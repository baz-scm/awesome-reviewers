{
  "schiller-manuel": {
    "repos": [
      "TanStack/router"
    ],
    "entries": [
      {
        "slug": "router-algorithm-implementation-trade-offs",
        "title": "algorithm implementation trade-offs"
      },
      {
        "slug": "router-api-backward-compatibility",
        "title": "API backward compatibility"
      },
      {
        "slug": "router-avoid-repeated-object-creation",
        "title": "Avoid repeated object creation"
      },
      {
        "slug": "router-avoid-unnecessary-complexity",
        "title": "avoid unnecessary complexity"
      },
      {
        "slug": "router-avoid-unnecessary-workflow-restrictions",
        "title": "avoid unnecessary workflow restrictions"
      },
      {
        "slug": "router-choose-efficient-algorithms",
        "title": "Choose efficient algorithms"
      },
      {
        "slug": "router-clear-contextual-error-messages",
        "title": "Clear contextual error messages"
      },
      {
        "slug": "router-comprehensive-test-validation",
        "title": "comprehensive test validation"
      },
      {
        "slug": "router-configuration-option-consistency",
        "title": "Configuration option consistency"
      },
      {
        "slug": "router-dependency-version-specification",
        "title": "Dependency version specification"
      },
      {
        "slug": "router-enhance-code-clarity",
        "title": "Enhance code clarity"
      },
      {
        "slug": "router-ensure-documentation-clarity",
        "title": "Ensure documentation clarity"
      },
      {
        "slug": "router-environment-aware-api-usage",
        "title": "environment-aware API usage"
      },
      {
        "slug": "router-environment-aware-configuration-management",
        "title": "Environment-aware configuration management"
      },
      {
        "slug": "router-intentional-async-usage",
        "title": "Intentional async usage"
      },
      {
        "slug": "router-maintain-backward-compatibility",
        "title": "Maintain backward compatibility"
      },
      {
        "slug": "router-maintain-comprehensive-documentation",
        "title": "Maintain comprehensive documentation"
      },
      {
        "slug": "router-memoize-for-render-optimization",
        "title": "memoize for render optimization"
      },
      {
        "slug": "router-react-hook-dependencies",
        "title": "React hook dependencies"
      },
      {
        "slug": "router-test-real-user-interactions",
        "title": "test real user interactions"
      },
      {
        "slug": "router-use-descriptive-conflict-free-names",
        "title": "Use descriptive, conflict-free names"
      },
      {
        "slug": "router-use-parameterized-testing",
        "title": "Use parameterized testing"
      },
      {
        "slug": "router-validate-before-accessing",
        "title": "validate before accessing"
      },
      {
        "slug": "router-validate-configuration-schemas",
        "title": "validate configuration schemas"
      }
    ],
    "comments": {
      "router-validate-before-accessing": [
        "should we not just return here if route is undefined?",
        "can we, instead of adding this function, enhance `deepEqual` to ignore properties that are explicitly set to `undefined`?"
      ],
      "router-avoid-repeated-object-creation": [
        "create the regex only once"
      ],
      "router-avoid-unnecessary-workflow-restrictions": [
        "yeah I think this check is not necessary, would be great to remove it"
      ],
      "router-maintain-backward-compatibility": [
        "we must avoid breaking changes.\r\ninstead, I suggest supporting both signatures like this (pseudocode):\r\n\r\n```tsx\r\nfunction useBlocker(\r\nblockerFnOrOpts: BlockerFn | BlockerOpts,\r\ncondition?: boolean | any)\r\n{\r\nif (typeof blockerFnOrOpts === 'function') {\r\n// function was passed\r\n}\r\nelse {\r\n// opts were passed\r\n}\r\n}\r\n```\r\n\r\nthen deprecate the old syntax and get rid of it with v2"
      ],
      "router-maintain-comprehensive-documentation": [
        "please document those new properties in docs/api/router/RouterOptionsType.md"
      ],
      "router-choose-efficient-algorithms": [
        "is there as solution that does not need a proxy?",
        "what exactly is the relevant change here?\r\nwould `TDefaultFrom extends RoutePaths<TRouteTree> | string = string,` result in the same improvement?\r\n\r\njust trying to understand this better"
      ],
      "router-ensure-documentation-clarity": [
        "yes, a more aggressive rewrite is in order. are you up for it?",
        "please add \"defaults to `routerOptions.defaultOnCatch`\" like we have it for the other props that default to router default props "
      ],
      "router-comprehensive-test-validation": [
        "do we really need a  separate e2e test for this?  isn't the generator test sufficient?"
      ],
      "router-validate-configuration-schemas": [
        "can't we just call `getConfig` down there and remove this function?",
        "please pass in the directory as an optional argument here\r\n\r\n```tsx\r\nexport async function getConfig(inlineConfig: Partial<Config> = {}, configDirectory?:string): Promise<Config> {\r\nif (configDirectory === undefined) {\r\n    configDirectory = process.cwd();\r\n}\r\nconst configFilePathJson = path.resolve(configDirectory, 'tsr.config.json')\r\n\r\n...\r\n"
      ],
      "router-configuration-option-consistency": [
        "please remove this section.\r\n\r\n`deleteNodes` is not a public API (yet) and thus shall not be documented",
        "rename this prop to `defaultOutputPath` to follow our usual config scheme of defaults that can be overriden"
      ],
      "router-use-parameterized-testing": [
        "use `it.each` please",
        "please use the `describe.each` syntax: https://vitest.dev/api/#describe-each",
        "then `test.each`? https://vitest.dev/api/#test-each",
        "can we reduce the verbosity one more step?\r\n\r\n```tsx\r\ndescribe('with a default config', () => {\r\nit.each([\r\n  ['single-level],\r\n  ['flat'],\r\n  // etc\r\n])('should wire-up the routes for a \"%s\" tree', async (folderName) => {\r\n    const config = await setupConfig(folderName)\r\n    await generator(config)\r\n    // ...\r\n});\r\n}\r\n```\r\n\r\nOR\r\nwe could iterate over all folders instead of enumerating them in the test?"
      ],
      "router-clear-contextual-error-messages": [
        "we don't have preview builds yet. however, you can run\r\n\r\n```shell\r\npnpm run dev\r\n```\r\n\r\nin the root router folder, and the same command in one of the example folders to test your changes locally",
        "> It's curious that `opts` wasn't used at all before (apart from `select`), so `StrictOrFrom` didn't really do anything at runtime?\r\n\r\nyeah looks strange, probably not meant that way. good catch.",
        "how about logging a stack trace? then it should be clear what's going on"
      ],
      "router-enhance-code-clarity": [
        "i would add all the imports for `createLink`, `Button` and `Link` here"
      ],
      "router-intentional-async-usage": [
        "why no `await Promise.all()`?\r\npersonally, i like async/await syntax more than \"then\".\r\nunless there is a technical reason?",
        "since we are awaiting this, shouldn't `this.startViewTransition` then return a promise?",
        "if I remove the `await` and this disable comment, eslint does not complain"
      ],
      "router-memoize-for-render-optimization": [
        "we cannot remove these memos as it kills a lot of render optimizations (as you saw you had to change the render count test)",
        "done"
      ],
      "router-dependency-version-specification": [
        "this should not be necessary as it is a dep of `@tanstack/react-router`",
        "should we also add `arktype` as peer dependency to express compatibility with some version?\r\ne.g. what happens if the internal structure of arktype changes as of some version?",
        "so what happens if a major version of arktype changes this? currently our package does not express which arktype version it is compatible to."
      ],
      "router-environment-aware-api-usage": [
        "wouldn't `path.posix.join` instead just work?"
      ],
      "router-algorithm-implementation-trade-offs": [
        "wouldn't it be much simpler to replace using\r\n\r\n```tsx\r\nnew RegExp(`^${basepath}`)\r\n```\r\n\r\nnotice the `^` to ensure it only replaces the basepath if it occurs at the start of the string",
        "1. true, did not think about that\r\n2. hmm I'd say not super likely, but possible\r\n3. I wasn't optimizing for runtime performance, just wanted to keep the code complexity down if possible\r\n\r\nthanks for the detailed response"
      ],
      "router-avoid-unnecessary-complexity": [
        "then just remove the href omit if it's not necessary. might have just been copied over from another example ",
        "we need a more descriptive name for this config option since this only affects route files",
        "`routeFileFormatter` is better!\r\n\r\n@SeanCassiere often has good naming ideas, so let's wait for him"
      ],
      "router-api-backward-compatibility": [
        "if this required now, that's an API break. please either update the docs or make it backwards compatible\r\n\r\nalso name of the property should be the same as previous, `blockerFn`\r\nOR you can add a new property, mark the old one as deprecated but still accept and use it if is presented",
        "same here. you can add a new property, mark the old one as deprecated but still accept and use it if is presented",
        "```suggestion\r\n- Specify if you are the resolver that the hook returns should be used or whether the information in your `shouldBlockFn` is enough to determine blocking\r\n```"
      ],
      "router-react-hook-dependencies": [
        "why do we need to track if a link was clicked?\r\nwhat happens if we have two link instances pointing to the same target, this then won't work, right?\r\n\r\ni think the solution must be independent from click tracking etc, and also must work when programatically preloading / navigating without clicking on a link",
        "this most likely needs to be fixed inside of `router.ts`",
        "ok please put this in a separate PR then",
        "@leoyli\ncan you please create a PR to fix this issue?\nideally with a test 😁"
      ],
      "router-environment-aware-configuration-management": [
        "it's also called `__TSR_SSR__` now"
      ],
      "router-use-descriptive-conflict-free-names": [
        "renamed r to router, v is gone\r\nadded a comment to c ( the cleanup function), this is called per script injection and thus benefits from a short name",
        "probably best to use US spelling? `behavior` without `u`",
        "Can you please choose different names than `TFrom` and `TTo` ? those are usually used in the navigation API for the `from` / `to` props\r\n\r\naside from that, LGTM",
        "naming should be improved, how about `routeFileIgnorePattern`?"
      ],
      "router-test-real-user-interactions": [
        "please use getByTestId and use `data-testid=\"...\"`\r\nwe try not to use getByRole in new tests anymore",
        "it allows to change text, tags etc without having to update the getByRole calls\r\nyou just need to make sure the test-id is correctly specified"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 123,
      "following": 14
    }
  },
  "agaudreault": {
    "repos": [
      "argoproj/argo-cd"
    ],
    "entries": [
      {
        "slug": "argo-cd-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "argo-cd-check-nil-before-access",
        "title": "Check nil before access"
      },
      {
        "slug": "argo-cd-choose-appropriate-synchronization-primitives",
        "title": "Choose appropriate synchronization primitives"
      },
      {
        "slug": "argo-cd-complete-configuration-examples",
        "title": "Complete configuration examples"
      },
      {
        "slug": "argo-cd-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "argo-cd-consolidate-rbac-permissions",
        "title": "consolidate RBAC permissions"
      },
      {
        "slug": "argo-cd-design-extensible-apis",
        "title": "design extensible APIs"
      },
      {
        "slug": "argo-cd-document-network-requirements",
        "title": "document network requirements"
      },
      {
        "slug": "argo-cd-document-observability-prerequisites",
        "title": "Document observability prerequisites"
      },
      {
        "slug": "argo-cd-explicit-security-controls",
        "title": "explicit security controls"
      },
      {
        "slug": "argo-cd-extract-testable-units",
        "title": "Extract testable units"
      },
      {
        "slug": "argo-cd-prefer-early-returns",
        "title": "Prefer early returns"
      },
      {
        "slug": "argo-cd-provide-comprehensive-explanations",
        "title": "Provide comprehensive explanations"
      },
      {
        "slug": "argo-cd-remove-unnecessary-elements",
        "title": "Remove unnecessary elements"
      },
      {
        "slug": "argo-cd-standardize-commit-tracing-metadata",
        "title": "standardize commit tracing metadata"
      },
      {
        "slug": "argo-cd-structured-logging-practices",
        "title": "structured logging practices"
      },
      {
        "slug": "argo-cd-use-clear-descriptive-names",
        "title": "Use clear, descriptive names"
      },
      {
        "slug": "argo-cd-use-configuration-constants",
        "title": "Use configuration constants"
      },
      {
        "slug": "argo-cd-use-descriptive-constants",
        "title": "Use descriptive constants"
      },
      {
        "slug": "argo-cd-validate-configuration-appropriateness",
        "title": "Validate configuration appropriateness"
      }
    ],
    "comments": {
      "argo-cd-extract-testable-units": [
        "extract all the test setup code to a function, the method should be generic enough so that it accepts a liveObj and returns only the client. You can then provide the faeClient to the \"import\" method (needs refactoring) and validate the result by doing a Get on the liveObj after the import.",
        "This test seems to re-implement the logic of the run function.\r\n\r\nThe unit test should be able to test a unit of code for which it can mock its dependencies. In this case, we want to test the NewImportCommand `Run` function. However, it is quite complex to provide a \"mock\" of the Kubernetes dependency this way. Instead, extract a function that you can test, that will receive a fake Kubernetes client and the  command arguments. This way, you can unit test that code.\r\n\r\nExample: `func (opts *importOpts) executeImport(client *dynamic.DynamicClient)`"
      ],
      "argo-cd-api-documentation-clarity": [
        "```suggestion\r\n* `pullRequestState`: PullRequestState is an additional MRs filter to get only those with a certain state. By default all states. Default: \"\" (all states). Valid values: `\"\"`, `opened`, `closed`, `merged` or `locked`. (Optional)\r\n```"
      ],
      "argo-cd-document-network-requirements": [
        "Link to the ingress documentation. The underlying user infrastructure may diverge too much from the example app.\r\n\r\n```suggestion\r\nThe api path `/api/webhook` of the `argocd-applicationset-controller` service on the `webhook` named port must be configured as part of your [ingress](./ingress.md).\r\n```",
        "You can add the fields to the example above. (for both generators)\r\n\r\n```yaml\r\n        # If true, skips validating the SCM provider's TLS certificate - useful for self-signed certificates.\r\n        insecure: true\r\n        # Reference to a ConfigMap containing trusted CA certs - useful for self-signed certificates. (optional)\r\n        caRef:\r\n          configMapName: argocd-tls-certs-cm\r\n          key: azure-devops-ca\r\n```"
      ],
      "argo-cd-complete-configuration-examples": [
        "This PR should update the default kustomize manifests in `manifests/` to use that variable and mount the argocd-redis secret"
      ],
      "argo-cd-document-observability-prerequisites": [
        "Format table + add info block that all `argocd_github_api` commands will only be enabled when the flag is configured."
      ],
      "argo-cd-use-descriptive-constants": [
        "Maybe unrelated to this feature, but I think the usage of \"Default\" here is confusing now that inheritance happens. It is not the \"Default Service Account\", it is the \"Service Account\" to use for that destination as far as I understand.\r\n```suggestion\r\n                        <div className='columns small-5'>ServiceAccount</div>\r\n```"
      ],
      "argo-cd-check-nil-before-access": [
        "Based on the implementation of registerDexHandlers, if SSO is not configured, ssoClientApp will be nil. The auth middleware should handle the nil use case. Make sure to add an additional unit test to validate that (TestWithAuthMiddlewareWhenSSONotConfigured)",
        "Should always return nil when claims is missing\r\n\r\n```suggestion\r\n\tclaim, err := m.GetExpirationTime()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get 'exp' claim: %w\", err)\r\n\t}\r\n```",
        "`app.Spec.Destination.Server` should be received in parameters bacuse it can be `nil` if destination by name is used"
      ],
      "argo-cd-explicit-security-controls": [
        "Should we set the default behavior to `false` (reverse the flag) so we are \"secure\" by default. Since the Sync action has to be called by a client, users will be able to upgrade Argo CD with the new behavior without affecting their Application. \r\n\r\nIf users need tome to apply the override permissions, then they can disable the flag."
      ],
      "argo-cd-provide-comprehensive-explanations": [
        "You should document the caveats for the user such as conflict with an HPA, or conflict when auto-sync is enabled and replicas are defined as code."
      ],
      "argo-cd-remove-unnecessary-elements": [
        "don't unnecessarily quote yaml. If quoting is necessary, use single quote."
      ],
      "argo-cd-use-configuration-constants": [
        "It is already possible to configure which resources update to ignore with customizations. https://argo-cd.readthedocs.io/en/stable/operator-manual/reconcile/ . Have you tried to use this feature?\r\n\r\nAdditionally, if you do not manage EndpointSlice manually, you can fully omit them from the watched resources. https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion"
      ],
      "argo-cd-use-clear-descriptive-names": [
        "nitpick on the name, but having a \"type\" be a validation regexp is not really intuitive.\r\n\r\n```suggestion\r\n            [\"format\"] = \"^[0-9]*$\",\r\n```"
      ],
      "argo-cd-structured-logging-practices": [
        "This change would mean refactoring the variables so we have a map of Applications instead of a list. This change is outside the scope of this refactor",
        "any ways to have a logger with some context like the revision / and git repo ?\r\n\r\nIt will be hard to know what caused that error"
      ],
      "argo-cd-prefer-early-returns": [
        "Invert if condition to return early. It will be more readable now that the method grew in size"
      ],
      "argo-cd-design-extensible-apis": [
        "The list stats should be part of an ApplicationListResponse object specific to the server/application/application.proto.\r\n\r\n",
        "Create another method on the interface, or provide another interface param to the middleware to `GetToken(r *http.Request) string`",
        "Instead of having a new VerifyJWT method on the existing provider interface, shouldn't you have a new implementation of the provider interface for JWT token? or have a new provider totally in something like /util/jwt/provider.go"
      ],
      "argo-cd-validate-configuration-appropriateness": [
        "Missing a check to set `obj.spec.syncPolicy.automated.enabled = false` if it is currently true"
      ],
      "argo-cd-comprehensive-function-documentation": [
        "out of scope of this refactor",
        "any change suggestion on the current docstring?",
        "Description updated to reflect the intent of the function and added doc for newRevisionHasChanges param usage. "
      ],
      "argo-cd-consolidate-rbac-permissions": [
        "The controller already has a cluster role with the permissions in https://github.com/agaudreault/argo-cd/blob/88c3fd61daf9832f12a1766d3ff37a1521d02ca8/manifests/cluster-rbac/applicationset-controller/argocd-applicationset-controller-clusterrole.yaml#L77-L88 \r\n\r\nThese will only be given for cluster install and not namespace install. This PR should consolidate both."
      ],
      "argo-cd-choose-appropriate-synchronization-primitives": [
        "I think when it timeouts, this will call `cancel()` on the context which will in turn close the `appEventCh` causing the for-range loop below to break. The last call will be `_ = printFinalStatus(app)`.\r\n\r\nThis would mean that the AfterFunc should make sure that\r\n1. It is not also calling `printFinalStatus` ✅ \r\n2. it should set `refresh = false` to make sure that the last call to `printFinalStatus` will not refresh the app.\r\n3. it should call `app, err = appClient.Get(ctx, &application.ApplicationQuery` to update the `app` (without refresh) so it is used by printFinalStatus.\r\n\r\nI haven't debugged if it is really what the execution does, but it should be testable in a unit test similar to `TestWaitOnApplicationStatus_JSON_YAML_WideOutput`.\r\n\r\nThere are also a few other problem with the code like the connection not being closed in the AfterFunc, and potential race conditions with refresh and app that might now require a lock. TBD",
        "immediately call `defer cancel()`",
        "Does the value of the timestamp really matter? Or what matters is that we received the log correctly?\r\n\r\n\"timestamp <= now\" seems more reliable. I don't think it is works mocking now, but ideally this is what should be done.",
        "updated to use struct type"
      ],
      "argo-cd-standardize-commit-tracing-metadata": [
        "By default, support (or not) Argocd-related-commit-type as a header. If it is not specifid, hydrator can decide to always assume git\r\n\r\n```suggestion\r\n{\r\n  \"references\": [\r\n    {\r\n      \"type\": \"commit\"\r\n      \"author\": \"Author Name <author-email>\",\r\n      \"sha\": \"<code-commit-sha>\",\r\n      \"message\": \"Commit message of the code commit\",\r\n      \"repoURL\": \"https://git.example.com/owner/repo\",\r\n      \"date\": \"2025-06-09T13:50:18-04:00\"\r\n    }\r\n  ]\r\n}\r\n```",
        "```suggestion\r\n  --trailer \"Argocd-reference-commit-message: Commit message of the code commit\" \\\r\n```"
      ]
    },
    "profile": {
      "location": "Saguenay",
      "company": "@Intuit @argoproj",
      "blog": "",
      "site_admin": false,
      "followers": 26,
      "following": 1
    }
  },
  "yottta": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-clear-relationship-descriptions",
        "title": "Clear relationship descriptions"
      },
      {
        "slug": "opentofu-contextualize-security-findings",
        "title": "Contextualize security findings"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-phased-migration-paths",
        "title": "Document phased migration paths"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-explicit-versus-dynamic-configurations",
        "title": "Explicit versus dynamic configurations"
      },
      {
        "slug": "opentofu-log-effectively-for-debugging",
        "title": "Log effectively for debugging"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-optimize-cicd-workflows",
        "title": "Optimize CI/CD workflows"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-protect-infrastructure-secrets",
        "title": "Protect infrastructure secrets"
      },
      {
        "slug": "opentofu-review-consistency-assumptions",
        "title": "Review consistency assumptions"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      }
    ],
    "comments": {
      "opentofu-separate-configuration-lifecycles": [
        "Thanks a lot for the input! Good catches with these 2 prompt parts.\r\nWhen it comes to the prompts, I would go with the first approach. The second option, where we could skip the deprecated variables, could block some users from using the configuration correctly, so I am not sure about it.\r\n\r\nAs for the `tofu show --json planfile`, yeah, totally agree.\r\n",
        "For the json output, added the changes in here: 55cfcce17bba824b8c642da1745c86c77b788ae7 .\r\nThe results will look like this. This is what you were suggesting, right?\r\n```json\r\n    \"root_module\": {\r\n      \"module_calls\": {\r\n        \"modcall\": {\r\n          \"source\": \"./mod1\",\r\n          \"expressions\": {\r\n            \"this_is_my_variable\": {\r\n              \"constant_value\": \"given value from modcall\"\r\n            }\r\n          },\r\n          \"module\": {\r\n            \"variables\": {\r\n              \"this_is_my_variable\": {\r\n                \"default\": \"default value\",\r\n                \"description\": \"This is a variable for the old way of configuring things.\",\r\n                \"deprecated\": \"This variable will be removed on 2024-12-31. Use another_variable instead.\"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n```\r\n----------\r\nFor prompts, added the changes in here: dd0cfdc949026216495b090a984c7753fed33e86\r\nAnd the results will look like this. Is it right? Not fully confident with these changes 🤔 \r\n![image](https://github.com/user-attachments/assets/11775498-f8ef-44f9-8d7a-057025af2be8)\r\n",
        "Thanks. Was not sure about it either.\r\nI applied that in a4c9d758df57f524f11d288127096a6353e8d5db.\r\nNow, it looks like this.\r\n<img width=\"1094\" alt=\"Screenshot 2025-03-07 at 18 17 38\" src=\"https://github.com/user-attachments/assets/2cf05f3f-03a0-4922-a9da-5b66bce2c0ac\" />\r\n"
      ],
      "opentofu-document-reference-standards": [
        "Yeah, seen it. I had the same question when I added this. Let's wait for others.\r\nPersonally, I consider that issues should be linked, since there should be no PR without an associated issue, especially PRs that require a changelog.\r\n\r\nSaying that because in general, the issue is the place where decisions are taken in terms of design and general functionality. I know that from the PR we can navigate back to the issue, but IMO, the client facing information should be the issue (aka the functional information) and not the PR (aka the technical approach)."
      ],
      "opentofu-document-phased-migration-paths": [
        "Thanks. Added 7b88c6f84ddb90024fff88eec0fa7f8f3fa7ad6f",
        "Good point! Thanks for the input. Fixed in 12adf6200ff60dfe12597285173e08990cf5b3cb"
      ],
      "opentofu-review-consistency-assumptions": [
        "Really good point!\r\nFrom a quick check, I see that some already implemented it but there are still some that didn't yet.\r\nAdded a warning about it e22385a328c24a07904fa422aad15849b57ff047",
        "I used terms like \"digest\", \"md5\" or \"digest file\" for the part where OpenTofu is writing an md5 hash of the statefile.\r\nFor example, looking into the dynamoDB, there is the md5 sum of the state file. Can be easily checked like this:\r\n![image](https://github.com/user-attachments/assets/7dc94a57-407b-4531-b6db-02bc2e24d10b)\r\n\r\n\r\n[Here](https://github.com/opentofu/opentofu/blob/6614782/internal/backend/remote-state/s3/client.go#L352C24-L374) is where this is handled in OpenTofu.\r\n\r\nIn that `Digest updates` section, I am talking about this part.",
        "Updated this section due to not needed to handle this digest file anymore.\r\nThis should be handled only when the DynamoDB locking is enabled. That's due to having two sources of information when it comes to locking and it was used to ensure that the state object from the S3 bucket was not altered separately, without acquiring a lock.",
        "Added in 38f1eb921775d9ac783fffe8c8cb3d2b11a67419"
      ],
      "opentofu-minimize-api-surface": [
        "Thanks! Really good point! Really kind of you to provide some code that actually worked flawlessly.\r\nSo applied in 538dec6f1ad081f9b78bcbb614cf595869b21a6f.",
        "`question`\r\nThis was a linting error and I totally agree with it, to have the json fields explicitly named.\r\nWas there a reason till now not to have this with json tags?",
        "You identified that correctly in [one of your comments](https://github.com/opentofu/opentofu/pull/2521#discussion_r1961379081), it was excluded before.\r\nAnd now, since I am using that struct again in my new changes, I encountered the same issue. I could have used the nolint directive as well, but I would prefer to have the actual issue fixed.\r\nLet's see what others are saying.",
        "Thanks for the ideas and the exchange. Fixed both points in 26f76c25053af5028e5ffba68ca5d5e64a2d38bc in order to match the default behavior."
      ],
      "opentofu-explicit-versus-dynamic-configurations": [
        "This is a first approach where I used the GH API to get all the protected branches (which are only the ones that are for the versions released of OpenTofu) and out of those exclude the the unmaintained versions.\r\nPros:\r\n* is using a reliable and well-known API that we can fully trust.\r\n\r\nCons:\r\n* manual maintenance. Quite ugly to update this.\r\n",
        "The update on the site is done automatically as seen [here](https://github.com/endoflife-date/release-data/tree/main?tab=readme-ov-file).\r\n![image](https://github.com/user-attachments/assets/01e8497d-1e49-4104-9ed8-21c39e9a4928)\r\n_That checkmark is for auto update_",
        "We are keeping only the hardcoded strategy to hold the control on what branches we run the flow against.\r\nSee [this](https://github.com/opentofu/opentofu/pull/2636#discussion_r2033011305) suggestion.",
        "This second approach is using https://endoflife.date/opentofu and is nice in terms that we don't really need to maintain this workflow anymore, once merged.\r\nPros:\r\n* no maintenance needed\r\n\r\nCons:\r\n* adds dependency on a 3rd party system",
        "We are keeping only the hardcoded strategy to hold the control on what branches we run the flow against.\r\nSee [this](https://github.com/opentofu/opentofu/pull/2636#discussion_r2033011305) suggestion.",
        "We added this section in the [CONTRIBUTING.RELEASE.md](https://github.com/opentofu/opentofu/pull/2636/files#diff-f3a80a44a166a40dd17304f62b1399e0a1477a0af3cea60dc41250f550b5ef07R248) for it.\r\nLet's see if this will be enough."
      ],
      "opentofu-document-intent-and-limitations": [
        "`suggestion`\r\nCould we maybe include a url here? 🤔 \r\nMaybe? https://opencontainers.org/posts/blog/2024-03-13-image-and-distribution-1-1/#manifest-maximum-size\r\nI am not sure which is the official one."
      ],
      "opentofu-optimize-cicd-workflows": [
        "Configured this to run once a week, on Sunday morning. Also, weird minute section configuration due to this mention from [GH actions docs](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule):\r\n> The `schedule` event can be delayed during periods of high loads of GitHub Actions workflow runs. High load times include the start of every hour. If the load is sufficiently high enough, some queued jobs may be dropped. To decrease the chance of delay, schedule your workflow to run at a different time of the hour.",
        "Perhaps we could change this later to fit our future needs.",
        "Changed in d7bd8375e8a0c9ccc46fc607eb2591595c0b865b"
      ],
      "opentofu-log-effectively-for-debugging": [
        "`suggestion`\r\nWe could at least log the error if occurs.",
        "Could we log it at least? 🤔 ",
        "`suggestion`\r\nFor being able to adjust this parsing in the future, we could have a trace log here writing the `fullName`. This way, in case somebody is encountering \"unknown\" traces, we could ask for the logs and adjust this parsing accordingly.",
        "`suggestion`\r\nA trace log maybe? Maybe somebody really wants to debug this if something is not working for them.",
        "I am thinking that for somebody that is playing around with setting this up, could upload the artifacts with a wrong artifactType and then have a hard time figuring it out.",
        "```suggestion\r\n\t\t\tlogger.Printf(\"[WARN] failed to fetch provider package; retrying\")\r\n```",
        "I would say that this is more of a warning than an info 🤔 Personally, I see INFO as an action that finished and the log needs to show the output of that. Finished in this context mean one of the following:\r\n* that it finished successfully\r\n* it finished with an error but there is a another possible success case on handling that error\r\n\r\nThe fact that an action failed and it's retried, is more of a failure, but not yet an interruptible one."
      ],
      "opentofu-safe-lock-patterns": [
        "Shouldn't `Sync` be under the the mutex lock too? This PR is changing that behaviour. Is this on purpose?",
        "`note`\r\nThis locking specific order (s3 first, dynamo second) is done this way keep the feature parity. From the tests ran locally, terraform is also doing both in case locking is enabled for s3+dynamodb and is doing those in that specific order.\r\nTerraform flow from the [localstack](https://www.localstack.cloud/) logs:\r\n```\r\nlocalstack.request.aws     : AWS iam.GetUser => 200\r\nlocalstack.request.aws     : AWS s3.ListObjectsV2 => 200\r\nlocalstack.request.aws     : AWS s3.PutObject => 200 <- acquire s3 lock\r\nlocalstack.request.aws     : AWS dynamodb.PutItem => 200 <- acquire dynamo lock\r\nlocalstack.request.aws     : AWS s3.HeadObject => 200\r\nlocalstack.request.aws     : AWS s3.GetObject => 206\r\nlocalstack.request.aws     : AWS dynamodb.GetItem => 200\r\nlocalstack.request.aws     : AWS s3.HeadBucket => 200\r\n...\r\nlocalstack.request.aws     : AWS s3.HeadBucket => 200\r\nlocalstack.request.aws     : AWS s3.GetBucketTagging => 404 (NoSuchTagSet)\r\nlocalstack.request.aws     : AWS s3.GetObject => 200\r\nlocalstack.request.aws     : AWS s3.DeleteObject => 204 <- release s3 lock\r\nlocalstack.request.aws     : AWS dynamodb.GetItem => 200\r\nlocalstack.request.aws     : AWS dynamodb.DeleteItem => 200 <- release dynamo lock\r\n```",
        "Good question! I was asking myself the same thing, but I see that `context.TODO` is used everywhere in this package.\r\nI am not aware yet, but maybe there are some plans on tackling this. Any ideas @cam72cam?"
      ],
      "opentofu-specify-configuration-behaviors": [
        "I am not aware of this. Could you expand it a little bit please?",
        "Oh, this is what you meant! Ok. I misread your initial comment.\r\nUpdated in 2695d6c58dc739ceae192242733ddea987cdf450",
        "3fe9247caedeef0099138111dece7a18065cda2b",
        "Added information in 11a29cb050fd95ab55b5f6d5df01bba3c73af854.\r\nA variable will not become ephemeral strictly from referencing an ephemeral value. In order for it to be able to work with ephemeral values, it needs to be configured specifically.",
        "f603db886aa3d830534b5cc68f7e415e58e3d343",
        "Applied in 6c43f94d46a40ebf22b652cb08d42bd5ba7f4179"
      ],
      "opentofu-contextualize-security-findings": [
        "`question`\r\nI see no label that we could assign to these issues. Any idea of one? Or should we create a new one?",
        "@cam72cam created a new label called `govulncheck` for these. Added in 4963f351b5e15206be46ce015fcb048605810576",
        "Would be possible, indeed. That's what I wanted to do initially.\r\nThough, I opted on including the workflow run url instead in the summary of the issue. ([Example of a vulnerability affecting v1.7](https://github.com/opentofu/opentofu/actions/runs/14333067260/job/40173357398#step:6:18627))\r\nThis way, whoever is working on fixing that vulnerability, will be able to inspect the findings of govulncheck.\r\n\r\nExtracting and writing entire stacktraces might be a little bit noisy on the issues, IMO, even though it will make the life easier for the one that works on solving it.\r\nAnother thing, is that the issues are reported per vulnerability and it reports all the versions that are affected, but different versions could have different stacktraces on how is calling the affected code.\r\n\r\nAnd yes, if we want to do this, would be advisable to do it in another language.",
        "Yes, that will work for sure because the `--search` argument in `gh issue list` is actually a query string, not only the title.\r\nSo whatever will be added after the title or in the description of it will not affect the search.\r\n\r\nThough, I am not sure that I understand why this would be better compared with adding a comment into the issue.\r\n\r\nBTW, had a conversation with @cam72cam and we will try to use security advisories instead of issues for this functionality. I will try to do it today and also test your idea that you suggested here.",
        "About the security advisories. Played around with the [API](https://docs.github.com/en/rest/security-advisories/repository-advisories) to see what it can do, but sadly it's quite restrictive and there is not really a  straight-forward way to test the changes. The API key that I can generate it can access only the published opentofu security advisories. Tried to test with one of my repos, but since I don't have any published/closed advisories, was returning only the draft one.\r\nEven further, the API to work with advisories is supporting filtering only on the `state` field which is not enough for our use case.\r\n\r\nTalked with @cam72cam and we are going with issues.",
        "Great arguments for the title topic! I totally agree with the issue to advisory flow.\r\nI would like to go forward with this approach and then reiterate later if we find out that the work with this flow is hard or not clear enough.\r\n\r\nThis is rising some ideas that could help with the flow for others not involved in the development of this:\r\n* I would add in the description of the ticket a note on how to work on the issue. We can have a short file documenting this and we can point to that in each ticket description, or we can have just a small note like:\r\n   > [!NOTE] \r\n   > * _Additional information can be added to the title as long as the original keywords are kept._\r\n   > * _Check also the pipeline run linked to get a better understanding of the source of the vulnerability for each version._\r\n* I would also lock the issue. What do you think about this? This kind of ticket should be handled by the core team IMO. Therefore, locking the issue will allow conversations only from the core team, avoiding pollution of the conversation and the investigation of it."
      ],
      "opentofu-craft-actionable-errors": [
        "```suggestion\r\n\t\toc := configOutputs[outputName]\r\n\t\tif prevStateOutput.Sensitive && !oc.Sensitive {\r\n\t\t\tdiags = diags.Append(&hcl.Diagnostic{\r\n\t\t\t\tSeverity: hcl.DiagWarning,\r\n\t\t\t\tSummary:  \"Output change in sensitivity\",\r\n\t\t\t\tDetail:   fmt.Sprintf(\"Sensitivity of the output %q changed. By doing so, the value will not be obfuscated anymore.\", oc.Name),\r\n\t\t\t\tSubject:  oc.DeclRange.Ptr(),\r\n\t\t\t})\r\n\t\t}\r\n```\r\n\r\nBy using a different way to initialise a diagnostic, we can provide that `Subject` field that will allow providing a better guidance to the user.\r\nThe change above will generate a warning similar to this one:\r\n![Screenshot 2025-06-02 at 15 15 32](https://github.com/user-attachments/assets/b30521cc-2517-406c-b76b-305ccee184a4)\r\nWith that indication of where the output that the warning is talking about can be located.",
        "Before\r\n![Screenshot 2025-03-17 at 11 20 04](https://github.com/user-attachments/assets/af02e9c1-e367-4df8-b169-fc264297c689)\r\nAfter\r\n![Screenshot 2025-03-17 at 11 22 52](https://github.com/user-attachments/assets/7ad66b64-607f-4c7d-b5f8-e8aca7b0efe3)\r\nI would suggest to enhance this to make it even better, like this:\r\n![Screenshot 2025-03-17 at 11 24 16](https://github.com/user-attachments/assets/d5fc77bc-1c4e-467d-b9bf-12da1e605924)\r\n```suggestion\r\n       \terrs = append([]error{fmt.Errorf(\"decryption failed for all provided methods\")}, errs...)\r\n\terrMessage := errors.Join(errs...).Error()\r\n```",
        "Indeed, that will not work as expected.\r\nYou could make use of a function like this:\r\n```golang\r\nfunc keyProvidersStack(stack []config.KeyProviderConfig) ([]string, hcl.Diagnostics) {\r\n\tres := make([]string, len(stack))\r\n\tvar diags hcl.Diagnostics\r\n\tfor i, cfg := range stack {\r\n\t\taddr, diag := cfg.Addr()\r\n\t\tdiags = diags.Extend(diag)\r\n\t\tif diag.HasErrors() {\r\n\t\t\tres[i] = \"<unknown>\"\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tres[i] = string(addr)\r\n\t}\r\n\treturn res, diags\r\n}\r\n```\r\n\r\nAnd then, before this `return` statement, you can do this:\r\n```golang\r\naddr, diags := keyprovider.NewAddr(cfg.Type, cfg.Name)\r\nstackAddrs, diag := keyProvidersStack(append(stack, cfg))\r\ndiags = diags.Extend(diag)\r\n```\r\nAnd this line will become\r\n```golang\r\nDetail: fmt.Sprintf(\"Cannot load %s due to circular reference between key providers. Stack trace %s\", addr, strings.Join(stackAddrs, \" -> \")),\r\n```\r\n\r\nEnding up in something similar to this. _Note: the message presented is not a real use case, that cannot happen._\r\n![Screenshot 2025-03-17 at 12 13 17](https://github.com/user-attachments/assets/acb90ebd-0a9c-4e76-945c-ddec0c43a479)\r\n",
        "We could have done this, but right now the key provider ([XOR](https://github.com/opentofu/opentofu/tree/main/internal/encryption/keyprovider/xor)) that could have been used to reproduce this is not included in the default allowed providers.\r\nI tested this with some slight temp modifications to the source code. ",
        "`suggestion`\r\n```suggestion\r\n\t\t\treturn desc, fmt.Errorf(\"unsupported OCI artifact type - empty\")\r\n```\r\n\r\nOr something more specific to let the user know about the actual cause? 🤔 ",
        "Good arguments for this. Thanks.\r\nAnd yes, the updated message seems more suitable for the particular situation.",
        "`suggestion`\r\n```suggestion\r\n\t\treturn nil, fmt.Errorf(\"manifest content digest (%s) does not match resolved digest %s\", gotDigest, desc.Digest)\r\n```\r\nNot sure about this 🤔 ",
        "Yes, that's more than a reasonable justification. Thanks. As I was saying, I was not sure about this suggestion.",
        "`suggestion`\r\n```suggestion\r\n\t\treturn selected, fmt.Errorf(\"ambiguous manifest has multiple descriptors for platform %s and version %s\", target, version)\r\n```"
      ],
      "opentofu-clear-relationship-descriptions": [
        "Wanted to add the suggestions individually but the signoff of the commit is not working correctly from the GH UI, so I added all your suggestions in one commit: 4ee1ec72ed946b32d50637bb893c4c754d1b6e4e"
      ],
      "opentofu-protect-infrastructure-secrets": [
        "> That raises another question, we should explain how we shouldn't use this with state encryption (due to the ephemerality) because some people may think \"This is great for secrets, i can use it for my passphrase\" and have issues.\r\n\r\nI am not sure that I really get what you are suggesting here. Could you expand it more please?",
        "Please check this out: ae712fdaf7a5ba4b69cfcf0e65d960df7d301dac",
        "In 91f105241ac49c3c20ba51d52306be62c7ac4c03"
      ],
      "opentofu-clear-concise-documentation": [
        "`question`\r\nAnd if setting `include = [\"registry.opentofu.org/mycorp/*\"]` then it means that the template should include interpolation only for the `type`?\r\nIf so, I will try to add just a small mention of it.\r\n",
        "Great points! Let's go with the current shape."
      ],
      "opentofu-proper-span-lifecycle": [
        "Agree, but really error prone this part. Isn't there a way to extract the content of the `for` loop into a new function?\r\nIf somebody will ever come and add a new conditional `continue` or worse, a `break` or a `return`, that span will never be closed and will create a havoc in the traces.",
        "This call here should be removed. Check that `span.End()` is called a little bit down the road."
      ],
      "opentofu-prevent-backing-array-surprises": [
        "Interesting that this conditional is rewritten in a different manner than [this](https://github.com/opentofu/opentofu/pull/2798/files#diff-b7f5b7c669ea7bb97e98d899d204d9e6bf48a00d18cb90109447eca05997704dR235) was in the current PR.\r\nAny particular reason of this difference? I am curious about the reason around having 2 different ways of writing these conditionals."
      ]
    },
    "profile": {
      "location": "Brasov, Romania",
      "blog": "",
      "site_admin": false,
      "followers": 14,
      "following": 9
    }
  },
  "mwilsnd": {
    "repos": [
      "maplibre/maplibre-native"
    ],
    "entries": [
      {
        "slug": "maplibre-native-buffer-bounds-validation",
        "title": "Buffer bounds validation"
      },
      {
        "slug": "maplibre-native-conditional-observability-instrumentation",
        "title": "Conditional observability instrumentation"
      },
      {
        "slug": "maplibre-native-cross-platform-ci-validation",
        "title": "Cross-platform CI validation"
      },
      {
        "slug": "maplibre-native-descriptive-named-constants",
        "title": "Descriptive named constants"
      },
      {
        "slug": "maplibre-native-document-platform-requirements",
        "title": "Document platform requirements"
      },
      {
        "slug": "maplibre-native-enforce-clear-data-ownership",
        "title": "Enforce clear data ownership"
      },
      {
        "slug": "maplibre-native-externalize-config-values",
        "title": "Externalize config values"
      },
      {
        "slug": "maplibre-native-follow-modern-c-guidelines",
        "title": "Follow modern C++ guidelines"
      },
      {
        "slug": "maplibre-native-group-related-properties",
        "title": "Group related properties"
      },
      {
        "slug": "maplibre-native-handle-errors-by-severity",
        "title": "Handle errors by severity"
      },
      {
        "slug": "maplibre-native-lock-responsibly-always",
        "title": "Lock responsibly, always"
      },
      {
        "slug": "maplibre-native-modern-c-style-practices",
        "title": "Modern C++ style practices"
      },
      {
        "slug": "maplibre-native-numerical-precision-considerations",
        "title": "Numerical precision considerations"
      },
      {
        "slug": "maplibre-native-optimize-compilation-flags",
        "title": "Optimize compilation flags"
      },
      {
        "slug": "maplibre-native-preallocate-collection-capacity",
        "title": "Preallocate collection capacity"
      },
      {
        "slug": "maplibre-native-prefer-safe-null-handling",
        "title": "Prefer safe null handling"
      },
      {
        "slug": "maplibre-native-template-instantiation-trade-offs",
        "title": "Template instantiation trade-offs"
      },
      {
        "slug": "maplibre-native-use-proper-logging",
        "title": "Use proper logging"
      }
    ],
    "comments": {
      "maplibre-native-follow-modern-c-guidelines": [
        "nit: `virtual` isn't required when the `override` keyword is present, high compiler warning levels can complain if they follow the core guidelines on [virtual, override and final](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines.html#c128-virtual-functions-should-specify-exactly-one-of-virtual-override-or-final)",
        "Prefer `using` over typedef",
        "syntax: `using CollisionBoxUBO = CollisionUBO;`"
      ],
      "maplibre-native-enforce-clear-data-ownership": [
        "I had to look up how to do this actually, haha. It extracts a value from the container and move it to a shared pointer inside the lambda's capture group. Doing it this way means we can keep using the iterator for the rest of the container.",
        "From when I worked on it, I believe `extract().mapped()` was my best option to prevent a copy of the element while also removing it from the container. Other options didn't look like they'd work.\r\n\r\nIt may have that lambda issue, I never noticed the other instance in `TileCache`. I also haven't seen the destructor running on the main thread. To be safe, I'd say do the same here as in `TileCache`.",
        "> Is `tiles.extract((tilesIt++)->first)` important here, vs. `tiles.extract(tilesIt++)`? It's doing an extra key search.\r\n\r\nIf it compiles and works correctly then no, I suppose it isn't.",
        ">It seems to be called somewhat rarely.\r\n\r\nIf it gets called at all then I say we should fix it, there is always a possibility of threading issues if that is allowed to happen.",
        "Have you considered what might happen if the caller holds on to the returned `string_view` while another thread (or even the same one later on) triggers a reallocation of `buffer`?\r\n\r\nIt might be prudent to couple the read lock with the returned string_view to at least reduce such a possibility.",
        "Yes, a copy would be the safest option if you feel that is acceptable. I would consider the frequency at which strings are fetched from the buffer to determine if constructing `std::string`s is an acceptable performance trade-off.",
        "We can just take a read lock here. If we don't find our string in the cache, we release the read lock and take a write lock to do our insertion."
      ],
      "maplibre-native-template-instantiation-trade-offs": [
        "I think in this case (and for `addCurrentVertex`), it's probably preferable to stick with `std::function`. What I expect to happen here is each unique lambda instantiates a new instance of this template. Since there is a lot of code in this template function, each new lambda given duplicates all this code. That quickly sends the trade-off in the other direction towards `std::function` the moment more than one lambda is passed to this template. Since each lambda is unique and the template approach is an almost certain inline, that prevents the compiler from de-duping the 95+% of the function body that is otherwise identical.",
        "I'm not super happy with this, but I can't specialize a template member function that also accepts a lambda without using `std::function` to erase the lambda type.",
        "The problem stems from the polymorphism of LayerGroupBase - a template virtual function isn't possible, so we need to know what kind of layer group we're dealing with to then invoke the right instance's visit template.",
        "I don't think so. There are two aspects to overhead with `std::function`, allocation and indirection. We probably weren't incurring allocation overhead in any of these changes, but we were paying the indirection cost.\r\n\r\n`std::function`'s type erasure works by using virtual dispatch. The compiler can't inline these virtual function calls as the functions provided could vary and *possibly* carry an allocated closure.",
        "+1 extract this to a function. In particular, comment the usage of `somePrime` as the FNV prime (assuming this is where it came from).",
        "Looks good."
      ],
      "maplibre-native-externalize-config-values": [
        "Should these limits be configurable at all? They could be set via build flags/preprocessor values.",
        "Can this be guarded by `#ifdef MLN_USE_TRACY `? We shouldn't try and load this extension unless we're building for instrumentation with Tracy"
      ],
      "maplibre-native-preallocate-collection-capacity": [
        "How often do we actually see reallocation happening here?"
      ],
      "maplibre-native-lock-responsibly-always": [
        "Can we add an option to wait forever by passing 0? Would it also make sense to reset the timeout if some events in the queue are processed but more are enqueued as a result?"
      ],
      "maplibre-native-handle-errors-by-severity": [
        "Should this throw? We need an allocator if we're going to render stuff.",
        "We should probably throw since assert will get dropped in release builds. Calling stubs should always terminate."
      ],
      "maplibre-native-group-related-properties": [
        "Is the intention here to only do polylines? I think this could be made a bit more clear what \"mode\" we're in, perhaps break these parameters out to a struct and pass it to `addPolyline` instead of having them here."
      ],
      "maplibre-native-optimize-compilation-flags": [
        "Build with `-Oz` here to prevent the inliner behavior from distorting the optimal size configuration. Otherwise things may get aggressively inlined which distorts the metric we're interested in.\r\n\r\nAdditionally, if we can, build only for armv8/AArch64. By default I think the xcframework should have both binaries in it (x86-64 and armv8). Bloaty gets confused by that and tends to pick one, usually x86-64 in my testing. Arm binaries are going to be bigger just by virtue of the instruction set, so make sure we're comparing arm."
      ],
      "maplibre-native-modern-c-style-practices": [
        "Yup, looks like I don't even need to specify it with Clang 16.",
        "Use braces on multi-line conditionals",
        "Prefer `static_cast<GLint>`"
      ],
      "maplibre-native-prefer-safe-null-handling": [
        "They ultimately mean the same thing, and at least in MSVC/Windows land, NULL is still used in a lot of places. I do agree though a chance to modernize while we're in here is good."
      ],
      "maplibre-native-buffer-bounds-validation": [
        "Do we want to check that `size + offset` is within the buffer bounds here?"
      ],
      "maplibre-native-conditional-observability-instrumentation": [
        "Could this be put behind a preprocessor macro so it only happens when built with Tracy support?"
      ],
      "maplibre-native-use-proper-logging": [
        "This was changed from Error to Debug intentionally, some of the tests use a [log observer](https://github.com/maplibre/maplibre-native/blob/88917b18065f6c7ee5d11ab2e109e0fab7af6edf/src/mbgl/util/logging.cpp#L53-L56) and only [Debug is omitted](https://github.com/maplibre/maplibre-native/blob/88917b18065f6c7ee5d11ab2e109e0fab7af6edf/src/mbgl/util/logging.cpp#L79). If XOpenDisplay fails during a test using the [FixtureLogObserver](https://github.com/maplibre/maplibre-native/blob/main/test/src/mbgl/test/fixture_log_observer.hpp), the test will fail because of the error level used in this retry message."
      ],
      "maplibre-native-descriptive-named-constants": [
        "Could you assign these indices to names to help clarify? ie `constexpr size_t LinePropUpdateFlags = 0;` (or with an enum),",
        "Can you store this in a named variable to add context, ex `constexpr uint32_t maximumVertexBindingCount = 16;`"
      ],
      "maplibre-native-cross-platform-ci-validation": [
        "A couple things:\r\n- bwb is really slow, it isn't building in parallel. There might be a way to fix that though.\r\n- bwb fails to locate a provisioning profile which is why we switched to bwx. I tested building the demo app and it still fails here in bwb mode."
      ],
      "maplibre-native-document-platform-requirements": [
        "This header won't be found for bazel builds, can you move it inside the `#if MLN_RENDER_BACKEND_OPENGL` block and additionally guard it with `defined(MLN_USE_TRACY )`?"
      ],
      "maplibre-native-numerical-precision-considerations": [
        "I think it's fine for now, if we start having to do this in other places we should add some test constants though."
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 13,
      "following": 1
    }
  },
  "tristan957": {
    "repos": [
      "ghostty-org/ghostty",
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "ghostty-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "ghostty-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "ghostty-document-configs-comprehensively",
        "title": "Document configs comprehensively"
      },
      {
        "slug": "ghostty-generate-dynamic-configurations",
        "title": "Generate dynamic configurations"
      },
      {
        "slug": "ghostty-in-tree-build-configurations",
        "title": "In-tree build configurations"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-use-descriptive-identifiers": [
        "```suggestion\r\nPREWARM_LABEL = \"compute_ctl_lfc_prewarm_requests_total\"\r\nOFFLOAD_LABEL = \"compute_ctl_lfc_prewarm_offload_requests_total\"\r\n```\r\n\r\nCapitalize constants",
        "I typically think that booleans can be overloaded. For instance, you have to write this docstring to explain what `with_compute_ctl` means. I like to use enums in such cases.\r\n\r\n```\r\nfrom enum import Enum\r\n\r\nclass LfcQueryMethod(Enum)\r\n    COMPUTE_CTL\r\n    POSTGRES\r\n```\r\n\r\nMakes it a little more obvious. But I will leave it up to you, merely a suggestion on improving readability"
      ],
      "ghostty-generate-dynamic-configurations": [
        "Should we add all releases?"
      ],
      "neon-clear-consistent-identifier-names": [
        "One last comment. It is `pgbouncer`. That's the name of the binary. Using the correct name makes it easier to search and find references.",
        "Please be consistent about either `endpoint_storage_auth_token` or `endpoint_storage_token`."
      ],
      "neon-hierarchical-semantic-naming": [
        "New metrics related to compute should be prefixed with `compute_`",
        "I would prefer that we start prefixing Postgres metrics with `postgres_` but it isn't something that we've discussed as a team."
      ],
      "neon-structure-endpoints-for-rest": [
        "Instead of 4 different routes, let's consolidate down to 2.\r\n\r\n```\r\nGET /prewarm_lfc - get the status\r\nPOST /prewarm_lfc - make the request\r\n\r\nGET /prewarm_lfc_offload - get the status\r\nPOST /prewarm_lfc_offload - make the request\r\n```\r\n\r\nI assume that `offload` means to send the current state of the LFC to endpoint storage. Can we just drop the `prewarm` prefix so it is just `lfc_offload`. I don't see why `prewarm` needs to be there.\r\n\r\nIf ^ is a good suggestion to you, I'd like to see the routes changed to:\r\n\r\n```\r\nGET /lfc/prewarm\r\nPOST /lfc/prewarm\r\n\r\nGET /lfc/offload\r\nPOST /lfc/offload\r\n```",
        "No need to use JsonResponse for empty bodies. Returning the status only is perfectly fine,",
        "```suggestion\r\n        StatusCode::ACCEPTED.into_response()\r\n```",
        "Since this is an async request, `Accepted` would be the more appropriate status code."
      ],
      "neon-proper-option-type-usage": [
        "This should continue to be an `Option<String>`. It's optional to pass it, and defaulting the value to empty string is meaningless.",
        "Please make `error` an `Option<String>`"
      ],
      "ghostty-in-tree-build-configurations": [
        "> We can expand and have a .Devel variant for debugging then, similar to many GNOME apps.\r\n\r\nI think this is a good idea.",
        "Let's restrict this to certain branches, at least main for now"
      ],
      "neon-document-parameter-choices": [
        "Also, please don't use an empty string for an ID."
      ],
      "neon-avoid-flaky-tests": [
        "I know this wasn't added in this PR, but this is a flaky test waiting to happen. I suggest using a database other than the `postgres` database for this test.",
        "Feel free to fix it in a subsequent PR."
      ],
      "neon-keep-files-focused-small": [
        "Would love to see the path pulled into a constant in a `pgbouncer` module (`compute_tools/src/pgbouncer.rs`).",
        "```suggestion\r\npub const PGBOUNCER_PIDFILE: &str = \"/tmp/pgbouncer.pid\";\r\n```\r\n\r\nAlso, just move this to a pgbouncer.rs file. It doesn't make sense to put this constant in pg_helpers.rs.",
        "Please follow pre-established pattern of the use statement at the top and avoid wildcard imports.",
        "As stated in a previous comment, please move all actual logic into the `ComputeNode` struct and just have the handlers call those functions.",
        "It seems like we have a good pattern going where the `ComputeNode` methods become small wrappers around functions in other files. I agree with this suggestion.",
        "Oh, I can't remember if it is, but that is also a good idea!"
      ],
      "ghostty-document-configs-comprehensively": [
        "Maybe it would be easier for users if we just blackholed this option and warned in the logs?"
      ],
      "ghostty-centralize-configuration-values": [
        "Instead of the linker script solution, can we just add a PR to check for the library name here?\r\n\r\nhttps://github.com/ghostty-org/ghostty/blob/95daca616db5c24d7bb37fd5a3ac2f8762bb4ead/src/build/SharedDeps.zig#L117",
        "The Zig build system doesn't allow you to pass the equivalent of `-lbz2`?",
        "Yeah I don't know enough about the Zig build system to say for sure unfortunately. Hopefully someone else can step in to assist, but linker script is still fine if it can't.",
        "I'm down for this pending what @jcollie and @pluiedev think"
      ],
      "ghostty-descriptive-consistent-naming": [
        "Feels like we should be more consistent with action names.\r\n\r\n```\r\nTOGGLE_SPLIT_ZOOM\r\nPWD\r\nRELOAD_CONFIG\r\nBELL\r\n```\r\n\r\nMaybe `RING_BELL` would be more consistent?"
      ],
      "neon-minimize-unnecessary-allocations": [
        "Seems like it would be easier to implement this function in terms of `major_version_num()`, but I'll leave it up to you!",
        "This is very strange to me. At the previous call site that I reviewed, we already spawned and cloned, and now we are doing it again. Please avoid that. Then you can probably inline the `impl` version of the function."
      ],
      "neon-flexible-documented-configurations": [
        "Follow-up PR here: https://github.com/neondatabase/cloud/pull/30120. It looks like we would overwrite the options sent from the control plane side. What are your thoughts on control plane vs compute_ctl changes?",
        "Seems like we may already overwrite `default_transaction_read_only=false`. I would need to verify that.",
        "Ok, I will take a look at the code and investigate. Thanks!",
        "PRs to remove the TODO:\r\n- https://github.com/neondatabase/cloud/pull/30274\r\n- https://github.com/neondatabase/neon/pull/12261\r\n\r\nAnd then the pseudocode that you wrote actually already exists at https://github.com/neondatabase/neon/blob/118e13438df173b98c83bea853e346ebbe00eab3/compute_tools/src/compute.rs#L363-L366.",
        "Ok, I missed that. I'll put up a PR for discussion.",
        "See the implementation at https://github.com/neondatabase/neon/pull/12262."
      ],
      "neon-configuration-context-alignment": [
        "Thanks for catching this. I agree with you."
      ]
    },
    "profile": {
      "location": "Austin, TX",
      "company": "@databricks",
      "blog": "https://tristan.partin.io",
      "site_admin": false,
      "followers": 80,
      "following": 48
    }
  },
  "krrishdholakia": {
    "repos": [
      "BerriAI/litellm"
    ],
    "entries": [
      {
        "slug": "litellm-ai-model-data-validation",
        "title": "AI model data validation"
      },
      {
        "slug": "litellm-ai-provider-documentation-completeness",
        "title": "AI provider documentation completeness"
      },
      {
        "slug": "litellm-api-parameter-consistency",
        "title": "API parameter consistency"
      },
      {
        "slug": "litellm-avoid-code-duplication",
        "title": "Avoid code duplication"
      },
      {
        "slug": "litellm-avoid-expensive-operations",
        "title": "avoid expensive operations"
      },
      {
        "slug": "litellm-background-task-coordination",
        "title": "background task coordination"
      },
      {
        "slug": "litellm-configurable-security-settings",
        "title": "configurable security settings"
      },
      {
        "slug": "litellm-document-configuration-purpose-clearly",
        "title": "Document configuration purpose clearly"
      },
      {
        "slug": "litellm-ensure-database-consistency",
        "title": "Ensure database consistency"
      },
      {
        "slug": "litellm-follow-consistent-naming-patterns",
        "title": "Follow consistent naming patterns"
      },
      {
        "slug": "litellm-follow-documentation-standards",
        "title": "Follow documentation standards"
      },
      {
        "slug": "litellm-incremental-dependency-updates",
        "title": "Incremental dependency updates"
      },
      {
        "slug": "litellm-minimize-core-dependencies",
        "title": "minimize core dependencies"
      },
      {
        "slug": "litellm-mock-tests-in-testslitellm",
        "title": "Mock tests in tests/litellm"
      },
      {
        "slug": "litellm-prefer-generic-api-patterns",
        "title": "Prefer generic API patterns"
      },
      {
        "slug": "litellm-prefer-lightweight-observability-integrations",
        "title": "Prefer lightweight observability integrations"
      },
      {
        "slug": "litellm-prefer-openai-compatibility",
        "title": "prefer OpenAI compatibility"
      },
      {
        "slug": "litellm-prioritize-naming-clarity",
        "title": "Prioritize naming clarity"
      },
      {
        "slug": "litellm-safe-access-patterns",
        "title": "Safe access patterns"
      },
      {
        "slug": "litellm-safe-operations-with-fallbacks",
        "title": "Safe operations with fallbacks"
      },
      {
        "slug": "litellm-standardize-environment-versions",
        "title": "Standardize environment versions"
      },
      {
        "slug": "litellm-use-configuration-helper-utilities",
        "title": "Use configuration helper utilities"
      },
      {
        "slug": "litellm-use-proper-logging-mechanisms",
        "title": "Use proper logging mechanisms"
      }
    ],
    "comments": {
      "litellm-use-proper-logging-mechanisms": [
        "please remove any print statements",
        "having so many if/else blocks re: logging in the codebase can lead to bugs\r\n\r\ncan we use a more general pattern / function here which can ensure consistent behaviour? @B-Step62 @ishaan-jaff ",
        "why not just move to using the logging.info here? ",
        "A noqa is not okay for a print statement.\r\n\r\nPlease change this to a verbose_logger.info",
        "is webbrowser a new dep? ",
        "verbose_logger uses the logging library - so it should print automatically - you can also try `.warning` as i've seen that show up pretty consistently as well. \r\n\r\nWe try to minimize print's in the codebase, so this would ideally not be the approach - but if there are no other options, we can go forward with this. ",
        "no `.set_verbose` is the old approach and enables `DEBUG` mode for the logging library. \r\n\r\n`INFO` statements should not require any enabling ",
        "Hey @anthony-liner can we please move this to inside litellm_logging.py and add testing for this - so this change doesn't get accidentally reverted\r\n\r\nideally keeping this logic within the Logging class itself"
      ],
      "litellm-prefer-lightweight-observability-integrations": [
        "why does a user need to pip install your sdk to send you logs? \r\n\r\nIsn't it OTEL compatible? if so - a simple callback should work too? ",
        "@ayulockin i'm confused - why is the `weave` sdk required here? \r\n\r\nIf this is patching litellm then that would be **bad**, as it can cause unexpected errors. \r\n\r\nIt would be **preferred** if weave was written as a customlogger, and **did not** have an sdk requirement (e.g. using pure httpx like langsmith - https://github.com/BerriAI/litellm/blob/9644e197f760c5cb87ae7662dbedbbce1b04c6a8/litellm/integrations/langsmith.py)",
        "i'm confused - why does the user need to pip install the agentops package for this? \r\n\r\nit should just work with litellm, like langsmith does via httpx "
      ],
      "litellm-document-configuration-purpose-clearly": [
        "Hey when would you want to import from a yaml? \r\n\r\nI don't understand what the user is trying to achieve with this ",
        "Hey @msabramo where does this porting from yaml to db happen in the code? from what i can see it gets appended to a model list, but i didn't see a /model/new call in this PR "
      ],
      "litellm-mock-tests-in-testslitellm": [
        "move inside `test_litellm/` \r\n\r\nwe don't have volcengine credentials - so all testing should be mocked and be inside test_litellm ",
        "can you add a mock test w/ screenshot of this working?\r\n\r\nsimilar to this - https://github.com/BerriAI/litellm/blob/246e3bafc89e19297fa42aabbfca6d25058e5989/tests/llm_translation/test_azure_ai.py#L48\r\n\r\n\r\nideally this test would **not** add the vllm sdk as a dep on the ci/cd pipeline (so maybe use a magicmock object here) ",
        "please move your tests to `tests/litellm/` as ideally a mock test - so we can run them on future contributor PRs as well ",
        "@Ali-Razmjoo `tests/litellm` is all mock tests. Please mock the response if your goal is to add an e2e test here. \r\n\r\nHere's an example - https://github.com/BerriAI/litellm/blob/8c0054a77e862bbd11e9cb5f40063a6e2d0d696c/tests/litellm/test_main.py#L282",
        "please write your tests inside `tests/litellm` so the github action can run and validate they pass",
        "this is **not** a good mock test. it does not test the integration itself.\r\n\r\nPlease mock the openai call - not the `.completion` call. Eg. - https://github.com/BerriAI/litellm/blob/61d77040572c545956d32cdcf2e19ea6858f525e/tests/litellm/test_main.py#L351",
        "instead of skipping this test, can you make it a mock test - using magicmock? \r\n\r\nthis will at least prevent any regressions from happening\r\n\r\neg. - https://github.com/BerriAI/litellm/blob/e4566d7b1ca0e1a3610349eb249cf16216c3a96f/tests/local_testing/test_embedding.py#L199",
        "This is the wrong file to add contributor tests. \r\n\r\nPlease add it inside tests/litellm - https://github.com/BerriAI/litellm/tree/main/tests/litellm\r\n\r\nand place it in a corresponding `test_` file to the one you're trying to test",
        "in this case it would be `tests/litellm/integrations/arize/test_arize_utils.py` \r\n\r\nif this file doesn't exist, please make one, with the correct imports ",
        "this test will fail in prod. please make this a mock test, see `hosted_vllm/` example"
      ],
      "litellm-safe-operations-with-fallbacks": [
        "add try-excepts around this - so if the tests fail due to openai internal server errors / rate limit errors - it doesn't stop ci/cd "
      ],
      "litellm-ai-model-data-validation": [
        "fixed on main",
        "based on this - it should be 400k input tokens \r\n<img width=\"1082\" height=\"432\" alt=\"Screenshot 2025-08-10 at 12 46 25 AM\" src=\"https://github.com/user-attachments/assets/e8be2274-2fa2-4ff0-b4e4-66c351ae7ae9\" />\r\n",
        "the input_cost_per_Token and output_cost_per_token are incorrect \r\n<img width=\"1168\" height=\"491\" alt=\"Screenshot 2025-08-10 at 12 47 56 AM\" src=\"https://github.com/user-attachments/assets/5ac62e73-dd3a-44c2-acd1-1d15f4f0ef02\" />\r\n",
        "this should be in scientific notation. ",
        "The pricing `0.00021` is incorrect. It's 0.21/1m tokens\r\n<img width=\"1215\" height=\"210\" alt=\"Screenshot 2025-07-27 at 8 59 26 AM\" src=\"https://github.com/user-attachments/assets/6e2422a3-7611-4b39-b77e-8007895128be\" />\r\n",
        "provider should be `bedrock_converse` as these are converse supported models - https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html",
        "the provider should be mistral, if it's openrouter the model name should begin with `openrouter/` ",
        "this is incorrect. this model is for vertex ai. Not google ai studio. ",
        "if we pass a system prompt to deepseek on groq, will it work? ",
        "can you please cite a source for the changes, so it's easy to see where this data is being pulled from? (similar to vertex ai) ",
        "is this the model name returned by databricks? if so, can you share the response object you see? \r\n\r\ncontext - the model name returned by provider is what is used for cost tracking "
      ],
      "litellm-follow-consistent-naming-patterns": [
        "i'm confused \r\n\r\nnvidia_nim is a separate provider to nvidia, why is the import for nvidia nim config coming from nvidia? ",
        "so is nvidia nim inside nvidia folder? \r\n\r\nthis breaks the pattern for `llms/` where the provider name is the folder - https://github.com/BerriAI/litellm/tree/main/litellm/llms\r\n<img width=\"1351\" alt=\"Screenshot 2025-03-09 at 7 23 07 PM\" src=\"https://github.com/user-attachments/assets/bd45241c-138c-4692-bd05-2601fa2c9486\" />\r\n",
        "was there an older nvidianim config? if so - what happens to it? \r\n\r\nI'm unclear on whether you're modifying the existing - `nvidia_nim/` integration, or adding a new provider\r\n\r\nif it's a new provider, can we avoid any modification of the existing `nvidia_nim/` provider ",
        "nit: can we use a less verbose name e.g. `prisma_airs` ? ",
        "is this change backwards compatible? @vanities ",
        "is the current code (not your PR) failing for you? @vanities ",
        "the file should be `/chat/transformation.py` ",
        "the folder should also match the llm provider \r\n\r\nin your case, if you're saying it's `meta-llama`, then the folder should follow the same convention (not: `meta/`) ",
        "rename this test file to specify the file being tested in the litellm/ folder\r\n\r\ni assume this is for `embed/transformation`? \r\n\r\nso this should be `embed/test_transformation.py` or `embed/test_netmind_transformation.py` in case of conflicts",
        "follow same naming convention + filepath as in `litellm/` \r\n\r\nso it should be `tests/litellm/integrations/SlackAlerting/test_slack_alerting.py` "
      ],
      "litellm-avoid-code-duplication": [
        "this is not needed. \r\n\r\nprovider_config_manager on line 44 should already handle this \r\n\r\n```\r\n    if provider_config and request_type == \"chat_completion\":\r\n        return provider_config.get_supported_openai_params(model=model)\r\n```\r\n\r\nplease remove this",
        "i'm confused - why does this convert the message list to a prompt and then back to a message list? \r\n\r\nif it's just to make sure content is a string, then i believe it might be better handled like Azure AI - https://github.com/BerriAI/litellm/blob/7630680690fdb85a0c239c49fa9817793b55447d/litellm/llms/azure_ai/chat/transformation.py#L27",
        "the message transformation logic should also be handled in a separate `transformation.py` file, so it's easy for someone to see how their call is being modified  - e.g. https://github.com/BerriAI/litellm/tree/main/litellm/llms/azure_ai",
        "why does this file exist at the root? ",
        "this file should be inside a folder called `/chat` - so a user can click into mistral and know all the supported endpoints \r\n\r\n",
        "this is the wrong place for this change. it should be inside watsonx/chat/transformation or watsonx/completion/transformation."
      ],
      "litellm-prefer-openai-compatibility": [
        "if solar is openai-compatible, a separate class is not needed @Tokkiu \r\n\r\nSee the GROQ implementation for referencehttps://github.com/BerriAI/litellm/pull/2168",
        "i don't see why this needs to exist as a separate file / if-else block in all the assistant api endpoints. \r\n\r\nIf it's openai-compatible, all you need to do is update `get_llm_provider` like we do for groq \r\n\r\nand this should work without major changes. \r\n\r\nSee - https://github.com/BerriAI/litellm/pull/2168/files#diff-9661b0d8f1d9f48433f2323d1102e963d3098f667f7a25caea79f16fa282f6dd",
        "remove this hardcoding, it's sufficient to add to `openai_compatible_providers` on init.py",
        "please use `base_llm_http_handler.completion` instead",
        "can we use `base_llm_http_handler` here instead - see deepseek/fireworks ai/groq if blocks for reference \r\n\r\nopenai_like.. is our older implementation of that flow ",
        "please inherit from the OpenAILikeConfig - it should be safer, and handles typical rough edges around openai compatible providers",
        "no need for this, see line below - just register as an openai_compatible_provider ",
        "could we reuse BaseAWSLLM here? - https://github.com/BerriAI/litellm/blob/2361bd98b08ba847657b8556671f9a9a792a0c93/litellm/llms/bedrock/base_aws_llm.py#L43",
        "we should avoid adding bloat to this function, as it's quite large already. \r\n\r\nI think this can be achieved by updating `_get_potential_model_names` instead",
        "can we rely on the map for this - moving to use `supports_function_calling` util from the litellm.utils \r\n\r\nthis way, we can just update the model cost map for any future model additions ",
        "please use the ProviderConfigManager.get_provider_chat_config factory instead"
      ],
      "litellm-api-parameter-consistency": [
        "inconsistent param usage.\r\n\r\nThe example on sdk says api_base, that's what it should be here",
        "@colesmcintosh these are still not the right env vars - \r\n\r\nit should be `ANTHROPIC_AUTH_TOKEN` and `ANTHROPIC_BASE_URL` - https://docs.anthropic.com/en/docs/claude-code/llm-gateway#basic-litellm-setup"
      ],
      "litellm-prioritize-naming-clarity": [
        "reading this over - can we make this more precise, e.g. `maximum_spend_logs_retention_period`? \r\n\r\nIt's more verbose but also more precise (i know exactly what it means when i read it) "
      ],
      "litellm-ai-provider-documentation-completeness": [
        "@colesmcintosh i think the user wanted to see how to set search options in the config - which is currently missing in the docs ",
        "```yaml\r\n  - model_name: grok-3\r\n    litellm_params:\r\n      model: xai/grok-3\r\n      api_key: os.environ/XAI_API_KEY\r\n      web_search_options: {} # 👈 KEY CHANGE\r\n```"
      ],
      "litellm-use-configuration-helper-utilities": [
        "please append both strings separately",
        "this approach looks unmaintainable over time (see the number of places the new env var was added to)\r\n\r\n\r\ncan we instead use a helper function - maybe inside `llms/gemini/common_utils.py` - and use that? "
      ],
      "litellm-ensure-database-consistency": [
        "what is a scenario where we would expect the object id to already exist? @yeahyung ",
        "can you please add a test for this inside https://github.com/BerriAI/litellm/blob/main/tests/enterprise/enterprise_hooks/test_managed_files.py",
        "This could lead to issues. There are other places where team checks will rely on the member list within the team, so the user would see a team their user object might list, but they'd face errors when using it. \r\n\r\nDid you figure out what the underlying cause of the drift was? @colesmcintosh "
      ],
      "litellm-background-task-coordination": [
        "would this just keep all tasks in memory, forever? \r\n\r\nConcerned re: possible memory leak. ",
        "if you run every 20s and have a while true - couldn't you end up in situations where the job is still running while another is queued? "
      ],
      "litellm-follow-documentation-standards": [
        "this is wrong - it should render the image here @colesmcintosh \r\n\r\nUse the Image import like here - https://github.com/BerriAI/litellm/blob/2a6dab0d23c23f32a37604dff5ce642cc955c89b/docs/my-website/docs/observability/arize_integration.md?plain=1#L17"
      ],
      "litellm-standardize-environment-versions": [
        "why does your PR modify circle ci? ",
        "wouldn't a stable release also be of type latest? ",
        "It's the latest published release - nightly's are triggered via the api call from .circleci/config.yml"
      ],
      "litellm-prefer-generic-api-patterns": [
        "is there a more generic way we can support this, instead of a one-off header? \r\n\r\n@NoWall57 ",
        "there is no need for this handler. please follow the cohere integration, and use the base_llm_http_handler - https://github.com/BerriAI/litellm/blob/3875df666b8a11819eda86fdc35d582be4bd8db6/litellm/rerank_api/main.py#L183",
        "hey @Manouchehri can we move away from hardcoded api bases - e.g. gateway.ai.cloudflare.com? \r\n\r\na more generic implementation where api_base is set and used, would help simplify the code here ",
        "i understand, but can we move this to just a simple check if api_base is set, if so -> set the base_url\r\n\r\nif not, don't? ",
        "since v2 might be used for other cohere endpoints too - e.g. embeddings, can we do a more generic `cohere_v2/` provider instead\r\n\r\nthis will make it reusable in different places ",
        "there is no need for this if block, just register as `litellm.openai_compatible_providers` in init\r\n\r\nand it will route it into the openai block ",
        "you should add it in the validate_environment in the config, that's where headers can be set. ",
        "in this case, it would be better to have it use base_llm_http_handler.py \r\n\r\nhttps://github.com/BerriAI/litellm/blob/4a6dcd64ff1a6151d7e5fec022b3905265791fcd/litellm/main.py#L2286\r\n\r\nThis is where we are moving all llm calls to - this way all your custom logic exists only within your transformation.py file "
      ],
      "litellm-safe-access-patterns": [
        "this assumes description is always set and is a dictionary. \r\n\r\nthis is not always true. \r\n\r\ncan you please handle for the none case",
        "this cast is wrong - as we could pass a None value to _valid_user_id ",
        "i don't believe api_key is a guaranteed field of `litellm_params` did this pass your lint check? "
      ],
      "litellm-configurable-security-settings": [
        "why set all these headers? ",
        "Why add this on the anthropic to openai bridge?\n\nI don't think openai has the same restrictions"
      ],
      "litellm-avoid-expensive-operations": [
        "if moving to success_handler, make sure that uses an executor, so as to not cause high cpu consumption @B-Step62 ",
        "Can we have a max limit on this - to prevent a memory leak "
      ],
      "litellm-minimize-core-dependencies": [
        "there should be no modification to pyproject.toml"
      ],
      "litellm-incremental-dependency-updates": [
        "why is the google sdk being bumped in this PR? ",
        "Hey @keith-decker - thanks for the quick response. Since we don't use google's sdk for handling calls - is there a reason we need protobuf bumped?\n\nTrying to understand what could be using protobuf here. \n\nBumping the sdk might have other consequences, so just trying to make sure we really need to do it. ",
        "try reverting it, and see if it passes the unit tests on ci/cd - if so, then i think we can back off the change. Since I can't imagine why OTEL would need protobuf. "
      ]
    },
    "profile": {
      "blog": "krishdholakia.com",
      "site_admin": false,
      "followers": 322,
      "following": 17
    }
  },
  "danielroe": {
    "repos": [
      "nuxt/nuxt"
    ],
    "entries": [
      {
        "slug": "nuxt-benchmark-algorithmic-optimizations",
        "title": "Benchmark algorithmic optimizations"
      },
      {
        "slug": "nuxt-build-documentation-clarity",
        "title": "Build documentation clarity"
      },
      {
        "slug": "nuxt-cache-lifecycle-management",
        "title": "Cache lifecycle management"
      },
      {
        "slug": "nuxt-check-ssr-context",
        "title": "Check SSR context"
      },
      {
        "slug": "nuxt-conditional-component-bundling",
        "title": "conditional component bundling"
      },
      {
        "slug": "nuxt-configuration-method-selection",
        "title": "Configuration method selection"
      },
      {
        "slug": "nuxt-configuration-resolution-patterns",
        "title": "configuration resolution patterns"
      },
      {
        "slug": "nuxt-consistent-code-formatting",
        "title": "consistent code formatting"
      },
      {
        "slug": "nuxt-documentation-formatting-consistency",
        "title": "documentation formatting consistency"
      },
      {
        "slug": "nuxt-explicit-api-design",
        "title": "explicit API design"
      },
      {
        "slug": "nuxt-explicit-response-types",
        "title": "explicit response types"
      },
      {
        "slug": "nuxt-follow-vue-api-patterns",
        "title": "Follow Vue API patterns"
      },
      {
        "slug": "nuxt-handle-async-cancellation-properly",
        "title": "Handle async cancellation properly"
      },
      {
        "slug": "nuxt-optimize-array-operations",
        "title": "optimize array operations"
      },
      {
        "slug": "nuxt-optimize-cicd-configurations",
        "title": "optimize CI/CD configurations"
      },
      {
        "slug": "nuxt-organize-accessibility-attributes",
        "title": "organize accessibility attributes"
      },
      {
        "slug": "nuxt-precise-language-usage",
        "title": "precise language usage"
      },
      {
        "slug": "nuxt-prefer-null-comparisons",
        "title": "prefer != null comparisons"
      },
      {
        "slug": "nuxt-preserve-http-header-semantics",
        "title": "Preserve HTTP header semantics"
      },
      {
        "slug": "nuxt-safe-error-data-handling",
        "title": "safe error data handling"
      },
      {
        "slug": "nuxt-semantic-names-with-counters",
        "title": "semantic names with counters"
      },
      {
        "slug": "nuxt-strategic-component-loading",
        "title": "Strategic component loading"
      },
      {
        "slug": "nuxt-use-consistent-terminology",
        "title": "Use consistent terminology"
      },
      {
        "slug": "nuxt-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "nuxt-use-proper-http-utilities",
        "title": "Use proper HTTP utilities"
      },
      {
        "slug": "nuxt-validate-cicd-timing-checks",
        "title": "Validate CI/CD timing checks"
      },
      {
        "slug": "nuxt-vue-context-boundaries",
        "title": "Vue context boundaries"
      }
    ],
    "comments": {
      "nuxt-use-consistent-terminology": [
        "Although technically correct (as in, it's the `UserConfig` export from `vite`), this is less intuitive for users who need to know that this configuration is _vite_ configuration. Can you think of a better way of communicating it?",
        "```suggestion\nimport App from '~/app.vue'\n```\n"
      ],
      "nuxt-follow-vue-api-patterns": [
        "I did switch the auto imports in this PR. The reason the export remains is in case someone directly imports from #app/composables/id.\n\n(We will remove this file for 4.x in a separate PR marked as a breaking change.)",
        "ideally we should not only expose this but also use `useLink` internally within the setup function, to reduce code duplication and ensure the logic remains the same",
        "This matches `RouterLink` behaviour (which exposes a 'bound' composable which other libraries, like vuetify, can use to create custom links): https://router.vuejs.org/guide/advanced/composition-api.html#useLink."
      ],
      "nuxt-documentation-formatting-consistency": [
        "```suggestion\r\nprerenderRoutes('/api/content/article/name-of-article')\r\n```",
        "```suggestion\r\n  ssr: false,\r\n  nitro: {\r\n    hooks: {\r\n      'prerender:generate'(route) {\r\n        const routesToSkip = ['/index.html', '/200.html', '/404.html']\r\n        if (routesToSkip.includes(route.route) ) {\r\n          route.skip = true\r\n        }\r\n      }\r\n    }\r\n  }\r\n})\r\n```",
        "```suggestion\r\nWhen prerendering a client-rendered app, Nuxt will generate `index.html`, `200.html` and `404.html` files by default. However, if you need to prevent any (or all) of these files from being generated in your build, you can use the `'prerender:generate'` hook from [Nitro](/docs/getting-started/prerendering#prerendergenerate-nitro-hook).\r\n\r\n```",
        "```suggestion\r\nIf you deploy your app to [static hosting](/docs/getting-started/deployment#static-hosting) with the `nuxi generate` or `nuxi build --prerender` commands, then by default, Nuxt will render every page as a separate static HTML file.\r\n```",
        "```suggestion\r\n  <NuxtLink :to=\"{ name: 'posts-id', params: { id: 123 } }\">\r\n```"
      ],
      "nuxt-explicit-response-types": [
        "```suggestion\r\nPrerendered API routes in production may not return the expected response headers, depending on the provider you deploy to. For example, a JSON response might be served with an `application/octet-stream` content type. In this case, you may have to specify an explicit `responseType`.\r\n```",
        "```suggestion\r\nPrerendered API routes in production may not return the expected response headers, depending on the provider you deploy to. For example, a JSON response might be served with an `application/octet-stream` content type.\r\n```",
        "```suggestion\r\nAlways manually set `responseType` when fetching prerenderered API routes.\r\n```",
        "```suggestion\r\nAlways manually set `responseType` when fetching prerendered API routes.\r\n```"
      ],
      "nuxt-semantic-names-with-counters": [
        "If we want to support defaulting to the component name as a fetch key, as in linked issue, we will need a nested global fetchKey object to keep track of individual indices of each key. That is, the fetch state might look like:\r\n```js\r\n{\r\n  '0': { a: 'mystate' },\r\n  'MyComponent': { a: 'mystate' },\r\n  'MyOtherComponent': [{ a: 'mystate' }, { a: 'mystate2' }],\r\n  // or, better\r\n  'MyOtherComponent-0': { a: 'mystate' },\r\n  'MyOtherComponent-1': { a: 'mystate2' },\r\n  '1': { a: 'mystate' },\r\n}\r\n```\r\n\r\n(If we don't keep track of these indices separately, the same issue that triggered the original bug will reappear because the global numeric fetch key will get out of sync.)",
        "What would `getKey` API look like? Would it just return a valid incremented number unique to the string passed in?\r\n```js\r\ngetKey (keyToLookup: string): number\r\n```"
      ],
      "nuxt-handle-async-cancellation-properly": [
        "I think if an abortController is triggered, then we should also track this and avoid doing anything with the finished result of an asyncData. so it shouldn't update the data/status, just as if it were cancelled.",
        "this doesn't seem to result in this hook being called:\r\n\r\n```ts\r\nexport default defineNuxtConfig({\r\n  hooks: {\r\n    close: () => {\r\n      console.log('closing things')\r\n    },\r\n  },\r\n})\r\n```"
      ],
      "nuxt-consistent-code-formatting": [
        "```suggestion\r\n      hasUAVisualTransition ||\r\n      !isChangingPage(to, from)\r\n```"
      ],
      "nuxt-explicit-api-design": [
        "in honesty I'm not yet sure about this as an API. But I also might be missing something.\r\n\r\nWhat is the situation where passing `force` wouldn't be good enough?",
        "on the force option for refresh, could we just add the force option and basically ignore hasCachedData when forced? And always respect hasCachedData when calling refresh from watch?\r\n",
        "we should probably omit hash/query if they are not present",
        "```suggestion\r\n    if (options?.replace) {\r\n      return typeof to === 'string' ? { path: to, replace: true } : { ...to, replace: true }\r\n    }\r\n    return to\r\n```",
        "the key thing that's missing from the nitro implementation is the ability to get the route rules for an arbitrary path... (or we would just use it on the server side)\r\n\r\nmight it be possible for nitro to handle a path rather than just the current event?",
        "totally on-board with aligning them",
        "What about updating the signature so we call this with `start({ force: true})` (and also below with `finish`)? This makes it more explicit and allows passing more options in future.",
        "Sorry, to clarify I don't mean passing it as an option to `useLoadingIndicator` but to the `start` and `finish` functions.",
        "The fewer options that `useLoadingIndicator` accepts, the better, because they will influence the global singleton.\r\n\r\nI think we should pass the override to start/finish and calling `_hide` or `_reset` should instead clear any timeout that is still running. (We probably should have implemented it that way in the first place.)",
        "Thank you!"
      ],
      "nuxt-use-descriptive-names": [
        "```suggestion\n/// <reference path=\"./types/shared.d.ts\" />\n```\n\ni would also recommend renaming to shared-imports.d.ts for clarity",
        "what about just naming the flag `cookieStore` instead to identify the API that would be used? And linking to CookieStore API docs both here and in 1.experimental-features.md?",
        "Maybe better: `nodeModulesDirectories`?"
      ],
      "nuxt-preserve-http-header-semantics": [
        "this is an already-serialised 'set-cookie' header, not a raw value. what we're doing here is very similar to the mergeHeaders function in h3:\r\n\r\nhttps://github.com/unjs/h3/blob/67af0575eb2078ba4c2ed1c535dfc168a4e578e6/src/response.ts#L101-L111",
        "the issue is that we want to _preserve_ the existing headers, as they are, rather than flattening them into a single `set-cookie` header.\r\n\r\nmerging to iterate, but let me know if you have any concerns 🙏 "
      ],
      "nuxt-strategic-component-loading": [
        "This will only have an effect if plugins are asynchronous. I think I would first advise avoiding any async activity in a plugin, and then fall back to making plugins `parallel`.",
        "let's start with a single page. we can update the plugins PR from Julien to augment this page + later see if we need to split.",
        "lazy hydration with `hydrate-never` will still hydrate things on client-side navigation unless it was part of the initial render.",
        "if Julien says we should omit server components for now, let's do omit that section for now.\r\n\r\nIt's still worth mentioning lazy hydration.",
        "I'm worried we might give the wrong impression, as these all take place in the same network request, with additional server components rendered with 'internal' fetches.\r\n\r\n```suggestion\r\nBe careful when nesting islands within other islands as each island adds some extra overhead.\r\n```"
      ],
      "nuxt-benchmark-algorithmic-optimizations": [
        "I'm not sure this is quite right. Do we only want to scan variables if the code has comments? And I think we should respect 'scope' rather than looking for any variable anywhere in the code.\r\n\r\nIn general I'd prefer to avoid using regexp for this. I feel a parsing-based scope-tracking approach would be sounder (but we can make that performant by only parsing the code if it code contains some of the 'reserved' names - if not we have a 'fast path' without parsing)",
        "by this point `array[last]` will always be undefined because you've reduced the size of the array.\r\n\r\nhere's a functioning solution:\r\n\r\n```suggestion\r\n      const lastItem = data[data.length - 1]\r\n      if (i < --data.length) {\r\n        array[i] = lastItem\r\n```\r\n\r\nhowever, it's not faster than the existing implementation (see [benchmark](https://jsbenchmark.com/#eyJjYXNlcyI6W3siaWQiOiJXR01CMEJLVXgwbUJDYVc3NmFHSVciLCJjb2RlIjoibGV0IGEgPSBEQVRBXG5hID0gZmlsdGVyKGEsIGkgPT4gaSAlIDUwID09PSAwKVxuYSA9IGZpbHRlcihhLCBpID0-IGkgJSAxMCA9PT0gMClcbmEgPSBmaWx0ZXIoYSwgaSA9PiBpICUgMiA9PT0gMCkiLCJuYW1lIjoiZmlsdGVyIiwiZGVwZW5kZW5jaWVzIjpbXX0seyJpZCI6Ik9VSnozNU1QTkdhWVZ2eVo3S3A1UiIsImNvZGUiOiJsZXQgYSA9IERBVEFcbmEgPSBmaWx0ZXJJblBsYWNlKGEsIGkgPT4gaSAlIDUwID09PSAwKVxuYSA9IGZpbHRlckluUGxhY2UoYSwgaSA9PiBpICUgMTAgPT09IDApXG5hID0gZmlsdGVySW5QbGFjZShhLCBpID0-IGkgJSAyID09PSAwKSIsIm5hbWUiOiJmaWx0ZXJJblBsYWNlIiwiZGVwZW5kZW5jaWVzIjpbXX0seyJpZCI6InctMm8ybWt5VzBmUXZpdi1CTUdmbiIsImNvZGUiOiJsZXQgYSA9IERBVEFcbmEgPSB0dXJib0ZpbHRlckluUGxhY2UoYSwgaSA9PiBpICUgNTAgPT09IDApXG5hID0gdHVyYm9GaWx0ZXJJblBsYWNlKGEsIGkgPT4gaSAlIDEwID09PSAwKVxuYSA9IHR1cmJvRmlsdGVySW5QbGFjZShhLCBpID0-IGkgJSAyID09PSAwKSIsImRlcGVuZGVuY2llcyI6W10sIm5hbWUiOiJ0dXJib0ZpbHRlciJ9XSwiY29uZmlnIjp7Im5hbWUiOiJCYXNpYyBleGFtcGxlIiwicGFyYWxsZWwiOmZhbHNlLCJnbG9iYWxUZXN0Q29uZmlnIjp7ImRlcGVuZGVuY2llcyI6W119LCJkYXRhQ29kZSI6Imdsb2JhbFRoaXMuZmlsdGVyID0gZnVuY3Rpb24gZmlsdGVyKGRhdGEsIHByZWRpY2F0ZSkge1xuICByZXR1cm4gZGF0YS5maWx0ZXIocHJlZGljYXRlKVxufVxuXG5nbG9iYWxUaGlzLmZpbHRlckluUGxhY2UgPSBmdW5jdGlvbiBmaWx0ZXJJblBsYWNlKGRhdGEsIHByZWRpY2F0ZSkge1xuICBmb3IgKGxldCBpID0gZGF0YS5sZW5ndGg7IGktLTsgaT49MCkge1xuICAgIGlmICghcHJlZGljYXRlKGRhdGFbaV0sIGksIGRhdGEpKVxuICAgICAgZGF0YS5zcGxpY2UoaSwgMSlcbiAgfVxuICByZXR1cm4gZGF0YVxufVxuXG5nbG9iYWxUaGlzLnR1cmJvRmlsdGVySW5QbGFjZSA9IGZ1bmN0aW9uIHR1cmJvRmlsdGVyKGRhdGEsIHByZWRpY2F0ZSkge1xuICBmb3IgKGxldCBpID0gZGF0YS5sZW5ndGg7IGktLTsgaSA-PSAwKSB7XG4gICAgaWYgKCFwcmVkaWNhdGUoZGF0YVtpXSwgaSwgZGF0YSkpIHtcbiAgICAgIGNvbnN0IGxhc3RJdGVtID0gZGF0YVtkYXRhLmxlbmd0aCAtIDFdXG4gICAgICBpZiAoaSA8IC0tZGF0YS5sZW5ndGgpIGRhdGFbaV0gPSBsYXN0SXRlbTtcbiAgICB9XG4gIH1cbiAgcmV0dXJuIGRhdGE7XG59XG5yZXR1cm4gWy4uLkFycmF5KDUwMDAwKS5rZXlzKCksLi4uQXJyYXkoNTAwMDApLmtleXMoKSwuLi5BcnJheSg1MDAwMCkua2V5cygpXSJ9fQ))\r\n\r\nand @antfu is right - the performance improvement is probably not noticeable in a Nuxt app, and we do probably need to keep the order of the items in the array. (but I'd be open for metrics showing otherwise!)",
        "Unfortunately I don't think that would work with a Set - though maybe we could use an object and iterate over its keys? What do you think about perf tradeoff?"
      ],
      "nuxt-configuration-method-selection": [
        "no, nuxt.config is only read when building/dev, not when starting a production server "
      ],
      "nuxt-build-documentation-clarity": [
        "```suggestion\r\n\r\nWhen prerendering a client-rendered app, Nuxt will generate `index.html`, `200.html` and `404.html` files by default. However, if you need to prevent any (or all) of these files from being generated in your build, you can use the `'prerender:generate'` hook from [Nitro](/docs/getting-started/prerendering#prerendergenerate-nitro-hook).\r\n```"
      ],
      "nuxt-optimize-array-operations": [
        "this array might have either 1 or 2 members - is it worth optimising it for 1?",
        "I'm pretty sure there's no benefit to switching from `.map` as this already is able to optimise by creating an array with the correct number of elements.",
        "I'm not sure this is a more performant approach... Let's skip this particular change for now.",
        "Understood. TIL. However this is build time code (while bundling up templates) and never impacts a user. I am happy to keep it as is."
      ],
      "nuxt-cache-lifecycle-management": [
        "I was wanting to avoid fragility in case you changed case of headers or added some additional ones that would break JS-enabled error pages (as we render in Nuxt).",
        "`Cache-Control` doesn't seem to be lowercase, fyi, in dev, though it is in prod",
        "would it resolve it just to do something like:\r\n\r\n```suggestion\r\n  return JSON.parse(JSON.stringify(extractedMeta))\r\n```",
        "yes, we'd also have to update it there too",
        "we could use klona",
        "you had some nice tests in the previous commit…. is it worth keeping them?",
        "excellent point. on it 👍 ",
        "actually, seems I already implemented at https://github.com/nuxt/nuxt/commit/4f85cff8d94c575c5f4238a4a8f8472beb1bce5d. Review very appreciated 🙏 ",
        "Agreed!"
      ],
      "nuxt-check-ssr-context": [
        "cc: @huang-julien "
      ],
      "nuxt-precise-language-usage": [
        "```suggestion\r\n- `fn`: The function to run once. It can be asynchronous.\r\n```"
      ],
      "nuxt-validate-cicd-timing-checks": [
        "Can this be forged? ie force push an 'older' commit?"
      ],
      "nuxt-use-proper-http-utilities": [
        "works well!"
      ],
      "nuxt-optimize-cicd-configurations": [
        "```suggestion\r\n          ref: ${{ steps.pr.outputs.head_sha }}\r\n          fetch-depth: 1\r\n```",
        "```suggestion\n    if: ${{github.event_name == 'push' || github.repository == 'nuxt/nuxt'}}\n```\n"
      ],
      "nuxt-prefer-null-comparisons": [
        "@manniL I also had to check, but I think it's good:\r\n\r\n![CleanShot 2024-03-10 at 10 23 02@2x](https://github.com/nuxt/nuxt/assets/28706372/d7c719dc-5c6f-4384-bcc7-04d60037fff6)\r\n",
        "what else could it be?"
      ],
      "nuxt-conditional-component-bundling": [
        "this implementation will result in always bundling `<NuxtPage>`, even if the `vue-router` integration isn't enabled.\r\n\r\nwe should probably protect it so that if no routes have the `isolate` metadata then there will be no changes to the bundle at all"
      ],
      "nuxt-safe-error-data-handling": [
        "```suggestion\n        ssrError.data = destr(ssrError.data)\n```\n"
      ],
      "nuxt-vue-context-boundaries": [
        "composables shouldn't be called at the top level ('ambiently' in JS) - this relies on being able to access the Nuxt context and will fail on the server.",
        "```suggestion\r\nCurrently, the Nuxt context is only accessible in [plugins](/docs/guide/directory-structure/plugins), [Nuxt hooks](/docs/guide/going-further/hooks), [Nuxt middleware](/docs/guide/directory-structure/middleware) (if wrapped in `defineNuxtRouteMiddleware`), and [setup functions](https://vuejs.org/api/composition-api-setup.html) (in pages and components).\r\n```",
        "```suggestion\r\nFor example, doing `export myState = ref({})` would result in state shared across requests on the server and can lead to memory leaks.\r\n```"
      ],
      "nuxt-configuration-resolution-patterns": [
        "I think we could probably just do:\r\n\r\n```suggestion\r\n    const extraPageMetaExtractionKeys = nuxt.options?.experimental?.extraPageMetaExtractionKeys || []\r\n```\r\n\r\n... as the issue is presumably that it's not defined, rather than set to a string or something like that",
        "not sure I understand \r\n\r\nah. I get what you're saying. that's actually the default behavior of nuxt schema if you pass a raw object",
        "```suggestion\r\n          const [srcDir, assetsDir] = await Promise.all([\r\n            get('srcDir'),\r\n            get('dir.assets'),\r\n          ])\r\n          return {\r\n            [basename(assetsDir)]: resolve(srcDir, assetsDir),\r\n            ...typeof val === 'object' ? val : {},\r\n          }\r\n```",
        "This is also shared in Nitro:\r\n\r\nhttps://github.com/unjs/nitro/blob/e4f687d32d4d6ec7bd7b011a59edb374b477fbf4/src/core/config/resolvers/runtime-config.ts#L31-L39\r\n\r\nBy doing this, the value is dropped out of `runtimeConfig` entirely and can't be overridden at runtime. See https://github.com/nuxt/nuxt/pull/18586 for more context.\r\n\r\nI recall doing it before, but if we want to allow setting these values to undefined/null values, we likely need to (re)investigate where the keys are being 'dropped' when serialised.",
        "A little tweak required.\r\n\r\nif it's not v4 compat, then by default this should be `<srcDir>/server`. In v4 compat, it should be `<rootDir>/server`. If the user provides it then in either case it should be resolved relative to `<rootDir>`.",
        "we should probably also check if `vue.script.propsDestructure` is set, for backwards-compatibility",
        "Nice find! I think:\n\n```suggestion\n        [basename(publicDir)]: resolve(srcDir, publicDir),\n```\n",
        "Previously `dir.public` was resolved relative to `srcDir`. For backwards compatibility `publicDir` is now fully resolved elsewhere in the schema, so effectively the first argument to `resolve` is ignored. But in the case a user sets it to `some-dir` this will be respected as `<srcDir>/some-dir`.",
        "I don't think we have a `nuxt.options.cacheDir` option set, so this should probably be something more like:\r\n\r\n```suggestion\r\n      $resolve: async (val, get) => val ?? resolve(await get('rootDir'), 'node_modules/.cache/vite'),\r\n```"
      ],
      "nuxt-organize-accessibility-attributes": [
        "```suggestion\n      <h1 class=\"flex flex-col gap-y-4 items-center justify-center\" aria-label=\"Nuxt {{ version }}\">\n```",
        "```suggestion\n          <svg role=\"img\" aria-label=\"Nuxt\" class=\"h-8 sm:h-12\" xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 800 200\">\n```"
      ]
    },
    "profile": {
      "location": "Edinburgh, UK",
      "blog": "https://roe.dev",
      "site_admin": false,
      "followers": 5182,
      "following": 167
    }
  },
  "chia7712": {
    "repos": [
      "apache/kafka"
    ],
    "entries": [
      {
        "slug": "kafka-avoid-overly-specific-examples",
        "title": "avoid overly specific examples"
      },
      {
        "slug": "kafka-avoid-unnecessary-object-creation",
        "title": "avoid unnecessary object creation"
      },
      {
        "slug": "kafka-catch-specific-exceptions",
        "title": "catch specific exceptions"
      },
      {
        "slug": "kafka-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "kafka-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "kafka-condition-based-network-synchronization",
        "title": "condition-based network synchronization"
      },
      {
        "slug": "kafka-defensive-null-validation",
        "title": "Defensive null validation"
      },
      {
        "slug": "kafka-improve-code-readability",
        "title": "Improve code readability"
      },
      {
        "slug": "kafka-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "kafka-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "kafka-optimize-collection-conversions",
        "title": "Optimize collection conversions"
      },
      {
        "slug": "kafka-optimize-data-structures",
        "title": "optimize data structures"
      },
      {
        "slug": "kafka-prefer-modern-collection-apis",
        "title": "prefer modern collection APIs"
      },
      {
        "slug": "kafka-use-condition-based-waiting",
        "title": "Use condition-based waiting"
      },
      {
        "slug": "kafka-use-parameterized-logging",
        "title": "Use parameterized logging"
      },
      {
        "slug": "kafka-validate-configuration-dependencies",
        "title": "validate configuration dependencies"
      },
      {
        "slug": "kafka-validate-configurations-early",
        "title": "Validate configurations early"
      },
      {
        "slug": "kafka-validate-network-state",
        "title": "validate network state"
      }
    ],
    "comments": {
      "kafka-avoid-unnecessary-object-creation": [
        "we could reuse the `partition` instead of creating new `AlterShareGroupOffsetsResponsePartition`, right?",
        "Could you please use `Collections.unmodifiableSet` instead to avoid extra deep copy?",
        "If it is hotspot, we should not generate optional object.\r\n```java\r\n            for (int brokerId : brokers) {\r\n                var broker = image.cluster().broker(brokerId);\r\n                if (broker != null && !broker.fenced() && broker.listeners().containsKey(listenerName.value()))\r\n                    res.add(brokerId);\r\n            }\r\n```",
        "Perhaps we don't need to create this temporary list.\r\n```java\r\n        Set<String> allNames = new HashSet<>();\r\n        allNames.addAll(MirrorCheckpointConfig.CONNECTOR_CONFIG_DEF.names());\r\n        allNames.addAll(MirrorSourceConfig.CONNECTOR_CONFIG_DEF.names());\r\n        allNames.addAll(MirrorHeartbeatConfig.CONNECTOR_CONFIG_DEF.names());\r\n        return allNames;\r\n```\r\n\r\nor\r\n\r\n```java\r\n        return Stream.of(\r\n                    MirrorCheckpointConfig.CONNECTOR_CONFIG_DEF.names(),\r\n                    MirrorSourceConfig.CONNECTOR_CONFIG_DEF.names(),\r\n                    MirrorHeartbeatConfig.CONNECTOR_CONFIG_DEF.names()\r\n                )\r\n                .flatMap(Set::stream)\r\n                .collect(Collectors.toSet());\r\n```"
      ],
      "kafka-optimize-algorithmic-complexity": [
        "Perhaps we could leverage `iterator` to avoid iterating through all items twice.\r\n```java\r\n        var iter = finalizedFeatureLevels.keySet().iterator();\r\n        while (iter.hasNext()) {\r\n            var featureName = iter.next();\r\n            if (newFinalizedLevels.containsKey(featureName) ||\r\n                featureName.equals(MetadataVersion.FEATURE_NAME) ||\r\n                featureName.equals(KRaftVersion.FEATURE_NAME)) {\r\n                continue;\r\n            }\r\n            removeFinalizedFeatureLevelMetric(featureName);\r\n            iter.remove();\r\n        }\r\n```"
      ],
      "kafka-improve-code-readability": [
        "```java\r\n    assertNotEquals(JoinGroupRequest.UNKNOWN_MEMBER_ID, memberId)\r\n    assertNotEquals(JoinGroupRequest.UNKNOWN_GENERATION_ID, memberEpoch)\r\n```\r\n`assertNotEquals` could show more readable messages.",
        "Could you please move those code to separate method? That can reduce the complexity and it can be test easily."
      ],
      "kafka-validate-configuration-dependencies": [
        "Can two brokers with two replicas fix the issue you mentioned?",
        "Perhaps we should ensure the exposed metrics are always based on current image. The benefit is the exposed finalized versions will be consistent to current image, and we won't  need to use `minimumProduction` which puts us in a weird position\r\n",
        "> If the feature does not have a finalized level, it does not have a finalizedLevel metric?\n\nYes, that is what I meant"
      ],
      "kafka-optimize-data-structures": [
        "```java\r\n    private DescribeTopicsResult describeTopicsResult(Collection<String> topics, int numOfPartitions) {\r\n        var topicDescriptions = topics.stream().collect(Collectors.toMap(Function.identity(),\r\n                topic -> new TopicDescription(topic, false, IntStream.range(0, numOfPartitions)\r\n                .mapToObj(i -> new TopicPartitionInfo(i, null, List.of(), List.of()))\r\n                .toList())));\r\n        return AdminClientTestUtils.describeTopicsResult(topicDescriptions);\r\n    }\r\n```",
        "```java\r\n        List<Integer> offlineReplicas = new ArrayList<>(0);\r\n        for (var brokerId : partition.replicas) {\r\n            var broker = image.cluster().broker(brokerId);\r\n            if (broker == null || isReplicaOffline(partition, listenerName, broker))\r\n                offlineReplicas.add(brokerId);\r\n        }\r\n        return offlineReplicas;\r\n```",
        "If we allow reusing the previous commit when matching subsets, the returned `Map<TopicPartition, OffsetAndMetadata>` will contain unrelated topic partitions. Therefore, should we adjust the return map of `consumer#committed` to match the input?"
      ],
      "kafka-avoid-overly-specific-examples": [
        "for another, perhaps we could use `openjdk:17` for this example to avoid using the explicit os version."
      ],
      "kafka-validate-network-state": [
        "if there is a topic having three partitions, and the `t-2` partition is offline, then if `partitionsToReset` is `t-0,t-1`, `filterNoneLeaderPartitions` will return `t-2`, causing the tool to fail. Is it expected?",
        "```\r\nchia7712@chia7712-ubuntu:~/project/kafka$ ./bin/kafka-consumer-groups.sh \\\r\n  --bootstrap-server 172.20.10.2:20001 \\\r\n  --reset-offsets \\\r\n  --to-earliest \\\r\n  --execute \\\r\n  --group perf-consumer-19460 \\\r\n  --topic chia:1\r\n\r\nError: Executing consumer group command failed due to The partitions \"chia-2\" have no leader\r\norg.apache.kafka.common.errors.LeaderNotAvailableException: The partitions \"chia-2\" have no leader\r\n```\r\n\r\nIt is indeed a bug that unrelated topic partitions could fail the tool.",
        "`filterNoneLeaderPartitions` needs to get fixed since it could return unrelated topic partitions.",
        "we don't need to modify the input `topicPartitions` if `filterNoneLeaderPartitions` returns the partitions having leaders."
      ],
      "kafka-use-condition-based-waiting": [
        "Could you please use \"wait for condition\" instead of time-based waiting? "
      ],
      "kafka-validate-configurations-early": [
        "It would be cool if `FeatureCommand` and `StorageTool` could leverage the exception from `fromVersionString`. After fixing KAFKA-19545, the `MetadataVersion.fromVersionString` will be used only by tools only, so it could throw `TerseFailure` instead of `IllegalArgumentException`, which could then be directly reused by tools.",
        "umm, `TerseFailure` is in core module, which can't be accessed by tool module. Also, `StorageTool` is still in core module, so `StorageTool` and `FeatureCommand` can't share the same `terse` exception for now. I guess another simple solution is they could just reuse the exception message thrown by `fromVersionString` :)\r\n\r\n\r\n\r\n"
      ],
      "kafka-comprehensive-test-coverage": [
        "It would be useful to have a unit test to loop over 1000 times, using random input to ensure before/after methods have same output.",
        "Could you please add unit test to ensure the maximum value of capacity of buffer is equal to `maxMessageSize`?"
      ],
      "kafka-maintain-naming-consistency": [
        "According to the naming, it should pass `false` instead of `true`, shouldn't it?"
      ],
      "kafka-use-parameterized-logging": [
        "the last parameter could be handled well if it is an exception object. Perhaps, we could use `{}` instead of `{}:{}`?",
        "Perhaps we should add the `e`  to the logger\r\n```java\r\nLOGGER.error(\"Encountered error while deleting {}\", file.getAbsolutePath(), e);\r\n```",
        "`LOG.error` could accept exception directly. Perhaps we could replace `due to: %s` by `LOG.error(msg, ex)`?"
      ],
      "kafka-centralize-configuration-values": [
        "sounds good"
      ],
      "kafka-optimize-collection-conversions": [
        "This will create many temporary collections. How about `partitionState.isr.asScala.map(_.toInt).diff(outOfSyncReplicaIds)`",
        "ditto",
        "ditto"
      ],
      "kafka-condition-based-network-synchronization": [
        "Perhaps it could be replaced by `TestUtils.waitForPartitionMetadata(brokers, partition2.topic(), partition2.partition())`?"
      ],
      "kafka-catch-specific-exceptions": [
        "Perhaps the log could include the exception?",
        "@k-raina catching the `Throwable` is not a major issue, so there is no hurry to include it in the 4.1.0 release. Perhaps you could address @apoorvmittal10 comment in this PR, and then we could backport it to 4.1.1  ",
        "Perhaps we could have an `unsupportedTypes` collection to filter out types according to `NoClassDefFoundError`? for example:\r\n```java\r\n\r\n                if (e instanceof NoClassDefFoundError) unsupportedTypes.add(compressionType);\r\n```\r\n`ClientTelemetryUtils.preferredCompressionType` could leverage the collection `compressionType` to avoid using  unsupported compression. for example:\r\n```java\r\n    public static CompressionType preferredCompressionType(List<CompressionType> acceptedCompressionTypes, Set<CompressionType> unsupportedTypes) {\r\n        if (acceptedCompressionTypes == null) return CompressionType.NONE;\r\n        // Broker is providing the compression types in order of preference. Grab the\r\n        // first one.\r\n        return acceptedCompressionTypes.stream()\r\n                .filter(t -> !unsupportedTypes.contains(t))\r\n                .findFirst()\r\n                .orElse(CompressionType.NONE);\r\n    }\r\n````",
        "Did you mean specify the accurate exception, like `IOException | NoClassDefFoundError`,  instead of `Exception`? \r\n\r\n> Catching Throwable like this isn't safe.\r\n\r\nCould you share the details of your concern with me?"
      ],
      "kafka-prefer-modern-collection-apis": [
        "`toMap` returns the mutable map, so `new HashMap` is unnecessary.",
        "```java\r\nreturn List.of(InternalMirrorResource.class);\r\n```"
      ],
      "kafka-defensive-null-validation": [
        "`QuorumFeatures.defaultSupportedFeatureMap` exclude the feature if the `enableUnstableVersions` is false and production version is zero. Hence, should we iterate `supportedFeatureRanges` instead of `Feature.PRODUCTION_FEATURE_NAMES` to avoid a possible NPE?"
      ]
    },
    "profile": {
      "location": "Taiwan",
      "blog": "www.chia7712.tw",
      "site_admin": false,
      "followers": 202,
      "following": 5
    }
  },
  "bolinfest": {
    "repos": [
      "openai/codex"
    ],
    "entries": [
      {
        "slug": "codex-avoid-hard-coded-configuration",
        "title": "Avoid hard-coded configuration"
      },
      {
        "slug": "codex-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "codex-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "codex-contextualize-dont-swallow",
        "title": "Contextualize, don't swallow"
      },
      {
        "slug": "codex-document-non-obvious-aspects",
        "title": "Document non-obvious aspects"
      },
      {
        "slug": "codex-extract-reusable-logic",
        "title": "Extract reusable logic"
      },
      {
        "slug": "codex-leverage-style-tools",
        "title": "Leverage style tools"
      },
      {
        "slug": "codex-minimize-blocking-operations",
        "title": "Minimize blocking operations"
      },
      {
        "slug": "codex-organize-code-top-down",
        "title": "Organize code top down"
      },
      {
        "slug": "codex-pin-external-action-dependencies",
        "title": "Pin external action dependencies"
      },
      {
        "slug": "codex-prevent-command-injection",
        "title": "Prevent command injection"
      },
      {
        "slug": "codex-proper-packagejson-structure",
        "title": "Proper package.json structure"
      },
      {
        "slug": "codex-provider-agnostic-api-design",
        "title": "Provider-agnostic API design"
      },
      {
        "slug": "codex-secure-cicd-pipelines",
        "title": "Secure CI/CD pipelines"
      },
      {
        "slug": "codex-semantic-naming-patterns",
        "title": "Semantic naming patterns"
      },
      {
        "slug": "codex-structure-configurations-properly",
        "title": "Structure configurations properly"
      },
      {
        "slug": "codex-workspace-version-configuration",
        "title": "Workspace version configuration"
      },
      {
        "slug": "codex-write-comprehensive-test-assertions",
        "title": "Write comprehensive test assertions"
      }
    ],
    "comments": {
      "codex-avoid-unnecessary-operations": [
        "Since the other two cases always `flush()`, there should never be anything new to `flush()` in this case, no?",
        "I would skip the `flush()`: all this needs to do is `ack.send(())`.",
        "If we are going to ad logic to control the render late, we should be doing this at a higher level in the TUI so it applies globally, no?",
        "Right, but in general, it's possible the top-level event loop gets too many `AppEvent::Redraw` requests, so why not do the throttling there?",
        "I think you want `let prompt: Cow<'a, Prompt>` if you can to avoid the `clone()`? So in the consequent, it's `Cow::Borrowed` and in the alternative, it's `Cow::Owned`?"
      ],
      "codex-contextualize-dont-swallow": [
        "Why do we ignore all the io errors instead of returning `io::Result<()>`?",
        "can it just be this for a string literal?\r\n\r\n```suggestion\r\n        .context(\"failed to load bash grammar\")?;\r\n```",
        "I think this is debatable whether we should proceed in this case. Certainly the program will continue to work, so I guess it's fine? On the other hand, if you are trying to test the wire protocol and you sent bad data, perhaps we should crash so you fix it?",
        "If this is happens, this is a logical error where our treesitter dependency is not set up correctly, so we should not swallow this error.",
        "Would with_context from anyhow work here?"
      ],
      "codex-proper-packagejson-structure": [
        "This should be in `devDependencies`."
      ],
      "codex-organize-code-top-down": [
        "This is admittedly a nit, but personally, I would rewrite this as a `match self.tx.send(RolloutCmd::Shutdown { ack: tx_done }).await` to avoid using the `return` keyword. I try to reserve the use of `return` when you really need an \"early return\" in a long function. But if using a single expression is an option, I find that to be cleaner because then it's more \"straight line\" code (though admittedly `match` branches...).",
        "For small helper functions, particularly ones that are private to the file, please declare them _after_ the functions that use them. I strongly prefer declaring the \"most important stuff\" at the top of the file and \"details\" (which includes functions like this) at the top of the file.",
        "Is there a reason this logic isn't added to `dispatch_codex_event()` instead? Much of the reason to have the `dispatch_codex_event()` helper is to keep the length of `run()` down. In a new top-level function, there will be less indenting and the code should be easier to read, as well.",
        "Let's do this before `match hunk` since it is used in all three cases."
      ],
      "codex-provider-agnostic-api-design": [
        "I think it's more appropriate for these to be:\r\n\r\n```suggestion\r\nconst DEFAULT_STREAM_IDLE_TIMEOUT_MS: u64 = 300_000;\r\nconst DEFAULT_STREAM_MAX_RETRIES: u64 = 10;\r\nconst DEFAULT_REQUEST_MAX_RETRIES: u64 = 4;\r\n```",
        "Some of this code duplicates code in `core/`. Note that the model provider might only support one of Responses or the chat completions API, so it is not safe to assume the above code is an option.\r\n\r\nThat said, I recently learned that there is a _non-stateful version of the Responses API_ where  you pass input as the array of all messages the same way you do for chat completions. So perhaps we could expose a function in `core/` that takes a `Config` and a list of messages like you have here and uses the `config.model_provider` and a `Client` to make the request?\r\n\r\nAdmittedly this is complex enough that it is probably appropriate to do it in a separate PR."
      ],
      "codex-centralize-configuration-management": [
        "I have been trying to eliminate support for environment variables in favor of using configuration. Can we just added a config option (prefixed with \"experimental\" like `experimental_resume`) for this?",
        "I think this could be argued either way, but I think there is a technical reason to do this as early as possible (before we know what the `Config` even is), which is that setting environment variables for the current process is not thread-safe, so it should really be done before any threads have been created.\r\n\r\nI just reworked this so that `load_dotenv()` is now called before we set up the Tokio runtime.",
        "I think we should only be looking at the `OPENAI_` environment variables for the built-in OpenAI provider, not all providers, right?",
        "Taking a step back, I'm not sure we should be honoring `OPENAI_STREAM_IDLE_TIMEOUT_MS`, `OPENAI_REQUEST_MAX_RETRIES`, or `OPENAI_STREAM_MAX_RETRIES` at all. As best I can tell, these are not \"standard\" OpenAI environment variables, but ones that we made up for Codex?\r\n\r\nI've been trying to maintain a consistency where the `Config` is the \"one true way\" to configure things, so supporting a small handful of environment variables confuses that.",
        "```suggestion\r\n    // Configure retry behavior explicitly to avoid mutating process-wide\r\n```",
        "```suggestion\r\n    // Configure retry behavior explicitly to avoid mutating process-wide\r\n```"
      ],
      "codex-extract-reusable-logic": [
        "First, we should carve out a general notification abstraction rather than duping this logic into the middle of a UI component."
      ],
      "codex-workspace-version-configuration": [
        "outside the scope of this PR, but I guess all of our `Cargo.toml` files should have `version = { workspace = true }`?"
      ],
      "codex-write-comprehensive-test-assertions": [
        "I would just `assert_eq!()` using a string literal that, yes, is a copy of `COMPACT_SUMMARY_TEMPLATE`. Again, I would use `r#`.",
        "Maybe use `find(|t| t.get(\"name\").as_ref() == Some(\"srv.dummy\")` on `tools.iter()` or something like that and then do an `assert_eq!()` on the value returned from `find()`?",
        "For both of these tests, can we just assert the entire string/serde_json::Value that we get back? I realize this means that we will have to update this test if we change the default tools, but I think having a test that verifies _everything_ (and effectively documents what we send on the wire) is worth that maintenance cost.",
        "This is a natural thing to do, but whenever possible, please just do one big `assert_eq!()` rather than doing a bunch of piecemeal `assert_eq!()` calls, so something like:\r\n\r\n```suggestion\r\n        assert_eq!(events, vec![\r\n            Ok(ResponseEvent::OutputItemDone(ResponseItem::Message { role: \"assistant\" }),\r\n            Ok(ResponseEvent::OutputItemDone(ResponseItem::Message { role, \"assistant\" }),\r\n            Ok(ResponseEvent::Completed { response_id: \"resp1\", token_usage: None })\r\n        ]);\r\n```\r\n\r\nI think you can also potentially do an `iter` / `collect()` on `events` to convert `Vec<Result<T>>` into `Result<Vec<T>>`.\r\n\r\n",
        "For clarity, would you mind just doing `assert_eq!` on the full struct? Admittedly, I think you need to `derive(PartialEq)` on `MaybeApplyPatchVerified` and some things to do that.\r\n\r\nWhen possible, I find that to be much stronger than doing `assert_eq!` on individual fields."
      ],
      "codex-leverage-style-tools": [
        "I'm about to run CI on this PR, but seeing this makes me suspicious that you haven't run `pnpm run format` in this folder or maybe not `pnpm run lint`? If you use VS Code and have the ESLint and Prettier extensions configured correctly, you should get alerted to these issues in the Problems pane and generally don't have to run those scripts explicitly.",
        "This should have stayed a `switch` statement because we have ESLint rules set up to ensure `switch` statements are exhaustive so that if a new variant of the `ApprovalPolicy` enum is introduced we are forced to address the existing callsites."
      ],
      "codex-pin-external-action-dependencies": [
        "This one is a bit better:\r\n\r\nhttps://github.com/codespell-project/codespell-problem-matcher",
        "Copy/paste error: that should have been https://github.com/codespell-project/actions-codespell. It has 21 forks and 80 stars, so it feels like at least some folks are keeping an eye on it.",
        "I am opting to keep both. I am saying that instead of referencing the actions by tag (`@v1`, `@v2`), I would like to reference them by full commit hash so the behavior of the action cannot change out from under us.",
        "This one is fine: this is GitHub's responsibility and if this does something malicious, the whole Internet will break anyway. So please keep this as `@v4` so it's easier to eyeball that we're doing something canonical."
      ],
      "codex-secure-cicd-pipelines": [
        "Hmm, this does not appear to have seen much interest or activity:\r\n\r\nhttps://github.com/codespell-project/codespell-problem-matcher\r\n",
        "Right, I trust that it works today. What I have less confidence in is a bad actor coming in, somehow gaining control of `codespell-problem-matcher`, changing the `v1` tag to point at something else, and anyone noticing."
      ],
      "codex-prevent-command-injection": [
        "Second, we should absolutely not be using `exec()` with a single string arg that appears to be subject to injection risks. At a minimum, we should be using `spawn()` with a list of strings."
      ],
      "codex-minimize-blocking-operations": [
        "Can you use tokio::Command and make this `async` instead? It's cheaper to create tokio tasks than POSIX threads. You should then update `collect_git_info()` to make all these calls in parallel.",
        "I appreciate the timeouts in `collect_git_info()`, though if I am reading it correctly, I suppose this could add ~6s to startup in the worst case? It would be nice to figure out how to make this truly async, since `RolloutRecorder::new()` is on the critical path to startup.\r\n\r\nThe challenge seems to be that we have these lines below:\r\n\r\n```rust\r\n    recorder.record_item(&meta).await?;\r\n    Ok(recorder)\r\n```\r\n\r\nThat is, we don't want `new()` to exit until the first item has recorded and now that is dependent on `collect_git_info()`. Certainly this is fixable, but the bookkeeping may be a bit ugly. What do you think?",
        "Actually, what if we move `collect_git_info(cwd).await` into the lambda passed to `tokio::task::spawn` and then ensure it is written to `file` before the `while let Some(line) = rx.recv().await` loop starts?\r\n\r\nYou could also increase the `git` timeout to 5s maybe?",
        "FYI, if you write it as a single statement, then there is no intermediate `running_requests_id_to_codex_uuid` reference that has to be _dropped_ to release the lock, so the `Drop` happens implicitly as part of the statement executing.\r\n\r\n```suggestion\r\n    running_requests_id_to_codex_uuid.lock().await.running_requests_id_to_codex_uuid.insert(request_id.clone(), session_id);\r\n```",
        "This is slightly better because it results in holding the lock for a shorter amount of time.\r\n\r\nThat extra level of scoping around the use of `self.pending_redraw.lock()` and `flag` ensures that after `*flag = true`, the lock is dropped.\r\n\r\nAlso, `Arc::clone()` is less canonical than just invoking `.clone()`, in my experience.\r\n\r\n```suggestion\r\n        {\r\n            #[allow(clippy::unwrap_used)]\r\n            let mut flag = self.pending_redraw.lock().unwrap();\r\n            if *flag {\r\n                return;\r\n            }\r\n            *flag = true;\r\n        }\r\n\r\n        let tx = self.app_event_tx.clone();\r\n        let pending_redraw = &self.pending_redraw.clone();\r\n        thread::spawn(move || {\r\n            thread::sleep(REDRAW_DEBOUNCE);\r\n            tx.send(AppEvent::Redraw);\r\n            #[allow(clippy::unwrap_used)]\r\n            let mut f = pending.lock().unwrap();\r\n            *f = false;\r\n        });\r\n```"
      ],
      "codex-structure-configurations-properly": [
        "I think we need a different name and maybe a level of two of depth.\r\n\r\nThat is, this not configuring the general \"output\" of Codex CLI. (To me, this name implies the \"output\" that I see as a user.) It is configuring something extremely specific: truncation parameters for the `shell` tool call.\r\n\r\nThis makes me wonder if should have a top level config entry named `\"tools\"` where keys are tool names that point to dictionaries of arbitrary properties for configuring that tool, so the JSON would look like:\r\n\r\n```json\r\n\"tools\": {\r\n  \"shell\": {\r\n    \"maxBytes\": 12410,\r\n    \"maxLines\": 256\r\n  }\r\n}\r\n```\r\n",
        "I don't think we should be loading the config at arbitrary places in the code. That means the config could change over the course of the run, which in some cases could be helpful, but in other cases I think could be quite dangerous because it could unintentionally change the behavior of a long-running agent.\r\n\r\nToday, `loadConfig()` is called once in `cli.tsx` (as it should be) and then it should be threaded through from there."
      ],
      "codex-document-non-obvious-aspects": [
        "Should we keep this comment?",
        "Please add a docstring explaining what is being tested.",
        "Could you add docstrings for this test and the other test? Admittedly, there is a lot of code required just to setup these tests, so it's not 100% obvious what is being tested. That is, this line seems to be the key bit that is producing the behavior that we are verifying at the end of the test:\r\n\r\n```rust\r\n.set_body_raw(sse_message(\"Hello, world.\"), \"text/event-stream\")\r\n```",
        "Maybe a docstring to explain what the return value represents?",
        "I have tried to avoid these sorts of comments. For example, in `bottom_pane.rs`, we have:\r\n\r\n```rust\r\n/// Number of terminal rows consumed by the textarea border (top + bottom).\r\nconst TEXTAREA_BORDER_LINES: u16 = 2;\r\n```"
      ],
      "codex-semantic-naming-patterns": [
        "Using `{}` as a placeholder in this way seems very confusing to me as a Rust person because it's not being used natively by `format!()`. Please use something like `SUMMARY_TEXT` instead so it's more obvious that something is meant to be replaced.",
        "These should not have an `openai_` prefix, but should be generally applicable to all providers, right?",
        "Similar to a comment I made on another PR, please list all of these helper functions below the tests. The tests are the most important thing in this file.",
        "Also, I would name the variable `codex_home` rather than `dir`.",
        "`fully_qualified_tool_name()` and `try_parse_fully_qualified_tool_name()` must be symmetric. It is not clear that this is the case given this implementation.\r\n\r\nSomeone told me that, empirically, the model doesn't care about the names of the functions all that much and therefore, we could SHA1 the long name or something and things would still work.\r\n\r\nAnother solution that is somewhat stateful, but more readable for users, would be to get the full list of tool names and only attempt to \"fully qualify them\" when there is a naming collision."
      ],
      "codex-avoid-hard-coded-configuration": [
        "I was thinking of going in the other direction where we would replace `--full-auto` with `--dangerously-auto-approve-everything` since the Docker container *is* the sandbox. Is your motivation to add a weaker approval mode or a stronger one?\r\n\r\nI guess from your screenshot you want to run it with `--suggest`?",
        "@fouad-openai do you want this to print something else if `--native` was passed to indicate publishing to a different tag?",
        "I believe it is possible for `ALLOWED_DOMAINS` to be the empty string if this script were run with `ALLOWED_DOMAINS=` (explicitly defining the `ALLOWED_DOMAINS` env var as the empty string), so out of an abundance of caution, maybe we should do this before `docker run`:\r\n\r\n```\r\nif [ -z \"$ALLOWED_DOMAINS\" ]; then\r\n  echo \"Error: ALLOWED_DOMAINS is empty.\"\r\n  exit 1\r\nfi\r\n```\r\n\r\n"
      ]
    },
    "profile": {
      "company": "Meta",
      "blog": "",
      "site_admin": false,
      "followers": 393,
      "following": 9
    }
  },
  "maxbrunsfeld": {
    "repos": [
      "tree-sitter/tree-sitter",
      "zed-industries/zed"
    ],
    "entries": [
      {
        "slug": "tree-sitter-algorithm-and-data-optimization",
        "title": "Algorithm and data optimization"
      },
      {
        "slug": "tree-sitter-api-pattern-consistency",
        "title": "API pattern consistency"
      },
      {
        "slug": "tree-sitter-assert-null-before-access",
        "title": "Assert null before access"
      },
      {
        "slug": "tree-sitter-automate-frequent-releases",
        "title": "automate frequent releases"
      },
      {
        "slug": "tree-sitter-complete-technical-documentation",
        "title": "Complete technical documentation"
      },
      {
        "slug": "tree-sitter-consistent-formatting-preferences",
        "title": "consistent formatting preferences"
      },
      {
        "slug": "tree-sitter-consolidate-related-logging",
        "title": "consolidate related logging"
      },
      {
        "slug": "tree-sitter-cross-platform-configuration-examples",
        "title": "Cross-platform configuration examples"
      },
      {
        "slug": "tree-sitter-ensure-semantic-naming-clarity",
        "title": "Ensure semantic naming clarity"
      },
      {
        "slug": "tree-sitter-measure-performance-implications",
        "title": "measure performance implications"
      },
      {
        "slug": "tree-sitter-optimize-frequent-operations",
        "title": "Optimize frequent operations"
      },
      {
        "slug": "tree-sitter-optimize-memory-usage-patterns",
        "title": "optimize memory usage patterns"
      },
      {
        "slug": "tree-sitter-prefer-compile-time-configuration",
        "title": "prefer compile-time configuration"
      },
      {
        "slug": "tree-sitter-provide-clear-error-context",
        "title": "Provide clear error context"
      },
      {
        "slug": "tree-sitter-respect-environment-overrides",
        "title": "Respect environment overrides"
      },
      {
        "slug": "tree-sitter-simplify-conditional-logic",
        "title": "Simplify conditional logic"
      },
      {
        "slug": "tree-sitter-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "tree-sitter-use-meaningful-identifiers",
        "title": "Use meaningful identifiers"
      },
      {
        "slug": "tree-sitter-use-meaningful-prefixed-names",
        "title": "Use meaningful prefixed names"
      },
      {
        "slug": "tree-sitter-validate-algorithm-boundaries",
        "title": "validate algorithm boundaries"
      },
      {
        "slug": "tree-sitter-validate-algorithmic-inputs",
        "title": "validate algorithmic inputs"
      },
      {
        "slug": "tree-sitter-validate-inputs-early",
        "title": "validate inputs early"
      },
      {
        "slug": "tree-sitter-write-explicit-concrete-tests",
        "title": "Write explicit concrete tests"
      },
      {
        "slug": "zed-test-through-public-apis",
        "title": "Test through public APIs"
      }
    ],
    "comments": {
      "tree-sitter-use-meaningful-prefixed-names": [
        "In C, these variant names are all added to the top-level namespace, so my usual practice is prefix them with the name of the enum, e.g. `TSQuantifierOneOrMore` (even though it is repetitive).",
        "Some minor naming suggestions:\r\n\r\n1. If you switched the `uint16_t parse_state;` with `uint8_t padding_bytes; uint8_t size_bytes;`, then all of the \"size\"-related fields would be together in `SUBTREE_4_BYTES`, and you could rename that macro to `SUBTREE_SIZE`, which indicates more about its meaning.\r\n2. Then, `SUBTREE_3_BYTES` would only contain two fields, and you could just inline them, instead of having that macro at all.\r\n2. I think `SUBTREE_6_BITS` could be renamed to just `SUBTREE_BITS`.",
        "I'd prefer to just spell out \"default\" fully. Can you change these functions to `*_default`?",
        "I think this field should be called `shift`. \"Shift\" and \"reduce\" are the two main types of actions."
      ],
      "tree-sitter-provide-clear-error-context": [
        "I think you could use anyhow's [`with_context`](https://docs.rs/anyhow/1.0.42/anyhow/trait.Context.html#tymethod.with_context) method to express this slightly more concisely:\r\n\r\n```rust\r\nlet content = fs::read_to_string(&location)\r\n    .with_context(|| format!(\"failed to read {}\", location.to_string_lossy()))?;\r\n// ...\r\n```"
      ],
      "tree-sitter-api-pattern-consistency": [
        "The main reason I lean toward keeping them separate is that callers will need to use them slightly differently. When creating bindings to this API, the `capture_names` need to stored in an array that will be used every time you run a query, whereas the `strings` only need to be loaded temporarily, while constructing a query.\r\n\r\nWith this approach, we can preallocate and populate the `captureNames` array using `ts_query_capture_count / ts_query_capture_name_for_id`, and preallocate and populate a separate `stringValues` array using `ts_query_string_count / ts_query_string_value_for_id`. Then, at the end of the query constructor function, we can simply drop the `stringValues` array.\r\n\r\n> It could still have all the capture names in the beginning, if there is a need to index a dense array of captures or something.\r\n\r\nThat's true. With that approach, we could at least *shrink* the array once we are done using the `strings`. But in order to do this, callers still need to know how many captures there are, so there still needs to be a separate `ts_query_capture_count` function.\r\n\r\nIn the end, I think the APIs are slightly more obvious in their meaning if `captures` and `strings` are just presented as two separate arrays, where each one has identical APIs for accessing its length and its contents.",
        "Yeah, good point. I'm writing the API with the assumption that FFI has some noticeable cost, based on experience with JavaScript via Node and WebAssembly. If you aren't keeping a managed copy of the captures array, then none of the things that I said above are applicable.\r\n\r\nI think I'll keep it separate for now, though I see your point about the potential reduction in API functions.\r\n\r\nOn a related note, in 1af85dc3f758e5b4e2179fec81f95b89dadc26e3 I did delete two APIs related to captures and string values, which I realized were unused by either of my bindings. So now there are slightly fewer 😁. "
      ],
      "tree-sitter-optimize-frequent-operations": [
        "This assertion change shows how failing to reuse nodes can destroy performance. We're now re-reading all the way to the end of the file, even though we should be able to reuse the unchanged code."
      ],
      "tree-sitter-automate-frequent-releases": [
        "Yeah, I agree. Right now it's hard to stay on top of releasing all of the latest changes. [This PR](https://github.com/tree-sitter/tree-sitter-java/commit/81148c0902125150d616ea30190a630c86974a7a) by @Luni-4 was a good start, adding automatic publishing to crates.io.\r\n\r\nI guess the ideal setup would be for all grammars to have something like that, and for the workflow to also publish other things as well, like the Node.js bindings and WASM builds to NPM, and at some point, the generated source code to a GitHub release, so that we could stop hosting it primarily in the git repository.",
        "Maybe the simplest approach is to pass the `.a` file as just another object file argument, rather than using the linker flags `-l` and `-L` at all.\r\n\r\n```sh\r\nclang++ -std=c++11 \\\r\n  $(find tree-sitter/out -name libcompiler.a)  \\\r\n  arithmetic_grammar.cc \\\r\n  -o arithmetic_grammar\r\n```\r\n\r\nThis works for me on macOS using clang? Does this work on your linux system?"
      ],
      "tree-sitter-use-meaningful-identifiers": [
        "Maybe write this integer in hexadecimal as `0x25A1`? It's probably still worth having a comment explaining that's the box character, but it would be slightly  more self-explanatory.",
        "Not a single-quoted character literal; just the plain integer `0x25A1`.",
        "What do you think of eliminating this typedef? It seems like it's only used in one place; also the name suffix \"Entry\" makes me think that it's going to be represent one element of some collection, which isn't the case here.",
        "For consistency with the other functions, could you rename these new methods so that they begin with the prefix `ts_language_`?"
      ],
      "tree-sitter-prefer-compile-time-configuration": [
        "The files `web_ui.html` and `playground.js` are actually always present, and checked into the repo. So I think you only need to check for the existence of `tree-sitter.js` and `tree-sitter.wasm`.\r\n\r\nI actually don't think you need any platform-specific logic in this script. On Windows, `wasm_files_present` will return false, because we don't build the wasm binding on Windows (at least not on CI). If some windows user *does* have a working emscripten toolchain, they should be able to build Tree-sitter with the wasm binding embedded.",
        "I think what we can do here is *replace* `#[cfg(unix)]` with `#[cfg(TREE_SITTER_EMBED_WASM_BINDING)]`. Basically, instead of a Posix/Windows distinction, we'll have the distinction between *embedded wasm files* and *non-embedded wasm files*.\r\n\r\nI think we can rename the `posix_resource` macro to something like `optional_resource` - a file which *may* or *may not* be embedded in the binary."
      ],
      "tree-sitter-validate-algorithmic-inputs": [
        "My intention was to for `each_referenced_state` to only call this callback with valid state indices, so we shouldn't have to check them here.\r\n\r\nI think all we need is your other fix to `each_referenced_state` itself, plus the one additional check I mention below.",
        "😬  Oh wow. Thanks for catching that.\r\n\r\nWe need to check one other condition here to ensure that `action` will have a meaningful `state_index` field:\r\n\r\n```cpp\r\nif ((action.type == ParseActionTypeShift && !action.extra) || action.type == ParseActionTypeRecover)\r\n  fn(&action.state_index)\r\n```\r\n\r\n**Background** - There's a special 'shift-extra' action that tells the parser to consume an 'extra token' (e.g. a comment). These actions don't change the parser's state, so they don't have a `state_index` field. I can't remember why I modeled these actions using a separate boolean `extra` field; it seems like there should just be a separate `ParseActionTypeShiftExtra` value for the `type` field. But anyway, with the current setup, only shift actions that are *not* `extra` will have a meaningful `state_index`."
      ],
      "tree-sitter-assert-null-before-access": [
        "It shouldn't be possible for `root` to be `NULL` here; would it make `scan-build` happy if we asserted that it was non-null before asserting about its ref_count?\r\n\r\n```c\r\nassert(root && root->ref_count > 0);\r\n```",
        "Yeah, this conditional was leftover from before https://github.com/tree-sitter/tree-sitter/pull/43, when the system tried to recover from malloc failure. I've removed that logic across the board, since malloc never even actually returns null on most systems, and apps that use tree-sitter aren't likely to have a fine-grained OOM recovery strategy anyway."
      ],
      "tree-sitter-algorithm-and-data-optimization": [
        "I think this test could use a different name, since `inlined` isn't really part of the issue. Maybe `parent_with_zero_length_node`? I think the issue is that there are zero-length nodes inside of the `suite`,  right?",
        "Yeah, I think the test should be called `test_parent_of_zero_width_node`. The `block` in this case (which is what you're testing the parent of) is a zero-width node, and that's why the problem was happening before.\r\n\r\nThe issue here doesn't have to do with inline subtrees. Inline subtrees are just a slightly more compact way of representing certain child-less subtrees, but it's not related to the problem.\r\n\r\nI think your fix (avoiding descending into nodes without children) helps in this case (and probably in most practical cases of this bug), but it's still possible that `ts_node_parent` could return the wrong value in the presence of zero-width nodes.\r\n\r\nFundamentally `ts_node_parent(node)` works by walking down the tree, starting from the root, trying to find the smallest possible node that contains the `node`'s byte range (and is not `node` itself). But if the node's byte range is empty, there are some problems. There might be *multiple* siblings that all contain that zero-length range. If there were multiple zero-length nodes in a row (which is uncommon), we could still get incorrect results here.\r\n\r\nI still think this fix improves the situation, and I'm glad you figured it out. I just think we should change the title of the test and 🚢 .",
        "I think this needs to return the number of remaining elements, so you’ll need to subtract off the current index."
      ],
      "tree-sitter-respect-environment-overrides": [
        "/cc @philipturnbull - I don't have libfuzzer set up locally on my linux VM to try this, but I've attempted to adjust this script (`build-fuzzers`) and the fuzzing driver program (`fuzzer.cc`) so that they work with the new project structure.\r\n\r\nA summary of the changes:\r\n\r\n* We don't use `gyp` to create Makefiles anymore. Since the bulk of the project (the CLI code) is now built using Cargo, I don't think we need a \"build system\" for the C code anymore. Instead I just have a script (`script/build-lib`) which builds the C library. That script should respect the `CFLAGS` environment variable.\r\n\r\n    That script will produce a static library in the root directory: `./libtree-sitter.a`.\r\n\r\n* The headers are now located in `lib/include` instead of `include`.\r\n\r\n* I'm no longer using the term \"runtime\" to refer to the runtime library. The header file is now just called `tree_sitter/api.h`.",
        "When you get a chance, would you mind taking a look at the new setup?",
        "Awesome! Thanks for those fixes.",
        "This check seems reliable; do we need to allow `TS_PTR_SIZE` as a compiler argument?",
        "Just curious - what is the effect of these two `LDFLAGS` lines (compared to not explicitly defining LDFLAGS at all)?",
        "I think I’d slightly prefer to leave these out then. Thanks for the explanation."
      ],
      "tree-sitter-use-descriptive-identifiers": [
        "What do you think of calling this field `capture_name`, or `expected_capture_name`, to indicate what the string represents?"
      ],
      "tree-sitter-validate-algorithm-boundaries": [
        "I think this would falsely *include* the node if it is empty, but disjoint from the range. For example, say we had an empty node at 0, but were looking for matches in the range `5 - 10`. I think that instead of special-casing byte offset 0, we may want to have a special-case where the node is empty and touches the range boundary.\r\n\r\nWhat about something like this?\r\n\r\n```c\r\nuint32_t start_byte = ts_node_start_byte(node);\r\nuint32_t end_byte = ts_node_end_byte(node);\r\nTSPoint start_point = ts_node_start_point(node);\r\nTSPoint end_point = ts_node_end_point(node);\r\nbool is_empty = start_byte == end_byte;\r\n\r\n// ...\r\n\r\nbool node_precedes_range =\r\n  parent_precedes_range ||\r\n  end_byte < self->start_byte ||\r\n  point_lt(end_point, self->start_point) ||\r\n  (!is_empty && end_byte == self->start_byte) ||\r\n  (!is_empty && point_eq(end_point, self->start_point));\r\n```",
        "I think this should be an **and**, not an **or**. To be counted, a node must be within the supplied byte range **and** within any supplied point range.",
        "Ah yes; you're quite right. Thanks for the explanation, and for making that code a bit clearer.\r\n\r\nWould you be willing to add a test for this in `query_test.rs`?",
        "Will this change silence the warning?\r\n\r\n```c\r\nfor (TSSymbol i = 0; (uint32_t)i < count; i++) {\r\n```\r\n\r\nI think I'd rather add a cast than an additional runtime check.",
        "Actually, I think a better fix would be to simply change line 90 to this:\r\n\r\n```\r\nuint16_t count = ts_language_symbol_count(self);\r\n```\r\n\r\nIn practice there will only ever be a few hundred \"symbols\" in a grammar. This is why we can use the type `uint16_t` to represent `TSSymbol`, which is good because it allows parser binaries to be much smaller. It follows that the variable `count`, which represents the number of distinct `TSSymbol` values in a parser, must itself be representable as `uint16_t`.\r\n",
        "I think it would be valid for those functions to return `uint16_t`, but it's also fine to keep them as `uint32_t`. For right now, I'd rather leave them as they are.\r\n\r\n> just changing the type of the `count` variable in this function won't really fix the issue but just hide it again.\r\n\r\nThere isn't *really* a bug here; all we need to do is help the static analyzer understand that `count` is, due to the way that the parsers are constructed, always going to be much less than `UINT16_MAX`.",
        "There are some auto-generated ones as well as those defined by the user.",
        "Yeah. A parser wouldn’t work at all if the limit was reached."
      ],
      "tree-sitter-complete-technical-documentation": [
        "Instead of line comments, why don't we add to the paragraph above? \r\n\r\n> In a point, rows and columns are zero-based. The `row` field represents the number of newlines before a given position, while `column` represents the number of bytes between the position and beginning of the line."
      ],
      "tree-sitter-simplify-conditional-logic": [
        "What do you think of shortening these functions up a bit by removing the `result` variable, and using early return statements in each case?\r\n",
        "Maybe now it'd be cleaner to unify these two conditionals:\r\n\r\n```c\r\nif (e) {\r\n  capture_quantifiers_delete(&field_capture_quantifiers);\r\n  if (e == PARENT_DONE) e = TSQueryErrorSyntax;\r\n  return e;\r\n}\r\n```",
        "I think that you simplify this code a little bit. I would suggest:\r\n\r\n1. Eliminating the `update_column_cache` parameter to `ts_lexer__do_advance`. Just have that method *always* update the column cache.\r\n\r\n2. Restructure this `ts_lexer__get_column` function so that it just populates the column cache if necessary, using the normal code path, and then returns its value:\r\n\r\n```c\r\nstatic uint32_t ts_lexer__get_column(TSLexer *_self) {\r\n  Lexer *self = (Lexer *)_self;\r\n  self->did_get_column = true;\r\n\r\n  if (!self->column_cache.valid) {\r\n    uint32_t goal_byte = self->current_position.bytes;\r\n\r\n    // back up to the beginning of the line\r\n    self->current_position.bytes -= self->current_position.extent.column;\r\n    self->current_position.extent.column = 0;\r\n    self->column_cache.valid = true;\r\n    self->column_cache.value = 0;\r\n\r\n    // advance back to the initial position\r\n    // ...\r\n  }\r\n  \r\n  return self->column_cache.value;\r\n}\r\n```"
      ],
      "tree-sitter-validate-inputs-early": [
        "The only problem with this is that, while the `didExceedMatchLimit` method is on the `Query` object, its behavior right now isn't scoped to that particular query. So it could give an unexpected result if you ran two different queries, and then called `didExceedMatchLimit` on the *first* query (admittedly, an unlikely sequence).\r\n\r\nWhat do you think of the following tweak, which would fix that minor issue:\r\n\r\nAt the ends of the functions `ts_query_matches_wasm` and `ts_query_captures_wasm`, we'd write a `did_exceed_match_limit` boolean as a third value to our `TRANSFER_BUFFER`. We'd have to cast it to a `void *`, which we're already doing with the result count. Then, on the JavaScript side, we'd just have to store that value as a boolean field on the `Query` object, and then return the same field value from the `didExceedMatchLimit` method.\r\n\r\nIt's slightly more complicated, but I feel like it's not too bad, so it'd be worth it to be able to avoid confusion if people call the methods in an unusual order."
      ],
      "tree-sitter-consistent-formatting-preferences": [
        "Can we not have these spaces after the `#` in preprocessor directives?",
        "Very minor - I think you can make this slightly shorter using [`operator+(const std::string &, const char *)`](http://www.cplusplus.com/reference/string/string/operator+/).\r\n\r\n```suggestion\r\n    auto lang_query_filename = binary_directory + \"/\" + TS_LANG_QUERY_FILENAME;\r\n```",
        "I think the combination `! -z` can be written `-n` (non-empty)."
      ],
      "tree-sitter-consolidate-related-logging": [
        "I don't think this `character` part needs to be logged; I think I left it in by accident. It looks like, if you removed this, you could make the macro simpler (no varargs).",
        "Could you put these in the same log message, like `start_token chars:%lu, row:%lu, column:%lu`?\n"
      ],
      "zed-test-through-public-apis": [
        "I think in this test, it would probably be good to have some assertions where the query (and the buffer) contain multibyte characters."
      ],
      "tree-sitter-cross-platform-configuration-examples": [
        "Since the `web-ui` command will still *work* without building the WASM (but it'll just require an internet connection to fetch the wasm from tree-sitter.github.io), maybe we should say:\r\n\r\n```md\r\nOptionally, build the WASM library. If you skip this step, then the `tree-sitter web-ui` command will require an internet connection. If you have emscripten installed, this will use your `emcc` compiler. Otherwise, it will use Docker:\r\n```"
      ],
      "tree-sitter-write-explicit-concrete-tests": [
        "This is a stylistic thing, but personally, I'd prefer to see this test written in a more concrete way, with two or three different hand-written queries, instead of the queries being generated via the for loops. It'd be a bit longer, but that way, we could make specific assertions about which matches we expect, for each query.\r\n\r\nI think just `(comment)*** @capture` and `(comment)??? @capture` would be enough.",
        "Very cool that you're verifying the graph logs!\r\n\r\nMy only suggestion is that I think we should perform this check in an explicit unit test of this functionality. Right now, this `LogSession` struct isn't used during a normal test run; it's just a tool that we use to debug failing tests (via `script/test THE_TEST_NAME -D`). And it's kind of an optional feature of the test suite: you can only use it if you have Graphviz installed on your system.\r\n\r\nWhat do you think about adding a specific unit test in `parser_test.rs`, something like this:\r\n\r\n```rust\r\n#[test]\r\n#[cfg(unix)]\r\nfn test_parsing_with_debug_graph_enabled() {\r\n    let mut parser = Parser::new();\r\n    let mut debug_graph_file = tempfile::tempfile().unwrap();\r\n    parser.print_dot_graphs(&debug_graph_file);\r\n\r\n    // validate the debug graph \r\n}\t\r\n```"
      ],
      "tree-sitter-measure-performance-implications": [
        "Does calling `delete` and `new` on every chunk slow down the tests at all? I'm wondering if we really need to do this everywhere, or if we can just do something custom for this one test, like calling `ts_document_set_input_string_with_length`.",
        "Ah ok, thanks for investigating!"
      ],
      "tree-sitter-ensure-semantic-naming-clarity": [
        "Why is the import called `TSParser`? I think it should just be `Parser`.",
        "Didn’t the default export used to be the Parser class? Seems like the the type and the class should be the same thing, not two different imports, similar to Node, Tree, etc.",
        "I still don’t get what Parser is in this code. In that code you wrote, is Parser not identical to TSParser, since the helper just re-exports Parser?",
        "Reading that, it looks like Parser and TSParser are just two different local names for the same class. Maybe I’m missing something very subtle though?",
        "I think the default export should just be the Parser class, which can be used as a type and also at runtime. I think that’s usually how classes are in Typescript.\r\n\r\nand we can eliminate the separate type declarations file I think",
        "I think it’s fine if it’s not all part of the Parser namespace anymore. Maybe we stop having a default export, and just export all of the classes from the top-level module?",
        "Sorry, I didn’t mean to convey that I was attached to the default export in particular, I care more about the classes (including Parser) having a single way to import them, that works for both for type declarations and for runtime.",
        "The `captures` and `matches` methods return arrays with different element types. I would call one of them `QueryCapture`, and one of them `QueryMatch`.\r\n\r\nA `QueryCapture` [has a `node` and a `name`](https://github.com/tree-sitter/tree-sitter/blob/99cd283e39d8dfb766fb365262fd08a419dd20a2/lib/binding_web/binding.js#L1044). A `QueryMatch` (as you wrote), has a `pattern` index, and a `captures` array, whose elements are `QueryCapture` objects."
      ],
      "tree-sitter-optimize-memory-usage-patterns": [
        "Optimization thoughts -\r\n\r\nCurrently, we allocate and free one of these lists on each sub-pattern of a query that contains a capture anywhere inside of it. And the length of the list is proportional to the numeric value of the largest capture id, so if a query has 50 captures, we'll allocate a 50-element list for any sub pattern *containing* the 50th capture.\r\n\r\nFor that reason, I wonder if it's worth making this `Array(uint8_t)`, and casting to `TSQuantifier` when reading from the array, so that we'll only use one byte per member. Currently, I think it'll represent `TSQuantifier` as an `int` (so 4 bytes).\r\n\r\nIt's a micro-optimization, but a pretty easy one to do. Thoughts?\r\n\r\n"
      ]
    },
    "profile": {
      "location": "Portland, OR",
      "blog": "",
      "site_admin": false,
      "followers": 1761,
      "following": 8
    }
  },
  "hiltontj": {
    "repos": [
      "influxdata/influxdb"
    ],
    "entries": [
      {
        "slug": "influxdb-avoid-flaky-test-patterns",
        "title": "Avoid flaky test patterns"
      },
      {
        "slug": "influxdb-centralize-workspace-configurations",
        "title": "Centralize workspace configurations"
      },
      {
        "slug": "influxdb-choose-appropriate-lock-primitives",
        "title": "Choose appropriate lock primitives"
      },
      {
        "slug": "influxdb-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "influxdb-clear-configuration-parameters",
        "title": "Clear configuration parameters"
      },
      {
        "slug": "influxdb-descriptive-semantic-naming",
        "title": "Descriptive semantic naming"
      },
      {
        "slug": "influxdb-document-complete-data-flows",
        "title": "Document complete data flows"
      },
      {
        "slug": "influxdb-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "influxdb-handle-errors-by-criticality",
        "title": "Handle errors by criticality"
      },
      {
        "slug": "influxdb-manage-complete-cache-lifecycle",
        "title": "Manage complete cache lifecycle"
      },
      {
        "slug": "influxdb-minimize-critical-path-allocations",
        "title": "Minimize critical path allocations"
      },
      {
        "slug": "influxdb-performance-conscious-metrics-implementation",
        "title": "Performance-conscious metrics implementation"
      },
      {
        "slug": "influxdb-prefer-explicit-nullability",
        "title": "Prefer explicit nullability"
      },
      {
        "slug": "influxdb-promote-code-clarity",
        "title": "Promote code clarity"
      },
      {
        "slug": "influxdb-secure-token-lifecycle",
        "title": "Secure token lifecycle"
      },
      {
        "slug": "influxdb-stable-schema-identifiers",
        "title": "Stable schema identifiers"
      },
      {
        "slug": "influxdb-use-structured-logging-fields",
        "title": "Use structured logging fields"
      }
    ],
    "comments": {
      "influxdb-stable-schema-identifiers": [
        "I don't know if using `enumerate` to determine the column ID is a good idea. I think that generally, columns are always appended, in which case, it is okay, but in the event that we allow for dropping columns, then this would change their order and mess up the IDs.\r\n\r\nWe probably need some way to generate the IDs, based on what was the largest already used ID for a given table, and then ensure that that ID remains fixed for the column it is applied to for all time.",
        "This could be done by flipping the hashmap to\r\n```rust\r\nHashMap<TableId, (Arc<str>, TableChunks)>\r\n```\r\nSerde would then `Serialize`/`Deserialize` it gracefully using derive.\r\n\r\nNot sure how gracefully that fits in to the broader change set but it would certainly be nice to not need the custom serialization code.\r\n\r\nIf you need to have both the name and ID in the hash key then you could define a new-type, e.g.,\r\n```rust\r\n#[derive(/* ... */, Serialize, Deserialize)]\r\nstruct WriteBatch {\r\n    /* ... */\r\n    table_chunks: HashMap<TableKey, TableChunks>,\r\n}\r\n\r\nstruct TableKey(Arc<str>, TableId);\r\n```\r\nThen, implement `Serialize`/`Deserialize` on `TableKey`, vs. having to do it for the whole `WriteBatch` type.",
        "JSON doesn't have a tuple, but `serde_json` will serialize/deserialize tuples as lists (see [here](https://docs.rs/serde_json/latest/src/serde_json/ser.rs.html#303-305)). I think part of the issue would be using `TableId` as the key in the map, since JSON doesn't support integer map keys. For that we would either need to de/serialize `TableId`s as strings, or not use JSON.",
        "Should this take the next available table ID, instead of using 0?",
        "(the same would be said for the other parsing function)",
        "Ah, I see, the code in validator is a bit confusing right now. For example, it used to update the in-memory catalog schema using a `Cow` to check that it had changes which is why this is here: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L514-L521\r\n\r\nBut it doesn't look like it uses the cow anymore. It does still update the catalog in its state here, though, by applying the catalog batch: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L182-L196\r\n\r\nAnd I believe that will result in the table being created, so I think it should probably be using the next ID instead of 0. Perhaps a test would help suss that out.",
        "Need to handle for collisions here, i.e., to ensure there are no duplicate fields, and also check for ordering. I don't think that race conditions are a concern since this method is invoked from a single event loop, and is processing rows from the buffer sequentially.\r\n\r\nA good test would be to have a cache on the table `foo` with keys `[t1]` and values `[f1, time]`, and then write the following LP that would add `f2` and `f3` value columns:\r\n```\r\nfoo,t1=a f1=1,f2=2,f3=3\r\nfoo,t1=b f1=1,f3=3,f2=2\r\n```\r\nAssuming that `t1=a` and `t1=b` have already been written to, and therefore each have a cache associated; each cache will have the new value columns `f2` and `f3` added, but they will be added in a different order to each respective cache:\r\n```\r\nt1=a -> f2,f3\r\nt1=b -> f3,f2\r\n```\r\nThen check that `RecordBatch`es spanning both `t1` key values can be produced and combined.",
        "Something worth noting is that the write buffer is validating the incoming writes, so we shouldn't have to worry about new fields being written with incompatible types in subsequent lines of LP. I do think ensuring that ordering discrepancies like above should be handled.",
        "https://github.com/influxdata/influxdb/pull/25125/commits/a129d003397bbb859a112980aee3de6286d1adcf switched to using `SchemaBuilder::try_merge` to prevent conflicts, and with the added test case, the above scenario will not cause issues.\r\n\r\n_Edit_: that commit did not produce the correct behaviour 🤦",
        "https://github.com/influxdata/influxdb/pull/25125/commits/2917fc149766ac349a4cb297f86ea7dfb6395797 Looks to have resolved this."
      ],
      "influxdb-manage-complete-cache-lifecycle": [
        "This test was removed because cache creation no longer behaves the same as before - previously if you made the same request to create a cache twice, the second one would succeed with a `204 NO CONTENT`, but have no effect; now, the second one fails with a `409 CONFLICT`.",
        "Yeah, only if it needs to prune would lock the inner map, but I agree, this is a little over-zealous.",
        "Each of these functions acquires their own write lock. Originally, they were called from separate places, but if they always get called at the same time like this, it might be better to do the evict and write in the same call, under the same write lock.",
        "> I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.\r\n\r\nThat sounds reasonable.",
        "Yes, good call, this only removes the values and is not walking up and cleaning up the maps. I'll address that with the other immediate issues in a follow-on PR."
      ],
      "influxdb-centralize-workspace-configurations": [
        "Might be good to keep the feature set at the workspace level, then don't need to set it in each individual cargo file.",
        "To clarify, it was previously set in the workspace `Cargo.toml`. Having it there could save someone a few head scratches if they pull one of the `v3` featured crates and find things broken, and I'm not sure we would ever have a crate that depends on one of these and does not use the feature."
      ],
      "influxdb-use-structured-logging-fields": [
        "It would be useful to log the `db_name` and `duration` provided.",
        "It would be useful to log the `db_name`",
        "```suggestion\r\n                info!(instance_id = ?instance_id, \"catalog not found, creating new instance id\");\r\n```\r\nJust encouraging use of `tracing`'s field syntax.\r\n\r\nThis is somewhat redundant with the `info!` emitted at the caller level, but I don't think it hurts to have.",
        "This will show up in the logs as `e=<error message>`, I would either format it inline or use a more descriptive name, e.g.,\r\n```rust\r\nerror!(error = %e, \"...\");\r\n```\r\nOr rename the `e` to `error`, such that it appears as `error=<error message>` in the logs."
      ],
      "influxdb-secure-token-lifecycle": [
        "Is it possible to assert that the new token is assigned a new unique `TokenId` from the previous deleted token? The catalog repository type should handle this but having a check in place would be good.\r\n\r\nWe also do hard deletion in other places (LVC, DVC, and triggers) and may not have such a check in place, but I think it is worth adding for tokens here if possible, given sensitive nature.",
        "Does this work because the `_admin` token has been deleted?\r\n\r\ni.e., normally this request to create the token would require a valid auth token, but the `_admin` token has been deleted, and there are no tokens to authenticate, so the create token request is allowed.",
        "Ah, right, that makes sense. Thanks for clarifying."
      ],
      "influxdb-avoid-flaky-test-patterns": [
        "Yeah, `insta` is compelling but not the  right tool here. I think @waynr was grappling with a similar issue in clustered so I may pick his brain to see if he landed on a solution.",
        "I opened https://github.com/influxdata/influxdb/issues/25493"
      ],
      "influxdb-document-complete-data-flows": [
        "A useful addition to this diagram would be to show the entry point for writes from user, i.e., where do writes go from the user (`wal buffer`?), via an arrow. Otherwise, it is not clear on the order of operations. If you could connect the numbers from the steps described below to locations / arrows on the diagram, that would be helpful."
      ],
      "influxdb-clear-configuration-parameters": [
        "The default should already have been 1 day: https://github.com/influxdata/influxdb/blob/e4cfbf71f78ce44072ec48f41b83721af6d21799/influxdb3_catalog/src/log/versions/v2.rs#L568\r\n\r\nI guess this is just for the purpose of documentation? Since, the coded default doesn't appear in the CLI output anywhere."
      ],
      "influxdb-performance-conscious-metrics-implementation": [
        "This will likely be an issue, since the HTTP endpoint that serves prometheus (`/metrics`) assumes a single registry: https://github.com/influxdata/influxdb/blob/be25c6f52b046e57ec909b815e5471d4c6bb4f19/influxdb3_server/src/http.rs#L734-L740\r\n\r\nIs the issue that using the same registry for multiple executors causes them to overwrite each other, or contend for locks with each other?",
        "Opened https://github.com/influxdata/influxdb/issues/25696"
      ],
      "influxdb-minimize-critical-path-allocations": [
        "Since this gets called in the write path for each line, might be worth returning a `Vec<&str>` or `Vec<Arc<str>>` to avoid the string copies. Or even a slice, if possible.\r\n\r\nFurthermore, having to do the lookup by ID for the name every time could also be avoided by holding the `Arc<str>` names around in the `TableDefinition` then just iterating over those directly.\r\n\r\nDepending on how far you take it, this could lead to a substantial change if you had to change the `TableDefinition` struct, so might be better for a follow-on if that's the case.",
        "This function no longer builds a new arrow schema on _every_ call. As a result, the arrow schema will only ever need to be rebuilt when new fields are added for caches that accept new fields, _or_ never for caches that have an explicit set of value columns. Furthermore, for the explicit case, it produces the value columns by iterating directly on the cache `IndexMap`, instead of iterating over the schema. The non-explicit case still needs to iterate over the schema and do a lookup to get column ID. Therefore, I suspect the explicit case will be considerably more performant.",
        "Changing this to use a `HashMap` instead of `BTreeMap` for faster lookups."
      ],
      "influxdb-prefer-explicit-nullability": [
        "This is a textbook use case for [`NonZeroUsize`](https://doc.rust-lang.org/std/num/type.NonZeroUsize.html), i.e.,\r\n```rust\r\n/// Must be greater than 0\r\n#[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\r\npub struct LastCacheSize(pub(crate) NonZeroUsize);",
        "No worries. We can refactor this later",
        "When I first refactored this, I had the `LastCacheKey` hold a `HashMap<Option<KeyValue>, LastCacheState>` to handle null key values, but switched to this behaviour to simplify it.\r\n\r\nI think It just needs to do that, i.e., use `Option<KeyValue>` instead of `KeyValue`, and then store the datatype in the `LastCacheKey`, because 1) key columns will always have a fixed data type, and 2) you can't rely on the `KeyValue` alone to get the data type when it is `None` (for creating `RecordBatch`es).\r\n\r\nI can open up an issue for this.",
        "The [`is_valid` function](https://docs.rs/arrow/latest/arrow/array/trait.Array.html#method.is_valid) indicates whether the value at given index is null or not - the data is still _valid_, if that makes sense. So, instead of `bail!`ing here, I think you can just `continue;` in the inner loop, leaving the cell as `Value::Null`."
      ],
      "influxdb-choose-optimal-data-structures": [
        "Using [`indexmap`](https://github.com/indexmap-rs/indexmap) makes this assertion (and the use of `insta` snapshots for serialization tests) possible.",
        "Maybe add a note on why `IndexSet` was used, i.e., for fast lookup in addition to iteration to the doc comment.",
        "Since I insert columns into the cache using the ordering of fields in the `schema` ([here](https://github.com/influxdata/influxdb/pull/25109/files#diff-6740f1021d631fa570625b9b27f8b4692fd0777c50eabcc435cda7793d0da049R155-R159)), then when producing a record batch out of the cache ([here](https://github.com/influxdata/influxdb/pull/25109/files#diff-6740f1021d631fa570625b9b27f8b4692fd0777c50eabcc435cda7793d0da049R184-R193)), `IndexMap` allows to iterate over the map directly while producing the correct order of columns for the schema.",
        "The links in that comment look to be out-of-date. But I have added docs/comments in the code to help explain this.",
        "@pauldix - while working on https://github.com/influxdata/influxdb/pull/25125 to enable caches that add new fields, I have found that the insertion order guarantee falls apart in scenarios where writes come in with different fields/orders for different key values. So, we can't rely on that, and I will be removing the comments about insertion order.\r\n\r\nI still like the use of `IndexMap` because it gives fast iteration over keys, as well as fast lookups (see [here](https://github.com/indexmap-rs/indexmap?tab=readme-ov-file#performance)), but if we want to avoid the dependency, or just optimize for lookup speed, then we could use a `HashMap` here.",
        "I guess you avoid the `entry` API on `buffered_data.database_buffers` here in order to avoid cloning `db_name` on every access?\r\n\r\nWe could consider using the `hashbrown` crate for its `HashMap` impl (which gets used here and there in IOx) and has the [`entry_ref` API](https://docs.rs/hashbrown/latest/hashbrown/struct.HashMap.html#method.entry_ref). I think there are a few places in this code where we could leverage that - I think introducing `hashbrown` can be a separate issue/PR though."
      ],
      "influxdb-promote-code-clarity": [
        "Could you move this code into a helper function since it is re-used in several places.",
        "Good call, I think I could do this by passing forward the `Arc<TableDefinition>` to the `MetaCacheExec` so it only pulls the column names when needed, e.g., for the `EXPLAIN` output. The table definition is already there so should not be too tricky, and yeah - given that part of the motivation for this whole execution plan implementation is to make it readable to the operator so I will get this in.",
        "https://github.com/influxdata/influxdb/issues/25582",
        "Yep, that works! I will apply it locally.",
        "Done in https://github.com/influxdata/influxdb/pull/25389/commits/71daada9884066ec8a2f7b2a1ac33d90fbf4cd99",
        "I moved out the system table related code to its own module in https://github.com/influxdata/influxdb/pull/25166/commits/7c1f4db1eea45f8b12cc6a013260fc668c658fc9 - 🧹 definitely cleaner.\r\n\r\nNo code was changed and CI is ✅ so I will get this merged."
      ],
      "influxdb-handle-errors-by-criticality": [
        "These unwraps can probably be changed to errors, but if any of these fails, it means that there is a race condition, so it might be that panicking is the right thing.",
        "FWIW - I think at most it would be `warn!`, since telemetry not sending is not a critical, _wake up your engineers in the middle of the night_ kind of error 😆 \r\n\r\nI have been fairly loose with the use of `error!` myself, which probably needs to be revisited."
      ],
      "influxdb-choose-appropriate-lock-primitives": [
        "I agree, I opened https://github.com/influxdata/influxdb/issues/25382 to look for alternatives.\r\n\r\nI think most straight-up LRU implementations will have this problem, given that they need to update the recency when getting an item (the popular [`lru` crate](https://crates.io/crates/lru/) that `clru` is based on is essentially the same API).",
        "Addressed in https://github.com/influxdata/influxdb/pull/25377/commits/ac8d7d3ba9ce25cf0dd04b4471e955e8ae4e1790",
        "Might be better to do the remove in `set_success` directly, so that it does it all under one lock.",
        "DashMap could definitely be useful here. I actually was using it at one point, so maybe I will re-introduce. I want to read more about how it works though.",
        "I switched over to using `DashMap` which hopefully will help reduce some lock contention.",
        "I changed it so that there is only one top-level lock, and now it is a three-level hashmap, so that it also stores multiple caches per table."
      ],
      "influxdb-descriptive-semantic-naming": [
        "Addressed this in https://github.com/influxdata/influxdb/pull/25722/commits/cd51bc2beda9a23446d694ee411e409871b1de39"
      ],
      "influxdb-follow-api-conventions": [
        "FWIW there is a [`Time::checked_sub` API](https://github.com/influxdata/influxdb3_core/blob/fd0e474a6c0af5ba867399d753f5df18f59907cb/iox_time/src/lib.rs#L149-L155) that you could use here to directly subtract the retention period `Duration` from `self.time_provider.now()`.",
        "Might be a case worthy of [`str::rsplit_once`](https://doc.rust-lang.org/std/primitive.str.html#method.rsplit_once) so that for, e.g., `foo:bar:apiv3_...`, it will only take `apiv3_...`.",
        "Several types are duplicated in this crate, e.g., `Format`, from the `influxdb3_server` crate. Perhaps we should pull `influxdb3_server` as a dependency of `influxdb3_client` and re-use them directly, or move the types to a central crate.\r\n\r\nBut this has bit us a couple times now.",
        "https://github.com/influxdata/influxdb/issues/24672",
        "With `reqwest`, you can use the [`query` API](https://docs.rs/reqwest/latest/reqwest/struct.RequestBuilder.html#method.query) to set the param, e.g.,\r\n```suggestion\r\n    /// Make a request to the `DELETE /api/v3/configure/database?db=foo` API\r\n    pub async fn api_v3_configure_db_delete(&self, db: impl AsRef<str> + Send) -> Result<()> {\r\n        let api_path = \"/api/v3/configure/database\";\r\n\r\n        let mut url = self.base_url.join(api_path)?;\r\n\r\n        let mut req = self.http_client.delete(url).query(&[(\"db\", db.as_ref())]);\r\n```"
      ]
    },
    "profile": {
      "location": "Canada",
      "blog": "trevorjhilton.com",
      "site_admin": false,
      "followers": 10,
      "following": 6
    }
  },
  "zanieb": {
    "repos": [
      "astral-sh/ty",
      "astral-sh/uv"
    ],
    "entries": [
      {
        "slug": "ty-configuration-documentation-clarity",
        "title": "Configuration documentation clarity"
      },
      {
        "slug": "ty-structure-documentation-logically",
        "title": "Structure documentation logically"
      },
      {
        "slug": "uv-balance-test-performance-considerations",
        "title": "Balance test performance considerations"
      },
      {
        "slug": "uv-clear-precise-documentation",
        "title": "Clear precise documentation"
      },
      {
        "slug": "uv-consistent-authentication-patterns",
        "title": "Consistent authentication patterns"
      },
      {
        "slug": "uv-declarative-constraints-over-runtime",
        "title": "Declarative constraints over runtime"
      },
      {
        "slug": "uv-enforce-strong-optional-types",
        "title": "Enforce strong optional types"
      },
      {
        "slug": "uv-environment-variable-best-practices",
        "title": "Environment variable best practices"
      },
      {
        "slug": "uv-make-errors-user-actionable",
        "title": "Make errors user actionable"
      },
      {
        "slug": "uv-names-should-be-descriptive",
        "title": "Names should be descriptive"
      },
      {
        "slug": "uv-optimize-cache-sharing-strategies",
        "title": "Optimize cache sharing strategies"
      },
      {
        "slug": "uv-optimize-cicd-commands",
        "title": "Optimize CI/CD commands"
      },
      {
        "slug": "uv-secure-configuration-defaults",
        "title": "Secure configuration defaults"
      },
      {
        "slug": "uv-short-circuit-evaluation-strategies",
        "title": "Short-circuit evaluation strategies"
      },
      {
        "slug": "uv-structure-for-readability",
        "title": "Structure for readability"
      },
      {
        "slug": "uv-test-deployment-edge-cases",
        "title": "Test deployment edge cases"
      },
      {
        "slug": "uv-use-direct-documentation-style",
        "title": "Use direct documentation style"
      }
    ],
    "comments": {
      "uv-enforce-strong-optional-types": [
        "`PythonMinorVersionLink::from_installation` will already return `None` on alternative implementations, and we can use `bool::then` to avoid repeating the default case here.\r\n\r\nSee https://github.com/astral-sh/uv/pull/13980"
      ],
      "uv-consistent-authentication-patterns": [
        "Does this work with `UV_PUBLISH_TOKEN`? Do they just ignore the username?",
        "Ah I see this is mentioned above. Maybe we can elevate that? https://github.com/astral-sh/uv/pull/14253/files#r2175124341",
        "```suggestion\r\n!!! important\r\n\r\n    If you use `--token \"$JFROG_TOKEN\"` or `UV_PUBLISH_TOKEN` with JFrog, you will receive a\r\n    401 Unauthorized error as JFrog requires an empty username but uv passes `__token__` for as\r\n    the username when `--token` is used.\r\n```"
      ],
      "uv-balance-test-performance-considerations": [
        "Given this test won't really change a lot, should we just check if commit info is available and have two snapshots? Using filters this way makes me a bit uncomfortable.",
        "They're just annoying during snapshot updates, so I try to avoid them in most cases.",
        "but if the filter is replacing basically the entire expected output, I'd either just skip the test or have a conditional.",
        "Yes, but we should squat or advocate for those names to be banned.",
        "It's just as bad, really. We try to make them reproducible wherever possible. That's mostly w.r.t. the external world though, internal changes causing snapshot changes are fine.\r\n\r\nHowever, I think this specific case is fine.",
        "You can use `.assert().success();` instead to avoid an empty snapshot.\r\n\r\nWe should test `python find --project` _before_ creating the virtual environment. In that case, we should ensure we're respecting the `requires-python` range from the child `pyproject.toml`.\r\n\r\nThen, we should test with a virtual environment.\r\n\r\nThen, we should also test we respect the `--project` root for `.python-version` file discovery too."
      ],
      "uv-environment-variable-best-practices": [
        "Thanks Ed!",
        "Similar to https://github.com/astral-sh/uv/pull/12246/files#r1999551644\r\n\r\n```suggestion\r\n    /// Only use uv-managed Python distributions.\r\n```\r\n\r\n(Sort of on the fence about \"installations\" vs \"distributions\" here but don't want users to be confused about whether we are \"installing\" here)\r\n\r\n",
        "I guess \"versions\" is actually the most consistent with other language?",
        "I wonder if we should also mention that this is equivalent to the `python-preference = \"only-managed\"` setting? I'm not sure. We might want to reference `python-preference` though unless we intend to later add a `managed-python = true | false` option to the settings to replace `python-preference`?",
        "I also think a `UV_MANAGED_PYTHON=1|0` variable might makes sense for these new options. It seems easier to use than `UV_PYTHON_PREFERENCE`.",
        "> It would be nice to use the same language consistently.\r\n\r\nYeah, but there are some places where it might be clearer to refer to installations or distributions. I think we should probably use \"versions\" when it's not confusing. It'd be good to audit other usages too, I'm sure there are more cases where we could switch to \"versions\".\r\n\r\nI think I'd prefer updating the ones here then opening an issue to standardize the language.\r\n\r\n> I'm hesitant to reference python-preference because the user might then try to learn two different ways to do the same thing \r\n\r\nSince there's not a configuration file setting for `managed-python`, I think it makes sense to explain how it's related? I wouldn't suggest the `--python-preference` CLI option.\r\n\r\n> I considered an env var, but the problem is we have a third option as a default.\r\n\r\nThinking out loud here...\r\n\r\nI assumed `UV_MANAGED_PYTHON=1` means `--managed-python` and `UV_MANAGED_PYTHON=0` means `--no-managed-python` while unset means default.\r\n\r\nLooking at some of our existing patterns, we do use `UV_NO_...` sometimes. I think most of them don't have an equivalent \"yes\" flag though. I might be okay with `UV_MANAGED_PYTHON=1` means `--managed-python` and `UV_NO_MANAGED_PYTHON=1` means `--no-managed-python`. I think `UV_MANAGED_PYTHON=0` not working would be surprising. That's how other boolean flags work today though, e.g.:\r\n\r\n```\r\n❯ UV_SYSTEM_PYTHON=0 uv python find\r\n/opt/homebrew/opt/python@3.13/bin/python3.13\r\n```\r\n\r\nbut as you said, that's for an option that's truly binary.\r\n\r\nI think I've loosely talked myself into using `UV_NO_MANAGED_PYTHON` / `UV_MANAGED_PYTHON` :)\r\n\r\n\r\n ",
        "```suggestion\r\n    /// Disable GitHub-specific requests that allow uv to skip `git fetch` in some circumstances.\r\n```"
      ],
      "ty-configuration-documentation-clarity": [
        "I'm hesitant to go into more detail here, it seems like a bit of a distraction and \"or similar\" is pretty load bearing. What's they key point we're trying to establish here?",
        "Haha see, I think the point is to provide that sort of tautological \"third-party modules is just a fancy word for dependencies\" statement since we're explaining to a beginner what we mean by this concept. I expect people to know what a \"dependency\" is. I can play with this language though...",
        "```suggestion\r\nThese are usually declared as dependencies in a `pyproject.toml` or `requirements.txt` file\r\nand installed using a package manager like uv or pip. Examples of popular third-party\r\nmodules are `requests`, `numpy` and `django`.\r\n```",
        "That's sort of not the target audience for _this_ documentation, but anyway I'm fine with changing it."
      ],
      "uv-short-circuit-evaluation-strategies": [
        "I would expect `@latest` to scan _all_ Python interpreters and select the latest version, whereas `@any` would stop on the first interpreter.\r\n",
        "`@any` isn't sorted by version during discovery though, because we short-circuit when we find an interpreter that meets the request. Sorting only makes a difference when selecting a Python download or iterating over managed Python installations."
      ],
      "uv-structure-for-readability": [
        "A dedicated `StandaloneIntepreter` type might be helpful to keep the logic about the `executable` to use in the `uv-python` crate."
      ],
      "ty-structure-documentation-logically": [
        "I think the existing structure of covering the default behavior (which is what most people will use) makes sense before discussing the fact that you can override that behavior? I am generally hesitant to cover the explicit options first here.\r\n\r\n> I think it's also important to state that the VIRTUAL_ENV variable will be implicitly set:\r\n\r\nThis makes sense to me.",
        "I don't love it :D ",
        "I'm trying to rephrase it for you, but this pattern is used in each section haha",
        "I think if we wanted them to be used by most users, I'd list them first, but the idea is that you don't need the setting.",
        "In the same way that we don't lead with \"here's how to use `src.root`\"",
        "I pushed a tweak, maybe it's a bit better?\r\n\r\nI'm trying to use the same language as the rest of the sections, e.g., \"By default, ...\" ",
        "Thanks for your lenience :) haha"
      ],
      "uv-names-should-be-descriptive": [
        "Nit: We use `s` as the name pretty consistently in this repository, it'd be nice to retain the style there.",
        "I think it's common for us to just write `from(...) -> Option<...>`, I don't think you need to encode the `maybe` in the name when the type system captures that.",
        "Agree that's a little confusing, but we do it pretty often and I'd prioritize consistency with the rest of the codebase.",
        "I'm loosely opposed to changing this, because we use this argument name all over the place. We do use `python_request` in some places? which you could use since you'll just shadow it later? I don't have strong feelings. It seems like this does improve readability here, but may be confusing if you're coming from elsewhere.",
        "Nit: Generally we don't shorten variable names, I'd just use `tool_request` or `tool_python_request`.",
        "Can you rename this variable to match, e.g., `with_requirements`?",
        "The name isn't great... open to suggestions.",
        "I like that",
        "Nit: Similar to https://github.com/astral-sh/uv/pull/12651/files#r2027441564, it seems fine to drop the `auth_` prefix here since these are the only indexes which exist in this context.",
        "(I think this applies broadly)"
      ],
      "uv-make-errors-user-actionable": [
        "Huh? Why is there a reference to a workspace at all here? This doesn't seem quite right, and seems bad enough that we need to improve it here because we're recommending installing a workspace member?",
        "Yeah it seems like we may want to drop that and open an issue to follow up on it?\r\n\r\n> In general even a single package is a workspace\r\n\r\nEven if this is true internally, we shouldn't leak that to the user. Most users don't need or use workspaces.",
        "Discussion on inclusion of the wheel filename is happening at https://github.com/astral-sh/uv/pull/13437#discussion_r2132744173",
        "👍 I'm happy to hand this off to you, unless you want me to write the version display.",
        "Why did this one change? There's no policy set here.",
        "I don't think we can throw an error there though, doesn't downstream logic rely on a successful request? (i.e., in https://github.com/astral-sh/uv/pull/12667#discussion_r2029115491)\r\n\r\nPerhaps I misunderstood the intent of this PR?",
        "Ah I understand the goal now, you want to make the case where no credentials were present distinct from the case where the credentials are wrong. Sorry it took me a while to get there. I think the snapshot changes should be like..\r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to a lack of valid authentication credentials (401 Unauthorized).\r\n\r\nto \r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to missing credentials (401 Unauthorized)\r\n\r\nand\r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to invalid credentials (401 Unauthorized)\r\n\r\nI think changing that error path entirely feels too risky and is hopefully unnecessary. \r\n",
        "(fwiw, I think this would have been clearer if the title was something like \"Distinguish between authentication failures due to missing vs invalid credentials\")",
        "Should this be \"Failed to determine the libc used on the current platform\"? This message seems too generic for a libc-detection specific problem.",
        "As structured, it needs to be present here or it's too broad / inaccurate. I also don't expect devs to know about libc flavors, but there should be an error above this explaining why we're looking for it. I'm also fine with it being structured differently.",
        "I think this is too specific? This can be raised for any managed Python error.\r\n\r\n```suggestion\r\n    #[error(\"Failed to discover managed Python installations\")]\r\n```\r\n\r\nIt seems misleading to suggest it's specifically related to \"matching the current platform\"."
      ],
      "uv-use-direct-documentation-style": [
        "We don't say \"simply\": https://github.com/astral-sh/uv/blob/main/STYLE.md#documentation",
        "Separately, do we want to recommend `uv sync`? The idea is you can just `uv run pytest` and start working. Maybe we want to recommend `uv sync` as a way to get things ready for an IDE?",
        "That makes sense. I think we might just want to reframe it a bit, I'll take a swing at the phrasing.",
        "Elsewhere, we stylize error codes as \"HTTP XXX\" instead of \"`XXX NAME`\". I do think the name is nice, I might do \"HTTP 403 Forbidden\" though? I'd probably omit the backticks, since you refer to the error codes in the subsequent text without them.\r\n\r\nYou can probably just say \"status code\" instead of \"response status code\".\r\n\r\nGenerally, I'd avoid using \"it\" to refer to uv if it's easy. So... \"uv will stop searching if a ... is encountered\" rather than \"uv will stop searching if it encounters ...\".\r\n\r\n",
        "We avoid referring to \"Users\" like this in the documentation. Instead, I'd say \"You can configure which error codes...\" or, more typically, \"To ignore additional error codes for an index, use the `...` setting.\""
      ],
      "uv-declarative-constraints-over-runtime": [
        "Why not just store `StatusCode` directly instead of the u16?",
        "Why does it need to implement `Serialize`?",
        "Stepping up a level, why is `Index` serializable? For writing it to the TOML file? If that's it, why _would_ we need this field to be serializable? A user can't construct it via the CLI. If truly necessary, why not use a wrapper type to implement serializability?",
        "It seems nice for type safety throughout and straightforward to implement? I don't feel super strongly, but the current approach of validating at the CLI level, but retaining the u16, then revalidating later seems quite weird."
      ],
      "uv-optimize-cache-sharing-strategies": [
        "Is the parallel part particularly important here? Isn't it helpful that the cache is shared regardless of concurrency?"
      ],
      "uv-clear-precise-documentation": [
        "I'm not sure how we'll want to reframe the documentation here.\r\n\r\nMaybe we want something like..\r\n\r\n> Run with all packages listed in the given requirements files.\r\n>\r\n> The following formats are accepted:\r\n>\r\n> - `requirements.txt`-formatted files\r\n> - `pyproject.toml`\r\n> - `setup.cfg` and `setup.py`\r\n> - `.py` files with PEP 723 metadata\r\n\r\nI'm not sure if we want to enumerate those everywhere? Are they all supported everywhere?",
        "Looks outdated :) we can probably update those holistically separately if you prefer.",
        "During upgrade, this is the directory to look for installations in, right?",
        "Should we change the doc then?",
        "Perhaps \"The directory Python installations are stored in\"?",
        "I think this is kind of confusing as written\r\n\r\n```suggestion\r\n    /// During an upgrade, uv will not uninstall outdated patch versions.\r\n```",
        "This is my first instinct\r\n\r\n```\r\n/// The environment was checked and required no updates.\r\n/// The environment was updated.\r\n/// A new environment was created.\r\n```",
        "I think I agree these should all be past-tense? Dry-run fills in the blank \"The environment would be ___\"",
        "We might want to say \"additional\" here (as discussed elsewhere). If you want to update that language all at once, it can happen here or in a separate pull requests.",
        "I think it's also fine to just say \"requirements\" without the floating (s)",
        "```suggestion\r\n    /// Whether to display the additional requirements installed with each tool.\r\n```\r\n\r\n(will require generated reference update)",
        "I would re-phrase (I think we use \"Whether\" when we only show one of the options)\r\n\r\n```suggestion\r\n    /// Disable use of uv-managed Python distributions.\r\n```\r\nI think we could say a bit more here, too, like\r\n\r\n> Instead, uv will search for a suitable Python installation on the system.\r\n\r\n\r\n",
        "See https://github.com/astral-sh/uv/pull/12246/files#r1999556537 — maybe we want to say \"Python versions\".",
        "```suggestion\r\n    /// Only show the version\r\n    ///\r\n    /// By default, uv will show the project name before the version.\r\n```"
      ],
      "uv-secure-configuration-defaults": [
        "```suggestion\r\nTo use the uv build backend in an existing project, add it to the `[build-system]` section in your `pyproject.toml`:\r\n```"
      ],
      "uv-optimize-cicd-commands": [
        "This is a good example of the breakage we'd cause with this change. I wonder if we can craft a GitHub search that captures if a second `uv venv` is common?",
        "We could grab the JSON output, I think — but the `readarray` reads clearer to me.",
        "uhh maybe we should attest the DockerHub ones, I'm not sure how if it works tbh.",
        "I think I'd prefer to do that afterwards",
        "I'm not sure we _want_ incremental builds in the release pipeline. A clean build seems like a good property, right?"
      ],
      "uv-test-deployment-edge-cases": [
        "I'm wary of adding a new test suite for this file, it's a big increase in scope for this task.",
        "(I'll see how painful it is though, especially with https://github.com/astral-sh/uv/pull/14184#discussion_r2175069753 this gets complicated to trust without tests)"
      ]
    },
    "profile": {
      "location": "Minneapolis, MN",
      "company": "@astral-sh",
      "blog": "",
      "site_admin": false,
      "followers": 760,
      "following": 4
    }
  },
  "fisker": {
    "repos": [
      "prettier/prettier"
    ],
    "entries": [
      {
        "slug": "prettier-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "prettier-angular-syntax-parsing",
        "title": "Angular syntax parsing"
      },
      {
        "slug": "prettier-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "prettier-cache-correctness-validation",
        "title": "Cache correctness validation"
      },
      {
        "slug": "prettier-cache-invalidation-strategy",
        "title": "cache invalidation strategy"
      },
      {
        "slug": "prettier-consistent-spacing-patterns",
        "title": "consistent spacing patterns"
      },
      {
        "slug": "prettier-document-ci-workflow-rationale",
        "title": "Document CI workflow rationale"
      },
      {
        "slug": "prettier-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "prettier-documentation-example-consistency",
        "title": "Documentation example consistency"
      },
      {
        "slug": "prettier-ensure-semantic-naming-accuracy",
        "title": "Ensure semantic naming accuracy"
      },
      {
        "slug": "prettier-environment-specific-error-handling",
        "title": "Environment-specific error handling"
      },
      {
        "slug": "prettier-maintain-api-backward-compatibility",
        "title": "maintain API backward compatibility"
      },
      {
        "slug": "prettier-measure-performance-impacts",
        "title": "Measure performance impacts"
      },
      {
        "slug": "prettier-modern-configuration-formats",
        "title": "Modern configuration formats"
      },
      {
        "slug": "prettier-organize-tests-properly",
        "title": "Organize tests properly"
      },
      {
        "slug": "prettier-prefer-efficient-algorithms",
        "title": "prefer efficient algorithms"
      },
      {
        "slug": "prettier-refactor-complex-conditions",
        "title": "refactor complex conditions"
      },
      {
        "slug": "prettier-test-all-variations",
        "title": "Test all variations"
      },
      {
        "slug": "prettier-use-cross-platform-commands",
        "title": "Use cross-platform commands"
      },
      {
        "slug": "prettier-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "prettier-use-descriptive-variable-names",
        "title": "Use descriptive variable names"
      },
      {
        "slug": "prettier-use-example-configuration-files",
        "title": "Use example configuration files"
      },
      {
        "slug": "prettier-validate-configuration-changes",
        "title": "validate configuration changes"
      },
      {
        "slug": "prettier-validate-configuration-values",
        "title": "validate configuration values"
      },
      {
        "slug": "prettier-verify-optional-chaining-necessity",
        "title": "Verify optional chaining necessity"
      },
      {
        "slug": "prettier-vue-syntax-parsing-robustness",
        "title": "Vue syntax parsing robustness"
      }
    ],
    "comments": {
      "prettier-use-cross-platform-commands": [
        "Please use `fs.writeFileSync` to write file.\r\n\r\nhttps://github.com/prettier/prettier/pull/16000#discussion_r1477057609\r\n\r\nhttps://github.com/typicode/husky/issues/1380",
        "I forgot this doesn't work on Windows...\r\n\r\nLet's use \r\n\r\n```js\r\nnode --eval \"fs.writeFileSync('.husky/pre-commit','npx lint-staged\\n')\"\r\n```\r\n\r\nSee https://github.com/prettier/prettier/blob/c8ba8dbca18858a7962184bbb3898502b9ec7cfb/docs/install.md?plain=1#L31-L40"
      ],
      "prettier-documentation-clarity-standards": [
        "Don't know, but feel hard to understand \"text\" and \"code\". Maybe @kachkaev or someone else have a better idea?\r\n\r\nReminder: Please sync this https://github.com/prettier/prettier/blob/e9b8764fc4c770aaa8e5d9b982bda40a83a4cf2a/website/playground/markdown.js#L30 before merge.",
        "Maybe we should add an \"Expected output\" section?"
      ],
      "prettier-refactor-complex-conditions": [
        "Why don't we destruct `value` here. Maybe also return early if `value === '' || !value.startsWith('-')` then we can check `value.charAt(1)` instead.",
        "?? ",
        "Sorry, I thought it's a `if`, but better rewrite to `if/else`\r\n\r\n```js\r\nreturn (foo && \"next\")\r\n```\r\n\r\nseems wired to me.",
        "```js\r\n  if (prevSibling &&\r\n    prevSibling.type === \"JSXExpressionContainer\" &&\r\n    prevSibling.expression.type === \"JSXEmptyExpression\" &&\r\n    prevSibling.expression.comments &&\r\n    prevSibling.expression.comments.some(\r\n      (comment) => comment.value.trim() === \"prettier-ignore\"\r\n    )) {\r\n    return \"next\";\r\n  }\r\n\r\n  return false;\r\n```",
        "```suggestion\r\n      const forceHardLine =\r\n        (parentParentParentNode.type === \"css-decl\" ||\r\n          (parentParentParentNode.type === \"css-atrule\" &&\r\n            parentParentParentNode.variable)) &&\r\n        node.groups.some((node) => node.type === \"value-comma_group\");\r\n```\r\n\r\nSame?",
        "`filter` + `some` can reduce to one `.some`.",
        "This change is not related, but the logic can be simplified to use `Array#some()`."
      ],
      "prettier-api-documentation-clarity": [
        "```suggestion\r\nPreviously, `languages` API for custom plugin only supported to infer parser based on the file basename or extension.\r\n\r\nPrettier main added `isSupported: (file: string) => boolean` function to allow plugin check if file is supported based on the full path (eg: files in a specific directory), the `file` parameter could be a normal path or a url string like `file:///C:/test.txt`.\r\n```",
        "`smae` -> `same`",
        "I think this signals what the arguments this function will receive, I don't think we should add this, many plugins define the following functions/methods with less parameters, we didn't mark they are optional. I'm not familiar with  TS, teach me if there is a good explanation.",
        "`parse(test)` was added by me, but I forgot where it is... ",
        "Found, https://github.com/prettier/prettier/issues/11888, we don't really support `parse(text)`."
      ],
      "prettier-validate-configuration-changes": [
        "if we have a `tests/format/flow-repo/esproposal_decorators.ignore/foo/format.test.js`, will it be unignored?",
        "I remember that I did something strange in eslint too https://github.com/prettier/prettier/blob/1652973553677de297782643ca0263f6331eb0d3/eslint.config.js#L21-L25\r\n\r\nDoes it work for prettier?",
        "As I tested, use the same pattern as https://github.com/prettier/prettier/pull/17632#discussion_r2160047355 works too.",
        "I agree with @sosukesuzuki , since this rule autofixable, \"error\" should be used.",
        "I think current settings for `no-console` is fine too.\r\n\r\nStory behind it https://github.com/prettier/prettier/pull/10322#issuecomment-779277567",
        "Yes,\r\n\r\n```console\r\n>yarn why chalk\r\nyarn why v1.22.17\r\n[1/4] Why do we have the module \"chalk\"...?\r\n[2/4] Initialising dependency graph...\r\n[3/4] Finding dependency...\r\n[4/4] Calculating file sizes...\r\n=> Found \"chalk@5.0.0\"\r\ninfo Has been hoisted to \"chalk\"\r\ninfo This module exists because it's specified in \"dependencies\".\r\ninfo Disk size without dependencies: \"68KB\"\r\ninfo Disk size with unique dependencies: \"68KB\"\r\ninfo Disk size with transitive dependencies: \"68KB\"\r\ninfo Number of shared dependencies: 0\r\n=> Found \"eslint-formatter-friendly#chalk@2.4.2\"\r\ninfo This module exists because \"eslint-formatter-friendly\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"vnopts#chalk@2.4.2\"\r\ninfo This module exists because \"vnopts\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"npm-run-all#chalk@2.4.2\"\r\ninfo This module exists because \"npm-run-all\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"jest-diff#chalk@4.1.2\"\r\ninfo This module exists because \"jest-diff\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-message-util#chalk@4.1.2\"\r\ninfo This module exists because \"jest-message-util\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-snapshot#chalk@4.1.2\"\r\ninfo Reasons this module exists\r\n   - \"jest-snapshot\" depends on it\r\n   - Hoisted from \"jest-snapshot#jest-diff#chalk\"\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-util#chalk@4.1.2\"\r\ninfo This module exists because \"jest-util\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"babel-jest#chalk@4.1.2\"\r\ninfo This module exists because \"babel-jest\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"eslint#chalk@4.1.2\"\r\ninfo This module exists because \"eslint\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-watch-typeahead#chalk@4.1.2\"\r\ninfo This module exists because \"jest-watch-typeahead\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"node-actionlint#chalk@4.1.2\"\r\ninfo This module exists because \"node-actionlint\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"cspell#chalk@4.1.2\"\r\ninfo This module exists because \"cspell\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@babel/highlight#chalk@2.4.2\"\r\ninfo This module exists because \"@babel#code-frame#@babel#highlight\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"@jest/core#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/transform#chalk@4.1.2\"\r\ninfo This module exists because \"jest-snapshot#@jest#transform\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/types#chalk@4.1.2\"\r\ninfo This module exists because \"@jest#globals#@jest#types\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-circus#chalk@4.1.2\"\r\ninfo This module exists because \"@prettier#jest-light-runner#jest-circus\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-cli#chalk@4.1.2\"\r\ninfo This module exists because \"jest#jest-cli\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-each#chalk@4.1.2\"\r\ninfo This module exists because \"@prettier#jest-light-runner#jest-each\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-matcher-utils#chalk@4.1.2\"\r\ninfo Reasons this module exists\r\n   - \"@types#jest#jest-matcher-utils\" depends on it\r\n   - Hoisted from \"@types#jest#jest-matcher-utils#jest-diff#chalk\"\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"snapshot-diff#chalk@4.1.2\"\r\ninfo Reasons this module exists\r\n   - \"snapshot-diff#jest-snapshot\" depends on it\r\n   - Hoisted from \"snapshot-diff#jest-snapshot#chalk\"\r\n   - Hoisted from \"snapshot-diff#jest-snapshot#jest-message-util#chalk\"\r\n   - Hoisted from \"snapshot-diff#jest-snapshot#jest-util#chalk\"\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-watcher#chalk@4.1.2\"\r\ninfo This module exists because \"jest-watch-typeahead#jest-watcher\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/console#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#@jest#console\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/reporters#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#@jest#reporters\" depends on it.info Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-config#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-config\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-resolve#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-resolve\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-runner#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-runner\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-runtime#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-runtime\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-validate#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-validate\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-jasmine2#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-config#jest-jasmine2\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n```\r\n\r\n\"vnopts#chalk@2.4.2\" and \"@babel/highlight#chalk@2.4.2\" are removed in build script.\r\n\r\nhttps://github.com/prettier/prettier/pull/12162 https://github.com/prettier/prettier/pull/12182",
        "`yarn dedupe` not available in `main` branch. It uses classic yarn.",
        "But we should dedupe before merge."
      ],
      "prettier-documentation-example-consistency": [
        "```suggestion\r\nWhat is `yarn exec` doing at the start? `yarn exec prettier` runs the locally installed version of Prettier. We’ll leave off the `yarn exec` part for brevity throughout the rest of this file!\r\n```",
        "```suggestion\r\nTo run your locally installed version of Prettier, prefix the command with `npx`, `yarn exec`, `pnpm exec`, or `bun exec`, i.e. `npx prettier --help`, `yarn exec prettier --help`, `pnpm exec prettier --help`, or `bun exec prettier --help`.\r\n```",
        "Don't forget https://github.com/prettier/prettier/blob/main/website/versioned_docs/version-stable/cli.md",
        "````suggestion\r\n<!-- prettier-ignore -->\r\n```md\r\n<!-- Input (--prose-wrap=always) -->\r\nThis folder has [VHS] tape files to create gifs for the [Widget Showcase]. To run them, install VHS from main (the theme and screenshot commands are not yet released).\r\n\r\n<!-- Prettier stable -->\r\nThis folder has [VHS] tape files to create gifs for the [Widget Showcase]. To run\r\nthem, install VHS from main (the theme and screenshot commands are not yet released).\r\n\r\n<!-- Prettier main -->\r\nThis folder has [VHS] tape files to create gifs for the [Widget Showcase]. To\r\nrun them, install VHS from main (the theme and screenshot commands are not yet\r\nreleased).\r\n```\r\n````",
        "````suggestion\r\n#### Fix a bug that incorrectly strips of commas in some cases (#14476 by @seiyab)\r\n\r\n<!-- prettier-ignore -->\r\n```css\r\n/* Input */\r\n@font-face {\r\n  src: url(RobotoFlex-VariableFont_GRAD,XTRA,YOPQ,YTAS,YTDE,YTFI,YTLC,YTUC,opsz,slnt,wdth,wght.ttf);\r\n}\r\n\r\n/* Prettier stable */\r\n@font-face {\r\n  src: url(RobotoFlex-VariableFont_GRADXTRAYOPQYTASYTDEYTFIYTLCYTUCopszslntwdthwght.ttf);\r\n}\r\n\r\n/* Prettier main */\r\n@font-face {\r\n  src: url(RobotoFlex-VariableFont_GRAD,XTRA,YOPQ,YTAS,YTDE,YTFI,YTLC,YTUC,opsz,slnt,wdth,wght.ttf);\r\n}\r\n````",
        "```suggestion\r\n/** @type {import(\"prettier\").Options} */\r\n```"
      ],
      "prettier-use-descriptive-variable-names": [
        "What is `xs`? `array` should be a better name.\r\n\r\nMaybe also `chunkSize` -> `size`",
        "It's common to name as size.\r\n\r\nhttps://github.com/ryancole/chunk/blob/39631771da7e5d3eb8f43d6ebcd52905c02b46ed/src/chunk.js#L4\r\n\r\nhttps://github.com/lodash/lodash/blob/6a2cc1dfcf7634fea70d1bc5bd22db453df67b42/src/chunk.ts#L22",
        "Should we name it as something like `splitByContinuousWhitespace`?"
      ],
      "prettier-use-example-configuration-files": [
        "Good point, I'm going to rename it as `settings.example.json`, and add a `prepare` script to copy to `settings.json` if it's not already exist.",
        "Turns out we can't use an extra script file to copy the file, because this will run when `yarn install prettier@prettier/prettier`",
        "yarn>=2 don't run `prepare`.",
        "I don't understand how `enableConstraintschecks` can do this. But yarn plugin should be able to do it.",
        "I'm going to use yarn plugin instead. Since we don't want add `yarn.config.cjs` for this."
      ],
      "prettier-vue-syntax-parsing-robustness": [
        "I think this is really bad, we are walking big vue SFC just to find the root script.(I think `.walk()` can't quit).\r\nIf we want allow ts only if ts script found, I think we can found them from the root.\r\n\r\nAlso, I'm not sure if we really need this check, I like things be safe, but I think we can always pase it as ts? I don't use ts myself, I'm fine with both add or remove this check.",
        "Why this write in template original..",
        "I've fixed this case here.",
        "Added a test for your case https://github.com/prettier/prettier/pull/12113/commits/6bbf42bb043622b0e30b68c342dec0c5ae01acb6"
      ],
      "prettier-angular-syntax-parsing": [
        "```suggestion\r\n  angularLetDeclaration: [],\r\n```\r\n\r\n`name` and `value` are not `Node`, they are stings."
      ],
      "prettier-prefer-efficient-algorithms": [
        "I didn't take a look at the AST, but are we checking if there is a space between current node and next node? How about simply compare `locEnd(iNode) !== locStart(iNextNode)`? Will it work?",
        "We already have so many hacks, one more doesn't matter.\r\n\r\n`Infinity + 1 === Infinity`",
        "`return (a.loc.start.line - b.loc.start.line) || (a.loc.start.column - b.loc.start.column)`",
        "We also have `.offset` added in https://github.com/prettier/prettier/pull/9626, so `a.loc.start.offset - b.loc.start.offset` or `locStart(a) - locStart(b)` like js printer",
        "```suggestion\r\n  return getChildren(node, options).next().done;\r\n```",
        "path.siblings, path.next, path.index should be used",
        "Same here, maybe `getCallArgumentSelector(index)`, allow index to be negative value (-1)."
      ],
      "prettier-maintain-api-backward-compatibility": [
        "`printDocToString` is a public API, let's not change the signature.",
        "I don't think we can drop support for assertions, Node.js already support import json file with that syntax, so people already use that syntax. Correct me if I'm wrong.",
        "Agree, we should be able to format runnable code.",
        "```js\r\nconst property = isNonEmptyArray(node.attributes) ? \"attributes\" : isNonEmptyArray(node.assertions) ? \"assertions\" : undefined;\r\n\r\nif (!property) {\r\n // ...\r\n}\r\n\r\nconst keyword = property === \"assertions\" || node.extra?.deprecatedAssertSyntax ? \"assert\" : \"with\";\r\n\r\n// ...\r\n```"
      ],
      "prettier-validate-configuration-values": [
        "The value `0` is invalid, discussed in https://github.com/josephfrazier/editorconfig-to-prettier/issues/1 and decided to keep it, but [`tabWidth=0` crashes](https://github.com/prettier/prettier/issues/7388), I think better to ignore instead.",
        "```suggestion\r\n      \"[error] `--cache-strategy` cannot be used without `--cache`.\"\r\n```\r\n\r\nDon't miss https://github.com/prettier/prettier/pull/12800#discussion_r889630196",
        "1, they can be in different directories, they can't be simply concated. We need to know what the patterns related to.\r\n\r\n```\r\n#.ignore1\r\nfoo\r\n\r\n#dir/.ignore2\r\nbar\r\n```\r\n\r\n2. In this case\r\n\r\n```\r\n#.ignore1\r\nfoo/a.js\r\n\r\n#.ignore2\r\n!foo/*\r\nfoo/b.js\r\n```\r\n\r\nshould `foo/a.js` be ignored? I think user may expect it be ignored, because it's ignored in `.ignore1` file. But if we treat this case like\r\n\r\n```\r\n#.ignore\r\nfoo/a.js\r\n!foo/*\r\nfoo/b.js\r\n```\r\n\r\nI'm not sure what's expected.\r\n",
        "Where is this filepath come from? There is no `filepath` flag in CLI at all.",
        "`filepath` only exists when format stdin, when formatting files, it doesn't exist.",
        "See `formatFiles` function, it calls `createIgnorerFromContextOrDie` on first line. I don't think it will work."
      ],
      "prettier-consistent-spacing-patterns": [
        "This can't be a boolean, It should be `\"line-start\" | \"line-end\"` or maybe `\"start\" | \"end\"` ?\r\n\r\nOr maybe change name to `experimentalOperatorLinebreak`, like [`operator-linebreak`](https://eslint.org/docs/latest/rules/operator-linebreak) rule.\r\n\r\n@sosukesuzuki I forgot how we decided.",
        "I'm fine with current name.",
        "I thought this PR is only add indentation back?\r\n\r\nI don't like this.",
        "`.icon-#{$size}` above didn't add space around",
        "I mean space inside `#{}`, this one has space around `$name`, but `.icon-#{$size}` don't.",
        "Looks like we are keeping spaces on stable version.\r\n\r\n**Prettier 2.1.2**\r\n[Playground link](https://prettier.io/playground/#N4Igxg9gdgLgprEAuEBiYASAhgAj-gwonAXx2AB0oDIAbCAJyRwbgBMBuKvEqq9YoOLYylavjqNmrTt1L9gQvNiWlycvJKYt2XcbyjoR68Zoj1tMvTxAAaEBAAOMAJbQAzslBYGDCAHcABR8ETxQsWn8sAE9PewAjBiwwAGs4GABlR2SXKABzZBgGAFc4ezgAW3j2NnYAGSx84qw8uAAxRgqsGFd85BAsYpgIOxAACxgK2gB1MZd4d2ywOAzQ+ZcAN3no-rB3OJBc9zgGGECkvK7kADMI4-sAK3cADwAhJNT0jKwKuDrcuA3O5lEBPZ4ZXJ5WhwACKxQg8CBtHuIGyDGODH67j2B0cDFyMGmLjYMDGyAAHAAGex4iDHaZJRz9PFwDEbQH2ACO8Pg5ycYQG7gAtFA4Ox2KNWNyXKxzi0rkhbsiQccKi5CiUVZDoXCEYDFcD7DAsPEiSSyUgAExGpIuWiQgDCEAqCpArIArKNiscAComsJKlEbUoASSgtVgGTA+OcAEFwxkYNFoUjjiQSEA)\r\n```sh\r\n--parser scss\r\n```\r\n\r\n**Input:**\r\n```scss\r\n#{$a                      } {\r\n    color: red;\r\n  }\r\n\r\n#{                      $a} {\r\n    color: red;\r\n  }\r\n#{           $a           } {\r\n    color: red;\r\n  }\r\n#{$a} {\r\n    color: red;\r\n  }\r\n```\r\n\r\n**Output:**\r\n```scss\r\n#{$a } {\r\n  color: red;\r\n}\r\n\r\n#{ $a} {\r\n  color: red;\r\n}\r\n#{ $a } {\r\n  color: red;\r\n}\r\n#{$a} {\r\n  color: red;\r\n}\r\n\r\n```"
      ],
      "prettier-organize-tests-properly": [
        "Better idea to test?",
        "Happy to see a separate test for this.",
        "If it's rejected, the test will fail, why we need that?",
        "You are right, I was not thinking right, I thought Jest will exit if one test fails."
      ],
      "prettier-test-all-variations": [
        "[`0d3bae1` (#17679)](https://github.com/prettier/prettier/pull/17679/commits/0d3bae1b2ad5b46b07af65d93b6bb279324021a0)",
        "I think we need more tests for different `--quote-props` options."
      ],
      "prettier-use-descriptive-names": [
        "Let's rename the first parameter to `text`, this function doesn't care if it's raw or cooked.",
        "```suggestion\r\nJS (ES Module)\r\n```",
        "I don't think so, the config is a single module, why `s`? I'm a native English speaker, I'll let other maintainer decide."
      ],
      "prettier-ensure-semantic-naming-accuracy": [
        "Can we deprecate these two methods in favor of `.key` / `.index` / `.node` getter?",
        "Where is `PropertyKey` ? Can't see."
      ],
      "prettier-document-ci-workflow-rationale": [
        "If the tests need build, it should be in prod test.",
        "https://github.com/prettier/prettier/blob/5f7aedc1cf0a1b3b4ec2c5a5ca0c09d1e0d00660/jest.config.js#L34",
        "```suggestion\r\n    name: Node.js ${{ matrix.node }} on ${{ matrix.os }}${{ matrix.FULL_TEST && ' (Full Test)' || '' }}\r\n```"
      ],
      "prettier-modern-configuration-formats": [
        "How about just one ESM version?",
        "Yes.",
        "> I will create an example repository then, because [azz/prettier-config](https://github.com/azz/prettier-config) is using a json file. And the new example will also use the exports field.\r\n\r\n@azz Will you accept PR change your example to ESM?",
        "```suggestion\r\nimport usernamePrettierConfig from \"@username/prettier-config\";\r\n\r\n/**\r\n * @type {import(\"prettier\").Config}\r\n */\r\nconst config = {\r\n  ...usernamePrettierConfig,\r\n  semi: false,\r\n};\r\n\r\nexport default config;\r\n```",
        "The recommended way to use plugin should be\r\n\r\n```js\r\nimport * as prettierPluginXml ...\r\n\r\nconst config = {\r\n  singleQuote: true,\r\n  plugins: [prettierPluginXml],\r\n};\r\n```\r\n\r\nBut `vscode-prettier` cannot work since they transfer config to worker.\r\n\r\nMy suggestion is to use an absolute path/url instead, but `import.meta.resolve` is under `--experimental-import-meta-resolve` flag. Not sure what's the best way here.",
        "> I'm gonna open a separate issue there.\r\n\r\nThe issue is here in Prettier.",
        "> Fortunately import.meta.resolve by itself is not under a flag,\r\n\r\nWe still support Node.js v14.",
        "> What do you think about merging this PR without this section about sharing configurations with plugins? It could be added later in a follow up PR easily.\r\n\r\nThe example should work for everyone, give me a while to think about it.",
        "Sorry for leaving this for such a long time. Let's merge with current example.",
        "Shouldn't editor also do autocomplete to json file? https://github.com/SchemaStore/schemastore/blob/master/src/schemas/json/prettierrc.json",
        "![image](https://user-images.githubusercontent.com/172584/204206097-ddd48433-790e-4b1d-b0d0-ecebc2cff834.png)\r\n![image](https://user-images.githubusercontent.com/172584/204206163-4ba68d07-fc7c-4090-b85d-0960762d0b97.png)\r\n"
      ],
      "prettier-measure-performance-impacts": [
        "```suggestion\r\n          !/\\S/.test(text.slice(locEnd(previousComment), locStart(node)))\r\n```",
        "JS is hard!",
        "Ha, this is the root cause... It will be hard to fix, this function mutates array because this comment https://github.com/prettier/prettier/blob/484ecde4307d3f535256ef83e9b09c5a0445bc77/src/document/printer.js#L506\r\n\r\nI try to solve this problem before, but didn't get time to finish https://github.com/prettier/prettier/pull/13315",
        "> Or is there no way to copy doc? \r\n\r\nCopy is not a good idea, the doc can be huge.\r\n\r\nLet's take my approach from #13315 . It creates mutable `fill` during print.",
        "Okay, I can take care of it, thanks for looking into the issue.",
        "Yes, it's complete, but I was waiting for a better solution :smile:",
        "```suggestion\r\n    // eslint-disable-next-line unicorn/prefer-at -- `Array#at` is slow on Node.js v16 and v18\r\n```\r\n\r\nSo we'll know when to remove it."
      ],
      "prettier-add-explanatory-comments": [
        "What kind of comment do you expect? Similar code exists\r\n\r\nhttps://github.com/prettier/prettier/blob/67e121a20b9d5d983970bdc2dcba5de2bd0f7beb/src/language-js/print/function-parameters.js#L246\r\nhttps://github.com/prettier/prettier/blob/67e121a20b9d5d983970bdc2dcba5de2bd0f7beb/src/language-js/print/object.js#L200",
        "[`e849c94`](https://github.com/prettier/prettier/pull/13735/commits/e849c94bbf45d4ceb502402fadf2ea425b5e4dc1)",
        "Can you add an example here to show what this condition supposed to match?",
        "Actually, I mean example for the code in this condition, not sure those `value-string`/`value-word` suppose to match."
      ],
      "prettier-cache-correctness-validation": [
        "Feel not safe without file content, but that will be slow.",
        "```suggestion\r\n      // If `createFromFile()` fails, it's probably because the format of cache file changed, it happened when we release v3.5.0\r\n```",
        "Can we add a test for stdin? So we won't accidentally use cache when formatting stdin."
      ],
      "prettier-environment-specific-error-handling": [
        "Make sense."
      ],
      "prettier-cache-invalidation-strategy": [
        "I'm not good at English, feel free to ignore.\r\n\r\nI guess this can be improved, maybe something like\r\n\r\n```\r\n#### Fix CLI crash when cache for old version exists (#17100 by @sosukesuzuki)\r\n\r\nPrettier 3.5 uses a different cache format than previous versions, Prettier stable crashes when reading existing cache file, Prettier main fixed the problem.\r\n```"
      ],
      "prettier-verify-optional-chaining-necessity": [
        "Will this index access cause problems for emojis or other characters\r\n? https://mathiasbynens.be/notes/javascript-unicode",
        ".at not only available on array, but we can check array and string only for now. We probably won't use TypedArray, and the transform only works for files (not packages) now. https://github.com/prettier/prettier/blob/4edb68ac50a847bef0d2968e1a1e4de643add7b0/scripts/build/transform/index.js#L13",
        "Since we are here. Let's remove this chaining, I don't think the `key` can be nullish here.\r\n\r\n```suggestion\r\n  const { name } = node.key;\r\n```"
      ]
    },
    "profile": {
      "location": "China",
      "company": "@d0d0dotcom ",
      "blog": "https://fiskercheung.com/",
      "site_admin": false,
      "followers": 418,
      "following": 64
    }
  },
  "dgozman": {
    "repos": [
      "microsoft/playwright"
    ],
    "entries": [
      {
        "slug": "playwright-api-consistency-and-decoupling",
        "title": "API consistency and decoupling"
      },
      {
        "slug": "playwright-api-documentation-precision",
        "title": "API documentation precision"
      },
      {
        "slug": "playwright-api-parameter-clarity",
        "title": "API parameter clarity"
      },
      {
        "slug": "playwright-avoid-redundant-operations",
        "title": "Avoid redundant operations"
      },
      {
        "slug": "playwright-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "playwright-document-build-configuration-changes",
        "title": "Document build configuration changes"
      },
      {
        "slug": "playwright-document-configuration-formats-explicitly",
        "title": "Document configuration formats explicitly"
      },
      {
        "slug": "playwright-document-network-limitations",
        "title": "Document network limitations"
      },
      {
        "slug": "playwright-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "playwright-explicit-undefined-checks",
        "title": "explicit undefined checks"
      },
      {
        "slug": "playwright-extract-repeated-logic",
        "title": "Extract repeated logic"
      },
      {
        "slug": "playwright-graceful-process-termination",
        "title": "graceful process termination"
      },
      {
        "slug": "playwright-justify-dependency-changes",
        "title": "Justify dependency changes"
      },
      {
        "slug": "playwright-prefer-css-over-javascript",
        "title": "prefer CSS over JavaScript"
      },
      {
        "slug": "playwright-prioritize-documentation-clarity",
        "title": "Prioritize documentation clarity"
      },
      {
        "slug": "playwright-proxy-configuration-precedence",
        "title": "proxy configuration precedence"
      },
      {
        "slug": "playwright-secure-authentication-state-files",
        "title": "Secure authentication state files"
      },
      {
        "slug": "playwright-synchronous-event-handlers",
        "title": "Synchronous event handlers"
      },
      {
        "slug": "playwright-testing-best-practices",
        "title": "testing best practices"
      },
      {
        "slug": "playwright-use-descriptive-identifier-names",
        "title": "Use descriptive identifier names"
      },
      {
        "slug": "playwright-use-modern-null-safe-operators",
        "title": "Use modern null-safe operators"
      },
      {
        "slug": "playwright-use-semantically-clear-names",
        "title": "Use semantically clear names"
      },
      {
        "slug": "playwright-validate-algorithmic-edge-cases",
        "title": "validate algorithmic edge cases"
      },
      {
        "slug": "playwright-validate-input-rigorously",
        "title": "validate input rigorously"
      },
      {
        "slug": "playwright-validate-inputs-early",
        "title": "validate inputs early"
      }
    ],
    "comments": {
      "playwright-testing-best-practices": [
        "I don't think `--list` is ever used by most testers.\r\n\r\n```suggestion\r\n- Open testing UI: `--ui`.\r\n```",
        "I feel like this is not a really useful strategy, because it is not stable. Whenever I add a test or move it between files, all my shards will be completely new. This introduces a lot of variability on CI, which from our experience is something to avoid. I wonder whether we can make this stable somehow?",
        "I guess that's fair. If you would like to opt-in into `round-robin` and it works for you, that's great. I still think it is less stable than I would prefer, but it doesn't hurt to have it as an option anyway."
      ],
      "playwright-comprehensive-test-coverage": [
        "Let's make this test a bit more involved:\r\n\r\n```\r\n- hidden\r\n  - visible\r\n    - neutral\r\n  - hidden\r\n    - visible\r\n  - neutral (no visibility change)\r\n    - visible\r\n      - hidden\r\n    - neutral\r\n```",
        "It is better to use `toHaveCount(3)` with the right number here, and `toHaveCount(0)` to check that snippets are hidden.",
        "Let's add a test for `expect(locator).toHaveClass('  baz   foo ', { partial: true })`."
      ],
      "playwright-use-modern-null-safe-operators": [
        "This one should use `item.testCase?.results[0]`, similar to the line 48."
      ],
      "playwright-synchronous-event-handlers": [
        "This is prone to race conditions and probably `EPERM` as well. Let's make it a try/catch inside `loadSource` instead?",
        "As a rule thumb, we can not afford async handling in an event handler. For example, the `popup` event below should be fired synchronously.",
        "Perhaps we can not await only when `format` is `commonjs` or something like that?",
        "```suggestion\r\n  const { timedOut } = await raceAgainstDeadline(() => transport.send('pushToCompilationCache', { cache }), monotonicTime() + 1000);\r\n```"
      ],
      "playwright-api-parameter-clarity": [
        "Now that we support `--user-data-dir` option in most commands, it looks like all the callers of `openPage` actually want to `allowPageReuse`. I think we can safely remove this argument.",
        "That's an interesting idea, but makes it harder to wire up on the browser-specific implementations.",
        "I guess we should pass `receiverLabel: 'page'` whenever we pass a page, or better yet auto-detect it here? Perhaps default to `receiverType.toLowerCase()`?",
        "I think you don't need to specify `contentType` here, it should be inferred from the `path`."
      ],
      "playwright-justify-dependency-changes": [
        "Can we stick with the old version we already had to avoid unnecessary deps churn?",
        "This seems to be the first time we declare `peerDependencies`, and we don't really like them. Where do we use them? Can we avoid declaring them as peers? Should we hard-depend on them, similar to depending on `vite-plugin-solid` in `@playwright/experimental-ct-solid`?"
      ],
      "playwright-proxy-configuration-precedence": [
        "I am afraid to touch this willy-nilly because it is used for the `npx playwright install`. Therefore I made the smallest change possible by respecting the user-supplied proxy if any. We can try to follow up with some opt-out env variable, or leave with it for one more release."
      ],
      "playwright-environment-variable-validation": [
        "```suggestion\r\n      ttyHeight = DEFAULT_TTY_HEIGHT;\r\n```"
      ],
      "playwright-validate-inputs-early": [
        "We should validate that `shardingMode` has one of the supported values.",
        "Let's throw when `-u` is passed explicitly to make this obvious?",
        "Let's throw a nice error when `!options.expectedText`, just in case?",
        "I think all our numbers are non-negative, so perhaps also check that?",
        "```suggestion\r\n      OptionValidators.validateNumber(value.slice(0, -1), { min: 1, max: 100 });\r\n```",
        "If you would like this to be really nice, we can print a better error message, e.g. `Expected a percentage between 1% and 100%`. Similarly in other places.",
        "```suggestion\r\n  ['--repeat-each <N>', `Run each test N times (default: 1)`, value => OptionValidators.validateNumber(value, { min: 1 })],\r\n```"
      ],
      "playwright-explicit-undefined-checks": [
        "```suggestion\r\n        if (params[name] !== undefined)\r\n          return params[name];\r\n```",
        "```suggestion\r\ntype HtmlReportOpenOption = NonNullable<Options['open']>;\r\n```",
        "Modern js allows to compare directly: `a.description === undefined`."
      ],
      "playwright-document-network-limitations": [
        "```suggestion\r\nExtensions only work in Chromium when launched with a persistent context. Use custom browser args at your own risk, as some of them may break Playwright functionality.\r\n\r\nGoogle Chrome and Microsoft Edge [removed the command-line flags needed to side-load extensions](https://groups.google.com/a/chromium.org/g/chromium-extensions/c/FxMU1TvxWWg/m/daZVTYNlBQAJ), so use Chromium that comes bundled with Playwright.\r\n```",
        "Let's have a list of limitations somewhere that would mention:\r\n- extra header will affect CORS options preflight;\r\n- requests on the server that were not made in response to the browser request will not be routed;\r\n- etc."
      ],
      "playwright-document-configuration-formats-explicitly": [
        "```suggestion\r\n| `PLAYWRIGHT_FORCE_TTY` | | Whether to produce output suitable for a live terminal. Supports `true`, `1`, `false`, `0`, `[WIDTH]`, and `[WIDTH]x[HEIGHT]`. `[WIDTH]` and `[WIDTH]x[HEIGHT]` specifies the TTY dimensions. | `true` when terminal is in TTY mode, `false` otherwise.\r\n```",
        "```suggestion\r\n| `PLAYWRIGHT_FORCE_TTY` | | Whether to produce output suitable for a live terminal. Supports `true`, `1`, `false`, `0`, `[WIDTH]`, and `[WIDTH]x[HEIGHT]`. `[WIDTH]` and `[WIDTH]x[HEIGHT]` specifies the TTY dimensions. | `true` when terminal is in TTY mode, `false` otherwise.\r\n```"
      ],
      "playwright-avoid-redundant-operations": [
        "Can we retain the same array instead of creating a new one? This is a memory-sensitive place, so minimizing the number of objects of any type is beneficial.",
        "Can we make this `100` to not waste cpu on the bots?"
      ],
      "playwright-graceful-process-termination": [
        "While this works, I think it's nicer to have a generic list of disposables to run at exit, sigint, etc. We can probably include esbuild contexts there, and calls `context.dispose()` to ensure esbuild native process is stopped."
      ],
      "playwright-extract-repeated-logic": [
        "We have a `wrapFunctionWithLocation` helper, can we reuse it here?",
        "I'd recommend to pass `data` here, since you are already retrieving it from the map on both call sites."
      ],
      "playwright-document-build-configuration-changes": [
        "Did you look at the generated code? How does it differ from babel's?"
      ],
      "playwright-api-documentation-precision": [
        "```suggestion\r\n- returns: <[ConsoleMessageType]<\"log\"|\"debug\"|\"info\"|\"error\"|\"warning\"|\"dir\"|\"dirxml\"|\"table\"|\"trace\"|\"clear\"|\"startGroup\"|\"startGroupCollapsed\"|\"endGroup\"|\"assert\"|\"profile\"|\"profileEnd\"|\"count\"|\"timeEnd\">>\r\n```",
        "A topic for the API review: does `not.toHaveClass('foo bar', { partial: true })` match the class name `foo baz`?"
      ],
      "playwright-prioritize-documentation-clarity": [
        "I think it's better to move the section about generic matchers to be after the list of web-first assertions.",
        "I don't think any of the commands below bring any value here, and they will eventually diverge from the source of truth. I'd remove them.",
        "While I like the explanation, I think that adding a suggestion \"you should use config option instead\" would benefit more users, because they will immediately understand what to do.",
        "I'd not mention the \"continue on failure\" behavior - that's expected. It is only important to mention that for serial mode, and avoid unnecessary details for the default mode.\r\n\r\n```suggestion\r\n* Running tests in order, retrying each failed test independetly.\r\n```"
      ],
      "playwright-use-descriptive-identifier-names": [
        "Choose a unique name, e.g. `alreadyRegistered`, otherwise this will conflict with other some other test.",
        "I do not think either electron or android were ever supported in our fixtures, so let's not add them.",
        "I'd inline this one to avoid an overly-generic `Options` type. Same for all other reporters.",
        "Shouldn't this be `HtmlReporterConfigOptions & CommonReporterOptions` instead?",
        "I am pretty sure we already have a `Location` type 😄 ",
        "`previousActionByEndTime`/`nextActionByStartTime`?",
        "nit: `typedArrayConstructors` to avoid unnecessary acronyms."
      ],
      "playwright-use-semantically-clear-names": [
        "Why are there multiple \"settings\" passed to a single checkbox?"
      ],
      "playwright-validate-input-rigorously": [
        "According to https://www.w3.org/TR/wai-aria-1.2/#aria-disabled, aria disabled only applies to elements with a suitable role, when that element or one of the ancestors (no matter the ancestor's role) has `aria-disabled=true`.\r\n\r\nDouble checking with DevTools accessibility panel:\r\n- In the following snippet `<h1>` is not disabled, because it has a `heading` role:\r\n  ```\r\n  <span aria-disabled=\"true\"> <h1>Click me!</h1> </span>\r\n  ```\r\n\r\n- In the following snippet `<button>` is disabled, because it has a suitable `button` role:\r\n  ```\r\n  <span aria-disabled=\"true\"> <button>Click me!</button> </span>\r\n  ```\r\n\r\n",
        "I'd like to be on the safe side and alter the actual content as less a possible. We know that `</` urls are broken anyway, so fixing them is definitely a win."
      ],
      "playwright-api-consistency-and-decoupling": [
        "I'd prefer that html report does not know about `rootDir` or anything filesystem-related. Can we instead fetch sources while building the report, and include them explicitly? This way, you can also open the report from anywhere at any time, and things will work."
      ],
      "playwright-secure-authentication-state-files": [
        "I would love the auth guide to not have a BIG RED WARNING on the first page!"
      ],
      "playwright-prefer-css-over-javascript": [
        "Instead of `useState()` and listening to mouseenter/mouseleave, perhaps we can just use css `:hover` to make the button visible on hover?",
        "I wonder whether the whole `isOpen` tracking can be replaced with\r\n```css\r\n.copy-request-dropdown:not(:hover) .copy-request-dropdown-menu {\r\n  display: none;\r\n}\r\n```"
      ],
      "playwright-validate-algorithmic-edge-cases": [
        "I think this does not work for `totalTestCount === 10`, yielding `1`.",
        "Can I opt-out of `deep-equal` by setting `contain` somewhere in the subtree? If so, this one should check that `template.containerMode !== 'contain'`. If not, we should probably throw in such a case."
      ]
    },
    "profile": {
      "location": "Sunnyvale, CA",
      "blog": "",
      "site_admin": false,
      "followers": 336,
      "following": 0
    }
  },
  "jmorganca": {
    "repos": [
      "ollama/ollama"
    ],
    "entries": [
      {
        "slug": "ollama-ai-dependency-decoupling-strategy",
        "title": "AI dependency decoupling strategy"
      },
      {
        "slug": "ollama-clear-recoverable-error-messages",
        "title": "Clear recoverable error messages"
      },
      {
        "slug": "ollama-complete-null-checks",
        "title": "Complete null checks"
      },
      {
        "slug": "ollama-comprehensive-test-coverage",
        "title": "Comprehensive test coverage"
      },
      {
        "slug": "ollama-descriptive-balanced-naming",
        "title": "Descriptive balanced naming"
      },
      {
        "slug": "ollama-extract-duplicated-code",
        "title": "Extract duplicated code"
      },
      {
        "slug": "ollama-follow-godoc-conventions",
        "title": "Follow GoDoc conventions"
      },
      {
        "slug": "ollama-guard-against-nil",
        "title": "Guard against nil"
      },
      {
        "slug": "ollama-loose-api-coupling",
        "title": "Loose API coupling"
      },
      {
        "slug": "ollama-optimize-ai-implementation-patterns",
        "title": "Optimize AI implementation patterns"
      },
      {
        "slug": "ollama-optimize-memory-allocations",
        "title": "Optimize memory allocations"
      },
      {
        "slug": "ollama-optimize-with-standard-library",
        "title": "Optimize with standard library"
      },
      {
        "slug": "ollama-platform-aware-configuration-documentation",
        "title": "Platform-aware configuration documentation"
      },
      {
        "slug": "ollama-purpose-reflecting-file-names",
        "title": "Purpose-reflecting file names"
      },
      {
        "slug": "ollama-reuse-buffers-strategically",
        "title": "Reuse buffers strategically"
      },
      {
        "slug": "ollama-use-idiomatic-go-flow",
        "title": "Use idiomatic Go flow"
      }
    ],
    "comments": {
      "ollama-optimize-with-standard-library": [
        "Given how rare this is, why don't we just use built-in sort for now. It will simplify the code a lot:\r\n\r\n```\r\n\tif k >= len(ts) {\r\n\t\tslices.SortFunc(ts, func(a, b token) int {\r\n\t\t\tswitch {\r\n\t\t\tcase a < b:\r\n\t\t\t\treturn -1\r\n\t\t\tcase a > b:\r\n\t\t\t\treturn 1\r\n\t\t\tdefault:\r\n\t\t\t\treturn 0\r\n\t\t\t}\r\n\t\t})\r\n\t\treturn ts\r\n\t}\r\n```",
        "This should be able to replace `partialSortLogits` and `sortLogits` below",
        "I know we passed on it previously, but it's worth seeing if we can use `container/heap`  ?\r\n\r\n````\r\nimport \"container/heap\"\r\n\r\n// tokenHeap implements heap.Interface\r\ntype tokenHeap []tokenInfo\r\n\r\nfunc (h tokenHeap) Len() int           { return len(h) }\r\nfunc (h tokenHeap) Less(i, j int) bool { return h[i].logit < h[j].logit } // min-heap based on logit\r\nfunc (h tokenHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\r\n\r\nfunc (h *tokenHeap) Push(x any) {\r\n\t*h = append(*h, x.(tokenInfo))\r\n}\r\n\r\nfunc (h *tokenHeap) Pop() any {\r\n\told := *h\r\n\tn := len(old)\r\n\titem := (*h)[n-1]\r\n\t*h = (*h)[:n-1]\r\n\treturn item\r\n}\r\n\r\n### Explanation:\r\n- `Pop` should remove and return the last element of the slice after shrinking it.\r\n- This properly maintains the heap's invariants defined by the other methods (`Len`, `Less`, `Swap`).\r\n\r\n### Usage example:\r\n\r\n```go\r\nh := &tokenHeap{}\r\nheap.Init(h)\r\n\r\n// Push elements onto the heap\r\nheap.Push(h, tokenInfo{id: 1, logit: 0.9})\r\nheap.Push(h, tokenInfo{id: 2, logit: 1.2})\r\n\r\n// Pop smallest element\r\nsmallest := heap.Pop(h).(tokenInfo)\r\n```\r\n````",
        "Let's use https://pkg.go.dev/slices#BinarySearchFunc vs re-implementing binary search",
        "I believe you can use `slices.Indexfunc` instead of this dependency, something like:\r\n\r\n```\r\nimport \"slices\"\r\n\r\nfunc maxIdx(vals []float64) int {\r\n    if len(vals) == 0 {\r\n        return -1\r\n    }\r\n    maxVal := slices.Max(vals)\r\n    return slices.IndexFunc(vals, func(v float64) bool {\r\n        return v == maxVal\r\n    })\r\n}\r\n```"
      ],
      "ollama-platform-aware-configuration-documentation": [
        "It may be simpler to keep this `Windows` but to mention in a warning below acceleration libraries aren't currently supported for ARM (vs two sections)",
        "The top of the doc (perhaps a bit too easy to glance over) has a link to TDM-gcc which works pretty well.",
        "(and llvm-mingw for ARM)"
      ],
      "ollama-use-idiomatic-go-flow": [
        "Nit: we can short circuit return above instead of the else",
        "nit/optional, but we could return early here\r\n\r\n```\r\nif c.windowSize == math.MaxInt32 {\r\n    return\r\n}\r\n\r\n...\r\n```\r\n"
      ],
      "ollama-optimize-memory-allocations": [
        "Thanks for the PR!\r\n\r\nWould it be possible to keep the argument based return value?"
      ],
      "ollama-guard-against-nil": [
        "Should we `ok` check this?",
        "As-is is ok if it can't happen",
        "Let's avoid the case where this is nil\r\n\r\n```\r\nvar vocab *model.Vocabulary\r\nif tp, ok := s.model.(model.TextProcessor); ok {\r\n  vocab = tp.Vocab()\r\n}\r\n\r\nif req.Grammar != \"\" && vocab != nil {\r\n\r\n}\r\n```",
        "Good catch. I'll remove the pointer.",
        "I wonder if there's a way we could get around the type assertion here via some other interface/struct design (maybe in a later PR as this all takes shape). Similar idea with `model.TextProcessor`."
      ],
      "ollama-reuse-buffers-strategically": [
        "this allocation can be one time?\r\n\r\n```\r\ntype weighted struct {\r\n\trng        *rand.Rand\r\n\ttransforms []transform\r\n\tbuf        []tokenInfo // reuse buffer\r\n}\r\n\r\nfunc (s *weighted) Sample(logits []float32) (int32, error) {\r\n\tif cap(s.buf) < len(logits) {\r\n\t\ts.buf = make([]tokenInfo, len(logits))\r\n\t}\r\n\ttokens := s.buf[:len(logits)]\r\n\t// rest of your logic\r\n}\r\n```"
      ],
      "ollama-purpose-reflecting-file-names": [
        "Yes, if this stays as a root level `Makefile` it should also build the project, ideally with GPU support"
      ],
      "ollama-complete-null-checks": [
        "@ParthSareen yes this should be in the `if (g != nullptr) {` block",
        "I would return a regular `std::string` here to avoid issues if the string gets deallocated or changed later by the caller\r\n\r\n```suggestion\r\nconst std::string ollama_vocab::token_to_piece(uint32_t token) {\r\n```"
      ],
      "ollama-optimize-ai-implementation-patterns": [
        "There may be multiple eog tokens, specifically EOS or EOT"
      ],
      "ollama-extract-duplicated-code": [
        "is `skipStartOfTurn` needed?"
      ],
      "ollama-loose-api-coupling": [
        "This is over coupling sampling with the `api.Options` type. Looser coupling is usually better. Much better that it accepts the raw float32/int values instead of passing `req` even deeper",
        "Instead of a list of transforms (`[]Transform`), why don't we just make helper functions like `topP` `topK` `softmax` and call them in the fixed order we use here (given the API is fixed)?"
      ],
      "ollama-comprehensive-test-coverage": [
        "This is a great start. I would take out the concept of tool tokens and have cases like:\r\n\r\n* valid prefix, invalid json\r\n* invalid prefix, valid json\r\n* valid prefix, valid json\r\n* 2+ objects\r\n\r\netc",
        "Great start on the tests! It will be quite a bit of tests, but we should also test the intermediate tool parsing steps (e.g. finding the template, finding the prefix/tool tokens) since we're bound to find edge cases in new models (vs these more top-down tests)",
        "Add tests for `0`",
        "No, we want to test this \"bottom-up\", testing 0 (or maybe a very very small number) here will ensure we have code coverage for `max(temp, 1e-7)`, which is different than testing to see if greedy is used (sample function)"
      ],
      "ollama-clear-recoverable-error-messages": [
        "```suggestion\r\nvar ErrNoVisionModel = errors.New(\"this model is missing data required for image input\")\r\n```",
        "Actually, this message is more of edge case vs a capability, it's great as is",
        "We may want to hint at missing data or similar. E.g. `this model is missing data required for image input`"
      ],
      "ollama-ai-dependency-decoupling-strategy": [
        "Yeah! Turned out to be little code, however the tradeoff is we have to keep using `llama` code for now. It's very important we work to remove this to use new engine tokenizers instead as this restricts us to loading GGUF files with specific KV keys. cc @ParthSareen "
      ],
      "ollama-descriptive-balanced-naming": [
        "Some of the variable and function names here are very long. Ideally 1 nameComponent, 2 if needed for clarity, but 3 is often a sign that better naming or structure can be used.\r\n\r\n```\r\nprefix, ok := toolPrefix(model.Template.Template)\r\nif !ok {\r\n  return nil\r\n}\r\n```",
        "```suggestion\r\n\tUseAuth = Bool(\"OLLAMA_AUTH\")\r\n```\r\n\r\nnit on potentially a more consistent name since we don't have `_USE_` anywhere else"
      ],
      "ollama-follow-godoc-conventions": [
        "I _think_ generally the doc for functions should start with the function name, e.g.\r\n\r\n```suggestion\r\n\t// SetLayer sets the active layer of the cache\r\n```",
        "More: https://go.dev/doc/comment",
        "A GoDoc comment here would be great! Since there are some preconditions to running this.",
        "What is `name` and `arguments` here? Are those the formats? I would add them to the GoDoc for this function. I'm not sure what their purposes is from reading the comment.\r\n",
        "Add comment as to why it's passed in by pointer here. Something like the length of the slice is modified to k",
        "pedantic but should we make these GoDoc comments above the fields?\r\n\r\n```\r\n//MainGPU is the gpu where xyz...\r\nMainGPU int\r\n```\r\n"
      ]
    },
    "profile": {
      "company": "@ollama",
      "blog": "ollama.com",
      "twitter_username": "jmorgan",
      "site_admin": false,
      "followers": 2521,
      "following": 1
    }
  },
  "sosukesuzuki": {
    "repos": [
      "prettier/prettier"
    ],
    "entries": [
      {
        "slug": "prettier-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "prettier-angular-syntax-parsing",
        "title": "Angular syntax parsing"
      },
      {
        "slug": "prettier-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "prettier-benchmark-performance-optimizations",
        "title": "Benchmark performance optimizations"
      },
      {
        "slug": "prettier-cache-correctness-validation",
        "title": "Cache correctness validation"
      },
      {
        "slug": "prettier-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "prettier-documentation-example-consistency",
        "title": "Documentation example consistency"
      },
      {
        "slug": "prettier-ensure-semantic-naming-accuracy",
        "title": "Ensure semantic naming accuracy"
      },
      {
        "slug": "prettier-environment-specific-error-handling",
        "title": "Environment-specific error handling"
      },
      {
        "slug": "prettier-maintain-api-backward-compatibility",
        "title": "maintain API backward compatibility"
      },
      {
        "slug": "prettier-organize-tests-properly",
        "title": "Organize tests properly"
      },
      {
        "slug": "prettier-prefer-efficient-algorithms",
        "title": "prefer efficient algorithms"
      },
      {
        "slug": "prettier-refactor-complex-conditions",
        "title": "refactor complex conditions"
      },
      {
        "slug": "prettier-test-all-variations",
        "title": "Test all variations"
      },
      {
        "slug": "prettier-use-descriptive-variable-names",
        "title": "Use descriptive variable names"
      },
      {
        "slug": "prettier-validate-configuration-changes",
        "title": "validate configuration changes"
      },
      {
        "slug": "prettier-validate-configuration-values",
        "title": "validate configuration values"
      },
      {
        "slug": "prettier-verify-optional-chaining-necessity",
        "title": "Verify optional chaining necessity"
      },
      {
        "slug": "prettier-vue-syntax-parsing-robustness",
        "title": "Vue syntax parsing robustness"
      }
    ],
    "comments": {
      "prettier-documentation-clarity-standards": [
        "Thank you for pointing it out. I would like to know if there is a better wording.",
        "looks good to me",
        "Since `standalone.mjs` does not exist in version 2.x, it would be good to clarify this.\r\n\r\n```suggestion\r\n- ES modules: `standalone.mjs`, starting in version 3.0 (In version 2, `esm/standalone.mjs`.)\r\n```"
      ],
      "prettier-refactor-complex-conditions": [
        "This isn't related directly to your change, but can you refactor this complex inline condition?\r\n\r\ne.g.\r\n\r\n```js\r\nif (value !== \"\" && canLineBreakBeConvertedToSpace(path, value, adjacentNodes)) {\r\n  if (isBreakable) {\r\n    return line;\r\n  }\r\n  return \" \";\r\n}\r\nif (isBreakable) {\r\n  return softline;\r\n}\r\nreturn \"\";\r\n```",
        "Also can you extract `canLineBreakBeConvertedToSpace` as an another function?",
        "> Do you think this is still too complex? Nested if looks ugly and verbose for me.\r\n\r\nThank you, the below code that you showed first looks enough readable to me (I prefer the first one than second one.)\r\n\r\n```js\r\nif (value !== \"\" && canLineBreakBeConvertedToSpace(path, value, adjacentNodes)) {\r\n  return isBreakable ? line : \" \";\r\n}\r\n// Space is not allowed, just \"\" or \"\\n\"\r\nreturn isBreakable ? softline : \"\";\r\n```"
      ],
      "prettier-api-documentation-clarity": [
        "Nits for Node.js beginners\r\n\r\n```suggestion\r\nStrings provided to `plugins` are ultimately passed to [`import()` expression](https://nodejs.org/api/esm.html#import-expressions), so you can provide a module/package name, a path, or anything else `import()` takes.\r\n```"
      ],
      "prettier-validate-configuration-changes": [
        "I prefer using `\"error\"` in even non CI environment.",
        "FYI Some other packages are duduped by `npx yarn-dedupulicate`. [90d6098](https://github.com/prettier/prettier/pull/13872/commits/90d6098cf250ec25714e10b15ba39594c24d04fe)"
      ],
      "prettier-documentation-example-consistency": [
        "Sounds good!"
      ],
      "prettier-use-descriptive-variable-names": [
        "Is this a just boolean value? If true I prefer naming as `shouldCache`. To clarify whether this is an object that has cached values or a just boolean:\r\n\r\n```suggestion\r\n  #shouldCache;\r\n```",
        "This function is only used in `config-searcher.js`, right? I prefer to move from `common.js` to `config-searcher.js` and name the function something like `createCachedSearchFunction`. The name createCachedFunction is too generic.",
        "The variable name clone sounds like a function, so cloned or clonedNode is preferable.",
        "I prefer to define the following function in `language-markdown/utils.js`:\r\n\r\n```js\r\nfunction isMarkdownlintComment(node) {\r\n  return (\r\n    node.type === \"html\" &&\r\n    node.value.startsWith(\"<!--\") &&\r\n    node.value(\"-->\") &&\r\n    node.value === \"markdownlint-disable-next-line\"\r\n  );\r\n}\r\n```\r\n\r\nI also prefer variable name `isPrevNodeMarkdownlintComment`.",
        "I prefer `whitespaceCharacters` or `whitespaceChars`"
      ],
      "prettier-vue-syntax-parsing-robustness": [
        "Thank you.\r\n\r\nIt is unsafe to use TS parser always. For example, The below code is valid as JS and TS.\r\n\r\n```ts\r\nconst foo = doSomething<T1 | T2>(param)\r\n```\r\n\r\n```js\r\n// as JS\r\nconst foo = (doSomething < T1) | (T2 > param);\r\n```\r\n\r\nTS and JS parse differently for such codes. Therefore, it is safe to parse as TypeScript only when the script tag explicitly specifies the use of TS.",
        "I've fixed to avoid using `ast.walk`. [35b2600](https://github.com/prettier/prettier/pull/12584/commits/35b2600c0d3b7d295dfc31ce0877c030bf8af51b)"
      ],
      "prettier-angular-syntax-parsing": [
        "We need to normalize node name `else if`. Angular allows `else         if` current our implementation does not:\r\n\r\n**Prettier pr-15606**\r\n[Playground link](https://deploy-preview-15606--prettier.netlify.app/playground/#N4Igxg9gdgLgprEAuEABAlgMwAQAoCU2wAOlAL7apwA2AznNo1nocGSADQgQAOM60WslABDAE5iIAdwAK4hEJQjqUkQE8hXAEZiRYANZwYAZREBbOABl0UOMkzL623QaPGeemwHNkMMQFc4LnozdF8AoJBab2o4AEV-CHh7R0iAK1oAD2MY+MTkpAc6SIBHfLgZSR5FEBFaAFpbOAATFs4QPxF0am8AYQgzMxFkWupqduioL1iAQRg-dC1-eBk4MWtbFOKuAAsYM2oAdR30eFoPMDhjBVP0ADdTtRGwWk0QO8CASShW2GMwMToPgzH7GGBqWJbJwgHiSeiHXQ8EawuD0MR3OxcGxomCVEReIZQyIeMRokYiKb+ajidqwmwwQ7oZowHbIAAcAAYuGI4GV0Dy8QThoVUlwYCItIzmaykAAmLj+egAFQliiK0LgZi0LVazUsFK8-nxcAAYhAxEN5t5ycsICAyGQgA)\r\n<!-- prettier-ignore -->\r\n```sh\r\n--parser angular\r\n```\r\n\r\n**Input:**\r\n<!-- prettier-ignore -->\r\n```html\r\n@if () {\r\n} @else   if () {}\r\n```\r\n\r\n**Output:**\r\n<!-- prettier-ignore -->\r\n```html\r\nError: Unknown block name: else   if\r\n    at is (https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:11151)\r\n    at Object.ba [as print] (https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:12544)\r\n    at Yn (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:30:7727)\r\n    at D (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:30:7547)\r\n    at s (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:30:7355)\r\n    at rt (https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:7858)\r\n    at https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:8860\r\n    at https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:26:1596\r\n    at Vt.each (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:26:1499)\r\n    at Vt.map (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:26:1576)\r\n```"
      ],
      "prettier-prefer-efficient-algorithms": [
        "@fisker You’re right—thanks! (Actually, we should be using `===` instead of `!==`.)\r\n\r\nI’ve also noticed a more fundamental problem: the location info for value-words that start with a hyphen is brokwn. For the `--foo` node, both `locStart` and `locEnd` are reported two characters too far forward.\r\n\r\nThis looks like a parser bug. I worked around it with a helper function called `fixValueWordLoc` ( https://github.com/prettier/prettier/pull/17398/commits/41838a2430ee3d000b1db9ec01b5dbedd7d818c5 )\r\n\r\nDo you have a better idea?"
      ],
      "prettier-maintain-api-backward-compatibility": [
        "You are right. However, Import Assertions was Stage 3 and was dropped from TC39. Therefore, it would make sense for us to drop its syntax. And I think v3 is a good opportunity to do so.",
        "Thank you @fisker and @nicolo-ribaudo. I got it.",
        "[f1d6a38](https://github.com/prettier/prettier/pull/14863/commits/f1d6a3865d9f523fe311025a2522c145b50d0507)"
      ],
      "prettier-validate-configuration-values": [
        "[fefe40a](https://github.com/prettier/prettier/pull/12800/commits/fefe40a7b51edbee5a09aabe2b9802d63ade40af)",
        "?\r\n```suggestion\r\n// To prevent `chalk` module from being included in the `standalone.js` bundle,\r\n// it will take that as an argument if needed.\r\nconst getFlagSchema = (colorsModule) =>\r\n```"
      ],
      "prettier-organize-tests-properly": [
        "This is actually for both invalid and valid. We should separate tests like:\r\n\r\n```js\r\ndescribe(\"doc builders\", () => {\r\n  test(\"Invalid usage\", () => {\r\n    // tests\r\n  });\r\n  test(\"Valid usage\", () => {\r\n    // tests\r\n  });\r\n});\r\n```",
        "This will not be called if `runPrettier` is rejected. So I think we should call `runPrettier` in `try` block and call this in `finally` block.",
        "Because changing `process.version` affects out of this test case.\r\n\r\nIf we add the test like below under the `test(\"node version error\", async () => ...);`.\r\n\r\n```js\r\ntest(\"checks version\", () => {\r\n  expect(process.version).toBe(\"v14.18.2\");\r\n});\r\n```\r\n\r\nif current node version is `14.18.2` and `runPrettier` is rejected, this test is failed via:\r\n\r\n```\r\nchecks version\r\n\r\n    expect(received).toBe(expected) // Object.is equality\r\n\r\n    Expected: \"v14.18.2\"\r\n    Received: \"v8.0.0\"\r\n\r\n      116 |\r\n      117 | test(\"checks version\", () => {\r\n    > 118 |   expect(process.version).toBe(\"v14.18.2\");\r\n          |                           ^\r\n      119 | });\r\n      120 |\r\n```\r\n\r\nSuch tests are rarely written, but it's better to use `finally` just in case."
      ],
      "prettier-test-all-variations": [
        "If we don't have a test for non-promise besides this one, I think we should add a new test instead of modifying this one.",
        "[b78e90f](https://github.com/prettier/prettier/pull/15888/commits/b78e90f25226056de6c869411e7aabaac402f940)"
      ],
      "prettier-ensure-semantic-naming-accuracy": [
        "`PropertyKey` is defined in TypeScript (`/node_modules/typescript`)"
      ],
      "prettier-benchmark-performance-optimizations": [
        "We originally did not intend to add the `--cache-strategy` option and always intended to use `metadata`. However, following a comment from @7rulnik ( https://github.com/prettier/prettier/pull/12800#discussion_r878044023 ), we added the `--cache-strategy` option for CI use cases so that `content` can be used.\r\n\r\nI had thought there was a tradeoff between `metadata` and `content` as follows:\r\n\r\n|        | `metadata`  | `content`   |\r\n| ------ | ----------- | ----------- |\r\n| for CI | :no_good:   | :ok_person: |\r\n| perf   | :ok_person: | :no_good:   |\r\n\r\n\r\n\r\nHowever, following your comment, I investigated and found that there is actually not much difference in performance between the two.\r\n\r\n***\r\n\r\n`--cache-strategy=metadata`:\r\n\r\n```\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 16.51s.\r\n\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 6.56s.\r\n```\r\n\r\n`--cache-strategy=content`:\r\n\r\n```\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache --cache-strategy=content\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 16.46s.\r\n\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache --cache-strategy=content\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 6.81s.\r\n```\r\n\r\n***\r\n\r\nFrom this result I think we can remove the `--cache-strategy` option and always use `content`.\r\n\r\nWhat do you think? @7rulnik, @fisker",
        "I created following script:\r\n\r\n<details>\r\n  <summary>test.benchmark.js</summary>\r\n  \r\n```js\r\nconst Benchmark = require(\"benchmark\");\r\nconst { execaSync } = require(\"./vendors/execa\");\r\n\r\nconst runWithMetadata = () =>\r\n  execaSync(\"./bin/prettier.js\", [\".\", \"!test*\", \"--check\", \"--cache\"]);\r\n\r\nconst runWithContent = () =>\r\n  execaSync(\"./bin/prettier.js\", [\r\n    \".\",\r\n    \"!test*\",\r\n    \"--check\",\r\n    \"--cache\",\r\n    \"--cache-strategy\",\r\n    \"content\",\r\n  ]);\r\n\r\nconst clearCache = () =>\r\n  execaSync(\"rm\", [\"-f\", \"./node_modules/.cache/prettier/.prettier-cache\"]);\r\n\r\nfunction runSuite01() {\r\n  const suite01 = new Benchmark.Suite();\r\n\r\n  clearCache();\r\n  runWithMetadata();\r\n\r\n  suite01\r\n    .add(\"--cache-strategy=metadata\", function () {\r\n      runWithMetadata();\r\n    })\r\n    .on(\"cycle\", function (event) {\r\n      console.log(String(event.target));\r\n    })\r\n    .run();\r\n}\r\n\r\nfunction runSuite02() {\r\n  const suit02 = new Benchmark.Suite();\r\n\r\n  clearCache();\r\n  runWithContent();\r\n\r\n  suit02\r\n    .add(\"--cache-strategy=content\", function () {\r\n      runWithContent();\r\n    })\r\n    .on(\"cycle\", function (event) {\r\n      console.log(String(event.target));\r\n    })\r\n    .run();\r\n}\r\n\r\nrunSuite01();\r\nrunSuite02();\r\n\r\n```\r\n\r\n</details>\r\n\r\nThe results were as follows:\r\n\r\n```\r\n--cache-strategy=metadata x 0.30 ops/sec ±6.05% (5 runs sampled)\r\n--cache-strategy=content x 0.30 ops/sec ±0.94% (5 runs sampled)\r\n```\r\n\r\nI ran several other benchmarks and found no significant differences. This is strange.\r\n\r\nBut, ESLint cache (Almost the same mechanism as our cache) with similar results.\r\n\r\n***\r\n\r\n**ESLint with `--cache-strategy=metadata`:**\r\n\r\n```\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=metadata\r\n✨  Done in 16.27s.\r\n\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=metadata\r\n✨  Done in 3.05s.\r\n```\r\n\r\n**ESLint with `--cache-strategy=content`:**\r\n\r\n```\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=content\r\n✨  Done in 13.69s.\r\n\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=content\r\n✨  Done in 2.96s.\r\n```\r\n\r\n***\r\n\r\nI ran the benchmark using Prettier's source code this time, but the results may be different for projects where each file is more huge.",
        "(Even in the Babel source code, there was little difference in speed between `metadata` and `content`.)",
        "To maintain both `--cache-strategy=content` and `--cache-strategy=metadata` isn't hard. So I think we can keep both.",
        "I created new benchmark script https://gist.github.com/sosukesuzuki/ded586261381a6261e02a69b188a8e44\r\n\r\nResults ware follows:\r\n\r\n```\r\n======= 100 lines, --cache-strategy=metadata, 0ms ============\r\n======= 1000 lines, --cache-strategy=metadata, 1ms ============\r\n======= 10000 lines, --cache-strategy=metadata, 1ms ============\r\n======= 100000 lines, --cache-strategy=metadata, 0ms ============\r\n======= 100 lines, --cache-strategy=content, 17ms ============\r\n======= 1000 lines, --cache-strategy=content, 12ms ============\r\n======= 10000 lines, --cache-strategy=content, 13ms ============\r\n======= 100000 lines, --cache-strategy=content, 12ms ============\r\n```\r\n\r\nThis benchmark script measures execution time when cache is enabled. There is only one file of interest, and each test combines the number of lines in the file with the cache strategy.\r\n\r\nAccording to this, performance still seems to be better with `--cache-strategy=metadata`(However, the number of lines does not seem to matter much). I think the reason why the previously posted benchmarks did not show a valid difference is probably due to overhead in areas other than formatting.",
        "> But I definitely think we should default to content.\r\n\r\n@lydell  Why you prefer `content`? Because it works on CI?",
        "I've updated the documentation to use `content` by default. For normal projects, there is no performance difference, and `content` is more convenient."
      ],
      "prettier-add-explanatory-comments": [
        "Can we add comment  like `// Block names that can have parameters` ?",
        "It seems unreadable to me... Can you add comment or extract these conditions to variables?",
        "I expect comments that describes what concrete syntax each matcher represents.",
        "Can you add a comment that explains why `\"stylus\"` is here? e.g.:\r\n\r\n```suggestion\r\n  // Prettier does not officially support stylus.\r\n  // But, we need to handle `\"stylus\"` here for printing a style block in Vue SFC as stylus code by external plugin.\r\n  // https://github.com/prettier/prettier/pull/12707\r\n  if (lang === \"stylus\") {\r\n    return inferParserByLanguage(\"stylus\", options);\r\n  }\r\n```",
        "What do you think the following comment?\r\n\r\n```js\r\n// For example, there is the following key-value pair:\r\n//\r\n//   \"xs-only\": \"only screen and (max-width: #{map-get($grid-breakpoints, \"sm\")-1})\"\r\n//\r\n// \"only screen and (max-width: #{map-get($grid-breakpoints, \" is a \"value-string\"\r\n// and \"sm\" is a \"value-word\"\r\n// We should not insert any spaces and lines here.\r\n```"
      ],
      "prettier-cache-correctness-validation": [
        "For minimum speed and security, why not check the length of the file content rather than the content of the file?",
        "Thanks, I've added `--cache-strategy` option!",
        "[9cf4a31](https://github.com/prettier/prettier/pull/12800/commits/9cf4a312aafa1037d48d37f034c857018aa3aeb4)"
      ],
      "prettier-environment-specific-error-handling": [
        "I think assertions for production environments should be considered separately from assertions for development environments.\r\nTherefore, I propose to add a simple validation like the one below and remove the relevant part from `assertComment` and wrap `assertComment` call in `if (process.env.NODE_ENV !== \"production\")`:\r\n\r\n```js\r\nif (!isLineComment(comment) && !isBlockComment(comment)) {\r\n    throw new TypeError(`Unknown comment type: \"${comment.type}\".`);\r\n}\r\n```"
      ],
      "prettier-verify-optional-chaining-necessity": [
        "I am wondering if we should include brand checking for safety..."
      ]
    },
    "profile": {
      "location": "Tokyo, Japan",
      "company": "@ubie-oss",
      "blog": "https://sosukesuzuki.dev",
      "twitter_username": "__sosukesuzuki",
      "site_admin": false,
      "followers": 581,
      "following": 15
    }
  },
  "ritchie46": {
    "repos": [
      "pola-rs/polars"
    ],
    "entries": [
      {
        "slug": "polars-appropriate-error-handling",
        "title": "Appropriate error handling"
      },
      {
        "slug": "polars-ci-workflow-configuration-best",
        "title": "CI workflow configuration best"
      },
      {
        "slug": "polars-consistent-naming-standards",
        "title": "Consistent naming standards"
      },
      {
        "slug": "polars-defer-expensive-operations",
        "title": "Defer expensive operations"
      },
      {
        "slug": "polars-evaluate-algorithmic-complexity-tradeoffs",
        "title": "Evaluate algorithmic complexity tradeoffs"
      },
      {
        "slug": "polars-explicit-configuration-precedence",
        "title": "Explicit configuration precedence"
      },
      {
        "slug": "polars-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "polars-extract-duplicated-code",
        "title": "Extract duplicated code"
      },
      {
        "slug": "polars-favor-clarity-over-brevity",
        "title": "Favor clarity over brevity"
      },
      {
        "slug": "polars-feature-flag-compatibility",
        "title": "Feature flag compatibility"
      },
      {
        "slug": "polars-hide-implementation-details",
        "title": "Hide implementation details"
      },
      {
        "slug": "polars-names-reveal-clear-intent",
        "title": "Names reveal clear intent"
      },
      {
        "slug": "polars-optimize-data-transformations",
        "title": "Optimize data transformations"
      },
      {
        "slug": "polars-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "polars-organize-tests-efficiently",
        "title": "Organize tests efficiently"
      },
      {
        "slug": "polars-prevent-cryptic-errors",
        "title": "Prevent cryptic errors"
      },
      {
        "slug": "polars-prevent-deadlock-conditions",
        "title": "Prevent deadlock conditions"
      },
      {
        "slug": "polars-safe-null-handling",
        "title": "Safe null handling"
      }
    ],
    "comments": {
      "polars-optimize-data-transformations": [
        "You don't have to:\r\n\r\n\r\n```rust\r\n matches!(\r\n        function,\r\n        FunctionExpr::Range(RangeFunction::IntRange { .. })\r\n    );\r\n```\r\n",
        "I think we should do this check during the IR::conversion, in `resolve_groupby` here:\r\n\r\nhttps://github.com/pola-rs/polars/blob/05f2abbf1b7f76f0b34c3c552fc33aa6da186561/crates/polars-plan/src/plans/conversion/dsl_to_ir.rs#L1013",
        "Ah, right. I see that we first do a `with_columns` here. We should store the `index` column on line 1155 and do the `with_column` rewrite during IR conversion.\r\n\r\nI understand it's a bit more than you anticipated. I can do the pre-work for that later if you like?",
        "Don't create `tmp_df` as what it does now.\r\n\r\nLet's say we have `pivot_df: A, B, C` and we create `value_col: A` from `pivot_df`. By concatting/hstacking we create:\r\n\r\n`tmp_df: A, A, B, C`. Where we just want to pass the context of `pivot_df` to the aggregation. So we can pass  `pivot_df` directly to `expr.evaluate`.",
        "Could you clarify how you distinct \"scanned\" from \"read\"? \r\n"
      ],
      "polars-appropriate-error-handling": [
        "If it is an implementation error on our end, we can panic.",
        "Instead of panicking, this should just raise an unsupported error. I don't want to put PR links in error messages either."
      ],
      "polars-organize-tests-efficiently": [
        "Can this be done in-memory with `io.BytesIO()`. We prefer in-memory tests when possible as this is faster."
      ],
      "polars-ci-workflow-configuration-best": [
        "Don't publish to pypi if this fails.",
        "Don't we need to include all variants then? (E.g. is the exclude set not smaller than the include set?)",
        "Right, I think you're right. I also do need to think harder with the include set. :laughing: "
      ],
      "polars-explicit-configuration-precedence": [
        "We should soften this guarantee. We will try to run on this engine, but don't guarantee it. Both streaming and gpu have cpu fallbacks."
      ],
      "polars-evaluate-algorithmic-complexity-tradeoffs": [
        "I do wonder though if it is worth the extra binary bloat. The `is_nan` will be correctly predicted on every non-nan, until we hit it. On that mis prediction we are done, so I think it doesn't really matter and we can save a monomorphized function.",
        "This allocates a new vec. We should be able to gather without reallocing.",
        "I think this should be supported. For decimals we should extract the `Int128` in both the haystack and the needle. \r\n\r\nAnd for the nested types we should convert both to the row-encoding."
      ],
      "polars-safe-null-handling": [
        "We can check `is_nan` on the floats.\r\n\r\nWe can make our own `NonNan` wrapper type in `polars-utils`, which does this check on entry.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "Can this return `Option<bool>`? On `None` we don't know and we must continue. But on `Some<false>` we could already error early."
      ],
      "polars-optimize-memory-allocation-patterns": [
        "Maybe we can hoist the writer out of the while loop?"
      ],
      "polars-prevent-deadlock-conditions": [
        "We should use the `enter_polars` which handles the `allow_threads`."
      ],
      "polars-explicit-null-handling": [
        "This is wrong. If our sum doesn't ignore nulls, it doesn't propagate them, but replaces them with the identity: 0.\r\n\r\nThe horizontal semantics should be the same as the vertical semantics.",
        "I mean that our `sum` is agnostic to nulls. I think we made a mistake exposing this to `sum_horizontal` as our vertical sum is agnostic to nulls.",
        "Yeah, I think you're right. Consider it an observation. ;) Will take a look a bit later. "
      ],
      "polars-defer-expensive-operations": [
        "I don't think we need an extra benchmark for this.",
        "No, but the goal isn't to hit everything with benchmarks. ",
        "I understand what benchmarks do. :) I think the change is good, but I want to get rid of the in-repo benchmarks, I am not happy with them on the shared runners. We have on-premise benchmarks running. \r\n\r\nCan you remove this?"
      ],
      "polars-extract-duplicated-code": [
        "Can we move this to a separate function so that implementation is separate from dispatch?",
        "Can we add a `schema: Option<Schema>` argument to `prepare_expression_for_context` to reduce duplication.",
        "This is also converted in `IR::Scan` can we factor this out into a function?",
        "Can we do that in this PR? \r\n\r\nMake a function and factor out the shared arguments?",
        "I believe this code is exactly the same as in collect. Can we put it in a function?"
      ],
      "polars-favor-clarity-over-brevity": [
        "Let's make this required keyword arguments:\r\n\r\n`self, * , dtype: PolarsDataType | type[Any], endianness: Endianness = \"little\"`"
      ],
      "polars-feature-flag-compatibility": [
        "The feature is `bigidx`, not u64 idx in Rust."
      ],
      "polars-consistent-naming-standards": [
        "`use_abs_path`.\r\n\r\nWe have a convention that separate words are `_` separated. ",
        "Can we use python `None`  directly (so not part of this set.\r\n\r\nThe argument then becomes `arg: MaintainOrder | None`\r\n\r\nPS. I also think we should name it `MaintainOrderJoin`"
      ],
      "polars-names-reveal-clear-intent": [
        "Given that our schema conflicts with the catalog schema definition. Shall we name it `create_namespace` and mention in the docstrings that we mean catalog schema's for that?\r\n"
      ],
      "polars-hide-implementation-details": [
        "We call that the shape in public API of Polars:\r\n\r\nhttps://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Array.html\r\n\r\nNit:\r\n\r\nI also would not mention that array's are sequentially nested. I want users to think of NDArrays. That we sequentially nest them is an implementation detail.",
        "The Polars public API strongly prefers full names. So this should be `\"ir\", \"physical\"`. (I think IR is common enough to not write it out. :) )"
      ],
      "polars-prevent-cryptic-errors": [
        "The guard position should be at the generic location. I believe that is in `_parse_inputs_as_iterable`.",
        "Yes, that's good. In `select` we also should not accept dictionaries."
      ]
    },
    "profile": {
      "location": "Utrecht",
      "blog": "https://www.ritchievink.com",
      "site_admin": false,
      "followers": 1562,
      "following": 43
    }
  },
  "spytheman": {
    "repos": [
      "vlang/v"
    ],
    "entries": [
      {
        "slug": "v-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "v-avoid-breaking-api-changes",
        "title": "Avoid breaking API changes"
      },
      {
        "slug": "v-avoid-expensive-repeated-operations",
        "title": "avoid expensive repeated operations"
      },
      {
        "slug": "v-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "v-clear-technical-writing",
        "title": "Clear technical writing"
      },
      {
        "slug": "v-comprehensive-validated-examples",
        "title": "comprehensive validated examples"
      },
      {
        "slug": "v-configure-socket-blocking-behavior",
        "title": "Configure socket blocking behavior"
      },
      {
        "slug": "v-document-algorithm-behavior",
        "title": "Document algorithm behavior"
      },
      {
        "slug": "v-document-cryptographic-requirements",
        "title": "Document cryptographic requirements"
      },
      {
        "slug": "v-explicit-cryptographic-parameters",
        "title": "explicit cryptographic parameters"
      },
      {
        "slug": "v-explicit-null-checks",
        "title": "explicit null checks"
      },
      {
        "slug": "v-function-documentation-standards",
        "title": "function documentation standards"
      },
      {
        "slug": "v-improve-code-clarity",
        "title": "Improve code clarity"
      },
      {
        "slug": "v-informative-error-messages",
        "title": "informative error messages"
      },
      {
        "slug": "v-runtime-configurable-defaults",
        "title": "Runtime configurable defaults"
      },
      {
        "slug": "v-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "v-use-prod-for-performance",
        "title": "use `-prod` for performance"
      }
    ],
    "comments": {
      "v-function-documentation-standards": [
        "The new public function should be documented with a comment, that `v doc` can understand.\r\nUse something like this above the header:\r\n```v\r\n// get_queryset returns ...\r\n```",
        "It is only important for `v doc`, that it starts with the name of the method, the explanation after that can be whatever makes sense.",
        "```suggestion\r\n// gen_print_reg writes a string of size n stored in r to fd\r\n```",
        "I agree.",
        "@kimshrier what do you think?",
        "That is understandable, please excuse me for adding to your cognitive load.\r\n\r\nI think we can delete that comment for now, as @blackshirt suggested, and revise it later if needed.",
        "What is index in this context, a line, a byte offset from the start, or a utf character offset from the start?\r\n\r\nImho, rewrite the comment to be more informative, instead of just expanding on the name of the field.",
        "It is better to not have any comments, than ones that just repeat the information from the names."
      ],
      "v-improve-code-clarity": [
        "Those methods have no utility at all, since they just delegate to methods of the underlying field, that could be just accessed directly, if it is public.\r\n\r\nThe code of the implementation also seems extremely verbose for what it does.\r\n```v\r\n\tif s.status.is_valid() {\r\n\t\treturn true\r\n\t} else {\r\n\t\treturn false\r\n\t}\r\n}\r\n```\r\n^ that could be just: `return s.status.is_valid()` ...\r\n\r\n",
        "why is this defining a closure, just to call it right away?\r\nimho just inline the code",
        "imho there is no need for the second argument, since it essentially duplicates what `enable_echo`, but for each key check.\r\n\r\nThat `state.c_lflag &= ~C.ICANON` can be moved inside enable_echo(false) too.",
        "I also think that it will be clearer and easier for maintenance, if we have 2 functions:\r\n`pub fn key_pressed() int {`\r\nand\r\n`pub fn key_pressed_blocking() int {`\r\ninstead of a single one, with a bool argument to choose it - the places where you want each variant, are imho different, and having a mandatory boolean argument for each callsite does not seem very ergonomic to me.",
        "This is computed over 150 lines above the place where it is used.\r\nImho move it below, near the if condition that uses it.",
        "Is there a reason why it should miss an argname ?\r\nIn all existing cases, it produces one, and imho that makes reading the generated code and the potential errors that C compilers produce, much easier.\r\n\r\nImho just always write the name.",
        "It will also eliminate the bool argument to write_fn_ptr_decl , which is true in all currently existing cases, and simplify the PR diff.",
        "`@FN` is already a string, there is no need to interpolate it."
      ],
      "v-document-cryptographic-requirements": [
        "```suggestion\r\n// You should make sure, the seed bytes come from a cryptographically secure random generator,\r\n```"
      ],
      "v-configure-socket-blocking-behavior": [
        "```suggestion\r\n\t\t$if !net_nonblocking_sockets ? {\r\n\t\t\tconn.set_blocking(true)!\r\n\t\t}\r\n```",
        "`-d net_nonblocking_sockets` will not be defined by default, but will provide a way for people to opt in for the old behavior for their old projects, if needed.",
        "```suggestion\r\n\t\t$if !net_nonblocking_sockets ? {\r\n\t\t\tconn.set_blocking(true)!\r\n\t\t}\r\n```",
        "```suggestion\r\n\t$if !net_nonblocking_sockets ? {\r\n\t\tres.set_blocking(true)!\r\n\t}\r\n```",
        "No, it should be `res.set_blocking(true)!` , the PR is changing the default.",
        "Note that there is inversion, it is `$if !net_nonblocking_sockets ? {`, not `$if net_nonblocking_sockets ? {` .\r\nIn other words, that code inside the comptime if body, will be executed only if `-d net_nonblocking_sockets` *is not* passed, i.e. by default, which is what we want."
      ],
      "v-document-algorithm-behavior": [
        "```suggestion\r\n\t// the two shared secrets (derived by Alice, and derived by Bob), should be the same\r\n```",
        "How big (in bytes) are the derived secrets?",
        "Can we print both in the example?",
        "```suggestion\r\n    // If you wish to generate another curve, use: `pbkey, pvkey := ecdsa.generate_key(nid: .secp521r1)!`\r\n```"
      ],
      "v-clear-technical-writing": [
        "```suggestion\r\nModules names in .v files, must match the name of their directory.\r\n```",
        "```suggestion\r\nbelong to the same module `abc`. They should also start with `module abc`.\r\n```",
        "```suggestion\r\nSo in `abc/def/source.v` the first line will be `module def`, and not `module abc.def`.\r\n```",
        "```suggestion\r\n`abc.def`\r\n```",
        "```suggestion\r\nRefering to a module symbol such as a function or const, only needs module name as prefix:\r\n```",
        "```suggestion\r\nA function, located in `abc/def/source.v`, is called with `def.func()`, not `abc.def.func()`\r\n```",
        "```suggestion\r\nA .v file `./abc/source.v` must start with `module abc`. All .v files in this directory \r\n```"
      ],
      "v-explicit-cryptographic-parameters": [
        "afaik, `hash_config` already has a default of `.with_recommended_hash`.\r\nWhy not just skip it?",
        "i.e.:\r\n```suggestion\r\n\tsignature := pvkey.sign(message_tobe_signed)!\r\n```"
      ],
      "v-use-descriptive-names": [
        "```suggestion\r\n// Note: implementation available only on macOS, Linux and Windows. Otherwise,\r\n```",
        "The name of the exact kernel is only slightly related; the popular name for the OS/platform is more important for the callers imho.",
        "```suggestion\r\n\tstr_dollar_needs_rcbr     []bool = []\r\n```",
        "`is_` is usually used as a prefix for bools, not stacks of them, in the V codebase, and the expectation is that you can `if x.is_something {` later",
        "Renaming `file` to `file_content` is good, but `path` to `dir`, given the usage is not, since write_file expects a path or file name, not a dir in its first argument.\r\n",
        "In general, it is better to have different names than a pair of `file`/`path` used as parameters, since that can be a bit confusing, because both can mean the same or not, depending on the context.\r\n\r\nI suggest using `folder` or `dir` here, instead of `path` or use `filename` instead of `file`.",
        "Why `gen_str`?\r\nIt is a public method. I think that a longer, and more descriptive name like `generate_patch` may be better.",
        "Why is this called `icon`, but the config name is `window_icon`?\r\nImho if you make both `icon`, it will be less confusing for people, similar to how the other properties are named the same.",
        "it should return `int`, not bool",
        "The signature is ignored by the compiler currently, but is used by tooling to show usage tips and to infer the result type of the method, so it should be correct.",
        "imho `mark_var_as_used` was a clearer name",
        "That is a bit misleading, since `u8` is also a type name, and saves only 2 characters. Please use the full module name `utf8` below, and remove the module alias.",
        "It would be clearer if `s` is the actual string literal, i.e. rename `l` to `s` or to `lit`.\r\nWhat is now `s` could be `sptr` for example.",
        "Use names in English for the fields. `name` for example is perfectly fine.\r\n\r\nThe code base would become an unmaintainable mess, if everyone starts using their local names/terms :-| (in the code).",
        "the name is `get_enum_type_idx`, it should return int; if you make it return ast.Type, change the name too",
        "change the name to `typ_idx`, it is misleading otherwise",
        "or just `idx`",
        "There are no maps anywhere: the input is an array, the output is an array, the body of the function also does not use maps.\r\n\r\nA better name could be `flatten_array`?"
      ],
      "v-runtime-configurable-defaults": [
        "Use instead:\r\n`const indexexpr_cutoff := os.getenv_opt('VET_INDEXEXPR_CUTOFF') or { '10' }.int()`\r\n\r\nThe reason that os.getenv is better, is that with $d() those will be fixed once `v vet` is compiled, while with getenv_opt, they can be changed more conveniently at startup/runtime in most shells, with just: `VET_INDEXEXPR_CUTOFF=99 v vet .` for example.",
        "These should be customizable.\r\nAn easy way for that is with something like:\r\n`const indexexpr_cutoff = os.getenv_opt('VVET_INDEXEXPR', '10').int()`\r\nThe negative is that setting env variables on windows in cmd for one off commands is clumsier.",
        "```suggestion\r\nconst buff_size = int($d('gg_text_buff_size', 2048))\r\n```",
        "I am not sure if 2048 is a good default. Running `examples/gg/minimal.v`, it seems to result in apps that use ~8MB more RAM by default on my machine, compared to the same but with 512 (on master):\r\n```\r\ndelian     14166  3.5  0.2 141112 40524 pts/0    Sl+  14:24   0:00          |       |       \\_ /home/delian/code/v/rrrrrrrrrrrrrr\r\n```\r\nvs\r\n```\r\ndelian     14082  3.6  0.3 166648 47672 pts/0    Sl+  14:23   0:00          |       |       \\_ /home/delian/code/v/rrrrrrrrrrrrrr\r\n```\r\n\r\n\r\n",
        "On the other hand, it is certainly better in the sense that apps will not have weird artifacts and disappearing texts in much more cases, and with $d(), now people can tweak it easily, if they need something smaller or bigger.",
        "There is no need for that CLI parsing complication. You can pass the non default difftool through an environment variable, say `VDIFF_TOOL`, and it would be propagated to all the code that needs it."
      ],
      "v-avoid-expensive-repeated-operations": [
        "hm, that is weird ... json.encode() is used here, but in the generated code, the error is inside `json__decode__option_int` which is not used by json.encode, but by json.decode, which is not used by the main program.",
        "To me, it seems like the fix is in the wrong place - it will ensure that the `builtin` `_option_none` is always available to cgen, no matter if it is used or not.",
        "Yes, we do the same for `_option_ok` and `_option_clone` too, and they are not used by most short programs either.\r\n\r\nI think, that we should work on improving markused, so that the use of options is tracked, and add those only in cases, when they are used as well.",
        "i.e. I can merge this as it is, at the cost of a small increase in cgen output for small programs, but if we tracked the use of options, we could avoid it.",
        "put that condition first, since it is cheaper to check, compared to the string comparisons and the function calls",
        "imho, cache `c.g.fn_addr.keys()` in `c`, since calling .keys() for each call can become expensive for longer programs.",
        "If these are not modified, they can be extracted as constants too.\r\nIt will avoid allocating them for each `block()` call.",
        "```suggestion\r\n\tgroup := gname[..gname_len].bytestr()\r\n```\r\n`.bytestr()` creates a copy too, i.e. the clone() is superfluous",
        "`expr.expr.str()` is called several times in the code, it may be beneficial to store it in a local variable",
        "That seems very weird - you have already written the `{`, and then you are cutting it back.\r\n\r\nWhy not just generate different code depending on the flag, instead of doing that maneuver? ",
        "You are doing `sb.str()`, which empties `sb`, followed by `sb.writeln(fn_header)` which fills it back up.\r\nIt is needless copying.",
        "At the very least, you can call `sb.bytestr()` to get a string, without emptying it :-( .",
        "And you can do that before line 4415, so that you would not have to do the .all_before() etc.",
        "`strings.Builder` is an alias to `[]u8`, and has all the methods that are defined on `[]u8` too.",
        "why 50_000 ?",
        "I was just curious, because of the round number. You are right that it can be adjusted later if needed to avoid reallocations for the common cases.",
        "imho cache the information in another field in `ast.Struct`, about whether there are any option fields in a struct, during parsing, so that it is done just once, and then just use it here, instead of doing the loop for `any` potentially many times for each expression.",
        "```suggestion\r\n\tmut sb := strings.new_builder(s.len)\r\n```",
        "Each \\n is replacing a space (or several) -> s.len will be probably very close to the final result length.",
        "Can this check be done only inside __global declarations ?",
        "In this case, please add test cases for some of the other usages.",
        "What I do not like about this, is that it does relatively heavy operations for each cast, while the error and the results of those operations will be used only in a very limited amount of cases.\r\n\r\nAt the very least, the `c.expected_type.has_flag(.generic)` check should be done first, since it is fast.\r\n",
        "You could probably also use `to_sym.name` directly, not calling `c.table.type_to_str` just to check the first non `&` character in the result.",
        "After the .type_to_str() call, you are calling `.replace('&', '')`, giving you the same as `to_sym.name` afaik.",
        "A special case like this here:\r\n`if name == 'UTF-8' { return 65001 }` \r\nput before anything else, would be beneficial since half of the checks will be for it."
      ],
      "v-add-comprehensive-test-coverage": [
        "Please add new test functions, instead of modifying the existing ones, unless there is a bug. That makes reviewing a lot easier.",
        "Please, also add another test_ function, that sets the mutable parameter to a value != none, and asserts that after calling the function too.",
        "Please add assertions too.",
        "No. Keep the existing test, and add your own if you want.\r\nReplacing things like this is bad - it removes coverage for the existing functionality of `json.raw_decode`.\r\nIt is also very sloppy - the test name is still `fn test_raw_decode_string() {` ...\r\n\r\nJust keeps things working as much as possible,\r\nand add *new tests* please.",
        "Same - keep the current code, and add *new* tests/asserts.",
        "If it acts the same, even better - decode 2 times, once the old way, and once the new way, and compare the results.",
        "this lacks tests",
        "public functions should preferably have their own tests",
        "Please also add a case for comparing `''` and `''` and also for `''` vs non empty string, and the reverse.",
        "same, please also add assertions for the cases with empty strings",
        "0 is not a good value for testing, because the default initialisation also uses 0s.\r\nUsing another value like 1, 3, or 42 makes the test more sensitive/less likely to continue to work, if eventually a value was overwritten (wrongly) by 0s in the implementation.",
        "Please keep the old _test.v file, and add a new one.\r\n\r\nRefactoring the old one (not just adding new test cases) has a non-trivial chance of masking/changing what it tested before.",
        "Please put some assertions against the contents of `g.str()` and `t.str()` too.",
        "(something like `assert g.str().contains('a: 100')` etc)"
      ],
      "v-avoid-breaking-api-changes": [
        "That is a breaking change. Is it not needed anymore?\r\nThe commented example above suggests otherwise.\r\nCan you extract the example as a separate _test.v file, that does not start with `module ecdsa`, and thus can only access its public API?",
        "It can be added in example/ecdsa_seed_test.v, but then it will be less clear about what is it demonstrating, and linking to it specifically from the comments here will be a bit harder. That said, if you prefer it that way - sure.",
        "This is a breaking change too - `new_xof_digest` was public, and  is now both changed in name, and private.",
        "Imho keep the new private function, but also add back the public one as it was + its comments, just calling the private one."
      ],
      "v-explicit-null-checks": [
        "Please assert that at least green_arena.data is not nil, and that green_arena.head is not nil .",
        "I think that it needs `defer { C.XCloseDisplay(display) }` too.",
        "`XOpenDisplay` is also documented to return null on failure, that should be checked too",
        "Wow ... that is a good find ... it may be better to turn `fn_decl` into `?&ast.FnDecl` to force the compiler into checking all places, where it is used unguarded",
        "although that may have a non trivial performance impact",
        "set current.next to nil right after this line",
        "You are doing `free(current)` on the next line. It will not affect the loop."
      ],
      "v-comprehensive-validated-examples": [
        "this is now out of order, and if you remove the `oksyntax` tag, `v check-md` will fail (or if the user copy/pastes it in the playground/editor).",
        "One of the major benefits of self sufficient examples, is that with them you get this:\r\n![image](https://github.com/user-attachments/assets/5607c538-074b-4c56-899c-594d0319e5d7)\r\non https://docs.vlang.io/hello-world.html .",
        "And then, it is trivial to play with them, which makes learning V easier for newbies.",
        "These were more informative compared to a whole example for just `.try_pop()`, with a comment that duplicates the code ...\r\nRemove the new example, and restore the old code.",
        "No, it is not the same information at all. The code sample before showed several channel features, while later in your example, shows just this, that is related to the channel use:\r\n```v\r\nres := ch.try_pop(mut b) // try to perform `b = <-ch`\r\n```\r\n\r\nDo delete the new code, and restore the old in this particular case, I do not want to argue about it.",
        "The other examples are mostly fine, but this one loses information, that is important for understanding how to use channels.",
        "Instead of `codeblock`, use `v` here.\r\nThe benefit is that the tool `v check-md .` will then check the example on the CI,\r\nand that way it will be kept valid, and also consistent with the rest of the V examples in documentation (formatted, vetted, etc)."
      ],
      "v-choose-efficient-data-structures": [
        "there is no need for this, if you are only going to use the push/pop operations for the Stack - the builtin arrays are enough",
        "is not it a bit better if the integer comparison is before the more complex string one, due to the short-circuiting behavior of boolean expressions? ",
        "```suggestion\r\n\tmain_set map[T]bool\r\n```\r\n\r\nFor most types, like strings or pointers etc, sizeof(bool) < sizeof(T), and the map here is only used for its keys afaik.",
        "```suggestion\r\n\treturn g.main_set.keys()\r\n```",
        "imho do the intermediate calculations with i64, and only do one check at the end of the loop, right before the return of the result",
        "I am concerned about the performance for the most common case, which is currently 64 bit machines.",
        "imho use i64 (or just one power of 2 higher size if you prefer), for the cases of numbers that are < 64 bit. See also `vlib/strconv/utilities.v` and `vlib/strconv/structs.v`, for the already defined operations on `Uint128` for the 64 bit case.",
        "soft 64bit arithmetic on 32 bit machines, is still preferable in terms of performance than checking for overflows on each operation in the loop, generating errors, and then handling them in the caller (in the loop)",
        "That can get expensive for lots of files.\r\nUsing a map will scale better.",
        "Or you can just use `arrays.distinct()`.",
        "Why put `[]Any` first?\r\n\r\nThe first type is going to be used for default initialization of Any values, and []Any is a container, that is relatively more expensive to create/initialize compared to say int or bool . "
      ],
      "v-use-prod-for-performance": [
        "`-prod` should turn on `-O3 -flto`, so perhaps just this:\r\n```suggestion\r\n`v -cc gcc -prod -cflags \"-std=c17 -march=native -mtune=native\" .`\r\n```",
        "was there a problem without `-fno-inline-small-functions`?",
        "hm, that is good to know ... so the problem is triggered by something in it + gcc's -O3 🤔 "
      ],
      "v-informative-error-messages": [
        "what would you want them to be?",
        "That seems a bit unclear 🤔 \r\nCan the error also show the type that the compiler thinks `v` is too?\r\nSomething like this can be less puzzling:\r\n```\r\nvlib/v/checker/tests/match_generic_case_err.vv:16:4: error: return type mismatch, it should be `int`, but it is instead `string`\r\n```",
        "(not in the same PR, but in general)",
        "especially for generic code, I think it will help a lot",
        "Nice. If you keep information about the position of the opening brace, it can be included too.",
        "imho provide information in the error, that `]` was expected too",
        "adding the actual number, that was checked so far, in the error message, would be informative"
      ]
    },
    "profile": {
      "location": "Sofia, Bulgaria",
      "company": "DIA Soft",
      "blog": "http://blog.bulsynt.org",
      "site_admin": false,
      "followers": 334,
      "following": 325
    }
  },
  "anonrig": {
    "repos": [
      "cloudflare/workerd",
      "nodejs/node"
    ],
    "entries": [
      {
        "slug": "node-benchmark-before-optimizing-code",
        "title": "Benchmark before optimizing code"
      },
      {
        "slug": "node-document-non-intuitive-code",
        "title": "Document non-intuitive code"
      },
      {
        "slug": "node-document-with-precise-accuracy",
        "title": "Document with precise accuracy"
      },
      {
        "slug": "node-minimize-configuration-dependencies",
        "title": "Minimize configuration dependencies"
      },
      {
        "slug": "node-standardize-null-pointer-checks",
        "title": "Standardize null pointer checks"
      },
      {
        "slug": "node-use-modern-c-features",
        "title": "Use modern C++ features"
      },
      {
        "slug": "workerd-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "workerd-avoid-unnecessary-allocations",
        "title": "avoid unnecessary allocations"
      },
      {
        "slug": "workerd-choose-appropriate-logging-functions",
        "title": "Choose appropriate logging functions"
      },
      {
        "slug": "workerd-clear-descriptive-naming",
        "title": "Clear descriptive naming"
      },
      {
        "slug": "workerd-compatibility-flag-consistency",
        "title": "compatibility flag consistency"
      },
      {
        "slug": "workerd-comprehensive-assertion-testing",
        "title": "comprehensive assertion testing"
      },
      {
        "slug": "workerd-defer-async-callbacks",
        "title": "defer async callbacks"
      },
      {
        "slug": "workerd-http-protocol-compliance",
        "title": "HTTP protocol compliance"
      },
      {
        "slug": "workerd-maintain-consistent-patterns",
        "title": "maintain consistent patterns"
      },
      {
        "slug": "workerd-minimize-memory-operations",
        "title": "minimize memory operations"
      },
      {
        "slug": "workerd-network-resource-state-validation",
        "title": "Network resource state validation"
      },
      {
        "slug": "workerd-nodejs-api-compatibility",
        "title": "Node.js API compatibility"
      },
      {
        "slug": "workerd-optimize-ci-resource-usage",
        "title": "optimize CI resource usage"
      },
      {
        "slug": "workerd-prefer-nullish-coalescing-operators",
        "title": "prefer nullish coalescing operators"
      },
      {
        "slug": "workerd-prioritize-code-clarity",
        "title": "prioritize code clarity"
      },
      {
        "slug": "workerd-prioritize-descriptive-naming",
        "title": "Prioritize descriptive naming"
      },
      {
        "slug": "workerd-use-appropriate-api-methods",
        "title": "Use appropriate API methods"
      },
      {
        "slug": "workerd-use-appropriate-exception-types",
        "title": "Use appropriate exception types"
      },
      {
        "slug": "workerd-wrap-throwing-operations",
        "title": "Wrap throwing operations"
      }
    ],
    "comments": {
      "workerd-add-explanatory-comments": [
        "`this.finished` and other attributes which is deprecated in types/node package.",
        "You're right. A comment will be nice. I'll add."
      ],
      "workerd-compatibility-flag-consistency": [
        "Can we explicitly define these values that we use?",
        "I'll address this in a follow up. I've added a TODO to the compatibility_flags.d.ts file"
      ],
      "node-document-with-precise-accuracy": [
        "This avoids only for comments. This is actually to avoid inconsistencies. I've found places where default values in jsdoc and code doesn't match at all. "
      ],
      "workerd-optimize-ci-resource-usage": [
        "We're building it with custom flags (such as --config=benchmark), if we don't have a custom artifact, it will increase the duration of the runner. Are we ok with that? ",
        "If we want to reduce the duration, we should add a large runner as well: https://docs.github.com/en/actions/using-github-hosted-runners/using-larger-runners/managing-larger-runners#adding-a-larger-runner-to-an-organization\r\n\r\n@danlapid I believe you are the only one who can add this.",
        "Ideally it changes compile flags. It should run proper optimization flags etc. ",
        "Unfortunately not.\r\n\r\n```\r\n# configuration used for performance profiling\r\nbuild:profile --config=thin-lto\r\nbuild:profile --copt=\"-fno-omit-frame-pointer\" --copt=\"-mno-omit-leaf-frame-pointer\"\r\nbuild:profile --config=limited-dbg-info\r\nbuild:profile --strip=never\r\n\r\n# configuration used for performance benchmarking is the same as profiling for consistency\r\nbuild:benchmark --config=profile\r\n```",
        "Can we use 22 since v22 is LTS now?"
      ],
      "workerd-wrap-throwing-operations": [
        "I updated the code to imitate Node.js \r\n\r\n```js\r\n  async #onRequest(request: Request): Promise<Response> {\r\n    const { incoming, response } = this.#toReqRes(request);\r\n    try {\r\n      this.emit('connection', this, incoming);\r\n      this.emit('request', incoming, response);\r\n      return await getServerResponseFetchResponse(response);\r\n    } catch (error: unknown) {\r\n      response.destroy(error);\r\n      throw error;\r\n    }\r\n  }\r\n```"
      ],
      "workerd-nodejs-api-compatibility": [
        "Agreed. I'll update this.",
        "We can omit the getter (which will throw a typescript error, since we have \"implements _IncomingMessage\"), or have this getter throw an error of ERR_METHOD_UNIMPLEMENTED (which will fail if we do console.log(incoming).\r\n\r\nI think there is no harm in having this.",
        "I'm closing this for now, but LMK if this is a deal-breaker for you (by unresolving)",
        "Nice. `const headers = new Headers(sentHeaders);` works.",
        "ExportedHandler is not available here, but I've changed the name of the function."
      ],
      "workerd-choose-appropriate-logging-functions": [
        "Thanks! Updated."
      ],
      "workerd-avoid-unnecessary-allocations": [
        "Right now, we always create a new ReadableStream even though we override it in the next statement. This would avoid creating unnecessary readable streams.\r\n\r\n```suggestion\r\n    return options?.encoding === 'base64' ?\r\n      stream.pipeThrough(createBase64EncoderTransformStream()) : (this.bindingsResponse.body || new ReadableStream());\r\n```",
        "You can use the following since encodeArray returns an ArrayBuffer, which will be faster afaik:\r\n\r\n```\r\nnew Uint8Array(buffer, byteOffset, length)\r\n```",
        "This also applies to other places"
      ],
      "workerd-defer-async-callbacks": [
        "It's not needed since the response completion is already on the next tick.",
        "Nice catch, we need to do it in next tick."
      ],
      "node-document-non-intuitive-code": [
        "Since the method names are not easily readable, it is extremely hard to understand what this method does. Can you add a comment explaining what this method do, and how it does what it does?",
        "Can you add a documentation to here?\r\nReturning the `else` statement early will make this function much more readable.",
        "Can you add a comment to here explaining why you're checking for [?",
        "Yes, but a year from now, a contributor might have some hard time understanding the reasoning. "
      ],
      "node-use-modern-c-features": [
        "I recommend always using std::string_view since you'll always know when you'll copy (by calling std::string(val))\r\n\r\n```suggestion\r\n  const auto get_sections = [](std::string_view path) {\r\n```"
      ],
      "workerd-prioritize-code-clarity": [
        "Why do we need !!? If requireEsm has bool operator if (requireEsm) would be sufficient I believe?",
        "Nice. Thanks for the recommendation."
      ],
      "workerd-use-appropriate-exception-types": [
        "This should be DOMException not DOMDataCloneError\r\n\r\n> If [this](https://webidl.spec.whatwg.org/#this)'s [closed flag](https://html.spec.whatwg.org/multipage/web-messaging.html#concept-broadcastchannel-closed) is true, then throw an [\"InvalidStateError\"](https://webidl.spec.whatwg.org/#invalidstateerror) [DOMException](https://webidl.spec.whatwg.org/#dfn-DOMException).",
        "Thanks. Yes, I'll update."
      ],
      "workerd-comprehensive-assertion-testing": [
        "I've added/updated tests"
      ],
      "workerd-maintain-consistent-patterns": [
        "I've used Object.create(null) because unenv uses that. In here node.js uses __proto__: null. I can make them both use __proto__: null.",
        "Can we move this to internal/globals.d.ts file and include it in Cloudflare/tsconfig so we don't have to import it on every place"
      ],
      "workerd-prefer-nullish-coalescing-operators": [
        "Agreed",
        "```suggestion\r\n  return this._eventsCount > 0 ? Reflect.ownKeys(this._events || {}) : [];\r\n```",
        "```suggestion\r\n    this.errno = options.errno ?? 1; // EPERM\r\n```"
      ],
      "node-minimize-configuration-dependencies": [
        "Why did we start using NPX? I don't think we should depend on npm, anymore than we should. ",
        "> We are using npx because \"node --run\" doesnt support running from a package.json in a different folder.\n\nThis looks like a good contribution opportunity and deserves to be a separate issue. (Or a requirement for this to land)",
        "> We tried numerous workarounds, had to use npx.\n\nPackage manager neutrality is one thing and also having a non-nodejs managed process running on top of every build process doesn't seem ideal to me. We don't explicitly know npx behavior or if in the future the behavior will change in a non-destructive way that slips through our reviews. For example, whether or not if npx creates any subdirectory in the operating system is somewhat unknown to me. Even if it's not right now, what guards us from detecting such filesystem changes for caching for example?\n\nWe should keep the implementation as simple as possible. We can achieve the same solution without using npx.\n\nIf you still require npx behavior and if it's the only possible solution, I am still interested in blocking and waiting for someone to implement \"change directory when executing\" behavior to node --run (which I'm the author)",
        "> Please don't make this a requirement. But I agree it would definitely be a good issue to be worked on :)\n\nI can just implement this if no one is interested in working on, if you give enough time. :-) ",
        "> We're not in a hurry anyways.\n\nCan someone open an issue for this and assign me?",
        "The current old tooling uses npm, but doesn't use npx. I prefer to wait, but I'll not block.",
        "This file should be surrounded by a ifdef of is windows. We don't need to compile this file for non-windows environments.",
        "This file should also be surrounded by isWindows ifdef"
      ],
      "workerd-clear-descriptive-naming": [
        "```suggestion\nexport function beforeRequest(module: Module): void {\n```\n",
        "registerPortHandler might be too limiting, if we want to add a feature to this function in the future. What about preRegisterHandler()?",
        "> Add features like what?\r\n\r\nHaha, I can't predict future too. I just personally prefer a method that doesn't contain port in it. Maybe we can have a poll in team chat and discuss this.",
        "I've updated the implementation to use `nodeCompatHttpServerHandler`"
      ],
      "workerd-minimize-memory-operations": [
        "Byte string is a kj::string, and kj::string constructor just moves the value. So technically there is no unnecessary allocation here. "
      ],
      "workerd-use-appropriate-api-methods": [
        "I looked at it once again, and start_exec() isn't the correct command.\r\n\r\nStart container requires building an image. How are we going to build an image? cc @mikenomitch "
      ],
      "node-standardize-null-pointer-checks": [
        "We mostly call CHECK_NOT_NULL macro for this particular case.\r\n\r\n```suggestion\r\n  CHECK_NOT_NULL(path);\r\n```",
        "Agreed. Updated the pull-request."
      ],
      "workerd-prioritize-descriptive-naming": [
        "We are already in node namespace, can we remove the extra Node prefix?\n\n```suggestion\nconstexpr kj::StringPtr getMessage(ExceptionCode code, kj::StringPtr message) {\n```\n",
        "You can achieve the same thing with node::ExceptionType"
      ],
      "node-benchmark-before-optimizing-code": [
        "this adds an additional item to the stack trace whenever this function is called. we should not call this method on non-windows to avoid polluting the stack trace.",
        "Also, can't this method be cached?"
      ],
      "workerd-network-resource-state-validation": [
        "This has a bug. If this port is entangled with another port, we should disentangle first. If not, port3 that is entangled with port1 will still be entangled with port1, even though port1 is entangled with port2.\r\n\r\nStep 1 of entangle spec step: https://html.spec.whatwg.org/multipage/web-messaging.html#entangle",
        "Also disentangle should emit \"close\" event which in this case it never emits."
      ],
      "workerd-http-protocol-compliance": [
        "Referencing node.js doc:\r\n\r\n> The numeric representation of the remote port. For example, 80 or 21. Value may be undefined if the socket is destroyed (for example, if the client disconnected).\r\n\r\nhttps://nodejs.org/api/net.html#socketremoteport",
        "Ah, now I see. But I'm not sure if we should return a random value or a fixed value in here. What do you think? @jasnell ",
        "Thanks. Thats definitely helpful! Sounds like a task for the good old AI buddy. "
      ]
    },
    "profile": {
      "company": "@cloudflare",
      "blog": "https://yagiz.co",
      "twitter_username": "yagiznizipli",
      "site_admin": false,
      "followers": 1913,
      "following": 126
    }
  },
  "kamilmysliwiec": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-benchmark-before-optimizing",
        "title": "Benchmark before optimizing"
      },
      {
        "slug": "nest-choose-meaningful-identifier-names",
        "title": "Choose meaningful identifier names"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "nest-document-configuration-behaviors",
        "title": "Document configuration behaviors"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-graph-based-dependency-management",
        "title": "Graph-based dependency management"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-modern-null-safety-patterns",
        "title": "Modern null safety patterns"
      },
      {
        "slug": "nest-optimize-critical-path-iterations",
        "title": "Optimize critical path iterations"
      },
      {
        "slug": "nest-package-dependency-configuration",
        "title": "Package dependency configuration"
      },
      {
        "slug": "nest-parameterize-version-requirements",
        "title": "Parameterize version requirements"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-preserve-api-interface-stability",
        "title": "Preserve API interface stability"
      },
      {
        "slug": "nest-preserve-public-api-stability",
        "title": "Preserve public API stability"
      },
      {
        "slug": "nest-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "nest-prevent-race-conditions",
        "title": "Prevent race conditions"
      },
      {
        "slug": "nest-proper-asynchronous-error-handling",
        "title": "Proper asynchronous error handling"
      },
      {
        "slug": "nest-secure-hash-algorithms",
        "title": "Secure hash algorithms"
      },
      {
        "slug": "nest-standardize-logger-configuration-patterns",
        "title": "Standardize logger configuration patterns"
      },
      {
        "slug": "nest-standardize-null-safety-patterns",
        "title": "Standardize null safety patterns"
      },
      {
        "slug": "nest-strategic-dependency-configuration",
        "title": "Strategic dependency configuration"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-structure-exception-handling-patterns",
        "title": "Structure exception handling patterns"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      },
      {
        "slug": "nest-use-secure-hash-algorithms",
        "title": "Use secure hash algorithms"
      },
      {
        "slug": "nest-use-topological-sorting",
        "title": "Use topological sorting"
      }
    ],
    "comments": {
      "nest-configurable-log-formatting": [
        "```suggestion\r\n  json?: boolean;\r\n```",
        "similarly to log levels, we should be able to specify (globally) that the logger should use `json` format for all logs by default ",
        "I think this line:\r\n\r\n```\r\n`${pidMessage}${this.getTimestamp()} ${formattedLogLevel} ${contextMessage}${output}${timestampDiff}\\n`\r\n```\r\nshould be configurable. We could declare another protected method that receives these variables as input parameters (without colors applied). Perhaps that's actually how the `formatMessage` should look likely (we can move everything else back to the original location)",
        "This approach https://github.com/stanimirovv/nest/commit/89d97652caafc26c379ae0268229c4fc3caf3cf2# better fits our needs but we should also allow opt-in coloring the messages. In this case, we should probably declare another dedicated method (let's say `colorize`) that takes the same set of arguments as `formatMessages` and adds colors. This method should be executed from within the `formatMessages` and have `protected` modifier to make it feasible to call it (if needed) from within the custom logger implementation.",
        "Yeah, I think as long as we make `getColorByLogLevel` protected as well (to make it accessible) it could take two arguments (one being a message to format and the second - optional - color to be used that defaults to `this.getColorByLogLever()`)"
      ],
      "nest-benchmark-before-optimizing": [
        "Wondering how fast it is compared to @napi-rs/blake-hash 🤔 \r\n\r\nAlso, on a side note, did you have a chance to compare https://github.com/Brooooooklyn/uuid to `@lukeed/uuid` (for generating uuids)?",
        "Why not filter + map? Performance should be marginal in this case (I'd think)",
        "Still, the performance impact should be negligible. filter & map is just cleaner & easier to read, not worth the optimization in this particular case "
      ],
      "nest-preserve-public-api-stability": [
        "This introduces a breaking change",
        "Updating a public interface with 2 additional methods that weren't there before introduces a breaking change as now every library (platform adapter) will have to update their `HttpServer` implementation as well (including these adapters that we don't control ourselves within the NestJS organization).\r\n\r\n\r\n\r\n",
        "We can alternatively update this method to be optional; in this case we won't have to wait "
      ],
      "nest-document-configuration-behaviors": [
        "```suggestion\r\n```\r\nRedis adapter intentionally wasn't being used in this example (it was meant to be simple and not require additional resources, e.g. Redis db)",
        "> My concern is that the adapter is included in the sample, but it is not clear how to actually use it.\r\n\r\nFair enough! Let's comment these lines out and leave a brief explanation just in case."
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "Can we wrap it within a `describe(\"valideteUser\")` block and then inside have to scenarios covered:\r\n\r\n- should return a user object when credentials are valid\r\n- should return null when credentials are invalid\r\n\r\nAnd likewise below with the `login()` method",
        "ping 🏓 ",
        "could you follow BDD style? Example:\r\n\r\n```typescript\r\ndescribe('when \"message\" is an object', () => { \r\n    it('should serialize an object', () => {\r\n    \r\n    });\r\n});\r\n```"
      ],
      "nest-modern-null-safety-patterns": [
        "Instead of adding a new condition here, we may just swap `||` expression with `??`.",
        "What do you think about using `isNil()` util function here instead? (to exclude `null` and `undefined` values)",
        "I think `isNil()` will be sufficient here :)"
      ],
      "nest-strategic-dependency-configuration": [
        "since we load this package lazily, i'd say we could probably declare `file-type` as a peer dependency instead",
        "we should also add the `peerDependenciesMeta` entry, flagging these deps as _optional_, see example here https://github.com/nestjs/graphql/blob/master/packages/apollo/package.json#L52"
      ],
      "nest-use-topological-sorting": [
        "This would introduce a major breaking change. Modules are supposed to be sorted by distance",
        "This change may lead to very tricky side-effects. Is there any way to somehow avoid this?",
        "What about circular dependencies? 😄 ",
        "What if the single module is imported by multiple modules that will override `distance` value several times?"
      ],
      "nest-follow-protocol-standards": [
        "Similarly to what Fastify did, should we expose the `forceCloseConnections` flag to control that behavior?",
        "Do we have any other ideas on how we could avoid introducing a breaking change? AFAIR `forceCloseConnections` is disabled by default (in Fastify) too.\r\n\r\nPerhaps we should expose a dedicated method at the adapter class level? So then you could do \r\n```typescript\r\nconst adapter = app.getHttpAdapter() as ExpressAdapter\r\nadapter.enableForceCloseConnections(); // adapter.forceCloseConnections = true; ?\r\n```\r\nOR maybe we should auto-enable it when someone calls `enableShutdownHooks()`?\r\n\r\n\r\n",
        "What makes this problematic is the fact that breaking change forces us to release a new major version (and so we'd have to postpone merging this PR a little) :( ",
        "SGTM ✅ ",
        "I'd say that we probably shouldn't introduce non-standard HTTP status codes. Better hardcode it inside a corresponding gRPC expection.",
        "Why do we presume that `TEMPORARY_REDIRECT` should be the default one?",
        "Make sense now :)"
      ],
      "nest-graph-based-dependency-management": [
        "This would introduce a major breaking change. Modules are supposed to be sorted by distance",
        "This change may lead to very tricky side-effects. Is there any way to somehow avoid this?",
        "What about circular dependencies? 😄 ",
        "What if the single module is imported by multiple modules that will override `distance` value several times?"
      ],
      "nest-use-secure-hash-algorithms": [
        "Let's do this (since it's faster)"
      ],
      "nest-structure-exception-handling-patterns": [
        "Not sure if I'm following. `process.exit` (force exit) shouldn't be necessary. We're also overriding the signal (the reason to kill the process) here",
        "This doesn't work even if you run `node dist/main` (no NestJS CLI involved)",
        "![image](https://github.com/user-attachments/assets/734ee5f9-ccbf-4330-a059-87879374a61e)\r\n\r\nCode \r\n\r\n```js\r\nconsole.log('Hello Node.js');\r\n\r\nprocess.on('exit', () => {\r\n  console.log('about to exit');\r\n});\r\n\r\nsetTimeout(() => {\r\n  console.log('killing');\r\n  process.kill(process.pid, 'SIGTERM');\r\n}, 1000);\r\n```\r\n\r\nNode v20",
        "Ah apologies, I forgot to include on SIGTERM listener @thomaschaaf ",
        "I still cannot reproduce your issue with the following code:\r\n\r\n```ts\r\nimport { NestFactory } from '@nestjs/core';\r\nimport { AppModule } from './app.module';\r\n\r\nasync function bootstrap() {\r\n  const app = await NestFactory.create(AppModule);\r\n  app.enableShutdownHooks();\r\n  await app.listen(3000);\r\n\r\n  console.log(`Application is running on: ${await app.getUrl()}`);\r\n\r\n  process.on('SIGTERM', async () => {\r\n    await app.close();\r\n    console.log('SIGTERM received');\r\n    process.kill(process.pid, 'SIGTERM');\r\n  });\r\n\r\n  process.on('exit', () => {\r\n    console.log('about to exit');\r\n  });\r\n\r\n  setTimeout(() => {\r\n    console.log('killing');\r\n    process.kill(process.pid, 'SIGTERM');\r\n  }, 1000);\r\n}\r\nbootstrap();\r\n```\r\n\r\nWith `node dist/main`:\r\n\r\n```\r\nApplication is running on: http://[::1]:3000\r\nkilling\r\nSIGTERM received\r\nabout to exit\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/63336e26-2562-456d-8893-e4f3a46adb64)\r\n\r\nWith `nest start`\r\n\r\n![image](https://github.com/user-attachments/assets/dd1829a9-f323-4ab6-b77c-e3fdebdb65f7)\r\n\r\nSame with `nest start --watch` (npm run start:dev)\r\n\r\n![image](https://github.com/user-attachments/assets/446e55d1-44f2-4a22-89e6-11d61279fe50)\r\n\r\n",
        "I can replicate the exact same behavior using plain Node.js (without the Nest framework/CLI), so I'm not sure if there's anything we need to change on our end.",
        "Wouldn't it be easier to define an object in which keys represent status codes and their values = classes of corresponding exceptions? e.g. `new HttpErrorByCode[statusCode](error)`?",
        "This:\r\n\r\n```typescript\r\nexport const HttpErrorByCode = {\r\n  [HttpStatus.BAD_REQUEST]: BadRequestException,\r\n  [HttpStatus.UNPROCESSABLE_ENTITY]: UnprocessableEntityException,\r\n```\r\nand:\r\n```typescript\r\nthis.exceptionFactory = (errors => new HttpErrorByCode[this.exceptionCode](errors));\r\n```\r\nshould be enough :) Any additional method/function which leverages `HttpException` is not needed. In addition, we shouldn't allow using `OK` or `CREATED` status. Let's just define an union with error status codes that can be used, e.g.:\r\n\r\n```typescript\r\nexport type ErrorHttpStatusCode = HttpStatus.BAD_REQUEST | HttpStatus.UNPROCESSABLE_ENTITY | etc.\r\n```\r\n"
      ],
      "nest-proper-asynchronous-error-handling": [
        "If this condition `err instanceof KafkaRetriableException && !isPromiseResolved` is true then Promise will both reject and resolve (which technically doesn't make much sense)",
        "Although I like the idea of introducing a new method that can be overridden, it's a breaking change, so we'd have to wait to merge this PR until the next major release. Hence, for the time being, I'd instead suggest just wrapping this logic in try..catch blocks.",
        "Sounds good to me @wSedlacek! Thanks for your work on this one 🙌 ",
        "I don't think that we should call both `.write()` and `.emit('error')` on each error."
      ],
      "nest-prevent-race-conditions": [
        "```suggestion\r\n      Object.assign(args[0] ?? {}, {\r\n        getPattern: () => this.reflectCallbackPattern(callback),\r\n      });\r\n```\r\nHmm.. I'm wondering if this won't cause some unexpected issues 🤔 If socket represents a single connection (and so its instance object is shared?), there's a probability that 2 messages from that socket might be processed asynchronously (independently). \r\n\r\nExample:\r\n- Connection to server is established (socket instance is created)\r\n- Message A is emitted \r\n  - we mutate the \"socket instance\" enhancing it with the \"getPattern()\" method\r\n  - before this message is processed, some asynchronous operation is triggered in (guard/interceptor/wherever)\r\n- In the meantime, message B is emitted\r\n  - we mutate the same \"socket instance\" again replacing the previous \"getPattern()\" method\r\n- Async operation (mentioned above) completes and we're back to processing message A.\r\n- In another interceptor/guard we call `client.getPattern()` when processing message A but it's already overridden with the message B's \"getPattern\" implementation\r\n\r\n",
        "If someone unsubscribes before this promise resolves, a memory leak would occur (`this.routingMap.set(packet.id, callback);`)",
        "@guiruiz it wouldn't because `serialize` isn't async and so `this.routingMap.set` is called synchronously.",
        "If someone applies 1 middleware multiple times, we should run it multiple times = no side-effects."
      ],
      "nest-use-consistent-control-structures": [
        "To remain consistent with the rest of the codebase\r\n```suggestion\r\n    if (areThereNoFileIn && this.fileIsRequired) {\r\n      throw this.exceptionFactory('File is required');\r\n    }\r\n    if (!areThereNoFileIn && this.validators.length) {\r\n      await this.validateFilesOrFile(value);\r\n    }\r\n```",
        "even though `void 0` is not necessarily required, please, let's stick with `{ ... }` at least"
      ],
      "nest-pin-dependency-versions": [
        "These may quickly get out of date - we will keep supporting v16 even after it's no longer a maintenance version (after September this year). That's why versions were hardcoded before "
      ],
      "nest-http-header-management": [
        "Why do we presume that `TEMPORARY_REDIRECT` should be the default one?",
        "Make sense now :)"
      ],
      "nest-use-consistent-curly-braces": [
        "To remain consistent with the rest of the codebase\r\n```suggestion\r\n    if (areThereNoFileIn && this.fileIsRequired) {\r\n      throw this.exceptionFactory('File is required');\r\n    }\r\n    if (!areThereNoFileIn && this.validators.length) {\r\n      await this.validateFilesOrFile(value);\r\n    }\r\n```",
        "nit(style): can we use if { } instead of inline ifs? just for the sake of consistency with the rest of the codebase",
        "even though `void 0` is not necessarily required, please, let's stick with `{ ... }` at least"
      ],
      "nest-choose-meaningful-identifier-names": [
        "`should` word indicates here that this method should return a boolean (and shouldn't update anything itself) - it's a common convention (is/has/should)",
        "> Can we have shouldFlushLogsOnOverride: boolean, and flushLogsOnOverride(): void method instead?\r\n\r\nSounds great @micalevisk!",
        "Can we make argument names somewhat more descriptive? For example, `transportOrExtras, extras`, etc.",
        "Can we rename it to \"id\" instead?\r\n```suggestion\r\n  async findOne(@Param(\"id\") id: string): Promise<Cat> {\r\n```\r\nSimilarly in the method below and in the corresponding service class",
        "this name is not very descriptive I guess. what about `activeShutdownSignals` or something like this?"
      ],
      "nest-secure-hash-algorithms": [
        "Let's do this (since it's faster)"
      ],
      "nest-package-dependency-configuration": [
        "since we load this package lazily, i'd say we could probably declare `file-type` as a peer dependency instead",
        "we should also add the `peerDependenciesMeta` entry, flagging these deps as _optional_, see example here https://github.com/nestjs/graphql/blob/master/packages/apollo/package.json#L52"
      ],
      "nest-use-factory-providers": [
        "With this change, you won't be able to use multiple multer modules in your project (due to equal hash tokens)."
      ],
      "nest-standardize-null-safety-patterns": [
        "Instead of adding a new condition here, we may just swap `||` expression with `??`.",
        "What do you think about using `isNil()` util function here instead? (to exclude `null` and `undefined` values)",
        "I think `isNil()` will be sufficient here :)"
      ],
      "nest-descriptive-identifier-names": [
        "`should` word indicates here that this method should return a boolean (and shouldn't update anything itself) - it's a common convention (is/has/should)",
        "> Can we have shouldFlushLogsOnOverride: boolean, and flushLogsOnOverride(): void method instead?\r\n\r\nSounds great @micalevisk!",
        "nit: mergePacketOptions/mergeRequesOptions? (to not confuse it with the \"client\" instance options) cc @tuxmachine ",
        "this name is not very descriptive I guess. what about `activeShutdownSignals` or something like this?"
      ],
      "nest-optimize-critical-path-iterations": [
        "Why not filter + map? Performance should be marginal in this case (I'd think)",
        "Still, the performance impact should be negligible. filter & map is just cleaner & easier to read, not worth the optimization in this particular case ",
        "Let's move this line (check) outside the callback function as otherwise it would be executed per each invocation of the route. This comment applies to other changes too 🙌 "
      ],
      "nest-explicit-default-configurations": [
        "Instead of adding two extra class properties, you could use helper `getOptionsProp()` method (see example below) which also takes a default value.",
        "could be potentially removed? (just use `undefined` when not explicitly defined)"
      ],
      "nest-preserve-api-interface-stability": [
        "This introduces a breaking change",
        "Updating a public interface with 2 additional methods that weren't there before introduces a breaking change as now every library (platform adapter) will have to update their `HttpServer` implementation as well (including these adapters that we don't control ourselves within the NestJS organization).\r\n\r\n\r\n\r\n",
        "We can alternatively update this method to be optional; in this case we won't have to wait ",
        "This interface shouldn't be modified with unessential props (it's a public API)."
      ],
      "nest-parameterize-version-requirements": [
        "These may quickly get out of date - we will keep supporting v16 even after it's no longer a maintenance version (after September this year). That's why versions were hardcoded before "
      ],
      "nest-standardize-logger-configuration-patterns": [
        "```suggestion\r\n  json?: boolean;\r\n```",
        "similarly to log levels, we should be able to specify (globally) that the logger should use `json` format for all logs by default ",
        "I think this line:\r\n\r\n```\r\n`${pidMessage}${this.getTimestamp()} ${formattedLogLevel} ${contextMessage}${output}${timestampDiff}\\n`\r\n```\r\nshould be configurable. We could declare another protected method that receives these variables as input parameters (without colors applied). Perhaps that's actually how the `formatMessage` should look likely (we can move everything else back to the original location)",
        "This approach https://github.com/stanimirovv/nest/commit/89d97652caafc26c379ae0268229c4fc3caf3cf2# better fits our needs but we should also allow opt-in coloring the messages. In this case, we should probably declare another dedicated method (let's say `colorize`) that takes the same set of arguments as `formatMessages` and adds colors. This method should be executed from within the `formatMessages` and have `protected` modifier to make it feasible to call it (if needed) from within the custom logger implementation.",
        "Yeah, I think as long as we make `getColorByLogLevel` protected as well (to make it accessible) it could take two arguments (one being a message to format and the second - optional - color to be used that defaults to `this.getColorByLogLever()`)"
      ],
      "nest-prevent-async-race-conditions": [
        "I think I misunderstood what this FR was about.\r\n\r\nDurable providers should not have access to the request-specific attributes (we shouldn't merge the payload with the request) simply because they are **durable** (meaning they are not removed after the request is finished processing). Merging the payload with request objects for durable providers will lead to memory leaks and unexpected behavior of the framework",
        "```suggestion\r\n      Object.assign(args[0] ?? {}, {\r\n        getPattern: () => this.reflectCallbackPattern(callback),\r\n      });\r\n```\r\nHmm.. I'm wondering if this won't cause some unexpected issues 🤔 If socket represents a single connection (and so its instance object is shared?), there's a probability that 2 messages from that socket might be processed asynchronously (independently). \r\n\r\nExample:\r\n- Connection to server is established (socket instance is created)\r\n- Message A is emitted \r\n  - we mutate the \"socket instance\" enhancing it with the \"getPattern()\" method\r\n  - before this message is processed, some asynchronous operation is triggered in (guard/interceptor/wherever)\r\n- In the meantime, message B is emitted\r\n  - we mutate the same \"socket instance\" again replacing the previous \"getPattern()\" method\r\n- Async operation (mentioned above) completes and we're back to processing message A.\r\n- In another interceptor/guard we call `client.getPattern()` when processing message A but it's already overridden with the message B's \"getPattern\" implementation\r\n\r\n",
        "If someone unsubscribes before this promise resolves, a memory leak would occur (`this.routingMap.set(packet.id, callback);`)",
        "@guiruiz it wouldn't because `serialize` isn't async and so `this.routingMap.set` is called synchronously."
      ]
    },
    "profile": {
      "location": "Poland",
      "company": "@nestjs ",
      "blog": "https://kamilmysliwiec.com",
      "twitter_username": "kammysliwiec",
      "site_admin": false,
      "followers": 8266,
      "following": 1
    }
  },
  "arvinxx": {
    "repos": [
      "lobehub/lobe-chat"
    ],
    "entries": [
      {
        "slug": "lobe-chat-account-for-model-variations",
        "title": "Account for model variations"
      },
      {
        "slug": "lobe-chat-add-unit-tests",
        "title": "Add unit tests"
      },
      {
        "slug": "lobe-chat-api-parameter-consolidation",
        "title": "API parameter consolidation"
      },
      {
        "slug": "lobe-chat-choose-hooks-wisely",
        "title": "Choose hooks wisely"
      },
      {
        "slug": "lobe-chat-configuration-file-consistency",
        "title": "Configuration file consistency"
      },
      {
        "slug": "lobe-chat-configuration-merging-precedence",
        "title": "Configuration merging precedence"
      },
      {
        "slug": "lobe-chat-consider-ssr-impact",
        "title": "Consider SSR impact"
      },
      {
        "slug": "lobe-chat-consistent-naming-patterns",
        "title": "consistent naming patterns"
      },
      {
        "slug": "lobe-chat-contextual-error-handling",
        "title": "Contextual error handling"
      },
      {
        "slug": "lobe-chat-css-utility-usage",
        "title": "CSS utility usage"
      },
      {
        "slug": "lobe-chat-dependency-version-management",
        "title": "Dependency version management"
      },
      {
        "slug": "lobe-chat-environment-variable-access",
        "title": "Environment variable access"
      },
      {
        "slug": "lobe-chat-explicit-environment-declarations",
        "title": "Explicit environment declarations"
      },
      {
        "slug": "lobe-chat-explicit-type-checking",
        "title": "Explicit type checking"
      },
      {
        "slug": "lobe-chat-extract-reusable-components",
        "title": "Extract reusable components"
      },
      {
        "slug": "lobe-chat-model-specification-accuracy",
        "title": "Model specification accuracy"
      },
      {
        "slug": "lobe-chat-nextjs-auth-configuration",
        "title": "Next.js auth configuration"
      },
      {
        "slug": "lobe-chat-optimize-database-column-types",
        "title": "Optimize database column types"
      },
      {
        "slug": "lobe-chat-optimize-store-selectors",
        "title": "Optimize store selectors"
      },
      {
        "slug": "lobe-chat-pin-docker-base-versions",
        "title": "Pin Docker base versions"
      },
      {
        "slug": "lobe-chat-protect-sensitive-data",
        "title": "Protect sensitive data"
      },
      {
        "slug": "lobe-chat-provide-contextual-guidance",
        "title": "Provide contextual guidance"
      },
      {
        "slug": "lobe-chat-provider-based-interface-design",
        "title": "Provider-based interface design"
      },
      {
        "slug": "lobe-chat-respect-browser-behavior",
        "title": "Respect browser behavior"
      },
      {
        "slug": "lobe-chat-simplify-error-flows",
        "title": "Simplify error flows"
      },
      {
        "slug": "lobe-chat-use-idempotent-migrations",
        "title": "Use idempotent migrations"
      },
      {
        "slug": "lobe-chat-use-reactive-hooks",
        "title": "Use reactive hooks"
      },
      {
        "slug": "lobe-chat-use-semantic-naming",
        "title": "Use semantic naming"
      }
    ],
    "comments": {
      "lobe-chat-protect-sensitive-data": [
        "ip 是敏感信息字段，先不存吧",
        "`keyVaults` stores the user's own provider apikey information. In the server-side DB implementation, we will encrypt and store keyVaults\r\n\r\n\r\n`keyVaults` 中存储了用户填写的自己的 provider apikey 信息，在服务端 DB 实现中，我们会将 keyVaults 加密存储。"
      ],
      "lobe-chat-extract-reusable-components": [
        "这个实现不对， server 方法怎么能引入 client 的 `getDetailsToken` 方法呢？如果是计算 token 的话这个方法应该抽到 utils 去",
        "这里的 config type 应该单独提出去",
        "如果拿 'Origin File Not Found' 这个做判断，我感觉得把这个抽成一个变量出来，而不是两边都字符串处理。否则万一后面谁改了这里，那边的判断就没用了",
        "withTimeout 这个方法抽到 util 里吧，然后把 naive 的也改下",
        "version 提成单独的变量吧，不需要函数这里控制。\r\n\r\nconst VERSION = 1;\r\n\r\nconst genImage = (url: string) =>\r\n  BRANDING_LOGO_URL || qs.stringifyUrl({ query: { v: VERSION }, url });",
        "这部分 TTS 和 Image 的实现是否一模一样？一样的话感觉没必要重写一遍。",
        "这里的实现逻辑并不合理。system agent 和 embedding 是两个不同的业务功能模块，不应该混在一起。单独写一个 parseFileConfig"
      ],
      "lobe-chat-configuration-merging-precedence": [
        "should use userAvatar first and then fallback to next-auth",
        "这里有个问题，如果 appearance.elements 里有 footerAction ，那么会被后面那个 `{}` 覆盖。",
        "合并应该做在这里，还是应该做到更加底层的位置？"
      ],
      "lobe-chat-explicit-type-checking": [
        "if it's optional, no need the `|| ''`"
      ],
      "lobe-chat-provider-based-interface-design": [
        "是不是可以考虑下优化这个props 了？按照 provider 来构建对象？\r\n\r\n`ga4: false | {  measurementId:string }`",
        "If you want to display the model name, it's better to have a `provider` props to know the enabled provider. This lookup way will have confilct as the order.\r\n\r\nMaybe a optional props with provider,if have this value to just get the model card, or use this lookup way. ",
        "ollama 的实现建议单独做一个 Ollama Checker ，不要揉进通用 Checker 里， 或者把 check 方法做成一个props也行",
        "有必要的。第一次进去出错，然后本地起来，然后用户的直觉上是再去点下 check 。 虽然功能能满足，但是用户行为习惯是不一样的。就像Chrome 的收藏夹一样：\r\n\r\n<img width=\"487\" alt=\"image\" src=\"https://github.com/lobehub/lobe-chat/assets/28616219/bbd6f1e0-5b22-4573-b6f8-908f481387ac\">\r\n\r\n当你点击的时候其实已经完成收藏了，但大部分人的习惯还是会去点下「完成」来确认收藏。是个安慰剂的按钮\r\n"
      ],
      "lobe-chat-dependency-version-management": [
        "这个为啥换成 0.x",
        "@canisminor1990  像这种地方不应该改掉。应该用 `^4.4`\r\n\r\n原因是现在在用的 zustand  `createWithEqualityFn` 方法是 4.4 版本起才会有的。\r\n\r\n虽然 ^4 能安装到最新版。但丢失了最小所依赖版本的语义。\r\n\r\n如果有人用 pnpm 的 `resolution-mode` 为 lowest，装完这里运行就会报错。",
        "升级的话感觉可以不用锁版本， `^0.34.2`"
      ],
      "lobe-chat-api-parameter-consolidation": [
        "there is a util to get s3 url `getFullFileUrl`. please replace with this method",
        "这个 inputStartAt 放在和 callbacks 同级变量里，第二个入参做成obj就是为了后续能够扩展的",
        "都3个参数了，感觉可以把 search 的第二个参数改成 params ?这样未来有其他参数就不用再额外加了",
        "使用 urlJoin 方法，不要手动拼接。 urlJoin 会自动处理掉 baseURL 以 `/` 结尾的情况，更加健壮"
      ],
      "lobe-chat-use-idempotent-migrations": [
        "看下其他的几个表（比如 0029 ），加一下防御性的语法：\n\n```\nCREATE TABLE IF NOT EXISTS\n```",
        "```\nALTER TABLE \"messages\" ADD COLUMN IF NOT EXISTS\n```\n\n这样多次执行的时候就不会报重复的错误了",
        "这里防御性一下，`CREATE TABLE IF NOT EXISTS`",
        "1. 换成语义化的名字，比如 add_opening_guide.sql ，然后 meta/_journal.json 那里也记得对应改下\r\n2. 稍微做点防御性编程，参考 https://github.com/lobehub/lobe-chat/blob/main/src/database/migrations/0019_add_hotkey_user_settings.sql 这个，加一下 `IF NOT EXISTS`"
      ],
      "lobe-chat-simplify-error-flows": [
        "这里套两层error 有点恶心，Origin File Not Found 是不是只有在\n\n```ts\n      const { filePath, file, cleanup } = await this.fileService.downloadFileToLocal(fileId);\n```\n\n这里才会出现？如果是的话是不是可以只包到这个层面？\n",
        "如果只是类型变化，不应该变更这一处的功能实现吧",
        "不对的，后面的response.ok= false 就是为了将此处的错误 response 转变成json error。 你现在这样直接 throw error response 反而是改坏了"
      ],
      "lobe-chat-use-reactive-hooks": [
        "应该在这个方法里这么写：\r\n\r\n```ts\r\nconst inboxMessages = useChatStore.getState().messagesMap[messageMapKey(INBOX_SESSION_ID, activeTopicId)] || []\r\n```\r\n\r\n而这么写的话又太长了，我建议收一个 selector 就叫 inboxActiveTopicMessages 用于获取这个数据。然后就可以收敛成：\r\n\r\n```ts\r\nconst inboxMessages = chatSelectors.inboxActiveTopicMessages(getChatStoreState())\r\n```\r\n\r\n这样在 Inbox 组件也不会受到 inboxMessages 变更带来的重渲染影响\r\n\r\n同时 inboxActiveTopicMessages 这个取数逻辑在其他地方也能复用",
        "另外这个写法也是有问题的。\r\n\r\n`useAgentStore.getState()`  这个不是一个响应式的写法，取出来的值是fixed的，要响应式，得改成\r\n\r\n```\r\nconst config = useAgentStore(agentSelectors.currentAgentChatConfig)\r\n```",
        "理论上有这一行就够了，它的作用就等于  \r\n\r\nuseEffect(() => {\r\n    form.setFieldsValue(settings);\r\n  }, [settings]);\r\n\r\n建议检查下这部分的实现为何不生效？",
        "不要使用 useEffect，改成用 useSWR 来获取数据 "
      ],
      "lobe-chat-contextual-error-handling": [
        "为啥还需要单独再加 error？ 不是有了一个全局的了么",
        "你是说比如 session 还在，但是里面会话那个 children 部分显示 error 这种？",
        "<img width=\"560\" alt=\"image\" src=\"https://github.com/lobehub/lobe-chat/assets/28616219/dcb8a8cf-c83b-4edd-a190-508aa95c2084\">\r\n\r\n最好是上面的「请求 Ollama 服务出错，请根据以下信息排查或重试」这条信息改为「未找到该本地模型，请通过 Ollama 下载后重试」",
        "「拉取」 改为 「下载」",
        "<img width=\"421\" alt=\"image\" src=\"https://github.com/lobehub/lobe-chat/assets/28616219/53de3adf-e80a-4030-998d-52d017a3cb82\">\r\n\r\n这块样式我后续优化下，基础体验是 OK 的。要是可以的话后续最好把下载速度和预计剩余时间也加下。"
      ],
      "lobe-chat-provide-contextual-guidance": [
        "我觉得需要说明的是如果在 Vercel上部署需要添加该配置。如果使用 lobe-chat-database 则已经默认添加了该项配置",
        "嗯 可以",
        "可以",
        "just `[model providers](/docs/self-hosting/environment-variables/model-provider)`"
      ],
      "lobe-chat-configuration-file-consistency": [
        "This is not ideal. please add GITHUB_TOKEN as fallback",
        "这里 8000 有问题，和 env 里 `CASDOOR_PORT=8000` 冲突了，怪不得我本地一直起不来"
      ],
      "lobe-chat-nextjs-auth-configuration": [
        "next-auth/react 用动态加载吧？\r\n\r\nconst { signOut } = awat import(\"next-auth/react\")"
      ],
      "lobe-chat-consistent-naming-patterns": [
        "这里的对象都使用小驼峰，embeddingModel,queryModel,rerankerModel",
        "配置文件用蛇形，代码里的变量用小驼峰，这样能区分出来",
        "建议拆一下，这个 auth改名为 clerkAuth，然后再加一个 nextAuth",
        "  `emailVerifiedAt: timestamptz('email_verified_at'),`\r\n\r\n保持命名风格统一",
        "由于这里变成了一个初始化方法，我建议改名叫 `initAgentRuntimeWithUserPayload` ，这样有宾语会更加清楚"
      ],
      "lobe-chat-choose-hooks-wisely": [
        "我在想这里既然都是取的瞬时值，没有必要做成 useXXXX 的形式吧，直接\n\n`sessionSelectors.currentSession(getSessionStoreState())`\n\n就是最新状态，而且更加灵活并且不会造成 rerender。\n\n这个方法可以写成 `getMainInterfaceAnalytics` 然后放到 Analytics 组件里？",
        "感觉搜索态是不是应该在这， `{searchTopics: data,  isSearchingTopic : !!keywords }`"
      ],
      "lobe-chat-account-for-model-variations": [
        "移动端下这个值太小了，会导致无法自动滚动的。所以我在移动端下 x2 了",
        "@sxjeru 主要是输出比较快的模型 （比如 gpt-3.5 / sonnet / groq），一次很容易出很多，短时间内就无法自动滚动了",
        "这块的输出逻辑可以看下之前一个 pr 的讨论 https://github.com/lobehub/lobe-chat/pull/1197\n\n我们是有做一个匀速输出的缓存，并且在完成生成后加速输出",
        "感觉没必要加这个判断。直接简单粗暴来就好了吧\r\n\r\n\r\n```ts\r\nconst ktoken = Math.floor(model.tokens / 1000)\r\n\r\nif ( ktoken <1000 ) return `${ktoken}K`;\r\n\r\nreturn Math.floor(ktoken / 1000) + \"M\";\r\n```",
        "> 后来我想了下在6月之后可能会出现无限 token 的模型，因为 RAG 和长期存储中间层的原因。\r\n\r\nRAG 也不会是无限空间的啊？也还是有上下文会话窗口限制的"
      ],
      "lobe-chat-css-utility-usage": [
        "这个可以用 `cx(styles.promptBox,styles.animatedContainer)`, cx 从 useStyles 里引入。\r\n\r\n`const { styles, cx } = useStyles()`",
        "后续如果是简单的 css，不包含 token 的情况下，可以这么写\r\n\r\n```ts\r\nimport { css,cx } from 'antd-style';\r\n\r\nconst  extraTitle = css`\r\n      font-weight: 300;\r\n      white-space: nowrap;\r\n    `;\r\n\r\n\r\n<div className={cx(extraTitle)} style={{ fontSize: extraSize }}>\r\n\r\n```",
        "这个计算逻辑是啥？感觉有点复杂了？",
        "可做为兜底方案，想想看有没有更加优雅的解法？ `56px - 13px - gap` 这种计算会非常脆弱，未来万一改了顶部的布局，这个方案就失效了。",
        "好的，我看下",
        "这里不应该加 pointer 。 pointer 一般是点击才加，这个只是hover有 tooltip",
        "哦 好的"
      ],
      "lobe-chat-environment-variable-access": [
        "just `process.env.AUTH_GOOGLE_CLIENT_ID` , no need the authEnv.AUTH_GOOGLE_CLIENT_ID",
        "这里要用 `llmEnv.XXX`",
        "accountId 作为参数传给 server端。然后 server 端的 LobeCloudflareAI 初始化时候 baseURL 的逻辑变成 accountId 拼接的？",
        "Add the server API KEY environment variable here, there are usually two: `ENABLED_XXX` and `XXXX_API_KEY`\r\n\r\n在此处添加服务端 API KEY 环境变量，一般会有两个： `ENABLED_XXX` 和 `XXXX_API_KEY` 。",
        "不要使用 getEnvironment，目前我们是通过 `@t3-oss/env-nextjs` 这个包，它已经内置了你的 getEnvironment 逻辑。因此只需要直接在 knowledgeEnv 文件中定义 `USE_UNSTRUCTURED_FOR_PDF` 即可\r\n\r\nhttps://github.com/lobehub/lobe-chat/blob/c67fb3347fdef64d4d0b70665860d1a19e39b7f0/src/config/knowledge.ts",
        "直接用 process.env.xxx",
        "```\r\n   clientId: process.env.AUTH_WECHAT_ID,\r\n    clientSecret: process.env.AUTH_WECHAT_SECRET,\r\n```\r\n就是不需要前面的 authEnv 了。旧的 authEnv 是为了兼容以前的写法。新的 provider 用新的方案",
        "I think we don't need this env any more. Just Environment Variable Inference ( #3701 ) is enough",
        "I mean because of `Environment Variable Inference`, so we don't need to defined it in the `authEnv`, and we also don't need to write these, because it will be handled automately by next-auth\r\n\r\n```js\r\nclientId: process.env.AUTH_XXX_ID,\r\nclientSecret: process.env.AUTH_XXX_SECRET,\r\nissuer: process.env.AUTH_XXX_ISSUER,\r\n```\r\n\r\n",
        "不需要加 `process.env.XXX`，因为 next-auth 会自动读这个值",
        "env 需要配在这里 https://github.com/lobehub/lobe-chat/blob/main/src/config/app.ts 然后读"
      ],
      "lobe-chat-respect-browser-behavior": [
        "这个不能改成mod，cmd+1 和浏览器的行为是冲突的，只能是 ctrl",
        "window下现在 按 ctrl +1 能正常切？\r\n\r\n改成 mod 会导致 mac 下这个功能失效的，因为 cmd + 1 是浏览器行为，改成 mod， mac 下没法用了\r\n\r\n",
        "我发现还是不能用 server redirect … 线上会抛错… 得改回 client 的…\r\n\r\n<img width=\"1574\" alt=\"image\" src=\"https://github.com/lobehub/lobe-chat/assets/28616219/6aa3879f-6eae-4cac-bba3-923c1ccda0f0\">\r\n"
      ],
      "lobe-chat-optimize-database-column-types": [
        "这个 id 感觉可以直接用 indentity ，不需要text",
        "对",
        "这里 enum 不用配置。另外这种 type 可以用 varchar 255 来限定使用1字节存储",
        "rbac_roles 和 rbac_permissions  两个主键 id 如果没有别的语义要求的话，感觉可以直接换成自增的 Identity ？\r\n\r\n```ts\r\n    id: integer('id').primaryKey().generatedAlwaysAsIdentity(),\r\n```"
      ],
      "lobe-chat-model-specification-accuracy": [
        "这个也是， Instruct 模型特指微调用于对话的版本，不加 Instruct 一般是指基座模型",
        "这个确定不用加 0 4k 吗",
        "> 实际可用的只有 authropic.claude 全系，和 meta.llama 全系\r\n\r\n因为 Bedrock 的实现很恶心…它不是想 OpenAI 那种换个模型就好了的，而是每个模型都得各自实现一遍… 贼离谱\r\n\r\n我之前因为精力原因就只做了 claude 和 llama",
        "这几个不建议动，GPT-4 模型估计都还有不少人在用的",
        "这几个都不重复的。 gpt-3.5-turbo-16k 是自动更新模型，比如下一个版本出来可能是 gpt-3.5-turbo-16k-0725 ，那么 gpt-3.5-turbo-16k 就会指向 `gpt-3.5-turbo-16k-0725` ，而 gpt-3.5-turbo-16k-0613 仍然是指向的是 0613 这个版本的",
        "1.0 应该不需要了，我记得是后面以1.5为主的。1.0应该要废弃掉了？ https://github.com/lobehub/lobe-chat/pull/2860",
        "那我们按 1.5-flash 、1.5-pro 、1.0 这样的顺序来排吧。然后应该可以默认把其他 1.0 隐藏掉不显示。",
        "+1",
        "这个不能删。是有人专门 PR 进来的： https://github.com/lobehub/lobe-chat/pull/2406 。说明他们有在用",
        "本身这个模型就是Google 官方放在 list 里的，我们内置没啥问题，没有默认开启对普通用户的是没啥影响的。而且也可能只是你的 apikey 没有这个权限呢？之前 gemini-1.5-pro 也是只有一小部分 apikey 才有权限的。",
        "这个不对的，它的参数里写的是 maxInput，tokens 这里是指的整个上下文长度，包含 input 和 output。所以是maxInput + maxOutput"
      ],
      "lobe-chat-explicit-environment-declarations": [
        "这个不配置的话默认就是0呀？",
        "那我其实建议你开两个项目（两个项目文件夹），web 用 next-auth 的 env，然后再复制一个 desktop 的出来。我自己就是这么搞的",
        "I think a better way is to add a Callout to show the self-built docker image need this env? as most clerk user doesn't need to add this env",
        "这里应该填写完整的NextAuth 环境变量，英文文档部分也同样如此",
        "这些都都漏了 `AUTH_` 前缀\r\n\r\n我在想既然和 Next Auth 一样了，感觉这些就不用单独提了。要不都删掉吧，只保留Next Auth 里没有的"
      ],
      "lobe-chat-use-semantic-naming": [
        "不应该叫 `right` 吧，RTL 的怎么办？用 use 、assistant 这样命名更加易于理解",
        "<img width=\"137\" alt=\"image\" src=\"https://github.com/lobehub/lobe-chat/assets/28616219/ff2558b5-d096-4cc5-b379-2eafd1d938a7\">\r\n\r\n这个样式不太合理，title 应该和润色中区分开来的",
        "title 还是应该叫语音输入吧",
        "点击停止识别应该要加的，只是之前一期没有做。但这是两个问题，分开来看，这个 PR 可以不用管点击停止的问题。\r\n\r\n既然都要加标题，建议都统一在左上角，不应该在这里包到按钮里面"
      ],
      "lobe-chat-optimize-store-selectors": [
        "这里不能这么取，会导致当消息 inboxMessages 变更时触发这个 Inbox 组件的重渲染。而理论上inboxMessages 变更时不应该影响Inbox组件的渲染。",
        "这些需要写成 \r\n\r\n```ts\r\nconst enableCompressHistory = useAgentStore( s => agentChatConfigSelectors.currentChatConfig(s).enableCompressHistory) \r\n```\r\n\r\n否则会有性能问题",
        "```ts\r\nimport isEqual from 'fast-deep-equal';\r\n\r\nconst remoteModels = useUserStore(modelProviderSelectors.remoteProviderModelCards(provider), isEqual\r\n  );\r\n```\r\n\r\n remoteModels 是一个数组，因此需要这么写来避免 rerender "
      ],
      "lobe-chat-add-unit-tests": [
        "错误解析方法是否应单独提一个 util 出来然后单独写单测？",
        "这几个处理方法需要补一下单测",
        "这个文件看上去是 next-auth db 的核心逻辑模块，需要补充单测",
        "stream 的部分需要补单测，不加的话基本上过段时间就看不懂了。\r\n\r\n参考 https://github.com/hezhijie0327/lobe-chat/blob/bedrock_mistral/src/libs/agent-runtime/utils/streams/openai.test.ts ，加一个基础的 text 和一个 tools 的",
        "这一行得补一个单测",
        "那还需要多一个启用 trace 的单测了",
        "请参考 https://github.com/lobehub/lobe-chat/blob/main/src/libs/agent-runtime/google/index.ts#L135-L172 这里，创建一个私有方法来做数据结构的转换，这样会干净一些。然后需要补充一下对应的数据结构转换的单元测试"
      ],
      "lobe-chat-consider-ssr-impact": [
        "CloudBanner 这里不应该加 ssr:false 的，cloudBanner 在布局里有高度，这么写会在 ssr 的渲染时候没这个条幅。但页面加载完又出现。带来高度跳动",
        "这里感觉不需要 dynamic",
        "感觉如果是 modal 是独立 route 的话可能就不用 ssr 了？是不是可以加 `ssr:false` 了？"
      ],
      "lobe-chat-pin-docker-base-versions": [
        "可以把 node 基线版本拉到 22 了。 目前 22已经是 LTS，我在 preview 上跑了一段时间了没遇到什么问题。",
        "node:lts 是否会存在持续更新的问题？比如假设现在是20，到某个时间之后自动升级到 22 ？",
        "我建议基础镜像固定版本，这个自动升级可能会有破坏性更新的风险"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 1824,
      "following": 55
    }
  },
  "afc163": {
    "repos": [
      "ant-design/ant-design"
    ],
    "entries": [
      {
        "slug": "ant-design-api-evolution-strategy",
        "title": "API evolution strategy"
      },
      {
        "slug": "ant-design-avoid-hardcoded-configuration-values",
        "title": "Avoid hardcoded configuration values"
      },
      {
        "slug": "ant-design-avoid-unnecessary-memoization",
        "title": "Avoid unnecessary memoization"
      },
      {
        "slug": "ant-design-configuration-documentation-standards",
        "title": "Configuration documentation standards"
      },
      {
        "slug": "ant-design-consistent-dependency-formatting",
        "title": "consistent dependency formatting"
      },
      {
        "slug": "ant-design-dependency-versioning-consistency",
        "title": "Dependency versioning consistency"
      },
      {
        "slug": "ant-design-enforce-ci-workflow-gates",
        "title": "Enforce CI workflow gates"
      },
      {
        "slug": "ant-design-handle-optional-values-safely",
        "title": "Handle optional values safely"
      },
      {
        "slug": "ant-design-markdown-formatting-consistency",
        "title": "Markdown formatting consistency"
      },
      {
        "slug": "ant-design-optimize-expensive-operations",
        "title": "Optimize expensive operations"
      },
      {
        "slug": "ant-design-pin-ci-dependencies-securely",
        "title": "Pin CI dependencies securely"
      },
      {
        "slug": "ant-design-react-component-api-clarity",
        "title": "React component API clarity"
      },
      {
        "slug": "ant-design-semantic-naming-consistency",
        "title": "Semantic naming consistency"
      },
      {
        "slug": "ant-design-simplify-complex-expressions",
        "title": "Simplify complex expressions"
      },
      {
        "slug": "ant-design-standardize-deprecation-documentation",
        "title": "Standardize deprecation documentation"
      },
      {
        "slug": "ant-design-test-network-performance",
        "title": "Test network performance"
      },
      {
        "slug": "ant-design-test-organization-standards",
        "title": "Test organization standards"
      },
      {
        "slug": "ant-design-use-semantic-descriptive-names",
        "title": "Use semantic descriptive names"
      },
      {
        "slug": "ant-design-use-semantic-naming",
        "title": "Use semantic naming"
      }
    ],
    "comments": {
      "ant-design-use-semantic-descriptive-names": [
        "要不要提一个专门的 `ClassNamesType<ButtonProps>` 这样？",
        "这样一个 Form 下面多个 Radio.Group ，是不是又会 name 重复。",
        "应该放 closable 里",
        "pos 这个命名不好",
        "> ClosableType 位于 components/_util/hooks/useClosable.tsx 下，属于 util，改动 ClosableType 的话会影响全局\r\n\r\n可以扩展此处类型就好。"
      ],
      "ant-design-simplify-complex-expressions": [
        "这个太复杂了，不要嵌套三元表达式。"
      ],
      "ant-design-configuration-documentation-standards": [
        "```suggestion\n- 🆕 Button 组件支持 `shape` 全局配置。[#54463](https://github.com/ant-design/ant-design/pull/54463) [@guoyunhe](https://github.com/guoyunhe)\n```",
        "```suggestion\n- 🆕 Upload 组件支持 `customRequest` 全局配置。[#54474](https://github.com/ant-design/ant-design/pull/54474) [@guoyunhe](https://github.com/guoyunhe)\n```",
        "```suggestion\n1. 使用  `@ant-design/icons@5.x` 配合 `antd@5.x`，而不是最新版本。\n```",
        "要写清楚是 Tooltip 的渲染节点。",
        "```suggestion\r\n- 🇨🇳 ColorPicker add `zh_HK` `zh_TW` locales. [#53440](https://github.com/ant-design/ant-design/pull/53440) [@mjsong07](https://github.com/mjsong07)\r\n```"
      ],
      "ant-design-consistent-dependency-formatting": [
        "```suggestion\r\n    \"@biomejs/biome\": \"^1.0.0\",\r\n```"
      ],
      "ant-design-use-semantic-naming": [
        "参考其他 token 命名，无需 pagination 前缀"
      ],
      "ant-design-avoid-unnecessary-memoization": [
        "这个 useMemo 意义不大，性能可能更糟糕。",
        "去掉 useMemo 即可。"
      ],
      "ant-design-enforce-ci-workflow-gates": [
        "这个风险有点高，是否有更好的办法。",
        "可以。",
        "如果跳过的话，就跑一次本地 ci 兜底如何？"
      ],
      "ant-design-api-evolution-strategy": [
        "```suggestion\n| classNames | 语义化结构 class | [Record<SemanticDOM, string> \\| (props)=> Record<SemanticDOM, string>](#semantic-dom) | - | 5.4.0 |\n```\n\n直接 props 放到顶层吧，info 里貌似也不会有别的什么东西。",
        "👌🏻",
        "是不是要等一下 https://github.com/ant-design/ant-design/pull/53742/",
        "```suggestion\r\n- 🗑 废弃 Cascader 组件的 `dropdown*` 等属性。[#53133](https://github.com/ant-design/ant-design/pull/53133) [@aojunhao123](https://github.com/aojunhao123)\r\n```",
        "不应该增加这个，应该用 styles.popup 。\r\n同理 popupClassName 也应该合并到 classNames.popup 里去。",
        "是的，不要再新增 popupStyle 这种马上就被废弃的 API 了。"
      ],
      "ant-design-test-network-performance": [
        "这里要判断一下国内镜像快不快。否则用户如果是海外的，访问 ant.design 虽然慢，但国内镜像对他来说可能更慢"
      ],
      "ant-design-pin-ci-dependencies-securely": [
        "@ant-design/ant-design-collaborators ",
        "@ant-design/ant-design-core ",
        "现在这个创建出来的 PR 不跑 ci，其实不太敢合。"
      ],
      "ant-design-standardize-deprecation-documentation": [
        "不能删，只能废弃，得保持兼容。",
        "```ts\r\n/**\r\n * @deprecated DeprecatedComponent 已被废弃，请使用 NewComponent 替代。\r\n */\r\n```"
      ],
      "ant-design-markdown-formatting-consistency": [
        "```suggestion\n通过 `classNames` 和 `styles` 传入对象/函数可以自定义按钮的[语义化结构](#semantic-dom)样式。\n```",
        "```suggestion\n  - 🐞 Fix Button hover/active text color of button with `variant=\"filled\"` and `href` set. [#54614](https://github.com/ant-design/ant-design/pull/54614) [@Komoszek](https://github.com/Komoszek)\n```",
        "```suggestion\r\n- ⌨️ 优化 Tour 的 `aria-*` 属性。[#53345](https://github.com/ant-design/ant-design/pull/53345) [@kiner-tang](https://github.com/kiner-tang)\r\n```"
      ],
      "ant-design-avoid-hardcoded-configuration-values": [
        "意义不大，现在 4px 是写在 js 里的，暂时没关联起来。"
      ],
      "ant-design-handle-optional-values-safely": [
        "```jsx\r\n  Object.assign({ a: 1 }, { a: undefined }) // { a: undefined }\r\n  extendsObject({ a: 1 }, { a: undefined }) // { a: 1 }\r\n```\r\nextendsObject 主要作用是跳过 undefined，如果你不需要这个特性，完全可以用原生方法替代。",
        "看看有没有必要存在。"
      ],
      "ant-design-optimize-expensive-operations": [
        "是的，这段代码可以通过使用 `requestAnimationFrame`（简称 `rAF`）来优化性能，使得 `resize` 操作更加流畅。以下是对代码的分析以及如何引入 `requestAnimationFrame` 的详细说明。\r\n\r\n---\r\n\r\n### **1. 当前代码的问题**\r\n在当前实现中，`adjustElementWidth` 函数直接修改了 DOM 元素的样式（`wrapper.style.width`）。如果这个函数被频繁调用（例如在用户快速调整窗口大小时），可能会导致大量的布局重绘（layout reflows）和样式重计算（style recalculations），从而影响性能。\r\n\r\n此外，直接操作 DOM 样式的方式可能会导致浏览器无法有效地合并多个样式更新，进一步降低性能。\r\n\r\n---\r\n\r\n### **2. 为什么需要 `requestAnimationFrame`？**\r\n`requestAnimationFrame` 是一种浏览器提供的优化机制，用于在下一帧渲染之前执行动画或视觉更新。它的主要优点包括：\r\n\r\n- **减少不必要的重绘**：通过将多次 DOM 更新合并到一帧内完成，避免频繁触发重绘。\r\n- **提高动画流畅性**：确保更新与屏幕刷新率同步，通常为 60 FPS（每秒 60 帧）。\r\n- **节省资源**：当页面不可见时（例如切换到其他标签页），`rAF` 会自动暂停，避免浪费 CPU 和 GPU 资源。\r\n\r\n因此，在 `resize` 场景中，使用 `requestAnimationFrame` 可以显著提升性能和用户体验。\r\n\r\n---\r\n\r\n### **3. 如何优化代码？**\r\n我们可以通过以下步骤对代码进行优化：\r\n\r\n#### **3.1. 使用 `requestAnimationFrame` 包装样式更新**\r\n将 `adjustElementWidth` 的逻辑放入 `requestAnimationFrame` 中，确保每次更新都与浏览器的渲染帧同步。\r\n\r\n#### **3.2. 防止重复调度**\r\n为了避免在短时间内多次调用 `handleResizeWrapper` 导致重复调度，可以使用一个标志变量（如 `isScheduled`）来确保每次只有一个 `rAF` 调度正在进行。\r\n\r\n#### **3.3. 改进后的代码**\r\n以下是优化后的代码实现：\r\n\r\n```typescript\r\nimport { TextAreaRef } from 'rc-textarea';\r\nimport React from 'react';\r\n\r\ntype ResizeWrapperHandler = (rcTextArea: TextAreaRef | null) => void;\r\n\r\nconst ELEMENT_GAP = 2;\r\n\r\nlet isScheduled = false; // 用于防止重复调度\r\n\r\nconst adjustElementWidth = (width: number, wrapper: HTMLElement): void => {\r\n  if (!isScheduled) {\r\n    isScheduled = true;\r\n\r\n    requestAnimationFrame(() => {\r\n      if (wrapper.offsetWidth - width < ELEMENT_GAP) {\r\n        // The textarea's width is increased\r\n        wrapper.style.width = `${width + ELEMENT_GAP}px`;\r\n      } else if (wrapper.offsetWidth - width > ELEMENT_GAP) {\r\n        // The textarea's width is decreased\r\n        wrapper.style.width = `${width + ELEMENT_GAP}px`;\r\n      }\r\n\r\n      isScheduled = false; // 重置调度标志\r\n    });\r\n  }\r\n};\r\n\r\nexport default function useHandleResizeWrapper(): { handleResizeWrapper: ResizeWrapperHandler } {\r\n  const handleResizeWrapper: ResizeWrapperHandler = React.useCallback((rcTextArea) => {\r\n    if (!rcTextArea) return;\r\n\r\n    const textAreaStyle = rcTextArea.resizableTextArea.textArea.style;\r\n    if (textAreaStyle.width.includes('px')) {\r\n      const width = parseInt(textAreaStyle.width.replace('px', ''), 10);\r\n      adjustElementWidth(width, rcTextArea.nativeElement);\r\n    }\r\n  }, []);\r\n\r\n  return { handleResizeWrapper };\r\n}\r\n```\r\n\r\n---\r\n\r\n### **4. 优化点详解**\r\n\r\n#### **4.1. `requestAnimationFrame` 的作用**\r\n通过将 `adjustElementWidth` 的逻辑放入 `requestAnimationFrame` 中，我们确保每次样式更新都与浏览器的渲染帧同步。这样可以避免在短时间内频繁触发重绘，从而提高性能。\r\n\r\n#### **4.2. 防止重复调度**\r\n通过引入 `isScheduled` 标志变量，我们确保即使 `handleResizeWrapper` 被频繁调用，也只会有一个 `rAF` 调度在进行中。这可以有效减少不必要的计算和 DOM 操作。\r\n\r\n#### **4.3. 性能提升的实际效果**\r\n- **减少重绘次数**：由于 `rAF` 将多次更新合并到一帧内完成，减少了布局重绘的频率。\r\n- **更流畅的动画**：`rAF` 确保更新与屏幕刷新率同步，避免卡顿。\r\n- **更低的资源消耗**：当页面不可见时，`rAF` 自动暂停，避免浪费资源。\r\n\r\n---\r\n\r\n### **5. 进一步优化建议**\r\n如果 `resize` 操作仍然不够流畅，可以考虑以下额外的优化措施：\r\n\r\n#### **5.1. 使用防抖（Debounce）**\r\n如果 `handleResizeWrapper` 被频繁调用（例如在窗口大小调整时），可以使用防抖技术来限制调用频率。例如：\r\n\r\n```typescript\r\nimport { debounce } from 'lodash';\r\n\r\nconst debouncedHandleResizeWrapper = debounce(handleResizeWrapper, 100);\r\n```\r\n\r\n然后在实际调用时使用 `debouncedHandleResizeWrapper`。\r\n\r\n#### **5.2. 使用 CSS 替代 JavaScript**\r\n如果可能，尽量通过 CSS 实现动态宽度调整。例如，使用 `flexbox` 或 `grid` 布局来自动调整元素宽度，而不是手动计算和设置样式。\r\n\r\n---\r\n\r\n### **6. 总结**\r\n通过引入 `requestAnimationFrame`，我们可以显著优化 `resize` 操作的性能，使其更加流畅。同时，结合防抖技术和 CSS 优化，可以进一步提升用户体验。如果你在实际使用中遇到其他问题，欢迎提供更多细节，我可以帮助进一步分析和优化！",
        "Yes, this code can be optimized for performance using `requestAnimationFrame` (rAF) to make the `resize` operation smoother. Below is an analysis of the code and detailed instructions on implementing rAF:\r\n\r\n---\r\n\r\n### **1. Issues with Current Code**\r\nThe current implementation directly modifies the DOM element's style (`wrapper.style.width`) in the `adjustElementWidth` function. If called frequently (e.g., during rapid window resizing), this can cause excessive layout reflows and style recalculations, degrading performance.\r\n\r\nDirect DOM manipulation also prevents browsers from batching style updates efficiently, further reducing performance.\r\n\r\n---\r\n\r\n### **2. Why Use `requestAnimationFrame`?**\r\n`requestAnimationFrame` is a browser optimization mechanism for executing animations or visual updates before the next repaint. Key benefits include:\r\n\r\n- **Reduced repaints**: Batches multiple DOM updates into a single frame.\r\n- **Smoother animations**: Syncs updates with the screen refresh rate (typically 60 FPS).\r\n- **Resource efficiency**: Pauses updates when the tab is inactive, saving CPU/GPU resources.\r\n\r\nUsing rAF in `resize` scenarios significantly improves performance and user experience.\r\n\r\n---\r\n\r\n### **3. How to Optimize the Code**\r\nFollow these steps to optimize:\r\n\r\n#### **3.1. Wrap Style Updates in rAF**\r\nEncapsulate `adjustElementWidth` logic in `requestAnimationFrame` to synchronize updates with browser repaint cycles.\r\n\r\n#### **3.2. Prevent Duplicate Scheduling**\r\nUse a flag (`isScheduled`) to ensure only one rAF callback runs at a time, even if `handleResizeWrapper` is called repeatedly.\r\n\r\n#### **3.3. Optimized Code**\r\n```typescript\r\nimport { TextAreaRef } from 'rc-textarea';\r\nimport React from 'react';\r\n\r\ntype ResizeWrapperHandler = (rcTextArea: TextAreaRef | null) => void;\r\n\r\nconst ELEMENT_GAP = 2;\r\n\r\nlet isScheduled = false; // Prevents duplicate scheduling\r\n\r\nconst adjustElementWidth = (width: number, wrapper: HTMLElement): void => {\r\n  if (!isScheduled) {\r\n    isScheduled = true;\r\n\r\n    requestAnimationFrame(() => {\r\n      if (wrapper.offsetWidth - width < ELEMENT_GAP) {\r\n        // Expand textarea width\r\n        wrapper.style.width = `${width + ELEMENT_GAP}px`;\r\n      } else if (wrapper.offsetWidth - width > ELEMENT_GAP) {\r\n        // Shrink textarea width\r\n        wrapper.style.width = `${width + ELEMENT_GAP}px`;\r\n      }\r\n\r\n      isScheduled = false; // Reset scheduling flag\r\n    });\r\n  }\r\n};\r\n\r\nexport default function useHandleResizeWrapper(): { handleResizeWrapper: ResizeWrapperHandler } {\r\n  const handleResizeWrapper: ResizeWrapperHandler = React.useCallback((rcTextArea) => {\r\n    if (!rcTextArea) return;\r\n\r\n    const textAreaStyle = rcTextArea.resizableTextArea.textArea.style;\r\n    if (textAreaStyle.width.includes('px')) {\r\n      const width = parseInt(textAreaStyle.width.replace('px', ''), 10);\r\n      adjustElementWidth(width, rcTextArea.nativeElement);\r\n    }\r\n  }, []);\r\n\r\n  return { handleResizeWrapper };\r\n}\r\n```\r\n\r\n---\r\n\r\n### **4. Key Optimizations Explained**\r\n\r\n#### **4.1. rAF Synchronization**\r\nBy wrapping updates in `requestAnimationFrame`, style changes align with the browser's repaint cycle, minimizing layout thrashing.\r\n\r\n#### **4.2. Duplicate Call Prevention**\r\nThe `isScheduled` flag ensures only one rAF callback runs at a time, even during rapid consecutive calls.\r\n\r\n#### **4.3. Performance Impact**\r\n- **Fewer repaints**: Batches multiple updates into a single frame.\r\n- **Smoother resizing**: Syncs with monitor refresh rate (60 FPS typical).\r\n- **Lower resource usage**: Pauses updates when the tab is inactive.\r\n\r\n---\r\n\r\n### **5. Further Optimization Suggestions**\r\n\r\n#### **5.1. Debounce Resize Events**\r\nIf `handleResizeWrapper` is called excessively (e.g., during window resizing), add debouncing:\r\n```typescript\r\nimport { debounce } from 'lodash';\r\n\r\nconst debouncedHandleResize = debounce(handleResizeWrapper, 100);\r\n```\r\n\r\n#### **5.2. CSS-Driven Resizing**\r\nWhere possible, use CSS for dynamic sizing (e.g., `flexbox`, `grid`) instead of JavaScript calculations.\r\n\r\n---\r\n\r\n### **6. Summary**\r\nUsing `requestAnimationFrame` significantly improves resize performance. Combined with debouncing and CSS optimizations, this approach ensures smooth, efficient UI updates. Share additional details if you encounter specific issues for further analysis!"
      ],
      "ant-design-dependency-versioning-consistency": [
        "用 `~0.1.1`\n"
      ],
      "ant-design-react-component-api-clarity": [
        "这里应该是 mask 而不是 previewMask 吧。mask 因为啥情况废弃了？",
        "```suggestion\r\nimport { createCache, extractStyle, StyleProvider } from 'antd/cssinjs';\r\n```\r\n\r\nHow about this way? cc @zombieJ",
        "还有一个思路是\r\n\r\n```jsx\r\nimport { cssinjs } from 'antd';\r\n\r\nconst { createCache, extractStyle, StyleProvider } = cssinjs;\r\n```",
        "```suggestion\r\n| footer | 底部内容，当不需要默认底部按钮时，可以设为 `footer: null` | ReactNode \\| (originNode: ReactNode, extra: { OkBtn: React.FC, CancelBtn: React.FC }) => ReactNode | - | renderFunction: 5.9.0 |\r\n```",
        "这样还简单点"
      ],
      "ant-design-test-organization-standards": [
        "最好避免完全靠这种方法独立测试类型的单测来做覆盖，这样可能导致正常使用 antd 组件时，某些代码没被覆盖到。\n\n尽量通过 antd 组件用法来触发行覆盖。",
        "不要一个 describe 只包一个 it\n\n留一个 `describe('CheckableTag', () => { ... });` 就好，所有相关 it 都可以放进来。"
      ],
      "ant-design-semantic-naming-consistency": [
        "这样看，folder 改成 nativeDirectory 是不是好理解一点。",
        "或者 `directory=\"native\"` 和 `directory=\"accept\"`。"
      ]
    },
    "profile": {
      "location": "Hangzhou, China",
      "company": "Alipay",
      "blog": "https://twitter.com/afc163",
      "site_admin": false,
      "followers": 7257,
      "following": 493
    }
  },
  "jacob314": {
    "repos": [
      "google-gemini/gemini-cli"
    ],
    "entries": [
      {
        "slug": "gemini-cli-add-tests-for-changes",
        "title": "add tests for changes"
      },
      {
        "slug": "gemini-cli-avoid-non-null-assertions",
        "title": "avoid non-null assertions"
      },
      {
        "slug": "gemini-cli-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "gemini-cli-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "gemini-cli-document-configuration-defaults-clearly",
        "title": "Document configuration defaults clearly"
      },
      {
        "slug": "gemini-cli-ensure-comprehensive-user-documentation",
        "title": "Ensure comprehensive user documentation"
      },
      {
        "slug": "gemini-cli-implement-resource-constraints",
        "title": "implement resource constraints"
      },
      {
        "slug": "gemini-cli-maintain-naming-consistency",
        "title": "Maintain naming consistency"
      },
      {
        "slug": "gemini-cli-minimize-performance-overhead",
        "title": "minimize performance overhead"
      },
      {
        "slug": "gemini-cli-never-ignore-errors-silently",
        "title": "Never ignore errors silently"
      },
      {
        "slug": "gemini-cli-optimize-react-hooks-usage",
        "title": "optimize React hooks usage"
      },
      {
        "slug": "gemini-cli-organize-code-by-responsibility",
        "title": "organize code by responsibility"
      },
      {
        "slug": "gemini-cli-prefer-lightweight-composable-apis",
        "title": "Prefer lightweight composable APIs"
      },
      {
        "slug": "gemini-cli-prefer-settings-over-environment",
        "title": "prefer settings over environment"
      },
      {
        "slug": "gemini-cli-prevent-concurrent-state-races",
        "title": "Prevent concurrent state races"
      },
      {
        "slug": "gemini-cli-prevent-react-race-conditions",
        "title": "Prevent React race conditions"
      },
      {
        "slug": "gemini-cli-reduce-nesting-complexity",
        "title": "reduce nesting complexity"
      },
      {
        "slug": "gemini-cli-secure-input-validation",
        "title": "Secure input validation"
      },
      {
        "slug": "gemini-cli-test-behavioral-differences",
        "title": "Test behavioral differences"
      }
    ],
    "comments": {
      "gemini-cli-avoid-non-null-assertions": [
        "tweak so you don't need the `!` after fileReadResult here and elsewhere. likely you can fix by a union type so fileReadResult is required when success is true. alternately add a case above that should never be it where you report an error if fileReadResult is undefined but success is true with a comment that it shouldn't occur.",
        "this change seems a little unrelated and generally agree with Gemini that it is safer to have this `throw`  unless this.contentGenerator can't possibly be null but the type system just can't figure it out."
      ],
      "gemini-cli-implement-resource-constraints": [
        "Thank you for adding this! I am really excited to get analytics to understand how bad user's memory usage usage issues.\r\nHowever, every 5 seconds is a bit too frequent. ideally we could only send this when the app is in use and even then we should rate limit a bit more to perhaps only send this metric at most once every minute. Some other tools only send analytics about memory usage at most once every hour.\r\nThe easiest way to detect that the app is in use would be to have an API the CLI package can call when the user types of an additonal message is added to history. \r\n\r\nAnother complementary option is to poll ever perhaps 10 seconds fetching the RSS side but only report if the RSS is at least 5% greater than the previous high water mark for the largest RSS side. Memory usage will bounce up and down when there are garbage collection, potentially due to garbage collection that happens purely due to tracking these events so you have to be careful you only send events when memory usage has increased relative to the largest value it has ever been rather than the previous value.",
        "can you comment where the 50 ms threshold comes from? I would worry we might miss that terminals support kitty when the users machine happens to be under heavy load but I also don't love adding >50ms of delay for all users who happen to not be using a kitty protocol terminal.",
        "unshift, shift, and slice are O(n).\r\nYou might want to use \r\nhttps://yomguithereal.github.io/mnemonist/fixed-deque\r\nor a doubly linked list to make these faster. shouldn't matter that much with a max of 1000 events but might be worth doing anyway to avoid an issue if max_events is raised."
      ],
      "gemini-cli-minimize-performance-overhead": [
        "nit: all of these cases are a bit verbose with repeated logic to track a start time, end time and then compute the duration. An alternate more terse api could be something like\r\n```\r\ntrackStartupPerformance(async () => {\r\n   await start_sandbox(sandboxConfig, memoryArgs);\r\n  },\r\n  'sandbox_startup`\r\n);\r\n```\r\nwhere the trackStartupPerformance method could have a no-op implementation if isPerformanceMonitoringActive()\r\nis false.\r\nAs a bonus that would also make it easy to connect this to the performance.measure API in debug builds so that the times are also visible on the chrome devtools timeline.\r\n",
        "please avoid timeouts in tests as slow tests slow everyone down. Can you use \r\n`await wait(1)` if a wait is really required for this to pass?",
        "comment applies to all wait calls added in the file"
      ],
      "gemini-cli-optimize-react-hooks-usage": [
        "we should be able to avoid toggleVimModeRef. Let me know if you need help figuring out how to avoid it and I can dig into it. would expect you can duplicate other patterns in this file and that with the right instructions Gemini CLI can likely help you remove this without introducing circular reference issues.\ngenerally we try to avoid useRef in the code base unless we strictly have to as the logic is easier to reason about when we don't."
      ],
      "gemini-cli-prefer-lightweight-composable-apis": [
        "this doesn't seem very algined with existing code with useInput calls throughout the application and InputPrompt that does its own non-ink standard keyboard processing. Given that I would suggest a solution that is lighter weight and instead allows callers to resolve whether specific input matches a specific keybinding rather than a solution that requires taking control of all useInput in the application."
      ],
      "gemini-cli-add-tests-for-changes": [
        "please add a test case that verifies the case you fixed is resolved. for example, write a test that would have failed previously due to the race condition you fixed.",
        "please add a test for this.",
        "wow good catch that we weren't merging mcp servers correctly previously. please add a test for this to prevent regressions.",
        "there is another pull request for this but I think your fix is cleaner so lets go with what you are doing.\r\nhttps://github.com/google-gemini/gemini-cli/pull/2231#pullrequestreview-2973500668",
        "please add tests for this new code in theme manager"
      ],
      "gemini-cli-ensure-comprehensive-user-documentation": [
        "I think we should add a message.\nThe message entering vim mode should tell you how to toggle between INSERT and NORMAL modes and the message leaving should clarify that you have left vim mode.",
        "love that this message is shown inline. I would still like to see a slightly longer message in the list of messages as well. that should be trivial to add removing the `// no message` comment and  adding a message with this content plus a reminder to run `/vim` again to leave the mode.\nThe message when leaving could be as simple as \"Exited Vim mode.`"
      ],
      "gemini-cli-secure-input-validation": [
        "rather than rolling our own shell escaping, it might make sense to use an existing npm package for this.\r\nhttps://www.npmjs.com/package/shescape?activeTab=code\r\nthis package isn't that popular so I am also open to just rolling our own or trying to find a popular package."
      ],
      "gemini-cli-test-behavioral-differences": [
        "make these tests a little more robust by adding some text to undo and redo and verifying that the text is actually undone and redone.",
        "agree this is worth testing. also add tests for paths with escaped spaces. That is needed as filenames can contain spaces.",
        "can you add a couple more test cases?\r\nOther cases of interest:\r\nThe non wrapping text exceeds the length of the MaxSizedBox.\r\nThe non wrapping text happens to have line breaks (ugly edge case for this)\r\nThe non wrapping text  exceeds the length of the MaxSizedBox.\r\nAdd a case with multiple rows in the MaxSizedBox just for clarity of how this may look with multiple lines with `...`",
        "also add a test case with emojis or other wide characters to make sure we are now handling that case correctly.",
        "Is this test actually checking the rendered text for this case. On my phone so sorry if I am missing something."
      ],
      "gemini-cli-maintain-naming-consistency": [
        "to align with how things will look in the future when we align the input processing with what is in text-buffer.ts I think we should change this to allowing the command both with and without shift. rather than allowing both 'c' and 'C'",
        "also need to handle escaped spaces in paths. check the existing logic for autocomplete for file paths to make sure the logic is aligned.",
        "nit: can you add support for 'j' and 'k' to match the existing ink library we were previously using?"
      ],
      "gemini-cli-reduce-nesting-complexity": [
        "nit: follow style of returning early to reduce nesting.",
        "nit: this conditional is getting a bit hard to read. as a fast follow can you refactor it to use a switch by AuthType or other technique to make it not as nested.",
        "200 is a bit of a magic number. make it a const while you are cleaning this up."
      ],
      "gemini-cli-organize-code-by-responsibility": [
        "can you move this escape logic into InputPrompt rather than having App.tsx deal with functionality that is focused on input prompt?",
        "can you move this out of App.tsx and into a separate file? I'd also suggest considering making this method be less coupled from react. After you sync to the latest you might want to check out how 'unhandledRejection' is now dealing with a similar case in gemini.tsx You could then start the process of checking for updates in gemini.tsx before the react app is even started using the same mechanism to send events to the app notifying it that updates have been accepted.",
        "nit: can you move this out of gemini into autoUpdate.ts or similar. could make testing easier and gemini.tsx is one of the tragedy of the commons files like app.tsx that it would be nice to try to keep small.",
        "please add these to TextBuffer rather than InputPrompt as nothing about this is specific to the InputPrompt",
        "this logic to execute the keystroke handlers seems independent on app. I would move it to keystrokeHandler.ts (also please rename that file to align with naming schemes for non tsx files in this project.\nWould then be nice to add a basic unittest for it that verifies cases such as if key.shift  and key.ctrl are handled correctly. seems like right now key.shift is ignored.",
        "grabbing the theme from the theme manager is making all of this code a bit more complex and isn't needed as colors.ts is actually doing this behind the scenes in each getter.",
        "this logic should go one level deeper in TextBuffer rather than InputPrompt as it does not require any of the additional data (e.g. autocomplete) that InputPrompt has.",
        "Thank you! Please let me know if there is anything I can do to help. Could really use this functionality",
        "this logic should move to text-buffer.ts and operate on the lines before wrapping.\r\nlets also be sure to implement this in a way that we can support highlighting blocks like [Image 1] with custom colors in the future. I'd suggest we move these complex changes to a second PR and land this first just supporting paste that adds a user visible `@some/path/to/image.png` in the input prompt.",
        "moving to text-buffer.ts will also make it easy to test this as that part of the code base has reasonable unit tests."
      ],
      "gemini-cli-centralize-configuration-management": [
        "can you put this in settings.ts / settings.json rather than adding as an env variable?\nI could also see the case for making this an env variable but would default to just surfacing it in settings.ts as that should be sufficient and it is easier if you don't need to merge settings from multiple sources.\nthe case for why this needs to be settable via settings.json is that some orgs might want to disable this for everyone in their org and that is easiest to accomplish via the existing mechanism for org level settings.json files.",
        "nit: still see reading an env variable rather than settings here.",
        "this is worth moving to its own file rather than keeping in app.tsx\nI'd like to reuse in InputPrompt and text-buffer.ts as well.\nIn addition, I'm hoping this can also be the basis to in the future configure keyboard shortcuts via a file and surface the up to date keyboard shortcuts in `/help`.",
        "this is a good place to be loading the custom themes. I suspect the call to load in ThemeDialog wasn't needed."
      ],
      "gemini-cli-document-configuration-defaults-clearly": [
        "document that it is saved to `~/.gemini/settings.json`"
      ],
      "gemini-cli-prevent-react-race-conditions": [
        "Gemini CLI should be able to help you refactor this hook to better align with the Gemini CLI style guide. I would encourage you to do the following to help get this to align better with the GEMINI.md in the project.\nI gave Gemini CLI the following prompt\n\"analyze @vim.ts focusing on ways it might not be aligned with this repos GEMINI.md which has specific instructions on how to use React\"\n\n\n   1. **State Management and Performance:** The hook uses multiple useState calls for interdependent state variables (mode, count, pendingG, pendingD, pendingC). This creates a complex\n      state machine that can lead to unnecessary re-renders. For instance, every keypress in NORMAL mode that is part of a count (e.g., typing \"12\") triggers a state update and\n      re-render. A more performant approach would be to manage the Vim state in a single useReducer or a useRef to avoid re-renders on every minor state change.\n[Jacob's thoughts] if we can solve this with useReducer that would be ideal. If not, I would suggest useRef. I would also worry about the current design breaking down if multiple key presses are sent in quick succession before react has time to recompute. what you are doing using useRef for entering and leaving insert mode solves it for that option but does not handle the other state.\n\n   2. **Overuse of `useCallback`**: Many of the functions wrapped in useCallback have empty dependency arrays, which suggests they do not depend on component props or state. While this\n      prevents them from being recreated on every render, it also adds boilerplate and can be a sign that these functions could be defined outside the component if they do not rely on\n       component-specific data.\n\n   3. **Complex Side Effects and State Synchronization:** The hook uses a useEffect to synchronize the mode state with a modeRef. This is a workaround for accessing the current mode\n      within the handleInput callback, which has a stale closure over the mode state. This pattern is a known \"code smell\" in React and can often be avoided by using useReducer or by\n      structuring the code to pass the latest state directly to event handlers.\nMy thoughts: if you are able to useReducer for this that would be ideal but I suspect it may not be feasible for your case.\n\n[Jacob's thoughts:] Ignore suggestion 4. I think your code is quite elegant given the difficulty of the problem it is solving. \n   4. Readability and Maintainability: The handleInput function is extremely large and contains a deeply nested switch statement. This makes it difficult to read, test, and maintain.\n      Breaking this logic down into smaller, more focused functions would significantly improve the code's clarity and align with the GEMINI.md's emphasis on simplicity and\n      readability.",
        "these two utilities should also go into text-buffer.ts as they also need to be integrated in with the reducer in text-buffer otherwise we are operating on stale input in the even that multiple keystrokes were handled in the same React event loop.",
        "this also needs to be integrated into text-buffer or we will hit issues when multiple keystrokes need to be processed in the same react event loop."
      ],
      "gemini-cli-choose-efficient-data-structures": [
        "same comment as bellow about useing\r\nhttps://yomguithereal.github.io/mnemonist/fixed-deque\r\nor a doubly linked list to avoid O(events.length) operations. "
      ],
      "gemini-cli-prefer-settings-over-environment": [
        "remove this env variable. I don't think this needs a setting but if it did, we should instead add it to settings.ts rather than checking env variables directly. that is the generally preferred way for us to manage user options that some users might want to opt into.",
        "we should also probably support force-model in \r\nsettings.ts so you can force it in your settings.json rather than via a command line argument",
        "sorry after some further thinking. Can we make this only be in settings.json to start out with? ",
        "it would make this a bit more annoying to use as settings.json is in the CLI package but what do you think of making this be in settings.ts rather than an env variable? this kindof seems like something I might want to set in settings.json. For example, maybe my project is really large so I want a specific value for it.\nin general I'm trying to keep most settings in settings.ts where feasible rather than encouraging more env variables or options in config controlled by args unless they strictly have to go that way. Happy to be convinced otherwise for this case.",
        "rather than adding this to config can we instead add it to settings.json with a property added to settings.ts.\r\nGenerally settings.json is a bette place for options that are small UI tweaks than command line flags. If there is a use case where a user wants to frequently change between showing the footer and not showing the footer then I would be open to including it in config.ts\r\n"
      ],
      "gemini-cli-never-ignore-errors-silently": [
        "please continue to log errors handling clipboard images so users can understand what went wrong.",
        "done",
        "one concern is that users of this setting will not be able to tell when errors have occurred. I would be more comfortable with the option if we still showed the number of errors that had occurred even when displayFooter is false."
      ],
      "gemini-cli-prevent-concurrent-state-races": [
        "this logic is not safe to perform here. instead use the existing api in text-buffer.ts or create one. that. is crucial to make sure this logic runs inside the reducer in text-buffer that ensures commands are executed properly and in order even when multiple key strokes are executed in the same react loop. the same concern applies to all of the other cases here that modify the state of the text. buffer. they need to instead call a method on text-buffer.ts to apply the transformation desired rather than reading the possibly stale state of the text buffer and performing operations on it.\nsorry this part of how text-buffer works is a little surprising but it is better than the alternatives where it becomes really difficult to be confident that react is dealing with the correct state.",
        "remove this helper completely as it is not safe given the fact that you want to handle multiple keystrokes one after the other so the buffer could be stale by the time you handle the second one if both keystrokes are handled in the same react event frame. sorry this logic has to be written in such a particular way to be robust but you are making changes to the trickiest part of the input system where we need to chain operations together carefully to avoid confusing user behavior.",
        "please refactor avoid this catch block. there should be ways to avoid any race condition here. if there are failure modes I would much rather have us check that this.events is empty that catch to swallow exceptions."
      ]
    },
    "profile": {
      "location": "Seattle",
      "company": "Google",
      "blog": "",
      "site_admin": false,
      "followers": 205,
      "following": 37
    }
  },
  "rosstimothy": {
    "repos": [
      "gravitational/teleport"
    ],
    "entries": [
      {
        "slug": "teleport-add-contextual-logging",
        "title": "Add contextual logging"
      },
      {
        "slug": "teleport-api-consistency-patterns",
        "title": "API consistency patterns"
      },
      {
        "slug": "teleport-api-parameter-encapsulation",
        "title": "API parameter encapsulation"
      },
      {
        "slug": "teleport-authenticated-health-checks",
        "title": "Authenticated health checks"
      },
      {
        "slug": "teleport-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "teleport-database-query-parameter-hygiene",
        "title": "Database query parameter hygiene"
      },
      {
        "slug": "teleport-design-comprehensive-metrics",
        "title": "Design comprehensive metrics"
      },
      {
        "slug": "teleport-design-intuitive-query-interfaces",
        "title": "Design intuitive query interfaces"
      },
      {
        "slug": "teleport-follow-platform-naming-standards",
        "title": "Follow platform naming standards"
      },
      {
        "slug": "teleport-minimize-authentication-complexity",
        "title": "Minimize authentication complexity"
      },
      {
        "slug": "teleport-minimize-unnecessary-allocations",
        "title": "minimize unnecessary allocations"
      },
      {
        "slug": "teleport-network-address-clarity",
        "title": "Network address clarity"
      },
      {
        "slug": "teleport-organize-code-by-functionality",
        "title": "Organize code by functionality"
      },
      {
        "slug": "teleport-parameterize-configuration-values",
        "title": "parameterize configuration values"
      },
      {
        "slug": "teleport-thoughtful-configuration-design",
        "title": "thoughtful configuration design"
      },
      {
        "slug": "teleport-use-appropriate-testify-assertions",
        "title": "Use appropriate testify assertions"
      },
      {
        "slug": "teleport-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "teleport-use-semantically-clear-names",
        "title": "Use semantically clear names"
      },
      {
        "slug": "teleport-use-structured-api-parameters",
        "title": "Use structured API parameters"
      }
    ],
    "comments": {
      "teleport-minimize-authentication-complexity": [
        "Is tsh login needed if we are explicitly passing in an identity file that we've gotten from tbot to tctl create below?"
      ],
      "teleport-database-query-parameter-hygiene": [
        "Why do we need the filter here if it isn't being consumed?",
        "I guess this poses the question of should the filter be applied in the cache/backend or the RPC layer. cc @okraport @espadolini "
      ],
      "teleport-api-consistency-patterns": [
        "Is this going to be exclusively available via the web UI? Is there any reason we can't add a CLI equivalent?",
        "The alternative to a new preset role is to require users create the Teleport Role themselves with the required permissions to enroll a new cluster as is currently the case today correct?"
      ],
      "teleport-use-structured-api-parameters": [
        "cc @okraport - Whatever we do we should standardize on, especially given that Luke is working on unifying our APIs while addressing our current lack of pagination support.",
        "@okraport has been working on standardizing our List APIs and might have some opinions on this",
        "We could always freeze the current ListBotInstances API and add ListBotInstancesV2 which followed that pattern."
      ],
      "teleport-use-semantically-clear-names": [
        "What do you think about taking the opportunity to use a different name here? Node has traditionally been used to mean ssh_service, but joining is not limited to ssh_service. What do you think about something like the following?\n\n```suggestion\n  // FriendlyName is the user-friendly name of the Teleport resource attempting to join.\n  string friendly_name = 3;\n```",
        "Should we take the opportunity to rename this to `system_role` to reduce any ambiguity?"
      ],
      "teleport-network-address-clarity": [
        "Is it even possible for proxies to be configured with conflicting public addresses? Maybe reword this to indicate that it cannot match the clusters public address if that is the case?\n```suggestion\n  # The public address must be a unique DNS name and not conflict with the Teleport cluster's public addresses.\n```"
      ],
      "teleport-organize-code-by-functionality": [
        "Is this package intended to ever be consumed outside of lib/join? Should we consider moving it into `join/internal/diagnostic`?",
        "It might also be worthwhile to convert these iterators over to iter.Seq at some point as well.",
        "Let's consolidate the implementations of UpsertAccessListWithMembers and UpsertAccessListWithMembersV2 by having one call the other.  \r\n\r\n```suggestion\r\nfunc (a *AccessListService) UpsertAccessListWithMembers(ctx context.Context, req accesslist.UpsertAccessListWithMembersRequest) (*accesslist.AccessList, []*accesslist.AccessListMember, error) {\r\nreturn a.UpsertAccessListWithMembersV2(ctx, accesslist.UpsertAccessListWithMembersRequest{...})\r\n}\r\n\r\nfunc (a *AccessListService) UpsertAccessListWithMembersV2(ctx context.Context, req accesslist.UpsertAccessListWithMembersRequest) (*accesslist.AccessList, []*accesslist.AccessListMember, error) {\r\n....\r\n}\r\n```",
        "I don't know that lib/services/local is the right home for this either. The local package is meant to house the storage layer for resources and I don't think there is anything particularly tied to the storage layer about this expression parser."
      ],
      "teleport-minimize-unnecessary-allocations": [
        "Do we need the intermediate slice?\n```suggestion\n\t\tfor _, proxyServer := range proxyServers {\n\t\t\tproxyAddr := proxyServer.GetPublicAddr()\n```",
        "I don't know that we should be preallocating these. This results in allocations for slices which should never have any items in the happy path of most user sessions.\r\n```suggestion\r\n\tvar errors []error\r\n```"
      ],
      "teleport-design-comprehensive-metrics": [
        "The main motivator for the metrics is to allow alerting when some number of resources are not healthy. Can we achieve that when unhealthy and unknown targets are grouped together? My main concern is that we disambiguate these, which then makes consuming the metrics and calculating the number of unhealthy/unreachable hosts harder. \r\n\r\nAs an admin, If I were to set up alerting for these health checks, would the distinction between unhealthy and unknown allow me to take different actions?",
        "There is one potential downside to the prometheus metrics in the current proposal: they only count resources that are covered by health checks. If I have 20 Kubernetes Clusters, but configure my health_check_resource to only match on labels of 10 of them I may never get alerted that any of the other 10 are offline. \r\n\r\nI think this is probably fine, as we can't count what we don't know, but it's maybe something we should call out better in the metrics documentation."
      ],
      "teleport-parameterize-configuration-values": [
        "🧐 \r\n```suggestion\r\n    proxy_server: example.teleport-test.com:443\r\n```"
      ],
      "teleport-thoughtful-configuration-design": [
        "CheckAndSetDefaults feels like the wrong place to initialize the healthcheck.Manager, I imagine what we instead want is CheckAndSetDefaults to return an error if a healthcheck.Manager was not configured.",
        "> That's the main motivation for choosing CheckAndSetDefaults for kube.\r\n\r\nWe don't need to follow the road paved by databases 100%. [CheckAndSetDefaults](https://github.com/gravitational/teleport/blob/e3396dcac52ccfd1707d51097e633eaa9377e96b/lib/kube/proxy/server.go#L134-L145) is also aborting when components are not injected properly. \r\n\r\nIn any case this is very much an implementation detail and likely a bit too granular for this RFD. We can discuss the appropriate place for this during implementation. For now I might advise to scale back calling out the exact locations that every change made during implementation will happen in this RFD.",
        "What would happen if I have a very large and verbose configuration?\r\n\r\n- Could this cause a heartbeat to exceed gRPC max message sizes?\r\n- Could this cause s heartbeat to exceed backend item size limits?"
      ],
      "teleport-add-contextual-logging": [
        "What do you think about logging some informational message if both of these errors are not nil? The only indication that users get if things are not working is from the message [here](https://github.com/gravitational/teleport/pull/58187/files#diff-76045a1d6e2c8f7d42000a7065c3c58579e87c33c0efd6f813701a28d970fefdR353) but that won't contain any context on _why_ user accounting is disabled. ",
        "What do you think about rewriting this to return early on success? Same suggestion for `Close` below.\n```suggestion\n\tvar errors []error\n\tif uacc.utmp != nil {\n\t\tif err := uacc.utmp.Login(ttyName, username, remote, loginTime); err == nil {\n\t\t\treturn &Session{\n\t\t\t\tuacc: uacc,\n\t\t\t\tutmpKey: ttyName,\n\t\t\t}, nil\n\t\t}\n\t\t\n\t\terrors = append(errors, err)\n\t}\n\t\n\tif uacc.wtmpdb != nil {\n\t\tkey, err := uacc.wtmpdb.Login(ttyName, username, remote, loginTime)\n\t\tif err == nil {\n\t\t\treturn &Session{\n\t\t\t\tuacc: uacc,\n\t\t\t\twtmpdbKey: ttyName,\n\t\t\t}, nil\n\t\t}\n\t\terrors = append(errors, err)\n\t}\n\t\n\treturn nil, trace.NewAggregate(errors...)\n```",
        "If wtmpdb exists then shouldn't it be viewed as the source of truth?",
        "The Ubuntu 24.10 VM I tested this on had all of /var/log/wtmp, /var/log/wtmpx, and wtmpdb - and I didn't install wtmpdb manually 😅 "
      ],
      "teleport-use-appropriate-testify-assertions": [
        "As of testify v1.10.0, require is now able to be used within a EventuallyWith assertion function. \n```suggestion\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Len(t, trackers, 1)\n\t\t\t\trequire.Equal(t, helpers.HostID, trackers[0].GetAddress())\n```",
        "Yeah, require will cause the function to return early if err is non-nil. At which point it will try again on the next tick."
      ],
      "teleport-api-parameter-encapsulation": [
        "Are we going to run into any issues by extending the current ListAccessList RPC with these new fields? What happens if a newer Teleport instance populates these fields and the request is received by an older Auth server that doesn't know about them? \r\n\r\nFrom a correctness standpoint, the scenario above would still deliver pages of access lists, though anything expecting sorting of filtering, like the UI, would result in a weird UX.",
        "Assuming there is a new RPC, would the UI be smart enough to do the right thing and fall back to existing behavior? Would the existing behavior be potentially worse than the weird UX or sorting not being honored?",
        "Can we name this ListAccessListsV2 and deprecate ListAccessLists?\r\n```suggestion\r\n  // ListAccessListsV2 returns a paginated, filtered, and sorted list of all access lists.\r\n  rpc ListAccessListsV2(ListAccessListsV2Request) returns (ListAccessListsV2Response);\r\n```"
      ],
      "teleport-comprehensive-function-documentation": [
        "I don't know that we should be annotating indexes with their current use cases. The comments may become completely irrelevant over time. I think it'd be more valuable to add comments that explain what the index does, i.e. allows for efficient sorting by the next audit date."
      ],
      "teleport-authenticated-health-checks": [
        "Is /readyz rate limited? Do we need to be concerned about similar behavior to https://github.com/gravitational/teleport/issues/58118?",
        "TCP might be a good starting point, though we've got at least one customer that has stated they want Kubernetes health checks to determine if the cluster is online AND if the Teleport agent is configured properly to communicate to the cluster. By using `/readyz` we can potentially identify cases where the agent was deployed with insufficient credentials and is unable to talk to the Kubernetes API.",
        "Tiago [echoed](https://github.com/gravitational/teleport/pull/58065#pullrequestreview-3163464098) the same sentiment. We likely need to go beyond is the Kubernetes API available in order to ensure that the agent can perform the operations needed for users to successfully connect to Kubernetes clusters via Teleport. ",
        "@rana let's get the RFD updated with this information.",
        "> Adding TLS checks is a focus for Kubernetes.\n\nIs this accurate? We don't so much want just a TLS handshake to be successful as we want to make fully authenticated requests to a particular endpoint of the Kubernetes API and examine the response right?"
      ],
      "teleport-follow-platform-naming-standards": [
        "I think we should suffix these with a _total to follow naming best practices: https://prometheus.io/docs/practices/naming/#metric-names\r\n```suggestion\r\nteleport_health_resources_total{type=\"kubernetes\"}\r\n# Returns 3, the expected number of healthy Kubernetes clusters\r\n\r\nteleport_health_resources_available_total{type=\"kubernetes\"}\r\n```",
        "Hrm yeah that is somewhat misleading advice in the naming best practices document. I'm fine omitting `_total` if that is reserved for counters.",
        "This enum should follow proto best practices. The linter would surely complain about these presently.\r\n```suggestion\r\n\t\tBOT_KIND_UNSPECIFIED = 0;\r\n\r\n\t\t// TBOT_BINARY means the bot is running the tbot binary.\r\n\t\tBOT_KIND_TBOT_BINARY = 1;\r\n\r\n\t\t// TERRAFORM_PROVIDER means the bot is running inside the Teleport Terraform\r\n\t\t// provider.\r\n\t\tBOT_KIND_TERRAFORM_PROVIDER = 2;\r\n\r\n\t\t// KUBERNETES_OPERATOR means the bot is running inside the Teleport Kubernetes\r\n\t\t// operator.\r\n\t\tBOT_KIND_KUBERNETES_OPERATOR = 3;\r\n```"
      ],
      "teleport-design-intuitive-query-interfaces": [
        "> A custom language parser will be used to provide instance-specific functions such as those in the table below.\r\n\r\nWhat is a custom language parser?",
        "Do you think including semver in the name will cause more harm than good? I could see some people not being aware of semantic versioning being very confused by this. Should we consider alternatives that are less technical and maybe align with things users of tctl inventory ls might already be familiar with?\r\n\r\nPerhaps something like `older_than(version, 18.1)` and `newer_than(version, 18)`?",
        "Do we need to support _or_equal though, you could get that via `newer_than(version, 17.999999.9999)` to find all releases that would include 18.0.0 and all newer releases.\r\n\r\nAlternatively we could add an `equal_to(version, 18.0.0)` predicate that could be combined via `newer_then(version, 18.0.0) || equal_to(version, 18.0.0)` to achieve the same behavior. "
      ],
      "teleport-use-descriptive-names": [
        "Minor naming nit: `diagnostic.Diagnostic` is repetitive ",
        "Renamed in [17b5b1b](https://github.com/gravitational/teleport/pull/58416/commits/17b5b1be3bc724e0a9f89b63d2c0354a389d1677)."
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 0,
      "following": 0
    }
  },
  "justinmk": {
    "repos": [
      "neovim/neovim"
    ],
    "entries": [
      {
        "slug": "neovim-api-consistency-patterns",
        "title": "API consistency patterns"
      },
      {
        "slug": "neovim-api-extensibility-parameters",
        "title": "API extensibility parameters"
      },
      {
        "slug": "neovim-avoid-error-masking",
        "title": "avoid error masking"
      },
      {
        "slug": "neovim-avoid-lua-ternary-traps",
        "title": "avoid Lua ternary traps"
      },
      {
        "slug": "neovim-avoid-unnecessary-configuration",
        "title": "avoid unnecessary configuration"
      },
      {
        "slug": "neovim-complete-function-documentation",
        "title": "Complete function documentation"
      },
      {
        "slug": "neovim-conditional-expensive-operations",
        "title": "Conditional expensive operations"
      },
      {
        "slug": "neovim-configuration-variable-organization",
        "title": "Configuration variable organization"
      },
      {
        "slug": "neovim-consistent-algorithm-interfaces",
        "title": "consistent algorithm interfaces"
      },
      {
        "slug": "neovim-consistent-naming-conventions",
        "title": "consistent naming conventions"
      },
      {
        "slug": "neovim-consolidate-network-apis",
        "title": "consolidate network APIs"
      },
      {
        "slug": "neovim-defer-expensive-operations",
        "title": "defer expensive operations"
      },
      {
        "slug": "neovim-document-connection-scope",
        "title": "Document connection scope"
      },
      {
        "slug": "neovim-documentation-accuracy-standards",
        "title": "Documentation accuracy standards"
      },
      {
        "slug": "neovim-documentation-formatting-standards",
        "title": "Documentation formatting standards"
      },
      {
        "slug": "neovim-follow-established-api-patterns",
        "title": "Follow established API patterns"
      },
      {
        "slug": "neovim-follow-protocol-standards",
        "title": "follow protocol standards"
      },
      {
        "slug": "neovim-initialize-before-dereferencing",
        "title": "Initialize before dereferencing"
      },
      {
        "slug": "neovim-optimize-algorithmic-complexity",
        "title": "Optimize algorithmic complexity"
      },
      {
        "slug": "neovim-prefer-concise-expressions",
        "title": "prefer concise expressions"
      },
      {
        "slug": "neovim-prevent-autocommand-reentrancy",
        "title": "prevent autocommand reentrancy"
      },
      {
        "slug": "neovim-prioritize-code-readability",
        "title": "prioritize code readability"
      },
      {
        "slug": "neovim-provide-helpful-documentation-context",
        "title": "Provide helpful documentation context"
      },
      {
        "slug": "neovim-reduce-test-verbosity",
        "title": "reduce test verbosity"
      },
      {
        "slug": "neovim-reuse-concurrency-infrastructure",
        "title": "reuse concurrency infrastructure"
      },
      {
        "slug": "neovim-security-warnings-need-guidance",
        "title": "Security warnings need guidance"
      },
      {
        "slug": "neovim-semantic-naming-over-implementation",
        "title": "Semantic naming over implementation"
      },
      {
        "slug": "neovim-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "neovim-validate-early-fail-fast",
        "title": "validate early, fail fast"
      }
    ],
    "comments": {
      "neovim-prefer-concise-expressions": [
        "please, use ternaries for simple cases. verbose code is a cost."
      ],
      "neovim-security-warnings-need-guidance": [
        "Let's adjust this to:\n```suggestion\n        .. 'Xfile is not trusted. To enable it, choose (v)iew then run `:trust`.'\n```"
      ],
      "neovim-use-descriptive-names": [
        "\"float\" is ambiguous. we should always say \"floatwin\" or win_float.\n```suggestion\nbool win_float_parse_option(WinConfig *config)\n```",
        "was this copied from vim? or generated by AI? :) we have lots of similar code like `parse_winborder`, `parse_win_config`, `parse_border_style`. why do we need even more ? \n\nand if it's really needed, the name should make it more clear that this is specifically for 'previewpopup'. The name `parse_float_option` is very unclear. A clearer name would be `parse_previewpopu_option`.\n\nAlso, should this live in `optionstr.c` ?\n",
        "@bfredl latest commit returns names like `cmdline_:` and `cmdline_/`. Is that what you were thinking?\r\n\r\nI think I'd prefer if we stuck to the standard plain `:` and `/` names, and these are simply understood (and documented) to mean \"cmdline window\". Then we have new identifiers for new window-types (e.g. `modal`, `modal-term`, ...). Normal windows are \"\" (empty string).",
        "`prompt_cur_input` ?\r\n\r\nalso a brief docstring helps when there is ambiguity. ",
        "either SGTM, slightly in favor of `prompt_gettext` since it aligns with `getcmdline()` (whereas `input*()` family of functions usually involves starting a prompt, not getting the current input?)"
      ],
      "neovim-document-connection-scope": [
        "We probably need to mention \"This only works if the UI started the server initially.\" \r\n\r\nIf the UI connected to some random remote endpoint, that is out of scope for now. We could think about how to support it if anyone shows interest."
      ],
      "neovim-defer-expensive-operations": [
        "```suggestion\r\n  `msg_history_clear` events. Performance benefit: reduces UI events traffic.\r\n```"
      ],
      "neovim-initialize-before-dereferencing": [
        "coverity:\r\n\r\n```\r\n*** CID 554963:           (FORWARD_NULL)\r\n/src/nvim/memline.c: 3484             in findswapname()\r\n3478                 if (swap_exists_action != SEA_NONE) {\r\n3479                   kv_printf(msg, _(\"Swap file \\\"\"));\r\n3480                   kv_printf(msg, \"%s\", fhname);\r\n3481                   kv_printf(msg, _(\"\\\" already exists!\"));\r\n3482                   char *run_but = _(\"&Open Read-Only\\n&Edit anyway\\n&Recover\\n&Quit\\n&Abort\");\r\n3483                   char *but = _(\"&Open Read-Only\\n&Edit anyway\\n&Recover\\n&Delete it\\n&Quit\\n&Abort\");\r\n>>>     CID 554963:           (FORWARD_NULL)\r\n>>>     Passing null pointer \"msg.items\" to \"do_dialog\", which dereferences it.\r\n3484                   choice = (sea_choice_T)do_dialog(VIM_WARNING, _(\"VIM - ATTENTION\"), msg.items,\r\n3485                                                    proc_running ? run_but : but, 1, NULL, false);\r\n3486     \r\n3487                   // compensate for missing \"Delete it\" button\r\n3488                   choice += proc_running && choice >= 4;\r\n3489                   // pretend screen didn't scroll, need redraw anyway\r\n/src/nvim/memline.c: 3492             in findswapname()\r\n3486     \r\n3487                   // compensate for missing \"Delete it\" button\r\n3488                   choice += proc_running && choice >= 4;\r\n3489                   // pretend screen didn't scroll, need redraw anyway\r\n3490                   msg_reset_scroll();\r\n3491                 } else {\r\n>>>     CID 554963:           (FORWARD_NULL)\r\n>>>     Passing null pointer \"msg.items\" to \"msg_outtrans\", which dereferences it.\r\n3492                   msg_outtrans(msg.items, 0, false);\r\n3493                 }\r\n3494                 no_wait_return--;\r\n3495                 kv_destroy(msg);\r\n3496                 xfree(fhname);\r\n3497               }\r\n```"
      ],
      "neovim-configuration-variable-organization": [
        "I like the \":\" idea to separate the variable part. (Though hopefully we don't need that in the future when we have [user-defined event keys](https://github.com/neovim/neovim/pull/34637#discussion_r2188886306).)",
        "[Rethinking this](https://github.com/neovim/neovim/pull/34388#discussion_r2196343686), can we just store the state on the client instead of creating per-client augroups?",
        "(Non-blocker; we can try it out and revisit later:) This could end up creating many top-level _lsp_...<client> vars. That's pretty noisy. It might be worth the trouble to define a single _lsp_enable table and manage things in there, e.g.\n\n```\n_lsp_enable = {\n  client = { [42] = true, [78] = true, ... },\n}\n```",
        "Actually, could we store this info on the `client` object itself? Any time the \"key\" is the client-id, it usually makes sense to store the data on the `Client`. Or is that not possible because of \"lifecycle\" requirements (maybe we need a `on_dispose` hook on the client?)",
        "Looking at this again, IIUC the per-client augroups (`get_client_augroup`) are just to enable/disable a specific client? And thus the augroup is a way to manage state. \n\nSeems like a much better pattern would be to store the state on the client itself. Could shove it in a `_foo` field for now. Then we just need 1 general `LspAttach` handler , which checks the `_foo` field on the relevant client object.",
        "> I like this because eventually we can also use a more general `features` property\r\n\r\nsame thought popped into my mind... :) related: https://github.com/neovim/neovim/issues/34659"
      ],
      "neovim-avoid-error-masking": [
        "why silent with \"!\"? that silences errors...",
        "use `vim.system():wait()`. that avoids fragile things like `vim.v.shell_error` :)",
        "`t.pcall_err`\n```suggestion\n    eq(':edit command in prompt buffer throws error',\n       t.pcall_err(api.nvim_command, 'edit'))\n```"
      ],
      "neovim-api-consistency-patterns": [
        "this existing test shouldn't require a change, because the enhancement to serverlist() should be opt-in, i.e. the user needs to pass something like:\n\n    vim.fn.serverlist({peer=true})",
        "```suggestion\n  -- TODO: Handle this generally (like vim.ui.open()), rather than overriding gf. #7282\n```"
      ],
      "neovim-conditional-expensive-operations": [
        "this is a perf cost, which should only be paid if it's actually necessary (i.e. depending on the `follow` param)",
        "perf nit: vim.ts is [lazy loaded](https://github.com/neovim/neovim/blob/7cd5356a6f89a46d83bbba9b7f6496b67f054629/runtime/lua/vim/treesitter.lua#L6) so let's wrap this to avoid eager-loading it:\n```suggestion\n    complete = function(...) return vim.treesitter.language._complete(...) end,\n```",
        "> I think ideally the full `DiagnosticChanged` autocmd should only be setup on demand if the default status line is actually used. Otherwise everyone pays the cost for the string formatting.\r\n\r\nOh, should we introduce this as `vim.diagnostic.status()` (compare `vim.lsp.status()`) ?\r\n\r\nAnd yes, making this conditional on whether the default statusline is used, makes sense.",
        "> `vim.lsp.status()` displays messages from all clients for all buffers.\r\n\r\nvim.diagnostics.status() doesn't need to exactly match how lsp.status works; it's just a reference for the \"shape\" + name.\r\n\r\n\r\n\r\n> If we decided to go with `vim.diagnostic.status()` would it be used in the statusline as is i.e.\r\n\r\nI think so. And then the [default 'statusline'](https://github.com/neovim/neovim/blob/6577d72d819dde32abeacd6a72d6ba64483f7ddc/src/nvim/options.lua#L8577-L8584) will reflect this too, which is Good. If there are \"performance\" issues, those need to be fixed internally in `status()`, e.g. by debouncing or whatever."
      ],
      "neovim-prevent-autocommand-reentrancy": [
        "Side-note: not a blocker for this PR, but need think about how to show an error if this logic fails. E.g. `:connect bogus` currently just exits without saying why."
      ],
      "neovim-optimize-algorithmic-complexity": [
        "this has triple-nested for-loops and doesn't say even 1 comment giving a hint about what it does."
      ],
      "neovim-complete-function-documentation": [
        "This is a whole new blob of code that future developers must study. We really need to give hints where we can. In this case, the key idea seems to be (correct if wrong):\n\n\n```suggestion\n  -- Compute positions, set them as extmarks, and store in diagnostic._extmark_id\n  -- (used by get_logical_location to adjust positions). \n  once_buf_loaded(bufnr, function()\n```\n\nNone of this will be obvious in the future. We need to leave hints.",
        "```suggestion\n--- Can also be shown with `:EditQuery`. [:EditQuery]()\n--- `:EditQuery <tab>` completes injected language names.\n```",
        "try this\r\n\r\n```\r\n--- Can also be shown with `:EditQuery`. `:EditQuery <tab>` completes injected language names. [:EditQuery]()\r\n```\r\n\r\nbtw, I'm confused about the mention of \"injected languages\" in your docs, since it looks like `_complete()` just iterates all available parsers, not specific to \"injected languages\" for the current buffer.",
        "need a `@return` tag. maybe helpful to look at `vim.system` example https://github.com/neovim/neovim/blob/e518666f1db110abcfc899e1227da949998cdd82/runtime/lua/vim/_editor.lua#L130",
        "the wording is kind of confusing, is this rewording correct?\r\n```suggestion\r\n--- @return string|boolean|nil Response body (sync) or `true` (async) on success; `nil` on failure\r\n```",
        "is there a 1-line docstring that can give insight into what this does? E.g. \"Sets a mark unless...\"",
        "missing `@since`\r\n\r\n```suggestion\r\n--- Computes the common range shared by the given ranges.\r\n```"
      ],
      "neovim-reuse-concurrency-infrastructure": [
        "could that be a flag on `vim.system` ? want to avoid teaching people to reach for similar things in different places. "
      ],
      "neovim-follow-protocol-standards": [
        "```suggestion\n  bool is_tcp = !!strrchr(server_address, ':');\n```"
      ],
      "neovim-avoid-unnecessary-configuration": [
        "no, special-purpose options are clusmsy and a maintenance burden.\n\nthe *actual* implementation is only a few lines long. since users have to find this documentation and add this option anyway, it's nearly equivalent to just documenting an event handler that they can copy into their config. "
      ],
      "neovim-validate-early-fail-fast": [
        "also need to handle the case where the +cmd didnt cause an exit nor an error ?",
        "> We could just call `getout(0)` if we get there by logging an error message\r\n\r\nlogging *and* then call `getout` makes sense 👍 ",
        "> I was initially thinking to just match against a list of possible \"quit\" commands (and combinations) and then checking against that\n\nThat's probably good enough for now.\n\nThe \"right\" way to do things is probably to setup some sort of `on_exiting` callback which performs `remote_ui_restart` only when Nvim is about to exit, wherever the `VimLeavePre` logic is."
      ],
      "neovim-consolidate-network-apis": [
        "side note: could drop this and use `vim.net.request()` now ?",
        "I suppose this can be a followup, but mentioning before I forget:\r\n\r\nwe will need to define new `NVIM_TEST_INTEG` env var and document it here: https://github.com/neovim/neovim/blob/f731766474901e5e345e0ca630315ef69122e556/test/README.md?plain=1#L436\r\n\r\nIt should be disabled by default. When enabled, it enables these tests which make network calls. Can use `t.skip()` to guard these tests, or define something like `skip_integ` similar to :  https://github.com/neovim/neovim/blob/f731766474901e5e345e0ca630315ef69122e556/test/testutil.lua#L837",
        "let's also have a test that tests text response and binary response (jpg or something)"
      ],
      "neovim-follow-established-api-patterns": [
        "`:help dev-naming` i would expect a `enable(enable:boolean)` pattern, not a stop() function\r\n\r\nalso agree that if we want a new module then it seems attractive to have a [format module](https://github.com/neovim/neovim/pull/34637#discussion_r2165069136) for formatting-related things. (We also discussed a `vim.lsp.workspace` module for workspace-related thing).\r\n\r\nBut ideally I think [`vim.lsp.enable` or a similar](https://github.com/neovim/neovim/pull/34637#discussion_r2166175192) top-level demuxer seems to be badly needed, instead of numerous `vim.lsp.x.enable()` things.",
        "if more filters are being added then it's time to make this a \"kwargs\" arg instead of adding more parameters.\r\n\r\nprobably `is_enabled({filter})` is best, though `is_enabled({bufnr}, {filter})` could be acceptable (and `bufnr=nil` would be \"all/any buffer\")\r\n\r\n```suggestion\r\nis_enabled({bufnr}, {filter})             *vim.lsp.inlay_hint.is_enabled()*\r\n```",
        "Isn't a filter kwargs table the common pattern? Not sure why `vim.lsp.completion.enable()` differs though.\r\n\r\n```suggestion\r\nenable({enable}, {filter})\r\n```",
        "> `semantic_tokens` doesn't either (but it also uses `start()` and `stop()` to be weird 😆\r\n\r\noh we might need to fix that. https://github.com/neovim/neovim/issues/34664\r\n\r\n> Seems like enabling/disabling features is kinda all over the place right now\r\n\r\n`:help dev-patterns` documents the expected `enable()` pattern.\r\n\r\n> will just take some more logic to handle global enabling/disabling\r\n\r\nif possible we should add a `vim._util` to help with that",
        "> supporting global enabling will add a lot of complexity (especially if we specify only one of `buf = nil` and `client_id = nil`. Is this a blocker or can it be put on hold?\r\n\r\nthere is no requirement to support any particular behavior, as long as the interface has the right pattern. i.e `enable(boolean)`",
        "Just to be clear, there is no requirement for the initial impl of this or other lsp features to fully support things like per-client enablement, etc. The only requirement is that the basic signature of `enable(enable:boolean, filter?:table)` is correct :) \r\n\r\n\"per-client\", \"per-buffer\" can be added later by expanding the `filter` kwargs."
      ],
      "neovim-provide-helpful-documentation-context": [
        "does the `This fails when changes have been made` note need to be updated? Also should mention `:confirm restart` as a hint.",
        "If it checks changes by default then :confirm is never needed? It probably makes sense to *not* check changes by default, then user can opt-in to checking changes via :confirm.",
        "docs should also show a +cmd example",
        "instead of mentioning this in a different place, add a line after line 1324 like\n\n    `:EditQuery <tab>` completes injected language names.",
        "should the module `@brief` mention what \"linked editing session\" is? Not obvious to me. Also linking to the spec section helps.",
        "non-blocker: should we mention something in lsp.txt (not the private fields, but just something like \"DiagnosticRelatedInformation is supported/included\")? I looked around and the best place would be a `@brief` in `lsp/diagnostic.lua` , then it will appear in the \"overview\" part of `:help lsp-diagnostic`\r\n\r\n"
      ],
      "neovim-api-extensibility-parameters": [
        "- We almost always want to accept `opts` and return `Dict`, unless we are 100% certain we won't need to expand the features in the future (which is usually wrong).\r\n- But in this case the returned value is a tabpage handle which (1) already represents an object, and (2) is symmetric with `nvim_open_win`.  So it's probably acceptable to return `Tabpage`.\r\n- For `opts` (or `config`?) I could imagine an arg that controls which tabpage to open next to (like `:[count]tabnew`).\r\n\r\n```suggestion\r\nTabpage nvim_open_tabpage(Buffer buffer, Boolean enter, Dict opts, Error *err)\r\n```",
        "FUTURE: we might want to give more control over how the server is stopped. E.g.:\r\n\r\n    :restart +qall\r\n\r\n(similar to `:help +cmd`). This also frees up \"!\" to be used for some other purpose."
      ],
      "neovim-reduce-test-verbosity": [
        "this is the same screen, isn't it? just use a multiline pattern to match the contents of 1 screen, instead of multiple expect() with `unchanged=true`.\n\nsame for the other cases below.\n\n```suggestion\n      screen:expect({ any = 'Allowed.*\\n.exrc' })\n```",
        "When the literal contents are known we usually don't use match. It tends to be more readable to write the literal contents, e.g. in this case it would look like (I think):\n\n    {1:~                          }|*2",
        "oh, yeah that makes sense.",
        "test is too verbose just to check that a commmand requires an argument. that only requires ~2 lines to check. also rename this to: \n```suggestion\n  it('validation', function()\n```\n\nthen we can add more validation later.",
        "is it possible for these tests to re-use a shared function, to make them less verbose?\r\n\r\nindirection can make things harder to read, but if these tests are mostly doing the same kind of thing, then using a shared function can be much more readable because it makes it obvious what is being tested in each case.",
        "are we actually testing anything here or is this just redundantly testing the existing behavior  of extmarks?\r\n\r\nis there a less verbose way to instead (1) assert that extmarks are mapped as expected to a diagnostic, and (2) just assume that extmarks work they way they should?\r\n\r\nsame question for all the other tests.",
        "instead of a separate test for this case, let's just add an extra step to the end of the one above, something like `enew` followed by `eq('', exec_lua(return vim.diagnostic.status()`.\n\nthat saves 20 lines of code, which is valuable.",
        "note: the args are flipped :) `eq()` takes the \"expected\" value as the first arg.",
        "the earlier lines in this test already make edits (and are more readable than \"asdf\"). wouldn't it make sense just to add `prompt_getinput` assertions after those screen tests, instead of peforming new edits here ?"
      ],
      "neovim-prioritize-code-readability": [
        "this logic looks suspicious. what is the purpose of the `metadata.conceal ~= nil` check, since if `metadata.conceal=false` it will fallthrough to the latter part anyway? also, use parens for clarity, since `a and b or c and d` is not obvious.",
        "not asking for an if statement, just parens",
        "use `('...'):format()` almost always. using `..` to concatenate usually ends up being less readable.",
        "> As a modest proposal, could we perhaps increase the column threshold from 100 to like 120?\r\n\r\nPushed an update.\r\n\r\nAre we ok with the changes to other test files, or should the 120 width be limited to certain files (idk if that's possible)"
      ],
      "neovim-semantic-naming-over-implementation": [
        "is there an `on_foo` name we can use? same for the others. \r\n```suggestion\r\ndeclaration({opts}, {on_result})                    *vim.lsp.buf.declaration()*\r\n```",
        "All of the callbacks in this pr need some sort of `on_` prefix (`:help dev-naming`). We use \"callback\" way too much. ",
        "> `on_` isn't necessary for continuations.\r\n\r\nIt is much easier to communicate one invariant rule rather than multiple, subjective rules. `on_x` tends to result in much more meaningful names, while also avoiding over-use of `callback`.\r\n\r\n> What is \"too much\"?\r\n\r\n`callback` and `cb` are used in places where an `on_x` name would be more useful and intuitive. And then people see that and just do even more of it. That is why it matters: people copy what existing code is doing, so isolated decisions spread out and multiply.",
        "> leads lots of variations of the same name: `on_exit`, `on_done`, `on_finished`, `on_result`, `on_complet`\r\n\r\nIf those are the same then they should align on one name.\r\n\r\n> It's also not obvious these names are callbacks... unlike `callback`.\r\n\r\nWhat happens when there is more than one callback? Then you need a qualifier anyway. So having a common prefix is extra helpful in that case.\r\n\r\nAnything prefixed with `on_` is a callback, I don't see how that is unclear.\r\n\r\n> but the `on_x` thing just seems like a specific thing you are over pushing for everything and seems to come directly from js. \r\n\r\nNo, this decision was made after experience from early days of the project where `_cb` prefix was used. `on_` is much more readable and conventional. You made a wrong assumption/speculation here.\r\n\r\nIt's a project guideline, and continually objecting to project guidelines is really not helpful. ",
        "> > What happens when there is more than one callback?\r\n> \r\n> Then it's not a continuation.\r\n\r\nA continuation is just a special case. And naming it differently leads to confusion and debates, which is exactly what I'm trying to avoid.\r\n\r\nMany people are just going to copy `callback` even when it's not a \"continuation\" or whatever other specific circumstances. A blanket rule is easier to follow and document.\r\n\r\n> Maybe if you're used to js.\r\n\r\nIt could be named `abc_` for all I care, what matters is that it's a common consistent prefix instead of a different subjective choice everywhere. \r\n\r\nIf it's hard for you to adapt to `on_` then you'll appreciate that it's even harder to document and learn when different kinds of callbacks/handlers should be named in which way in various contexts.",
        "> If there are multiple events then `on_` can be switched to.\r\n\r\nNot if it's a field, like in the case of `nvim_create_autocmd({callback=...})` which is a case where `on_x` was clearly more meaningful and readable.\r\n\r\nAnd \"we'll just do it differently here and there\" is what I'm trying to avoid.",
        "I added it because we had no guidelines and we have things named `handler`, `callback`, `x_cb`, `on_x`, and more (js uses \"listener\" btw). What's your point? I get criticized when there are no guidelines, and when there are...\r\n\r\nHow about this compromise:\r\n\r\n- `callback` for continuations (exactly one callback param, not multiple [like uv.new_work](https://github.com/neovim/neovim/blob/4745270bf124a960ccdffdddbb6c27b7bf36ba82/runtime/doc/luvref.txt#L3600) for example),\r\n- `on_x` for everything else\r\n\r\nand we can commit to this (no debates about special cases or personal preferences in the future)? Then I will update the docs.",
        "`on_response` sure would be more meaningful here. Instead the docs have to explain it.",
        "does `on_response` make sense as the name for this? \"exit\" is an implementation detail.  the fact that curl is used internally isn't relevant and shouldn't be surfaced in the request() interface (for now)."
      ],
      "neovim-documentation-accuracy-standards": [
        "the docstring looks wrong, it doesn't return anything"
      ],
      "neovim-consistent-naming-conventions": [
        "Let's stick to \"pos\" as the nonce for position/location. Jargon creep adds friction.\n\n```suggestion\nlocal function get_logical_pos(diagnostic)\n```",
        "In a description many different terms may be used to orient the reader. But in interface names, same concept should always have same name.",
        "avoid the `_cb` suffix, use an `on_` suffix. `:help dev-naming`",
        "`on_cmd` is fine",
        "The name FRProvider seems questionable. Why not name it `FoldRange` or `FoldState`? If it's just an object that does stuff, that's the typical pattern.\r\n\r\n\"Provider\" implies that there will be different backend implementations of a concept, typically platform-specific in some way. If we're only talking about polymorphism or a \"base class\", it's helpful to just name the base-class by its main purpose.",
        "> The suffix `Provider` is just to highlight that it have a common base class and contains methods.\r\n\r\nOk then \"Provider\" isn't the right name, for the reason mentioned in my above comment.\r\n\r\n`vim.lsp.folding_range.State` seems fine, but the name of the instance should probably be `foldState` or `foldRangeState`.",
        "would refresh() make sense here? that's a common name in other modules.",
        "the doc seems to imply this name:\n\n\n```suggestion\nlocal function get_confirm_update_plugins(bufnr)\n```",
        "start/end are the normal convention in the codebase\r\n\r\n```suggestion\r\n          local mark = { start_line = start_row, start_col = start_col, opts = opts }\r\n```",
        "since \"mode\" is pretty similar to \"modal\", maybe mention \"win\" in these new names. My reflex would be a prefix (`win_modal_active`, `win_cmdwin_active`, `win_modal_result`, `WinModalType`, ...) but I guess the prevalence of `cmdwin_xx` would justify `modalwin_xx` (this also complements `floatwin_xx`).",
        "handlers should be prefixed with `on_`. or if we think of this function as \"procedural\" we can just call it `show_message()` ?\r\n\r\n\r\n\r\n```suggestion\r\nlocal function on_show_message(params, ctx)\r\n```"
      ],
      "neovim-consistent-algorithm-interfaces": [
        "please search for \"predicate\", \"filter\", \"skip\", and any other related concept, in the :help docs to see which is the most common. I'm wondering if positive logic (\"match\") or negative logic (\"skip\") is more common and more intuitive. ",
        "> I borrowed the concept from `vim.fs.dir`\r\n\r\nOh, I missed that an existing `vim.fs` function already has this concept. In that case this seems fine.",
        "does returning `false` omit a directory (that matches the `vim.fs.dir` case)?\r\n```suggestion\r\n                 • {filter}? (`fun(dir: string): boolean`) Predicate that\r\n                   decides if a directory is traversed. Return true to traverse\r\n                   a directory, or false to skip. ...                   \r\n```",
        "```suggestion\r\n• |vim.fs.find()| accepts a `filter` predicate.\r\n```"
      ],
      "neovim-documentation-formatting-standards": [
        "```suggestion\r\n• |prompt-buffer| supports multiline input/paste, undo/redo, and o/O normal\r\n  commands.\r\n```"
      ],
      "neovim-avoid-lua-ternary-traps": [
        "ternary"
      ]
    },
    "profile": {
      "location": "Berlin",
      "blog": "https://sink.io",
      "twitter_username": "justinmk",
      "site_admin": false,
      "followers": 1615,
      "following": 12
    }
  },
  "tannergooding": {
    "repos": [
      "dotnet/runtime"
    ],
    "entries": [
      {
        "slug": "runtime-abstract-traversal-patterns",
        "title": "Abstract traversal patterns"
      },
      {
        "slug": "runtime-avoid-busy-waiting",
        "title": "Avoid busy waiting"
      },
      {
        "slug": "runtime-centralize-platform-configurations",
        "title": "Centralize platform configurations"
      },
      {
        "slug": "runtime-choose-appropriate-error-mechanisms",
        "title": "Choose appropriate error mechanisms"
      },
      {
        "slug": "runtime-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "runtime-decompose-complex-algorithms",
        "title": "Decompose complex algorithms"
      },
      {
        "slug": "runtime-document-configuration-intent",
        "title": "Document configuration intent"
      },
      {
        "slug": "runtime-document-function-contracts",
        "title": "Document function contracts"
      },
      {
        "slug": "runtime-enable-configurable-instrumentation",
        "title": "Enable configurable instrumentation"
      },
      {
        "slug": "runtime-honor-api-contracts",
        "title": "Honor API contracts"
      },
      {
        "slug": "runtime-maintain-configuration-compatibility",
        "title": "Maintain configuration compatibility"
      },
      {
        "slug": "runtime-maintain-consistent-formatting",
        "title": "Maintain consistent formatting"
      },
      {
        "slug": "runtime-maintainable-test-structure",
        "title": "Maintainable test structure"
      },
      {
        "slug": "runtime-model-actual-hardware-costs",
        "title": "Model actual hardware costs"
      },
      {
        "slug": "runtime-optimize-aligned-simd-operations",
        "title": "Optimize aligned SIMD operations"
      },
      {
        "slug": "runtime-optimize-build-dependency-chains",
        "title": "Optimize build dependency chains"
      },
      {
        "slug": "runtime-optimize-common-paths",
        "title": "Optimize common paths"
      },
      {
        "slug": "runtime-optimize-for-readability",
        "title": "Optimize for readability"
      },
      {
        "slug": "runtime-optimize-memory-access",
        "title": "Optimize memory access"
      },
      {
        "slug": "runtime-platform-agnostic-network-apis",
        "title": "Platform-agnostic network APIs"
      },
      {
        "slug": "runtime-platform-aware-algorithm-optimization",
        "title": "Platform-aware algorithm optimization"
      },
      {
        "slug": "runtime-simplify-code-expressions",
        "title": "Simplify code expressions"
      },
      {
        "slug": "runtime-specific-exceptions-with-context",
        "title": "Specific exceptions with context"
      }
    ],
    "comments": {
      "runtime-document-function-contracts": [
        "Fixed."
      ],
      "runtime-document-configuration-intent": [
        "They've been discussed in various blogs and are fairly well known in the space as the way to test fallback paths if your hardware supports the latest.\r\n\r\nI don't think it'd be the end of the world to drop them and the devs primarily using these switches could update, but didn't want to do so without discussion.\r\n\r\nWould the `--instruction-set` flags for R2R/NAOT be in the same boat? If we could remove the extras there it would also simplify things. I don't believe the flag is considered officially supported today, but does print out on the command line.",
        "Removed the config knobs and instruction-set switches"
      ],
      "runtime-maintain-consistent-formatting": [
        "nit: These should be moved up near the other `AVXVNNI` instructions so they're part of the `AVX` grouping rather than strictly the `AVX512` grouping checks.",
        "```suggestion\r\n    <GenAPITargetPath>$([MSBuild]::NormalizePath('$(MSBuildProjectDirectory)', '..', 'ref', '$(AssemblyName).netcore.cs'))</GenAPITargetPath>\r\n    <!-- SA1001: Commas should not be preceded by a whitespace; needed due to ifdef -->\r\n    <NoWarn>$(NoWarn);SA1001</NoWarn>\r\n```"
      ],
      "runtime-optimize-for-readability": [
        "The xplat helper intrinsics support operators and so we can make this \"more readable\" by just using `x & y`.",
        "Believe so, the fixer automatically applied them.",
        "```suggestion\r\n            if (dimension is 0 or -1)\r\n```"
      ],
      "runtime-specific-exceptions-with-context": [
        "We weren't entirely consistent about this throughout all of our APIs. Several of them validated `s` first and several others validated `style` first.\r\n\r\nI opted for making it consistent as `s` first since that is validating the arguments \"in order\"."
      ],
      "runtime-abstract-traversal-patterns": [
        "This was the cleanest/easiest way I could think of to do this.\r\n\r\nThe general idea is that we have several transforms we want to make to LIR before containment happens (because containment complicates these transforms). However, since we're in LIR form for these nodes at this point, we need to make sure the transform is safe to do.\r\n\r\nAn alternative would be to add a `pre` pass for lowering then do a separate `post` pass for containment, but that feels like a bigger/bulkier/riskier change.\r\n\r\nThere are some other transforms that would be nice to move \"here\" longer term, like the sequential `insertps` operation folding we do; or the recognizing of `AND(x, NOT(y))` we do; neither of which we want to do in HIR because it breaks or massively complicates other optimizations (like folding and operation negation) that we do.\r\n\r\nI'd be happy to make this a separate pass for .NET 11, if we feel that is better. But for .NET 10 I think this is a less risky approach."
      ],
      "runtime-optimize-common-paths": [
        "Instructions like `pextrw` which extract to a general purpose register or like `pinsrw` which insert from a general-purpose register don't take masks.",
        "Instructions like `pmaddwd` which are `INS_TT_FULL_MEM` don't have the `Input_*Bit` flag since they cannot support embedded broadcast and only ever take the full simd size.",
        "Checks throughout the JIT have been ordered under the presumption that `varTypeUsesIntReg` is the most likely occurrence. Where mask register handling is needed, `varTypeUsesMaskReg` is the middle case since we only need 1 additional check to cover all 3 cases. This allow the last case to `assert`.\r\n\r\nThis keeps 1-check for non-xarch, minimizes the total ifdefs when mask handling is needed, and ensures we need no more than 2 checks on xarch with the common case being 1 check. Ideally this helps keep it \"pay for play\".\r\n\r\nHaving `varTypeUsesIntReg` allowed a few other checks to become simplified/improved in general.",
        "I think it'd be better to use the `AuxiliaryJitType` and mark these as `SpecialCodeGen` than to add a bunch of extra table entries. More similar to how `Gather/Scatter` get handled.\r\n\r\nIf we were to do the extra table entries, I'd rather they be moved to the bottom of the file with the other extra intrinsics that don't directly map to managed API names. That way the lookups for managed APIs remain cheaper and clearer.",
        "That's the purpose of `AuxiliaryJitType`, to give a secondary type to use.\r\n\r\n`SimdBaseJitType` is the primary type and for most intrinsics is the sole type needed to determine which instruction to use. Some special intrinsics need a secondary type and that's the purpose of `AuxiliaryJitType` so that we have a way to track it where required.",
        "Most of these ISAs aren't meaningful to test anymore, especially since we aren't doing bringup of the ISAs.\r\n\r\nFor the most part, we just care about the unique ISA paths used in corelib or the JIT, which is primarily the SSE and AVX families that are already covering cases like FMA or BMI being disabled.\r\n\r\nI think we can even get it so that we don't need to test `nossse3, nosse41, and nosse42` as well; but we'd need to do a bit of cleanup in the JIT to achieve that.\r\n\r\n-- I'd like to get our support matrix down to effectively:\r\n* x86-64-v1: This is the baseline and is currently targeted by NAOT\r\n* x86-64-v2: This is everything up through SSE4.2+POPCNT\r\n* x86-64-v2 + AVX: This tests the VEX encoding and is known to have a decent user share\r\n* x86-64-v3: This is AVX2+FMA+BMI1+BMI2\r\n* x86-64-v4: This is AVX512 and covers the EVEX encoding\r\n\r\nFor x86-64-v2:\r\n* this is what is supported by the x64 on Arm emulation provided by Apple and Windows\r\n* it would be nice if NAOT could target this as the default, as its an 18 year old baseline\r\n* this is the target required by Win11 24H2 and boot is blocked if the ISAs aren't available\r\n* Win11 prior to 24H2 is documented as requiring SSE4.1 otherwise",
        "> I do not think we want to have the baseline supported piecemeal (it is unlikely we would be able to it correctly since it is impossible to test). If we want to raise the baseline, we should do it for the whole product. I would not be opposed to raising the product baseline to x86-64-v2.\r\n\r\n👍. I think it would provide a nice simplification in the JIT and libraries for what code paths we need to support. I don't want to raise the bar \"too high\" and negatively impact a significant number of customers, but I do think that `x86-64-v2` and `armv8.1-a` are reasonable baselines at this point.\r\n\r\n> Where is it documented?\r\n\r\nIn https://learn.microsoft.com/en-us/windows-hardware/design/minimum/minimum-hardware-requirements-overview, specifically under the Windows 11 document (last updated 2021)\r\n\r\n![image](https://github.com/user-attachments/assets/ad370ba3-9e00-49fc-8683-43d08338289a)\r\n\r\nWe got the notification that SSE4.2+POPCNT in 24H2+ when we had last reached out about Arm RDM support; although there's also been some news about the boot blocker that exists that went around last year as well.\r\n\r\nAzure, AWS, Google Cloud, and other major cloud providers that specify what hardware they provide are entirely on x86-64-v3 or later. There are also other 3rd party numbers out there, such as the Steam Hardware Survey, which shows 99.78% of reporting users have x86-64-v2 (while 97.31% have AVX and 94.66% have x86-64-v3).\r\n\r\n> When did Intel/Amd stop selling the last processor without x86-64-v2? It is the more interesting date for this discussion.\r\n\r\nFor Intel the last CPUs pre x86-64-v2 were discontinued in 2013, around the time x86-64-v3 was launched. This would have been the Bonnell microarchitecture (part of the Intel Atom lineup for low end machines). The Conroe/Merom and Penryn/Wolfdale chipsets had been discontinued in 2011-2012. AMD discontinued their 10h and Bobcat lineups in 2012-2013 as well.\r\n\r\nSo if we did try to raise the baseline in .NET 11, it should only be for 12-18 year old computers which are no longer supported by the CPU manufacturers, by MacOS, or by current Windows.\r\n"
      ],
      "runtime-maintain-configuration-compatibility": [
        "@jkotas are we planning any other R2R breaking changes this release? If so, it would be nice to go ahead and clean up (potentially just in a follow up PR) some of the R2R values; otherwise, I can log a tracking issue for it.\r\n\r\nThe two big things are that we have some unused gaps now that could be filled in and I don't see why we need to use unique IDs across X86 and Arm64 (that is `X86Base` and `ArmBase` can't currently both be R2R ID 1).\r\n\r\nFor the latter point, the general intent of the R2R flags is to track when user code has some `if (Isa.IsSupported) { }` check that isn't taken, because we have to assume it may cause different behavior. However, on x64 some `AdvSimd.IsSupported` check can never be true, so it can't ever cause a difference in behavior and so we shouldn't need to track it at all and so the x64 vs Arm64 R2R IDs should be able to overlap, with them differentiated by a general \"target architecture\" flag instead. \r\n\r\nI believe we should be able to do much like we do in NAOT where we simply track 1 bit per ISA on the relevant architecture. In other words, I expect we could essentially simplify this to almost the same as https://github.com/dotnet/runtime/pull/115983/files#diff-37e89a49ea22a08b2d0502ca3b1716e2dd1bd68cafd3eeb3cd45f5a4406601d3R219, which would also let us avoid needing to track \"specifiable\" or not and needing unique bit entries for cases like `gfni_v512` (which is tracked as `gfni + avx512` for NAOT). We should just need to track the baseline ISA in addition to what NAOT tracks, since that can be disabled for testing user fallbacks.",
        "I'll revert for now. However, ...\r\n\r\n> This would make r2rdump more complicated. Right now, r2rdump just does Enum.ToString() to dump the instruction set.\r\n\r\nI think it would be better to make it slightly more complex by taking this break long term and adding a compat handler to r2rdump to handle older image versions.\r\n\r\nRight now we're using 60 bits for R2R, when we should only need 34 total. If we split it by platform then it's 11 for Arm64 and 23 for x64. Just given the pending work around SVE and AVX, we will likely go over 64-bits in the next release and need to take a break anyways.\r\n\r\nI think the way NAOT has this setup is more flexible and was done because it came online later and was able to handle some of the scenarios that R2R didn't have the foresight to consider.\r\n\r\n> NAOT does not have the versioning problem like R2R. There is no mixing and matching of versions, etc.\r\n\r\nRight, but I think it's also something we can handle for R2R to make things overall cleaner moving forward. Not something we have to do now, but I expect we'll be forced to deal with it in the .NET 11 timeframe\r\n\r\n",
        "Reverted to preserve the compat for .NET 10"
      ],
      "runtime-optimize-memory-access": [
        "This is working around the lack of variable sized `ExtractMostSignificantBits`.\r\n\r\nIt's not pretty, but it works and ideally gets replaced in .NET 8 as part of the `VectorMask` work...",
        "We now expose helper intrinsics that directly operate on `ref`: `LoadUnsafe(ref T source, nuint elementOffset)`.\r\n\r\nThis helps avoid pinning, which can have measurable overhead for small counts and which can hinder the GC in the case of long inputs.\r\n\r\nIt likewise helps improve readability over the pattern we are already utilizing in parts of the BCL where we were using `Unsafe.ReadUnaligned` + `Unsafe.Add` + `Unsafe.As`.\r\n\r\n",
        "`ExtractMostSignificantBits` behaves just like `MoveMask` on x86/x64. This is also exposed by `WASM` as `bitmask`",
        "I've preserved the `Vector256` path given that it was already here and I would presume has undergone the necessary checks to ensure it is worth doing on x86/x64.\r\n\r\nArm64 doesn't support `V256` and so will only go down the `V128` codepath.",
        "This only validates the alignment is a power of 2 as that is relatively cheap. Also validating that `byteCount % alignment == 0` would be fairly expensive.",
        "I've simplified the alignment check to just `if ((alignment % (uint)sizeof(void*)) == 0)`, which will generate simply `test dl, 7` (or `test dl, 3` for 32-bit)",
        "I'll also add in a check for `byteCount` since that can just be `(byteCount & (alignment - 1)) == 0`",
        "Yes, it does. Thanks for the catch.\r\n\r\nWill update and add a test covering this scenario.",
        "Should be resolved now.",
        "`BinaryPrimitives.ReadInt64LittleEndian` is likely \"better\" than `BitConverter`, it's at least more explicit. `MemoryMarshal.Read` is likely something we want to avoid in \"safe\" code because of where it lives (even if it does bounds check/etc).\r\n\r\nToday, we basically have to pick between being \"safe\" or being \"efficient\". We can get semi-efficient for scalars, but that's potentially leaving 2-64x perf on the table, depending on platform/scenario.\r\n\r\nIf the JIT could elide bounds checks for `V128.Create(rospan)` then we have a safe way to operate on the data. We can alternatively centralize the unsafe code behind a core helper, for example we have `InvokeSpanIntoSpan` and a pattern that makes use of \"functional interfaces + generics\" to do this for `TensorPrimitives`. This at least reduces risk as the logic that accesses memory can be well tested and not duplicated everywhere. We'll never be able to get rid of *all* unsafe code either, something like `TensorPrimitives` which needs to handle large data needs to be able to pin so it gets access to non-temporal loads/stores, for example.\r\n\r\nI think our goal should therefore really be to move paths like these to use centralized helpers instead of trying to rewrite them to be \"safe\". We know they're important to perf and we know they benefit from unsafe today, so lets pull some of the `InvokeSpanIntoSpan` and `Aggregate` helpers so that cases like this are functionally rewritten to `=> Aggregate<T, IdentityOperator<T>, Crc32Operator<T>>(x)` and all the \"dangerous\" code is centralized. When the JIT finally gets support for eliding bounds checks over `V128.Create(rospan)` that one helper can be rewritten to use the safe pattern and the rest of the BCL implicitly gets it. We can add a number of `Debug.Assert`, memory access checks, and even potentially `opt-in` forced validation path (such as would lightup via a feature switch) to help ensure robustness in the interim.",
        "> I get that this pattern is centralizing the unsafe code, but I do not think that it reads well.\r\n\r\nIt doesn't today, but the long term goal would be to get something like `functional interfaces` into the language so that you can rather have something like `=> Aggregate(span, (previous, value) => Crc32(previous, value))` instead and the lambda binds to the `TOperation` given `where TOperation : IOperation<....>` so that it becomes \"prettier\"\r\n\r\nWe're not always going to be able to hit the trifecta of `safe`, `efficient`, and `pretty`. We can, however, strike a decent enough balance between them and as a long term goal push towards improving the scenarios where it falls down."
      ],
      "runtime-honor-api-contracts": [
        "Sign is supposed to throw for NaN (unlike CopySign which does not). It's a historical behavior/contract of the API."
      ],
      "runtime-choose-appropriate-error-mechanisms": [
        "We shouldn't assert unreached here. There are always scenarios where unused values get preserved, such as min-opts, and so we should prefer the typical pattern of `if (foundUse) { use.ReplaceWith(node); } else { node->SetUnusedValue(); }`",
        "Previously the JIT would fail fast with `The JIT compiler encountered invalid IL code or an internal limitation.`\r\n\r\nNow, it will assert in debug mode but will `throw PlatformNotSupportedException` at runtime.",
        "~This was changed to not use `impUnsupportedHWIntrinsic` and instead use `gtNewMustThrowException` directly.~\r\n\r\n`impUnsupportedHWIntrinsic` was renamed to `impUnsupportedNamedIntrinsic` and moved out of `FEATURE_HW_INTRINSIC`"
      ],
      "runtime-optimize-aligned-simd-operations": [
        "I have it erroring if `posix_memalign` isn't available as a fallback. `posix_memalign` is a very old (`_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600`) function and should be always available.\r\n\r\nLikewise, I don't believe trying to polyfill via `malloc`/`free` is reliable as there is no guarantee that `free` works with arbitrary pointers:\r\n> The behavior is undefined if the value of ptr does not equal a value returned earlier by malloc(), calloc(), realloc(), or aligned_alloc() (since C11).\r\n\r\nLikewise, there is no reliable way to backtrack from a given aligned `ptr` to the actual `ptr` that `malloc` did return.",
        "I'd like to see if any platforms don't actually have `posix_memalign` before falling back to storing the actual pointer before the returned pointer."
      ],
      "runtime-avoid-busy-waiting": [
        "I know this is existing, but this is a bug/memory safety violation.\r\n\r\nConsider the case where `location` refers to actually immutable memory. In such a case, `Volatile.Read` should succeed, but in actuality, `CompareExchange` will end up writing `0` in the case the value is actually `0`, which will cause an AV.",
        "We can't for double, but we could remove the unsafe code here for `ulong` by following the same pattern that uses `Interlocked.CompareExchange` and `Interlocked.Exchange` under the ifdef."
      ],
      "runtime-platform-aware-algorithm-optimization": [
        "> to always return false\r\n\r\n`AvxVnni*.X64.IsSupported` should return false on a 32-bit machine and on a 64-bit machine it should return the same result as `AvxVnni*.IsSupported`",
        "Is this implication going to exclude the CPUID for `AVX10_VNNI_INT`?\r\n\r\nIt wasn't clear if that would only be set for `AVX10.2` or if it was allowed to be separate"
      ],
      "runtime-platform-agnostic-network-apis": [
        "Unfortunately, it isn't this simple. Stuff on Unix has to go through the PAL layer so I need effectively something like the following in PAL instead:\r\n```cpp\r\n#if defined(__APPLE__)\r\n#define sincos __sincos\r\n#endif\r\n```"
      ],
      "runtime-choose-descriptive-names": [
        "I've renamed this in `IsRedundantMov` but not everywhere else (as `/* canSkip */ value` is used for almost every call to `emitIns_Mov`).",
        "This is in a lambda which implicitly captures the `insName` variable"
      ],
      "runtime-simplify-code-expressions": [
        "I think we could collapse `IsAVXVNNIInstruction(ins) || IsAVXVNNIINT8Instruction(ins) || IsAVXVNNIINT16Instruction(ins)` down into some `IsAvxVnniFamilyInstruction(ins)` given the places that are checking them"
      ],
      "runtime-optimize-build-dependency-chains": [
        "This is the actual interesting \"bit\" of the change.\r\n\r\nEach high level folder (Arm, General, X86) has a targets file that:\r\n1. Has a dependency on the relevant test generator to ensure it is built first\r\n2. A target that runs the test generator to ensure the necessary files exist, with relevant up to date checks to ensure it doesn't run unnecessarily\r\n3. A target that has to always run as part of compilation to read the list of generated tests and include them in the list of files csc should process",
        "The actual `GenerateTests.csx` was simply moved to be part of a csproj and to take a couple inputs to help filter the tests.\r\n\r\nIt's not necessarily the cleanest change, but it is by far one of the simplest. Building a source generator or otherwise refactoring things more would result in significantly more time and churn and would block the AVX-512 work until that was completed."
      ],
      "runtime-decompose-complex-algorithms": [
        "@kg, I don't see any obvious existing handlers for what is effectively \"bitcast\" (that is the input is returned directly with no change, the API only exists to satisfy the type system).\r\n\r\nDo you have a pointer to any similar handling that might exist?",
        "I ended up following what `op_UnaryNegation` does and adding handlers that basically do memcpy to handle it.",
        "This file ended up needing a bit of a refactoring as there were some assumptions in place that don't hold when supporting additional intrinsics.\r\n\r\nIn particular, there are various intrinsics where:\r\n* one of the SIMD types may not be generic at all (`Vector2`, `Vector3`, `Vector4`)\r\n* multiple generic types exist (`As<TFrom, TTo>`)\r\n* the return type may not be a 128-bit vector (`AsVector2`, `AsVector3`)\r\n\r\nSo, what I did here was I broke this `get_common_simd_info` method into two:\r\n* `get_common_simd_info`\r\n* `get_common_simd_scalar_arg`\r\n\r\nThe former now always gets the size of the input klass and secondly determines if it is a SIMD type and what the underlying element type is if so. While the latter identifies the first non-SIMD argument, if one exists.\r\n\r\n",
        "To support methods which return a value type, but where that isn't a 128-bit vector, this explicitly gets the return type from the signature to ensure there can be no accidents",
        "In here, we consistently query the relevant simd information of the return type and if it exists the simd information of the  first parameter (this is enough to correctly handle all the cases that currently exist).\r\n\r\nThere's actually quite a bit of logic in this function that *could* be moved down into `emit_common_simd_operations`, as APIs like `AndNot` exist for `Vector128<T>` and `Vector<T>`, they may also exist for types like `Vector4` in the future. I opted to not move that down in this PR, to try and keep the total churn under control.\r\n\r\nBut, I did add some basic validation that the encountered signatures are roughly as expected to help ensure we don't hit issues in the future as new overloads are introduced or the general SIMD support in Mono is expanded."
      ],
      "runtime-centralize-platform-configurations": [
        "> /Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk/usr/include/malloc/_malloc.h:50:10: note: 'aligned_alloc' has been marked as being introduced in macOS 10.15 here, but the deployment target is macOS 10.13.0\r\n\r\nand then the mono failure for C99 seem potentially problematic\r\n\r\n`posix_memalign` is the \"next best\" thing but has a restriction\r\n> The address of the allocated memory will be a multiple of alignment, which must be a power of two and a multiple of sizeof(void *).\r\n\r\n`memalign` might also be available, but is considered \"obsolete\" in newer versions and is not guaranteed to work with `free` (and there is no explicit counterpart for it)\r\n\r\nhttps://www.man7.org/linux/man-pages/man3/posix_memalign.3.html",
        "We build on 10.15 saying we target 10.13 and the headers have annotations saying \"this API is only available on 10.xx+\" so using them raises a warning.\r\n\r\nSimilarly to say building for 10.0.10240 (Threshold 1) on a machine running 10.0.19043 (21H1).",
        "There is probably some other CMake magic or define checks that could be done here, but I think that can be tracked via a separate issue for follow up.\r\n\r\nThis impacts a range of Apple products (iOS, M1, OSX, etc) and may not be trivial to do correctly on all platforms.",
        "https://github.com/dotnet/runtime/issues/54296",
        "Fixed."
      ],
      "runtime-enable-configurable-instrumentation": [
        "```suggestion\r\nRETAIL_CONFIG_DWORD_INFO(EXTERNAL_EnableAVXVNNIINT8,            W(\"EnableAVXVNNIINT8\"),         1, \"Allows AVXVNNI8+ hardware intrinsics to be disabled\")\r\nRETAIL_CONFIG_DWORD_INFO(EXTERNAL_EnableAVXVNNIINT16,           W(\"EnableAVXVNNIINT16\"),        1, \"Allows AVXVNNI16+ hardware intrinsics to be disabled\")\r\n```"
      ],
      "runtime-maintainable-test-structure": [
        "Were these intentionally added? They look to have dead code and aren't actually testing the APIs in question.\r\n\r\nWe have other tests which cover the CPUID checks"
      ],
      "runtime-model-actual-hardware-costs": [
        "I don't think these costs are \"accurate\" and likely deserve more investigation in the future. It also isn't accounting for SIMD/Mask. -- We have a similar issue with some of the costs in the gen tree computation, where a lot of them were modeled around the x87 FPU and never updated to account for modern SIMD considerations.\r\n\r\nHere's the rough costs of loads:\r\n* a 32-bit integer move: `encoding: 2 bytes; execution: 2-5 cycles`\r\n* a 64-bit integer move: `encoding: 3 bytes; execution: 2-5 cycles`\r\n* a 32-bit floating-point move: `encoding 4 bytes; execution: 4-7 cycles`\r\n* a 64-bit floating-point move: `encoding 4 bytes; execution: 4-7 cycles`\r\n* a 128-bit simd move: `encoding 3-4 bytes; execution: 4-7 cycles`\r\n* a 256-bit simd move: `encoding 4-5 bytes; execution: 5-8 cycles`\r\n* a 512-bit simd move: `encoding 6 bytes; execution: 5-9 cycles`\r\n\r\nStores tend to be up to more expensive than loads. You get roughly `4-10 cycles` for simd and floating-point, but nearly `2-10 cycles` for integers\r\n\r\n",
        "This logic hasn't really been applicable in a long time. We have single instruction spill/reload regardless of SIMD size, it's just that TYP_SIMD32/64 require spilling since the upper bits are volatile and we may only spill 128-bits instead of all bits, if possible (and which is no more expensive to do).\r\n\r\nA long time ago we did have more complex logic for upper save/restore, but now we can just follow the normal callee save/restore consideration instead.",
        "> varTypeIsSIMD(tree) is redundant with tree->TypeIs(TYP_SIMD32, TYP_SIMD64).\r\n\r\n`varTypeIsSIMD(tree)` is cheaper and ensures the common case is 1 check (just checking the `varTypeClassification`).\r\n\r\n> Why have tree->OperIsHWIntrinsic() branch as well?\r\n\r\nThere are nodes that produce scalar results or small types but which still operate on V256/V512. It's an `||` condition so its handling either `node that produces a simd32/64` or `hwintrinsic node that uses simd32/64`\r\n\r\n> This will not work for tree that contain SIMD expression but do not evaluate to a SIMD/HWI.\r\n\r\nFor some arbitrary block copies or other operations, potentially. Most usages of SIMD32/64 are done via hwintrinsics, so this is getting the important bits.\r\n\r\n> The comment mentions ...such constants..., but the code does not contain checks for constants, so it is confusing.\r\n\r\nWill fix.\r\n",
        "Talked with @kunalspathak and this is currently needed as we still try to do alignment when optimizations are disabled.\r\n\r\nWe may want to revisit that since OSR + TC should handle all the important cases and aligning debug code likely isn't worth the cycles required.",
        "This covers the crossgen scenario, correct?\r\n```suggestion\r\n    const bool  isReadyToRun           = opts.IsReadyToRun();\r\n    const bool  resolveTokens          = makeInlineObservations && (isTier1 || isReadyToRun);\r\n```",
        "Will switch to that. Could I get a summary of the difference between the two for future reference?",
        "```suggestion\r\n    const bool  isPreJit               = opts.jitFlags->IsSet(JitFlags::JIT_FLAG_PREJIT);\r\n    const bool  resolveTokens          = makeInlineObservations && (isTier1 || isPreJit);\r\n```",
        "Rather than take `emitDataAlignment` which only had `None`, `Preferred`, and `Required`, we just pass in the actual alignment we want. This is normally the same as the constant size, but it isn't required to be (as is the case for `SMALL_CODE` where we want an alignment of whatever is smallest and works for the architecture).",
        "It didn't when I checked but most of the methods that involve vector constants also don't contain other constants."
      ]
    },
    "profile": {
      "location": "Lake Stevens, WA",
      "company": "Microsoft",
      "blog": "",
      "twitter_username": "tannergooding",
      "site_admin": false,
      "followers": 425,
      "following": 1
    }
  },
  "ThisIsMissEm": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-api-parameter-design",
        "title": "API parameter design"
      },
      {
        "slug": "mastodon-avoid-redundant-computations",
        "title": "avoid redundant computations"
      },
      {
        "slug": "mastodon-batch-similar-operations",
        "title": "batch similar operations"
      },
      {
        "slug": "mastodon-centralize-configuration-management",
        "title": "centralize configuration management"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-comprehensive-authorization-checks",
        "title": "comprehensive authorization checks"
      },
      {
        "slug": "mastodon-consistent-route-nesting",
        "title": "consistent route nesting"
      },
      {
        "slug": "mastodon-extract-complex-logic",
        "title": "Extract complex logic"
      },
      {
        "slug": "mastodon-leverage-build-tool-automation",
        "title": "leverage build tool automation"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-minimize-html-attack-surface",
        "title": "Minimize HTML attack surface"
      },
      {
        "slug": "mastodon-network-resource-limits",
        "title": "Network resource limits"
      },
      {
        "slug": "mastodon-review-configuration-currency",
        "title": "Review configuration currency"
      },
      {
        "slug": "mastodon-safe-proxy-configuration",
        "title": "Safe proxy configuration"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-naming",
        "title": "Use semantic naming"
      },
      {
        "slug": "mastodon-use-semantic-null-handling",
        "title": "Use semantic null handling"
      }
    ],
    "comments": {
      "mastodon-batch-similar-operations": [
        "Here, if an application had say, 500 access tokens before being deleted, we'd previously publish 500 events in one redis pipeline to 500 pubsub topics.\r\n\r\nWith this change, we'd publish only one event with 500 access token IDs in the message body to a single pubsub topic.\r\n\r\nThis would apply for each multiple of 1000, so for 1500 access tokens, we'd send two publishes to a single topic."
      ],
      "mastodon-centralize-configuration-management": [
        "This line is a close copy of line 54 in application_controller, would it be better to lift it up to the application configuration layer instead? e.g., a `Rails.configuration.x.fetch_mode` and then set it to `none | actors | all`?"
      ],
      "mastodon-avoid-redundant-computations": [
        "This method previously did multiple things:\r\n- created the listener for the messages from Redis\r\n- subscribed to the topics in redis\r\n- attached a close handler in the case of http EventSource connections\r\n\r\nThis made the code difficult to reason about. It is still currently responsible for sending the message on to the output destination, but that can't really be avoided.\r\n\r\nThis does mean that for both WebSockets and EventSource connections we need to manage the pubsub manager subscriptions independently, but eventually we'll be able to replace that with a \"subscription manager\", much like WebSocket's already has, but abstracted for all connection types.\r\n\r\nThis will mean that processing a message from redis won't need N callbacks, but instead just the one that grabs the subscribed connections and sends the messages, making the streaming server lighter overall (currently we've some GC churn by constantly declaring complex functions such as this."
      ],
      "mastodon-use-semantic-naming": [
        "This should be deleted.. we reuse the `report_notes/report_notes` partial — we should probably rename that partial to `moderation_notes` or something, since it applies for Account Moderation Notes, Report Notes, and Instance Notes now.",
        "The localisation here for \"noop\" is \"Filtered\", which seems like the most reasonable value, because showing a moderator \"NOOP\" in the menu just doesn't really make sense, and usually this severity is used in combination with the `reject_*` fields in domain blocks, which are effectively \"filters\"",
        "Maybe, but given its a single use for now, I'd probably not add a method for it"
      ],
      "mastodon-network-resource-limits": [
        "It'd be better to probably use just a single topic, since the churn on subscribe/unsubscribe is going to be high otherwise — one typically isn't staying on a profile for a long period of time, but there are a significant number of \"view file, go back to notifications\" type interactions.\r\n\r\nBy using a per-account redis channel, we'd be encountering fairly significant subscribe/unsubscribe load on redis, so I'd advise against it. As it is, we've a proliferation of redis topics unnecessarily for other things (e.g., access token revocations), we should probably let the streaming server just do the filtering it needs.\r\n\r\nThe only case where we'd decide to do something different is if we were to use `pubsub numsub` to check if we should publish to redis at all in the first place."
      ],
      "mastodon-choose-appropriate-exception-types": [
        "I don't think this is really the right error to raise here, open to suggestions. Would it be worth adding something more specific to `lib/exceptions.rb` ? e.g., `Mastodon::InvalidDomainError` or something?\r\n\r\nThis should probably also raise if the normalized domain is invalid, though it looks like `Addressable::URI::InvalidURIError` will be raised from `TagManager.instance.normalize_domain` in that case?",
        "oddly enough, `TagManager.instance.normalize_domain` doesn't raise if the domain is blank or otherwise malformed..\r\n\r\nSo these are currently \"valid\" domains:\r\n- `''` (returns an empty string)\r\n- `.` (returns `.`)\r\n- `a.` (returns `a`)\r\n- `.a` (returns `.a`)",
        "So should we allow it and just kick the problem down the road to when the person using the admin panel goes to save the note? It will error at some point.\r\n\r\nMaybe we should have the validation actually be on the router? Ideally reusable by all the instance admin routes?",
        "Could we maybe do a constrain that's just `\\w+(.\\w+)+` or something? i.e., any word character? Though arguably that wouldn't work with non-normalized IDN",
        "How would we feel if I kept the method but removed the exception being thrown?",
        "Have removed the method."
      ],
      "mastodon-api-parameter-design": [
        "If I'm understanding what this code did correctly, then if the `forward_to_domains` option wasn't specified, then it'd always forward to the target_account's domain. We may want to reassess that, and instead rely on manual forwarding — I don't know if a missing API parameter should be seen as \"yes, let's forward this to a remote instance\"",
        "```suggestion\r\n        it 'removes the status and the attachment', :aggregate_failures do\r\n```",
        "This doesn't keep the report status filter, but probably should"
      ],
      "mastodon-use-semantic-null-handling": [
        "Then you'd not need string | null as the type — an empty string is as food as null in this case: there is no description to show."
      ],
      "mastodon-safe-proxy-configuration": [
        "This default of a string of multiple values actually caused proxyaddr to throw a server crashing error if the req.ip property was ever accessed",
        "This is exactly how Express adds ip onto requests",
        "Having request.ip both for http & websocket would allow us to support IP Blocks in streaming as well, should we want that"
      ],
      "mastodon-consistent-route-nesting": [
        "This is a little clunky because I nested the notes under the instances routes, so:\r\n\r\n```\r\n/admin/instances/:instance_id/notes/:id\r\n```\r\n\r\nWhere as both Report Notes and Account Moderation Notes are top-level routes in admin:\r\n\r\n```\r\n/admin/account_moderation_notes/:id\r\n/admin/report_notes/:id\r\n```\r\n\r\nI think those probably should have been nested originally.",
        "This allows people to permalink to a specific note"
      ],
      "mastodon-use-descriptive-specific-names": [
        "```suggestion\r\n  def forwardable?\r\n```",
        "Arguably we could change this signature to:\r\n```suggestion\r\n  def call(report, forwarder, forward_to_domains = [])\r\n```\r\n\r\nThe options usage is just from moving code across from ReportService"
      ],
      "mastodon-comprehensive-authorization-checks": [
        "What about if the requested `params.account_id` Account has blocked the requestee? Yes, filtering will happen at the per-message level, but we should probably do an authorization check here too."
      ],
      "mastodon-migration-data-dependencies": [
        "I wasn't sure on the exact way to do this migration, but I've tested it locally a bunch and it appears to work.\r\n\r\nThe goal is to inherit the previous setting for both, if a previous setting existed, otherwise use the defaults (allow timeline_preview_local, prevent timeline_preview_remote).\r\n\r\nReverting the setting in the down is also a bit chaotic, and also kinda unneeded because, well, do we ever do down migrations post-merge of a migration?",
        "I've been having a think about this further, and I'm not sure we actually even want these routes, instead I think we'd want like `GET /api/v1/timelines/preview/local` or something. That way we avoid all this complicated boolean logic."
      ],
      "mastodon-minimize-html-attack-surface": [
        "I'm not sure how to do this safely? Apparently markdown.render returns html as a string but it's not marked as safe for usage in templates?"
      ],
      "mastodon-review-configuration-currency": [
        "This entire switch is — basically the environment variables to find the root cert don't exist here (or aren't documented) so there's no way to align with ruby via DB_SSLMODE, because we require the root certificate to actually properly do this.\r\n\r\nDATABASE_URL can encode the path to that root certificate and automatically load it, which enables us to have a checkServerIdentity function ",
        "We could maybe grab the certificate path from `PGSSLROOTCERT` if set — it'd still be different, but at least then we'd be able grab it if defined.\r\n\r\nOtherwise we'd need to load from either `PGSSLROOTCERT`, or `~/.postgresql/root.crt` in the user's home directory on unix/mac, or on Microsoft Windows from `%APPDATA%\\postgresql\\root.crt`. (`%APPDATA%` may be [`os.homedir()`](https://nodejs.org/docs/latest/api/os.html#oshomedir) though I'm not sure sure).\r\n\r\nThis is based on https://www.postgresql.org/docs/current/libpq-ssl.html",
        "If none of those options exist (none are a readable file by the current user) then we would have to abort with an error, but otherwise we could \"discover\" the root certification just like libpq does.",
        "Yeah, so my understanding above was correct, here's the function in libpq that's called if `PGSSLROOTCERT` is not defined, to discover the default path of `~/.postgresql/` or `%APPDATA%\\postgresql\\`:\r\n\r\nhttps://github.com/postgres/postgres/blob/master/src/interfaces/libpq/fe-connect.c#L8123-L8175\r\n\r\nAnd then where it's called to load up the root certificate:\r\n\r\nhttps://github.com/postgres/postgres/blob/master/src/interfaces/libpq/fe-secure-openssl.c#L866-L969\r\n\r\n**Essentially let `rootCert` be `PGSSLROOTCERT` if defined, else construct a path to `/.postgresql/root.crt` relative to the home directory or `\\postgresql\\root.crt` relative to `%APPDATA%` (dependent on operating system)**\r\n\r\nFrom what I can tell `%APPDATA%` may have a few different locations, and there's no node.js API to resolve this as it's a C call to [`SHGetKnownFolderPath`](https://learn.microsoft.com/en-us/windows/win32/api/shlobj_core/nf-shlobj_core-shgetknownfolderpath) (as of windows vista). I have had someone suggest that there may be a `process.env.APPDATA` or `process.env.APP_DATA` value on windows, but I cannot verify this as I don't have a windows computer nor can I get a windows developer VM to test in (they've been unavailable on the microsoft website \"temporarily\" for the past 6 months)\r\n\r\nIf `rootCert` is `\"system\"` then that triggers using all of openssl's root certificate logic.\r\n\r\nElse, if `rootCert` has a value, do a `stats` syscall on `rootCert` and if it exists and is readable (I think the `stats` syscall returns non-zero if it's unreadable by the current user), then load that certificate via [`SSL_CTX_load_verify_locations`](https://docs.openssl.org/master/man3/SSL_CTX_load_verify_locations/).\r\n\r\nNode.js's crypto internals for tls do certificate loading slightly differently because they requiring loading the certifcate as a buffer from the filesystem first, before adding the buffer as a root cert if it's a valid certificate. But for all intents and purposes, I think these would be equivalent.\r\n\r\nSo if we just do the part in bold above, then we would have parity with libpq entirely, beyond what `pg-connection-string` offers with regards to compatibility when using just `DB_SSLMODE` without specifying a root certificate some how."
      ],
      "mastodon-leverage-build-tool-automation": [
        "My understanding is that these maybe could be deleted because Vite automatically preloads? (assuming the application was structured in a way to code-split effectively)"
      ],
      "mastodon-extract-complex-logic": [
        "the above block should probably be moved into a separate `def`"
      ],
      "mastodon-use-accessible-terminology": [
        "I'm using \"Internal Notes\" as the heading to emphasize that these aren't visible to the general public.\r\n\r\nThere is also still \"private_comment\" vs \"public_comment\" on DomainBlocks, but I suspect we'll see people opt not to use private comments in the future (plus they're usually junk because of how importing domain blocks works)",
        "Could also use \"Private Notes\" maybe, or \"Moderator Notes\", idk."
      ]
    },
    "profile": {
      "location": "Berlin, Germany",
      "company": "@Unobvious-Technology / @brandedcode ",
      "blog": "https://brandedcode.com/",
      "site_admin": false,
      "followers": 619,
      "following": 121
    }
  },
  "dummdidumm": {
    "repos": [
      "sveltejs/svelte"
    ],
    "entries": [
      {
        "slug": "svelte-analyze-transitive-dependencies",
        "title": "analyze transitive dependencies"
      },
      {
        "slug": "svelte-api-flexibility-balance",
        "title": "API flexibility balance"
      },
      {
        "slug": "svelte-async-cleanup-safety",
        "title": "async cleanup safety"
      },
      {
        "slug": "svelte-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "svelte-complete-code-examples",
        "title": "Complete code examples"
      },
      {
        "slug": "svelte-defensive-error-handling",
        "title": "defensive error handling"
      },
      {
        "slug": "svelte-document-configuration-hierarchies",
        "title": "document configuration hierarchies"
      },
      {
        "slug": "svelte-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "svelte-measure-performance-impact",
        "title": "Measure performance impact"
      },
      {
        "slug": "svelte-multi-indicator-configuration-detection",
        "title": "Multi-indicator configuration detection"
      },
      {
        "slug": "svelte-prefer-simple-code-patterns",
        "title": "prefer simple code patterns"
      },
      {
        "slug": "svelte-prefer-testing-libraries",
        "title": "prefer testing libraries"
      },
      {
        "slug": "svelte-preserve-user-input-focus",
        "title": "preserve user input focus"
      },
      {
        "slug": "svelte-realistic-documentation-examples",
        "title": "realistic documentation examples"
      },
      {
        "slug": "svelte-runtime-html-escaping",
        "title": "Runtime HTML escaping"
      },
      {
        "slug": "svelte-state-boundary-management",
        "title": "state boundary management"
      },
      {
        "slug": "svelte-use-modern-null-safe-operators",
        "title": "use modern null-safe operators"
      },
      {
        "slug": "svelte-write-clear-test-cases",
        "title": "Write clear test cases"
      }
    ],
    "comments": {
      "svelte-choose-descriptive-names": [
        "```suggestion\r\nexport default function read_expression(parser, opening_token, disallow_loose) {\r\n```",
        "I think so - I was hesitant to adopt the subscriber nomenclature because of it sounding too much like stores, but it explains the concept much better"
      ],
      "svelte-use-modern-null-safe-operators": [
        "Pretty sure you can reduce this to `/** @type {Derived} */ (reaction).parent?.f & DERIVED) !== 0`",
        "Of course you can?\r\n```js\r\nlet foo = { x: { i: 2 } };\r\nconsole.log((foo.x.i & 2) !== 0); // true\r\nconsole.log((foo.y?.i & 2) !== 0); // false\r\n```\r\nit doesn't work in the case of `=== 0` because then regardless of undefined or a number it's always 0, but it works in the `!== 0` case",
        "we should probably also adjust the call signature to allow undefined for the handler param, and adjust that for `export function event(...)` aswell ",
        "```suggestion\r\n\t\t\t? process.cwd?.()\r\n```"
      ],
      "svelte-state-boundary-management": [
        "Because it is unavoidable in some scenarios. Angular actually had a setting where writing inside effects was disallowed and they removed it because it caused so many headaches (and didn't actually help, most people just turned on the option)",
        "````suggestion\r\n\r\nThe fallback value of a prop not declared with `$bindable` is treated like a non-reactive POJO, and therefore also doesn't update the component when mutating its properties.\r\n\r\n```svelte\r\n<--- file: Child.svelte --->\r\n<script>\r\n\tlet { object = { count = 0 } } = $props();\r\n</script>\r\n<button onclick={() => {\r\n\t// has no effect if the fallback value is used\r\n\tobject.count += 1\r\n}}>\r\n\tclicks: {object.count}\r\n</button>\r\n```\r\n\r\nIn general, mutating props is discouraged, instead use callback props to make it easier to reason about state and changes to that state. If parent and child should share (and be allowed to mutate) the same object, then use the [$bindable]($bindable) rune.\r\n\r\n````",
        "```suggestion\r\n\r\n> [!NOTE] Fallback values are not turned into reactive state proxies (see [Updating props](#Updating-props) for more info)\r\n\r\n```"
      ],
      "svelte-runtime-html-escaping": [
        "```suggestion\r\nimport { escape_html } from '../../../escaping.js';\r\n```"
      ],
      "svelte-defensive-error-handling": [
        "I'd rather have us warn here and add `UNKNOWN`, else people might be stuck with not using certain operators once they land if they can't upgrade to a version that supports it (people could be on v5 and we only add the operator in v6)",
        "I think that's the wrong conclusion. If something is thrown we catch it, no exceptions (ha). So I'd rather adjust the catch logic to not have a stack in that case and also adjust the types to `unknown`",
        "Just as a thought: What if we wrap non-errors in an error object and put the original on some property? Or does that mess with user expectations too much? Not sure if doing that would help you have nicer/smaller logic in the code. It would at least help with attaching a stack, though that's probably not the most important thing.\r\n(feel free to resolve the conversation again if you think what we have now is better than wrapping the error)",
        "```suggestion\r\n\t// try-catch needed because this tries to read properties of `a` and `b`,\r\n\t// which could be disallowed for example in a secure context\r\n\ttry {\r\n```",
        "```suggestion\r\n\tif (DEV) {\r\n\t\t// prevent parent/child element state being corrupted by a bad render\r\n\t\treset_elements();\r\n\t}\r\n```"
      ],
      "svelte-realistic-documentation-examples": [
        "Is that code example necessary? It's taking up quite a bit of space, I think the \"hey watch out for that\" is enough, and then people can read up on how the `DataTransfer` API works (like I should have done, to not provoke this PR 😄 )",
        "Good clarification, but I think we also should have a code example showing how to do it if that's not what you want.\r\n\r\n````suggestion\r\nSince the `tooltip(content)` expression runs inside an [effect]($effect), the attachment will be destroyed and recreated whenever `content` changes. The same thing would happen for any state read _inside_ the attachment function when it first runs.\r\n\r\nIn case this is not the desired behavior, and you instead want the attachment to only be executed once on mount, then make sure to not read state eagerly inside the function body. Assuming `tippy` would be very expensive to setup and tear down (which it isn't), then you could rewrite the above example by passing a reference to the `content` variable and invoke it inside a nested effect:\r\n\r\n```svelte\r\n<!--- file: App.svelte --->\r\n<script>\r\n\timport tippy from 'tippy.js';\r\n\tlet content = $state('Hello!');\r\n\t/**\r\n\t * @param {() => string} content\r\n\t * @returns {import('svelte/attachments').Attachment}\r\n\t */\r\n\tfunction tooltip(content) {\r\n\t\treturn (element) => {\r\n\t\t\tconst tooltip = tippy(element);\r\n\t\t\t$effect(() => {\r\n\t\t\t\ttippy.setContent(content());\r\n\t\t\t});\r\n\t\t\treturn tooltip.destroy;\r\n\t\t};\r\n\t}\r\n</script>\r\n<input bind:value={content} />\r\n<button {@attach tooltip(() => content)}>\r\n\tHover me\r\n</button>\r\n```\r\n````",
        "Fine with me, I just want to have it mentioned somewhere",
        "Maybe something additional like \"note that if you mutate the value, you're mutating the original value, unless you do something like `$state.snapshot` to clone it\" here?\r\n\r\nAlso, can you copy that section over to `documentation/docs/03-runes/01-state.md`? That's where the docs for the final site live.",
        "The idea was that there's a compiler error version of this and so there's a bit of context why there's also a warning version of this (thought that's likely not very clear still, and so we can either just omit it [your suggestion] or expand on it by actually referencing the other variant)",
        "yeah that sounds good. My suggestion then (mirroring my other error text suggestion):\r\n```suggestion\r\nHTML restricts where certain elements can appear. In case of a violation the browser will 'repair' the HTML in a way that breaks Svelte's assumptions about the structure of your components. Some examples:\r\n\r\n- `<p>hello <div>world</div></p>` will result in `<p>hello </p><div>world</div><p></p>` for example (the `<div>` autoclosed the `<p>` because `<p>` cannot contain block-level elements)\r\n- `<option><div>option a</div></option>` will result in `<option>option a</option>` (the `<div>` is removed)\r\n- `<table><tr><td>cell</td></tr></table>` will result in `<table><tbody><tr><td>cell</td></tr></tbody></table>` (a `<tbody>` is auto-inserted)\r\n\r\nThis code will work when the component is rendered on the client (which is why this is a warning rather than an error), but if you use server rendering it will cause hydration to fail.\r\n```"
      ],
      "svelte-api-flexibility-balance": [
        "I briefly thought about that but it seemed unnecessary - in which case would you have existing attributes on a html tag but in such a way that you know which ones to then merge them in some way? Even if, the regex for adjusting the html attributes string would be straightforward. So I opted for making the simple case more ergonomic.\r\n\r\nIt _is_ an interesting question for SvelteKit specifically though, which currently sets `lang=\"en\"` in `app.html` by default. What would we do here? (regardless of whether we return a string or an object). The easiest would be to have `lang=\"en\"` after the string and rely on browser being forgiving about it (they ignore duplicate attributes) / the user removing it in case they set it themselves in `<svelte:html>`",
        "let's play it save and make this an options argument, so we can enhance this in the future of needed"
      ],
      "svelte-async-cleanup-safety": [
        "It needs to be there and it will break if you run the tests with the \"no async\" environment flag. It was added in https://github.com/sveltejs/svelte/pull/16198 and the logic basically says \"in async mode we want to depend on state read inside and effect in which that state was also created, but not in non async mode because it's a breaking change\""
      ],
      "svelte-prefer-simple-code-patterns": [
        "```suggestion\r\n\t\t\tstyle_directives.some((directive) => directive.metadata.expression.has_state);\r\n```",
        "Can we use a boolean instead of a number here? Seems like it's only used within one if block"
      ],
      "svelte-measure-performance-impact": [
        "Came here to say the same https://github.com/sveltejs/svelte/pull/15073#issuecomment-2604895320",
        "We can also do this, would handle the case described in https://github.com/sveltejs/svelte/pull/14116/files#r1876874053\r\n```suggestion\r\n\t\tconst previous = document.title;\r\n\t\tconst own = {};\r\n\t\t// @ts-expect-error\r\n\t\tdocument._last_title_setter = own;\r\n\t\tdocument.title = text;\r\n\r\n\t\treturn () => {\r\n\t\t\t// @ts-expect-error\r\n\t\t\tif (document._last_title_setter === own) {\r\n\t\t\t\tdocument.title = previous;\r\n\t\t\t}\r\n```",
        "Looking at how the code is generated, we should switch it up anyway - right now if your title would contain dynamic content, you would set it two times on an update - once to revert to the previous value, then right away again for the updated value. Wasteful."
      ],
      "svelte-prefer-testing-libraries": [
        "I think it's important to have a low level example to not immediately abstract away stuff. Then they have an easier time reasoning about the abstraction",
        "The reading flow is better if first the underlying primitive is shown and then the abstraction on top - I added a code example showing how to rewrite it using the testing library, to make the benefits more obvious.",
        "- you can programmatically instantiate slots now using `createRawSnippet`\r\n- what is the problem with events/use/bind?\r\n- \"DX of programmatically intantiating components\" - what is so bad about it? I mean, yes it's not as nice as declaratively doing that, but doing `mount`/`unmount` isn't so bad? And when it comes to selecting elements in order to interact with them, it's always going to be a bit of legwork (in e2e testing, too)",
        "The way to go about all these cases is to create a wrapper component that tests the actual component - I don't see a problem with that. It's how you would have to do this in _any_ framework.\r\nFurthermore, just not documenting how to do component testing because it's cumbersome sounds wrong. Testing components in isolation is something you'd want to do especially when creating a component library, for example. I'm open to having a `> Don't overdo this as component tests are generally harder to maintain` note at the end of the section.\r\nI'll adjust the docs to mention both these things.",
        "It's explained in their [setup docs](https://testing-library.com/docs/svelte-testing-library/setup/#vitest) - they add a teardown behind the scenes. Didn't want to get into details here which would just mean repeating their docs, rather visualize how the resulting code is more robust to changes and more through the lens of a user."
      ],
      "svelte-multi-indicator-configuration-detection": [
        "you should also check for `$:` statements",
        "```suggestion\r\n\t// Components not explicitly in legacy mode might be expected to be in runes mode (especially since we didn't\r\n\t// adjust this behavior until recently, which broke people's existing components), so we also bail in this case.\r\n\t// Kind of an in-between-mode.\r\n\tif (context.state.analysis.runes || context.state.analysis.maybe_runes) {\r\n```",
        "The problem with these is that I have seen these appear above typescript functions in the wild. `@type` on the other hand is a pretty strong indicator ",
        "> If they use JSDoc wouldn't they omit the lang=\"TS\"?\r\n\r\nYes, which is why there's the outer `||` in which case we see \"this is definitely TS because the lang tag is set\". This is only about the ambiguity of people using JSDoc but having a `tsconfig.json`\r\n\r\n> Makes sense. However, if someone specifies use_ts to be true or false, shouldn't that precedence over a check?\r\n\r\nIt will be set to `true` or `false` by `svelte-migrate` in https://github.com/sveltejs/kit/pull/12881 based on the presence (or absence) of tsconfig.json, which as pointed out above does not necessarily mean you're using TS, so the check is still needed"
      ],
      "svelte-preserve-user-input-focus": [
        "what's the reason for this change? Can't imagine this breaking anyone to be fair",
        "Pretty sure yes because things not read in a teardown can still cause bugs as seen in https://github.com/sveltejs/svelte/issues/16072 - we might need to make props signals after all, gonna investigate that soon",
        "```suggestion\r\n\t// For objects, we apply string coercion (which might make things like $state array references in the template reactive) before diffing\r\n```"
      ],
      "svelte-analyze-transitive-dependencies": [
        "```suggestion\r\nfix: don't consider children of rules when checking whether they are used or not\r\n```"
      ],
      "svelte-write-clear-test-cases": [
        "what I like to do in these tests is also have another function above that passes, so that you know \"this one doesn't throw, this one does throw, as expected\"",
        "Names are a bit confusing: these imports are not broken.\r\nIn general, it's probably better to move this test into the validation test suite, where you can more clearly test \"I expect this not to fail\""
      ],
      "svelte-complete-code-examples": [
        "yes, but prettier probably auto-formatted that"
      ],
      "svelte-document-configuration-hierarchies": [
        "```suggestion\r\n> Using an identifier or a rest element as the declarator for `$props` when compiling to custom elements without declaring `props` in the component options means that Svelte can't know which props to expose as properties on the DOM element. Consider explicitly destructuring all the props or add the `customElement.props` option.\r\n```",
        "previously the depth was unlimited. Are we confident with `3` being a right number which doesn't turn up false positives for people previously having no warning?",
        "If I were to design this from scratch I'd also take 3, it's more about people not seing a warning now which they previously didn't. But having such a deeply nested input is probably super rare. Maybe I'd go with 5.",
        "```suggestion\r\n- `svelte:options` now lets you set the `css: \"inject\"` compiler option on a per-component basis (**5.0.0-next.209**, [#12660](https://github.com/sveltejs/svelte/pull/12660))\r\n```"
      ],
      "svelte-documentation-clarity-standards": [
        "I find this sentence much harder to read/parse than the previous one. Can we find some kind of middle ground between what was there before and what's proposed?",
        "```suggestion\r\n- `<svelte:component>` is now unnecessary in runes mode and therefore is deprecated (**5.0.0-next.203/217**, [#12646](https://github.com/sveltejs/svelte/pull/12646) and [#12694](https://github.com/sveltejs/svelte/pull/12694))\r\n```",
        "```suggestion\r\nWhen using custom elements, you should still use `<slot />` like before. In a future version, when Svelte removes its internal version of slots, it will leave those slots as-is, i.e. output a regular DOM tag instead of transforming it.\r\n```"
      ]
    },
    "profile": {
      "location": "Germany",
      "company": "Vercel",
      "blog": "",
      "twitter_username": "dummdidumm_",
      "site_admin": false,
      "followers": 671,
      "following": 0
    }
  },
  "the-mikedavis": {
    "repos": [
      "helix-editor/helix",
      "tree-sitter/tree-sitter"
    ],
    "entries": [
      {
        "slug": "helix-api-documentation-accuracy",
        "title": "API documentation accuracy"
      },
      {
        "slug": "helix-avoid-hardcoded-configuration-values",
        "title": "avoid hardcoded configuration values"
      },
      {
        "slug": "helix-avoid-panics-gracefully",
        "title": "avoid panics gracefully"
      },
      {
        "slug": "helix-avoid-version-specific-documentation",
        "title": "avoid version-specific documentation"
      },
      {
        "slug": "helix-consistent-descriptive-naming-conventions",
        "title": "Consistent descriptive naming conventions"
      },
      {
        "slug": "helix-consistent-highlighting-patterns",
        "title": "consistent highlighting patterns"
      },
      {
        "slug": "helix-consistent-naming-conventions",
        "title": "consistent naming conventions"
      },
      {
        "slug": "helix-documentation-style-and-formatting",
        "title": "Documentation style and formatting"
      },
      {
        "slug": "helix-follow-established-conventions",
        "title": "Follow established conventions"
      },
      {
        "slug": "helix-minimize-allocations-and-syscalls",
        "title": "Minimize allocations and syscalls"
      },
      {
        "slug": "helix-omit-redundant-configuration",
        "title": "omit redundant configuration"
      },
      {
        "slug": "helix-optimize-query-performance",
        "title": "Optimize query performance"
      },
      {
        "slug": "helix-prefer-let-else-patterns",
        "title": "prefer let-else patterns"
      },
      {
        "slug": "helix-reduce-nesting-complexity",
        "title": "reduce nesting complexity"
      },
      {
        "slug": "helix-semantic-identifier-naming-patterns",
        "title": "Semantic identifier naming patterns"
      },
      {
        "slug": "helix-standardize-build-configuration-patterns",
        "title": "Standardize build configuration patterns"
      },
      {
        "slug": "helix-target-documentation-to-audience",
        "title": "Target documentation to audience"
      },
      {
        "slug": "helix-use-descriptive-names",
        "title": "use descriptive names"
      },
      {
        "slug": "tree-sitter-algorithm-and-data-optimization",
        "title": "Algorithm and data optimization"
      }
    ],
    "comments": {
      "helix-semantic-identifier-naming-patterns": [
        "Semantically it contains a function but syntactically it's a variable and it should be highlighted as one",
        "Same here and below on L270 about `#lua-match?`"
      ],
      "helix-api-documentation-accuracy": [
        "Some of the commentary is good like the above one about dance. I'd like to avoid commenting on how well the extensions work though like these two lines since the extensions might get better over time (and then this page is out of date) and how well the emulation works might be arguable\r\n\r\n(Same for the shells section below - we can probably drop the \"comments\" column from that table)"
      ],
      "helix-minimize-allocations-and-syscalls": [
        "The `to_string()` can move from the `map` in `let language_servers` to the `map` here on L349. That way we allocate only for names that match the input.",
        "```suggestion\r\n        Variable::Language => Ok(match doc.language_name() {\r\n            Some(lang) => Cow::Owned(lang.to_owned()),\r\n            None => Cow::Borrowed(\"text\"),\r\n        }),\r\n```\r\n\r\nmight as well avoid that extra allocation when there is no lang, even though it's small",
        "We should avoid reading the entire file into memory at once"
      ],
      "helix-consistent-naming-conventions": [
        "```suggestion\r\n\"markup.link.url\"             = { fg = \"markup_link_url_fg\", modifiers = [\"underlined\"] }\r\n```\r\n\r\nsmall typo here: when used in `modifiers` it should be `underlined` instead of `underline`",
        "It looks like these `selectionFG` colors should be switched to `selectionfg` to match the entry in the palette",
        "Let's call this something more specific in the `contrib` dir like `contrib/hx-deb-wrapper.sh` or something so it's clear what it's for. (I assume you can rename it back to `hx` here.)",
        "Yep that works too, I'd just like to avoid calling it \"hx\""
      ],
      "helix-consistent-highlighting-patterns": [
        "```suggestion\r\n(rule (unknownDirective) @attribute) @error\r\n```\r\n\r\nFor syntax errors in the past I believe we used `error` rather than `diagnostic.error`",
        "It's fine to highlight based on exact function/type names for things like the builtin highlights, but I don't like a huge pattern like this that tries to guess the highlight. We can't figure out whether the identifier is in this situation robustly and I don't want a really big highlight like this that tries to cover common cases - it will seem inconsistent when you write a custom higher order function",
        "Generally we do not highlight `(ERROR)` since it's very noisy when typing. In the future we might introduce configuration for it if it's desired (basically prepending `(ERROR) @error` to `highlights.scm` text) but in the meantime let's remove this pattern",
        "The captures need some tweaks to match the scopes we use: https://docs.helix-editor.com/master/themes.html#syntax-highlighting\r\n\r\nFor example `boolean` should be `constant.builtin.boolean`, `number` should be `constant.numeric.integer` or `constant.numeric.float`",
        "Some of the captures in this file will need to be adapted to the ones we use: https://docs.helix-editor.com/master/themes.html#syntax-highlighting\r\n\r\nFor example `@number` should become `@constant.numeric` and `@module` should become `@namespace`",
        "I think these two should be reversed? Now the less specific highlight should be higher in the file - I think `comment.line` will always overwrite `comment.block.documentation` here",
        "```suggestion\r\n  (#match? @comment.block.documentation \"^//!\"))\r\n```\r\n\r\n`#lua-match?` is neovim specific but this regex will work for `#match?`"
      ],
      "helix-use-descriptive-names": [
        "I'll add a note in the command description that it can be used to evaluate arguments with no effect",
        "This is meant to be generically about strings - we might define other specialized small strings in this module",
        "```suggestion\r\n    pub workspace_diagnostics: Vec<Severity>,\r\n```\r\n\r\nI would prefer a full name here so it's easy to read"
      ],
      "helix-avoid-hardcoded-configuration-values": [
        "This debounce doesn't relate to that: it debounces large updates to the word index. Instead what you'd want would be covered by configuration for the minimum word length to trigger completion (in the new `word` module).\r\n\r\nThe way I have the completion tuned now (with constants rather than configuration) avoids word completion most of the time so to type a long word I use `C-x` more often than not. I think it's ok for the trigger length to be configurable but I think the default configuration should try to avoid aggressively completing words and word completions should be lower priority than LSP completions."
      ],
      "helix-optimize-query-performance": [
        "Capturing the name as a namespace here can lead to some odd highlighting, for example\r\n\r\n```rust\r\nfn fun(param: u32) {\r\n    param::call();\r\n    param\r\n}\r\n```\r\n\r\n`call` is highlighted as `namespace`. Probably the highlighter should recognize that this capture comes from the locals queries and not attach any highlight to it, so that the regular highlights take over.",
        "I don't think it would take that large of a change - Query has functions that can be used to tell if the pattern is from the locals file or from the highlights file. I'll look into it.\r\n\r\nI'm a bit surprised that `#has-ancestor?` has poor performance since it should be bounded by the height of the tree. I suppose that can grow to be quite large though. I don't like it because it has you write node names and that is both inflexible (can't use the full pattern matching power of the query language) and error-prone since tree-sitter won't check that it's a valid node during query analysis. So I'd like to avoid adding it if possible. Some day I'll try to understand `query.c` and see if an arbitrary nesting operator is possible.",
        "```suggestion\r\n((identifier) @variable.builtin (#any-of? @variable.builtin \"this\" \"msg\" \"block\" \"tx\"))\r\n```\r\n\r\nwhen this was originally written I don't think we supported `#any-of?` but now we can replace the `#match?` with the equivalent `#any-of`"
      ],
      "helix-avoid-version-specific-documentation": [
        "`+stable` is only available if you're using rustup which is not always the case, for example you might use cargo/rustc from your package manager",
        "This text may age poorly as versions change. Let's say something a bit more generic like \"The CI may use a libc version greater than what your Ubuntu/Debian/Mint version requires - in this case you can build a `.deb` from source.\" and jump right into the directions."
      ],
      "tree-sitter-algorithm-and-data-optimization": [
        "Since we're just doing contains lookups, a hashset would be better, no? Or actually I guess you could use just a u32 because match IDs are incrementing integers..."
      ],
      "helix-follow-established-conventions": [
        "Configuring a formatter is ok but we shouldn't enable `auto-format` unless formatting for TLA+ is a standard practice\r\n\r\n```suggestion\r\n```",
        "We don't parse 3-character hex codes so this will need to be expanded to the 6 character version"
      ],
      "helix-documentation-style-and-formatting": [
        "```suggestion\r\nwhere `key` represents what you want to style, `fg` specifies the foreground color, `bg` the background color, `underline-style` the underline style, `underline-color` the underline color (only meaningful if an underline style is enabled), and `modifiers` is a list of style modifiers. `bg`, `underline` and `modifiers` can be omitted to defer to the defaults.\r\n```"
      ],
      "helix-omit-redundant-configuration": [
        "```suggestion\r\n```\r\n\r\nthis key can be omitted since `[]` is the default",
        "This `language-id` key can be omitted - by default the `name` will be used"
      ],
      "helix-standardize-build-configuration-patterns": [
        "For the build of `packages.helix` we can use the latest stable - that's what we do in the release builds as well. We only need the MSRV toolchain for the development shell",
        "```suggestion\r\n        cargoBuildType = \"release\";\r\n```\r\n\r\nYeah let's switch back to `release`. The LTO stuff (I assume) seems to be way more expensive than I thought. I'm seeing ~60s to build on `release` and ~330s to build on `opt` :/",
        "Should we use `rustc` and `cargo` from the attrs passed to `mkHelix` instead? I'm seeing a deprecation warning on accessing these",
        "For the package build (not the shell) we don't need to follow the MSRV, it's ok to build with latest stable. We do roughly the same in CI - the build workflow for PRs checks with the MSRV and then the release workflow uses whatever is the latest stable"
      ],
      "helix-reduce-nesting-complexity": [
        "It looks like each of these enum variants are storing the count, right? Let's separate the enum from the count and pass that as a separate parameter even if it means adding `#[allow(clippy::too_many_arguments)]` for some functions",
        "Stylewise we avoid `let` blocks because they lead to increased indentation. It's fine to let `label` be `mut` for this `for` loop scope"
      ],
      "helix-target-documentation-to-audience": [
        "For these we should tag as `comment.line.documentation` - Rust also has block doc comments ([reference](https://doc.rust-lang.org/reference/comments.html)) as\r\n\r\n```rust\r\n/**  - Outer block doc (exactly) 2 asterisks */\r\n```\r\n\r\nwhich we could tag as well"
      ],
      "helix-prefer-let-else-patterns": [
        "rustfmt won't help us fix this yet but I think the style guide suggests (https://github.com/rust-lang/rust/blob/c8ead2e693a22fe94c6b3edeb3f49c7e6aec3912/src/doc/style-guide/src/statements.md#single-line-let-else-statements):\r\n\r\n```rs\r\nlet Some(capabilities) = self.capabilities.get() else { return false };\r\n```"
      ],
      "helix-consistent-descriptive-naming-conventions": [
        "To align with other config, `underline_styles` and `underline_color` should probably be kebab-case and to align with the modifiers, the underline styles should be snake_case"
      ],
      "helix-avoid-panics-gracefully": [
        "Rather than a panic you can set an error in the statusline:\r\n\r\n```rust\r\nif count != 1 {\r\n    cx.editor.set_error(\"inserting multiple newlines not yet supported\");\r\n}\r\n```",
        "Instead of an `expect` let's `log::error!` when the config isn't set and `return true`"
      ]
    },
    "profile": {
      "location": "NYC",
      "company": "@AWS",
      "blog": "https://the-mikedavis.github.io/",
      "site_admin": false,
      "followers": 618,
      "following": 36
    }
  },
  "zombieJ": {
    "repos": [
      "ant-design/ant-design"
    ],
    "entries": [
      {
        "slug": "ant-design-api-evolution-strategy",
        "title": "API evolution strategy"
      },
      {
        "slug": "ant-design-avoid-hardcoded-configuration-values",
        "title": "Avoid hardcoded configuration values"
      },
      {
        "slug": "ant-design-avoid-unnecessary-memoization",
        "title": "Avoid unnecessary memoization"
      },
      {
        "slug": "ant-design-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "ant-design-consistent-dependency-formatting",
        "title": "consistent dependency formatting"
      },
      {
        "slug": "ant-design-deprecated-api-documentation",
        "title": "Deprecated API documentation"
      },
      {
        "slug": "ant-design-extract-common-patterns",
        "title": "Extract common patterns"
      },
      {
        "slug": "ant-design-handle-optional-values-safely",
        "title": "Handle optional values safely"
      },
      {
        "slug": "ant-design-jsdoc-deprecation-formatting",
        "title": "JSDoc deprecation formatting"
      },
      {
        "slug": "ant-design-optimize-expensive-operations",
        "title": "Optimize expensive operations"
      },
      {
        "slug": "ant-design-prevent-component-re-mounting",
        "title": "Prevent component re-mounting"
      },
      {
        "slug": "ant-design-react-component-api-clarity",
        "title": "React component API clarity"
      },
      {
        "slug": "ant-design-semantic-naming-consistency",
        "title": "Semantic naming consistency"
      },
      {
        "slug": "ant-design-simplify-complex-expressions",
        "title": "Simplify complex expressions"
      },
      {
        "slug": "ant-design-test-network-performance",
        "title": "Test network performance"
      },
      {
        "slug": "ant-design-test-organization-standards",
        "title": "Test organization standards"
      },
      {
        "slug": "ant-design-use-semantic-descriptive-names",
        "title": "Use semantic descriptive names"
      },
      {
        "slug": "ant-design-use-semantic-naming",
        "title": "Use semantic naming"
      }
    ],
    "comments": {
      "ant-design-use-semantic-descriptive-names": [
        "这个不对吧，如果我一个form 有多个 Radio Group，name 不能是相同的。",
        "应该是让用户自己传 name 然后 `useId(name)`。这样就是优先用特地传给 group 的name 了",
        "这个 name 不一定是合法的，可能是 number 可能是 string，也可能是 (number | string)[]，还要做一下转化，可以考虑 formName 作为前缀",
        "统一到 subMenu 上，这个不需要用户去理解 mode 和 semantic 的关系，都是同一个赋值就好"
      ],
      "ant-design-simplify-complex-expressions": [
        "做成解构吧，不要到处都是 `?.`",
        "interval 里面有好多 Date.now，直接抽个变量出来吧。还能让代码看起来简单一些。 reverse 有了以后看起来太绕脑子",
        "`type === 'countdown'` 出现了2次，抽一个变量"
      ],
      "ant-design-consistent-dependency-formatting": [
        "不要跑 update，只改你提升的。否则 blame 会乱掉"
      ],
      "ant-design-use-semantic-naming": [
        "直接 export default function useOrientation，这边这个 UseOrientation 我是举例子，其实不用维护这个类型。",
        "下面既然还需要额外判断干脆这里直接：\r\n\r\n```tsx\r\ndisplayName: 'timePicker' | 'datePicker'\r\n```"
      ],
      "ant-design-avoid-unnecessary-memoization": [
        "最好你传入的 classNames memo 一下，否则这里每次渲染都会跑一次。"
      ],
      "ant-design-api-evolution-strategy": [
        "升级到 v6 的话是不需要这个，v5 抬上来 console 会直接告诉你。"
      ],
      "ant-design-prevent-component-re-mounting": [
        "不能这么写，子元素会 re-mount。\r\n\r\n相关 issue 是什么？描述里关联的看起来不对。",
        "所以不能这么写。\r\n初始化一个 children，hover 后又 unmount children 再 mount 一个新的 children。对于有 mount effect 的子元素是 break 的。",
        "另外，React 官方是不推荐提前优化，只有在业务里确实遇到性能问题再做优化，开发环境的性能问题不用考虑。\r\n\r\n> https://react.dev/learn/render-and-commit#optimizing-performance\r\n> The default behavior of rendering all components nested within the updated component is not optimal for performance if the updated component is very high in the tree. If you run into a performance issue, there are several opt-in ways to solve it described in the [Performance](https://reactjs.org/docs/optimizing-performance.html) section. Don’t optimize **prematurely**!\r\n\r\n我测试了重现里的代码，在经过上次的优化后开发模式渲染 2000 个 Tooltip（重现中为 30 * 50 = 1500 个 Tooltip）：\r\n\r\n<img width=\"521\" height=\"140\" alt=\"截屏2025-09-02 11 45 42\" src=\"https://github.com/user-attachments/assets/0311f6f7-89e8-4f2c-af84-e270b9a815c7\" />\r\n\r\n其中性能损耗主要来自于 React Dev 自身：\r\n\r\n<img width=\"419\" height=\"76\" alt=\"截屏2025-09-02 11 46 33\" src=\"https://github.com/user-attachments/assets/ba877a5f-2093-4811-9bf5-02701ed8181c\" />\r\n\r\n第一个 run：\r\n<img width=\"445\" height=\"67\" alt=\"截屏2025-09-02 11 47 00\" src=\"https://github.com/user-attachments/assets/d084bce7-6c6d-4ebe-952b-7ee4ad735782\" />\r\n\r\n另一个 run:\r\n<img width=\"431\" height=\"74\" alt=\"截屏2025-09-02 11 47 14\" src=\"https://github.com/user-attachments/assets/326b70f1-a6d8-4fa7-94ef-2ecb9fee93c5\" />\r\n\r\n当编译完后（即使用 React Production Build）：\r\n<img width=\"408\" height=\"141\" alt=\"截屏2025-09-02 11 48 15\" src=\"https://github.com/user-attachments/assets/aac1fb52-67bd-4a7c-b172-dd2cd4e73240\" />\r\n\r\n",
        "这个 ref 是为了保持结构稳定，防止 motion 切换导致子元素来回卸载/重装",
        "那听起来可以判断 motion 是否是 boolean，如果是就记录并且包一层就好了？",
        "这里还是需要原来的 ref 来记录一下是否有渲染过 MotionCacheContext，如果有的话。无论是否相同，都需要渲染 MotionCacheContext",
        "这边定义直接是 boolean 就行了，不需要包 object，否则子元素里消费 Context 的时候上层变化会导致 memo 被穿透。",
        "和 MotionProvider 一样的封装时机即可，不是所有 CP 都需要一层层包这个的。否则这样子就会冲突：\r\n\r\n* CP motion=false\r\n  * CP motion=true\r\n    * CP (没有 motion)\r\n      * CP motion=true",
        "不对，这么写会导致每个 CP 都会包 MotionProvider。要添加 MotionProvider 的时机是：\r\n\r\n1. 检查当前是否设置了 `motion: boolean`\r\n2. 检查是否曾经有设置过 `motion: boolean`\r\n\r\n进阶优化则是在 `1` 处调整：\r\n\r\n```diff\r\n-- 检查当前是否设置了 `motion: boolean`\r\n++ 检查当前设置了 `motion: boolean` 并且与 `parent` 的 `motion: boolean` 不一致\r\n```"
      ],
      "ant-design-test-network-performance": [
        "条件里还需要判断用户当前网络是否成功打开官网了，如果慢的话才弹出提示。"
      ],
      "ant-design-avoid-hardcoded-configuration-values": [
        "参考一下 AI 的回复",
        "可以的，把 orientation 放到 CP 里~",
        "这个不太对，应该是先转成 obj，然后：\r\n```\r\nconst mergedMask = {\r\n  ...contextMaskConfig,\r\n  ...maskConfig,\r\n};\r\n```\r\n\r\n最后第一个参数返回 mergedMaskConfig.enabled 来确定是否展示 mask"
      ],
      "ant-design-jsdoc-deprecation-formatting": [
        "组件用尖括号表示，另外不需要后面的说明 `deprecated` 方法自己会包：\r\n\r\n```tsx\r\nwarning.deprecated(\r\n      true,\r\n      '<Statistic.Countdown />',\r\n      '<Statistic.Timer type=\"countdown\" />',\r\n);\r\n```\r\n\r\n另外没有写对应的 test case 来测试这个 warning"
      ],
      "ant-design-handle-optional-values-safely": [
        "info 是可选的话这里就不要 `!` 转换了，没覆盖到的容易出问题。可以写一个 `{ props: {} }` 预设对象出来",
        "这里逻辑梳理顺序：\r\n\r\n1. object & not null\r\n2. boolean\r\n3. default: `{}` 不需要提前设置 blur ",
        "这边不对，应该是 css var 不存在的话就 fallback 到 tooltipColor"
      ],
      "ant-design-optimize-expensive-operations": [
        "这样没有省性能，比如有两个 classNames:\r\n\r\n```tsx\r\nA {\r\n  root: 'a',\r\n}\r\n\r\nB {\r\n  body: 'b'\r\n}\r\n```\r\n\r\n原本分别遍历 `Object.keys` 时，只需要遍历 `A.a` + `B.b`。现在变成了 `A.a` + `A.b` + `B.a` + `B.b`。反而耗费了性能。"
      ],
      "ant-design-react-component-api-clarity": [
        "尴尬，`mask` 在走废弃流程。但是这个 previewMask 似乎有会多一次废弃，未来肯定还是 `mask` 属性。要不然做点 tricky 的事情：\r\n1. mask 如果是 object，则是新版本的配置\r\n2. mask 如果是 ReactNode，则是旧版本的用法，需要 warning"
      ],
      "ant-design-centralize-configuration-management": [
        "previewConfig 是有一个 usePreviewConfig 来处理的，不用自己额外再转一次"
      ],
      "ant-design-test-organization-standards": [
        "测试名字不对，不要把实现作为测试名。而是因为是要测试的内容。这里测试的应该是快速点击不应该触发多次 click 事件",
        "然后测试行为是 Button 非 loading 然后点击多次，检测 onClick 是否触发"
      ],
      "ant-design-extract-common-patterns": [
        "直接单独把 mask & cover 检查从遍历里抽出来单独写，不要在遍历里通过 if 抽",
        "这种样式比较多，可以抽一个出来作为通用 blur 样式",
        "这里很奇怪，如果使用的是 `height` 的话上面的 `borderBlockStart` 就没有必要。画线两种，要么画 border 上，要么画 content 上。然后我是推荐画 content 上，这样用户自定义实现渐变色会很方便。",
        "直接打平 params，这样压缩体积会小。类型靠 ts 来就好",
        "参数如下：\r\n\r\n```tsx\r\n(defaultOrientation, orientation, vertical) => orientation\r\n```\r\n\r\nvertical, ctxVertical 应该是不需要的，直接 vertical ?? ctxVertical 传进来就行了",
        "这个看起来可以合起来，另外也不需要 `includes` 了，两个变量 includes 反而会让体积变大：\r\n\r\n```tsx\r\nconst validOrientation = orientation ===  'horizontal' || orientation ===  'vertical';\r\nconst mergedOrientation = validOrientation ? orientation : (defaultVertical ? 'vertical' : 'horizontal');\r\nreturn [mergedOrientation, mergedOrientation === 'vertical'];\r\n```"
      ],
      "ant-design-semantic-naming-consistency": [
        "> rc-upload 已经发了。都应该跟原生保持一致，几年前非 directory 已经改了，这个是漏了应该。 #30403\r\n\r\n不是漏改，只是以前作为 feature 加的。另外不在文档正式透出前都能调整。\r\n\r\n> 这样看，folder 改成 nativeDirectory 是不是好理解一点。\r\n\r\n`directory` 名字是最好的，不过已经被占了。`dir` 名字也是被占的名字，才想的 folder。accept 检查逻辑其实一直都有问题（js 检查其实不能真的检查文件类型，比如那个 windows 系统不支持的 case），默认交还给原生的行为也是合理的。因为现在文档里说的 FAQ 其实根本不能用 [《文件夹上传在 Safari 仍然可以选中文件?》](https://ant-design.antgroup.com/components/upload-cn#%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8A%E4%BC%A0%E5%9C%A8-safari-%E4%BB%8D%E7%84%B6%E5%8F%AF%E4%BB%A5%E9%80%89%E4%B8%AD%E6%96%87%E4%BB%B6)\r\n\r\n",
        "AI 问了一下，似乎没有比 folder 作为新名字更好代替了。",
        "排序的话都是：\r\n1. 属性字母顺序\r\n2. 方法字母顺序\r\n\r\n其他的几个 markdown 也都是这样，都看看吧~"
      ],
      "ant-design-deprecated-api-documentation": [
        "按照 @Wxh16144 的推荐，所有废弃且仍然兼容的 API 应该是 ~~xx~~ 删除线，这样文档上就会删除，但是检查全部还能看到。",
        "组件大版本可以删，有需要看老的文档。这个部分已经固化了。"
      ]
    },
    "profile": {
      "location": "Shanghai, China",
      "company": "@alipay",
      "blog": "",
      "site_admin": false,
      "followers": 1383,
      "following": 8
    }
  },
  "daniel-lxs": {
    "repos": [
      "RooCodeInc/Roo-Code"
    ],
    "entries": [
      {
        "slug": "roo-code-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "roo-code-centralize-configuration-constants",
        "title": "Centralize configuration constants"
      },
      {
        "slug": "roo-code-conditional-debug-logging",
        "title": "Conditional debug logging"
      },
      {
        "slug": "roo-code-configure-with-care",
        "title": "Configure with care"
      },
      {
        "slug": "roo-code-consistent-localization-formatting",
        "title": "Consistent localization formatting"
      },
      {
        "slug": "roo-code-document-i18n-string-usage",
        "title": "Document i18n string usage"
      },
      {
        "slug": "roo-code-enforce-api-format-consistency",
        "title": "Enforce API format consistency"
      },
      {
        "slug": "roo-code-enforce-resource-usage-limits",
        "title": "Enforce resource usage limits"
      },
      {
        "slug": "roo-code-extract-reusable-patterns",
        "title": "Extract reusable patterns"
      },
      {
        "slug": "roo-code-extract-shared-code-patterns",
        "title": "Extract shared code patterns"
      },
      {
        "slug": "roo-code-internationalize-all-text",
        "title": "Internationalize all text"
      },
      {
        "slug": "roo-code-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "roo-code-optimize-algorithm-implementations",
        "title": "Optimize algorithm implementations"
      },
      {
        "slug": "roo-code-optimize-react-components",
        "title": "Optimize React components"
      },
      {
        "slug": "roo-code-preserve-error-context-chain",
        "title": "Preserve error context chain"
      },
      {
        "slug": "roo-code-prevent-timeout-race-conditions",
        "title": "Prevent timeout race conditions"
      },
      {
        "slug": "roo-code-prevent-unnecessary-processing",
        "title": "Prevent unnecessary processing"
      },
      {
        "slug": "roo-code-protect-shared-state-access",
        "title": "Protect shared state access"
      },
      {
        "slug": "roo-code-robust-error-handling",
        "title": "Robust error handling"
      },
      {
        "slug": "roo-code-use-structured-logging",
        "title": "Use structured logging"
      },
      {
        "slug": "roo-code-validate-model-capabilities-first",
        "title": "Validate model capabilities first"
      },
      {
        "slug": "roo-code-write-resilient-test-assertions",
        "title": "Write resilient test assertions"
      }
    ],
    "comments": {
      "roo-code-enforce-api-format-consistency": [
        "I noticed that `.avif` is included in `SUPPORTED_IMAGE_FORMATS` (line 44) but there's no corresponding MIME type mapping here. This means AVIF images will default to `image/png` which isn't correct.\n\nCould we add:\n```typescript\n\".avif\": \"image/avif\",\n```\n\nto ensure AVIF images are properly identified?",
        "The URL detection pattern might be too broad. Could a URL like `https://api.example.com/v1/embeddings-service` or `https://api.example.com/deployments-info/v1` be incorrectly classified as a full endpoint URL?\n\nWould it be safer to use more specific patterns? For example:\n- Check if the URL ends with `/embeddings` (with or without query params)\n- Use a regex pattern like `/deployments/[^/]+/embeddings` for Azure-style URLs\n\nThis would reduce the risk of false positives while maintaining compatibility with the intended services.",
        "The description says \"milliseconds\" but the value is actually stored in minutes and converted to milliseconds in the handler. This is inconsistent with the other timeout descriptions and could cause confusion. Also, the `.describe()` method isn't used elsewhere in the schema.\n```suggestion\n\topenAiApiTimeout: z.number().optional(),\n```",
        "The current implementation converts messages to plain text format, but this seems inconsistent with the original approach.\r\n\r\nThe comment on line 117 states \"Claude CLI doesn't accept JSON messages\", but looking at the [original implementation pattern](https://github.com/cline/cline/pull/4111/files#diff-c922b8fdacba11f1ecef526a5223718b0ea2c9d5aea01c653b1fd7afddea2ca5R23), it appears the CLI should accept JSON-formatted messages. The current text conversion approach:\r\n\r\n1. Loses message structure and metadata\r\n2. Makes it harder to handle complex message types\r\n3. Differs from how other providers handle messages\r\n\r\nCould you verify if the Claude Code CLI actually requires text format? If it does accept JSON, consider reverting to:\r\n```typescript\r\nconst args = [\r\n  \"-p\",\r\n  JSON.stringify(messages),\r\n  \"--system-prompt\",\r\n  systemPrompt,\r\n  // ... other args\r\n]\r\n```\r\n\r\nThis would also simplify the validation logic since JSON.stringify handles escaping automatically.",
        "The current implementation converts messages to plain text format, but this seems inconsistent with the original approach.\n\nThe comment on line 117 states \"Claude CLI doesn't accept JSON messages\", but looking at the [original implementation pattern](https://github.com/cline/cline/pull/4111/files#diff-c922b8fdacba11f1ecef526a5223718b0ea2c9d5aea01c653b1fd7afddea2ca5R23), it appears the CLI should accept JSON-formatted messages. The current text conversion approach:\n\n1. Loses message structure and metadata\n2. Makes it harder to handle complex message types\n3. Differs from how other providers handle messages\n\nCould you verify if the Claude Code CLI actually requires text format? If it does accept JSON, consider reverting to:\n```typescript\nconst args = [\n  \"-p\",\n  JSON.stringify(messages),\n  \"--system-prompt\",\n  systemPrompt,\n  // ... other args\n]\n```\n\nThis would also simplify the validation logic since JSON.stringify handles escaping automatically."
      ],
      "roo-code-avoid-hardcoded-configurations": [
        "The script could use a few updates to make it more reliable and easier to maintain:\r\n\r\n1. **Hardcoded extension ID**: Right now, the extension ID is hardcoded as `rooveterinaryinc.roo-cline`. Instead, it should be built dynamically from the `package.json` to avoid issues if the name or publisher ever changes:\r\n\r\n   ```javascript\r\n   const publisher = packageJson.publisher\r\n   const extensionId = `${publisher}.${name}`\r\n   ```\r\n\r\n2. **Missing VSIX file check**: Before trying to install the VSIX file, it's a good idea to check that it actually exists. Otherwise, the script might fail without a clear reason. Here’s a simple check to add:\r\n\r\n   ```javascript\r\n   if (!fs.existsSync(vsixFileName)) {\r\n     console.error(`VSIX file not found: ${vsixFileName}`)\r\n     console.error(\"Make sure the build completed successfully\")\r\n     process.exit(1)\r\n   }\r\n   ```\r\n\r\nAdding these checks will make the script more robust and easier to work with in the long run.\r\n"
      ],
      "roo-code-document-i18n-string-usage": [
        "I see that the oversized image error uses i18n properly, but this regular image notice is hardcoded. For consistency, could we move these strings to the translation files as well?\n\nFor example:\n```typescript\nconst noticeText = dimensionsInfo\n  ? t(\"tools:readFile.imageWithDimensions\", { dimensions: dimensionsInfo, size: imageSizeInKB })\n  : t(\"tools:readFile.imageWithSize\", { size: imageSizeInKB });\n```\n\nThis would ensure all user-facing strings are translatable and maintain consistency across the codebase."
      ],
      "roo-code-robust-error-handling": [
        "The error handling logic looks good, but could this be more robust? Currently it falls back to string conversion for non-Error objects. Would it be helpful to also capture additional context like the component stack or timestamp when the error occurred?",
        "The JSON.parse() here could throw an error if lastMessage.text contains invalid JSON. Could we wrap this in a try-catch block to handle potential parsing errors gracefully?\n\n```typescript\nlet followUpData;\ntry {\n  followUpData = JSON.parse(lastMessage.text || \"{}\")\n} catch (error) {\n  console.error('Failed to parse follow-up data:', error);\n  return;\n}\n```"
      ],
      "roo-code-use-structured-logging": [
        "We should clean this up before merging.\r\n\r\nThe `console.info` statements currently in the code (including the one at line 144 where `finalError` is logged) should be removed. If logging is still needed, consider using a proper logging service instead of `console` calls.",
        "I notice the error logging here uses different levels - `console.info` for connection failures (line 90) vs `console.warn` for other errors (line 92). In contrast, the LM Studio fetcher uses `console.error` for connection failures. Would it make sense to standardize the logging approach across both fetchers?"
      ],
      "roo-code-optimize-algorithm-implementations": [
        "**Important**: Starting pattern detection from length 1 could cause false positives. For example, \"AAAA\" would be detected as pattern \"A\" repeated 4 times, which overlaps with consecutive repetition detection.\n\nConsider starting from length 2 to avoid this overlap:\n```typescript\nfor (let patternLength = 2; patternLength <= maxPatternLength; patternLength++) {\n```\n\nThis ensures pattern detection only catches actual patterns like \"AB\", \"ABC\", etc., not single repeated elements."
      ],
      "roo-code-protect-shared-state-access": [
        "I noticed that while we check `supportsImages` at the beginning of the function (line 147), this value could theoretically change if the model is switched during execution. Should we consider moving this check closer to where we actually decide to include images?\n\nThe current implementation is likely fine for most cases, but for extra safety, we could store the model info check result and use it consistently throughout the function execution.",
        "I noticed a potential race condition here. The model info is fetched during message streaming, which could cause issues if multiple concurrent requests are made before the cache is populated. \n\nWould it be safer to fetch and cache the model info during handler initialization or before starting the stream? This would ensure consistent context window information across concurrent requests.",
        "The `modifyConversation` method attempts to acquire locks on two different files through nested calls to `modifyClineMessages` and `modifyApiConversationHistory`. This pattern could lead to:\n\n1. **Deadlocks** if another process tries to acquire these locks in reverse order\n2. **Data inconsistency** if one modification succeeds but the other fails\n\nIs this intentional? The proper-lockfile documentation recommends against holding multiple locks simultaneously. Consider either:\n- Using a single lock file for both operations\n- Implementing a two-phase commit pattern\n- Documenting why this approach is safe in your specific use case",
        "I noticed a potential edge case in the error handling here. If `ripgrepOperationPromise` fails and we set it to null (line 84), but another concurrent call comes in before line 90, it might start a new operation while the first caller is still in the catch block.\n\nCould this be addressed by setting `ripgrepOperationPromise = null` after the entire try-catch block completes? Or perhaps using a more robust state management approach?",
        "Is there a potential race condition here? The draft is only restored if `!inputValue`, but what happens if the component receives an initial value before the message handler is set up? Could we miss restoring a valid draft in that case?",
        "I'm a bit concerned about potential race conditions here. You're using a 100ms timeout in the provider while ChatView uses 50ms. If someone rapidly switches between windows, multiple focus events could queue up and cause unexpected behavior.\n\nMaybe we could use the same timeout duration in both places? Or even better, implement a proper debounce mechanism to handle rapid focus changes more gracefully.",
        "Good that you're clearing the timeout in dispose(), but what happens if the view gets disposed while a timeout is still pending? The timeout would still fire and try to access this.view which might be disposed.\r\n\r\nIt might be a good idea to add a check in the timeout callback to ensure the view still exists and hasn't been disposed before trying to post a message to it."
      ],
      "roo-code-configure-with-care": [
        "The current implementation uses the settings values without validating whether they're within reasonable bounds. For example, if someone manually edits their settings file and sets a negative number or an extremely large value, it could lead to unexpected behavior.\n\nIt might be worth adding validation like this:\n\n```ts\nconst maxImagesPerResponse = Math.max(1, Math.min(100, state?.mcpMaxImagesPerResponse ?? 20));\nconst maxImageSizeMB = Math.max(0.1, Math.min(50, state?.mcpMaxImageSizeMB ?? 10));\n````\n\nThis helps keep the values within safe operational limits, even in cases where the settings file is corrupted or manually modified.\n\n"
      ],
      "roo-code-internationalize-all-text": [
        "All these announcement strings need to be translated. Consider creating a helper function that uses the translation system:\n\n```typescript\nconst getAnnouncementText = (option: ContextMenuQueryItem, index: number, total: number) => {\n  const position = t(\"chat:contextMenu.position\", { current: index + 1, total });\n  \n  switch (option.type) {\n    case ContextMenuOptionType.File:\n    case ContextMenuOptionType.OpenedFile:\n      return t(\"chat:contextMenu.announceFile\", { \n        name: option.value || option.label, \n        position \n      });\n    // ... other cases\n  }\n};\n```",
        "This instruction text should also be translated:\n\n```typescript\n<div id=\"context-menu-instructions\" className=\"sr-only\">\n  {t(\"chat:contextMenu.instructions\")}\n</div>\n```"
      ],
      "roo-code-extract-reusable-patterns": [
        "The debouncing logic is implemented directly in this component. Have you considered extracting it into a reusable custom hook? This could help improve separation of concerns.",
        "The className logic here is getting quite complex with nested ternaries. Could we simplify this by extracting the condition to a variable?\n\n```tsx\nconst showShareButton = primaryButtonText === t(\"chat:startNewTask.title\") && currentTaskItem?.id;\nconst buttonClassName = showShareButton || secondaryButtonText \n  ? \"flex-1 mr-[6px]\" \n  : \"flex-[2] mr-0\";\n```\n\nThis would make the JSX cleaner and the logic more readable.",
        "The edit mode rendering logic is quite complex with multiple helper functions. Consider extracting the edit mode UI into a separate component for better maintainability:\n\n```typescript\nconst EditModeControls: React.FC<EditModeControlsProps> = ({ ... }) => {\n  // Edit mode specific UI logic\n};\n```\n\nThis would simplify the main component and make the code more modular.",
        "Could you extract this logic into a shared function to avoid duplication? The same pattern appears in `handleSuggestionClickInRow`. Something like:\n\n```typescript\nconst markFollowUpAsAnswered = useCallback(() => {\n  const lastFollowUpMessage = messagesRef.current.findLast((msg) => msg.ask === \"followup\")\n  if (lastFollowUpMessage) {\n    setFollowUpAnswered((prev) => new Set(prev).add(lastFollowUpMessage.ts))\n  }\n}, [])\n```",
        "Since there's timeout logic here as well similar to what's on ClineProvider. Have you considered extracting this into a shared utility function or hook? Something like `useDebouncedFocus()` could handle the timeout.",
        "The `handleSave` function is quite complex with nested conditions and multiple responsibilities. Would it be cleaner to extract the group update logic into a separate utility function?\n\nFor example:\n```typescript\nfunction updateMcpGroupOptions(\n  groups: GroupEntry[],\n  allowedList: string[],\n  deniedList: string[]\n): GroupEntry[] {\n  // Group update logic here\n}\n```\n\nThis would make the code more testable and easier to understand."
      ],
      "roo-code-centralize-configuration-constants": [
        "UI consideration: The component shows 8192 as the default value when no value is set. Would it be clearer to show the model's actual max tokens instead?\n\n```typescript\nconst displayValue = value ?? modelInfo?.maxTokens ?? 8192\n```\n\nThis would give users better context about what the model actually supports before they override it.",
        "I'm curious about why this is needed? I think `setApiConfigurationField` should handle these state changes by itself without having to process them manually."
      ],
      "roo-code-conditional-debug-logging": [
        "I noticed some `console.log` and `console.debug` statements here. Are these intended for debugging and should they be removed before merging?"
      ],
      "roo-code-optimize-react-components": [
        "Have you considered using a simpler state management approach here? Instead of tracking all answered follow-ups in a Set, you could track just the current active follow-up question's timestamp and clear it when answered. This would avoid the need to manage a growing collection of timestamps.\n\nFor example:\n```typescript\nconst [currentFollowUpTs, setCurrentFollowUpTs] = useState<number | null>(null)\n```\n\nThen check `message.ts === currentFollowUpTs` instead of using a Set."
      ],
      "roo-code-enforce-resource-usage-limits": [
        "Should we consider implementing a limit on the number of images that can be returned in a single response? Without a limit, a malicious or buggy MCP server could potentially return hundreds of images, causing performance issues. Maybe add a configurable maximum (e.g., 10-20 images) and log a warning if exceeded?",
        "With the increased limit of 500,000 files, have you considered the memory implications? Each FileResult object contains path, type, and label properties. For a project with 500K files, this could consume significant memory (potentially hundreds of MB).\n\nWould it make sense to:\n1. Make this limit configurable via VSCode settings?\n2. Implement streaming or pagination for extremely large file lists?\n3. Add memory usage monitoring/warnings?",
        "The accumulator still stores the entire conversation history, which matches the original behavior, so this isn't a regression. But it could become a memory concern for long-running chats.\r\n\r\nThis PR avoids re-parsing, which is great for performance. Should we try adding a bounded accumulator or sliding window to help with memory usage?",
        "While the accumulator is reset before each `attemptApiRequest`, consider adding a maximum size check as a safety measure against edge cases where the parser might be reused without proper reset:\n\n```typescript\nprivate readonly MAX_ACCUMULATOR_SIZE = 1024 * 1024; // 1MB limit\n\nprocessChunk(chunk: string): AssistantMessageContent[] {\n    if (this.accumulator.length + chunk.length > this.MAX_ACCUMULATOR_SIZE) {\n        throw new Error('Assistant message exceeds maximum allowed size');\n    }\n    this.accumulator += chunk;\n    // ...\n}\n```\n\nThis would prevent potential memory issues if the parser state isn't properly managed in all code paths.",
        "Should we consider adding a maximum draft size limit just in case? localStorage typically has a 5-10MB quota, and very large drafts could potentially cause issues. What do you think about truncating or warning when drafts exceed a reasonable size (e.g., 100KB)?"
      ],
      "roo-code-maintain-consistent-naming-patterns": [
        "I noticed the naming pattern inconsistency here. You've renamed `MAX_SEARCH_RESULTS` to `DEFAULT_MAX_SEARCH_RESULTS` (which is good!), but `SEARCH_MIN_SCORE` doesn't follow the same pattern.\n\nFor consistency, should we consider renaming `SEARCH_MIN_SCORE` to `DEFAULT_SEARCH_MIN_SCORE`? This would make it clearer that both are default values that can be overridden by user configuration."
      ],
      "roo-code-consistent-localization-formatting": [
        "No, that translation is correct since it's a placeholder to signify the mode"
      ],
      "roo-code-preserve-error-context-chain": [
        "This error handling pattern for ENOENT is duplicated in multiple places:\r\n- Here in `safeReadFile`\r\n- In `ClineProvider.updateContent`\r\n- In `custom-instructions.ts`\r\n- In `custom-system-prompt.ts`\r\n\r\nIt would be a good idea to consolidate this on a single helper function.",
        "What happens if the base64 data is corrupted or invalid? Would it be worth wrapping this in a try-catch to handle potential errors when constructing the data URL? This could prevent the entire tool response from failing due to a single corrupted image."
      ],
      "roo-code-prevent-timeout-race-conditions": [
        "Is there a potential race condition here? If the auto-approval timeout fires just as the user is typing/submitting a response, could both the manual response and auto-approval execute? The timeout clearing happens after the condition check, so there might be a small timing window.",
        "Is there a reason the timeout clearing happens inside the `clineAskRef.current` check rather than at the beginning of `handleSendMessage`? If we clear it unconditionally at the start, we could prevent any race conditions where the timeout fires while we're processing the user's input.\n\nThis would also simplify the logic since we wouldn't need to check for specific ask types.",
        "I notice there's a potential race condition here. If the component unmounts while the countdown is active, the interval cleanup happens, but what if the auto-approval in ChatView.tsx is still waiting? Could we consider using a shared cancellation mechanism or ensuring the ChatView's timeout is also cleared when this component unmounts?"
      ],
      "roo-code-write-resilient-test-assertions": [
        "The test verifies Windows path conversion logic but doesn't actually test that the Windows code path uses WSL or that temporary files are created and cleaned up. Would it be valuable to add more comprehensive tests that mock the `execa` call and verify the correct command is executed with WSL on Windows?",
        "Hey, @Githubguy132010\r\nI agree, we shouldn't mock the very thing we are trying to test. Can you rewrite the test to help us properly test your implementation.\r\n\r\n"
      ],
      "roo-code-validate-model-capabilities-first": [
        "Is it intentional that we're returning image data without checking if the current AI model supports images? I noticed that `maybeRemoveImageBlocks` in `src/api/transform/image-cleaning.ts` checks `apiHandler.getModel().info.supportsImages` before processing images. Should we add a similar check here to prevent sending image data to models that can't process it?\n\nFor example, we could check the provider's capability before including images in the response:\n```typescript\nconst provider = await cline.providerRef.deref();\nconst supportsImages = provider?.apiHandler?.getModel()?.info?.supportsImages ?? false;\nconst imagesToInclude = supportsImages ? allImages : [];\n```",
        "The default dimension is set to 768 for `gemini-embedding-exp-03-07`, but this model supports dimensions of 3072, 1536, and 768. Is 768 the intended default? The smaller dimension might impact embedding quality.\n\nAlso, would it be helpful to add validation somewhere to ensure only valid dimensions (3072, 1536, or 768) are accepted for this model?"
      ],
      "roo-code-prevent-unnecessary-processing": [
        "This useEffect runs on every selectedMenuIndex change during keyboard navigation, which could impact performance. Consider debouncing the announcements or only announcing when navigation pauses:\n\n```typescript\nuseEffect(() => {\n  if (!showContextMenu || selectedMenuIndex < 0) return;\n  \n  const timeoutId = setTimeout(() => {\n    // announcement logic here\n  }, 100); // Small delay to avoid rapid announcements\n  \n  return () => clearTimeout(timeoutId);\n}, [showContextMenu, selectedMenuIndex, /* other deps */]);\n```",
        "Consider memoizing the dialog components to prevent unnecessary re-renders:\n\n```typescript\nconst MemoizedDeleteMessageDialog = React.memo(DeleteMessageDialog);\nconst MemoizedEditMessageDialog = React.memo(EditMessageDialog);\n```\n\nThis could improve performance, especially when the parent component re-renders frequently."
      ],
      "roo-code-extract-shared-code-patterns": [
        "I notice the GC logic is duplicated between lines 636-658 and 674-694. Could we extract this into a separate method to follow DRY principles? Something like:\n\n```typescript\nprivate static async runBackgroundGC(git: SimpleGit, branchName: string): Promise<void> {\n    try {\n        this.log(`[${this.name}#deleteBranch] Running gc --prune=now after deleting branch ${branchName}`);\n        git.raw([\"gc\", \"--prune=now\", \"--quiet\"])\n            .then(() => {\n                this.log(`[${this.name}#deleteBranch] Background gc --prune=now completed for branch ${branchName}`);\n            })\n            .catch((gcError) => {\n                this.log(`[${this.name}#deleteBranch] ERROR: Background gc after deleting branch ${branchName} failed: ${gcError instanceof Error ? gcError.message : String(gcError)}`);\n            });\n    } catch (e) {\n        this.log(`[${this.name}#deleteBranch] ERROR: Failed to initiate gc: ${e instanceof Error ? e.message : String(e)}`);\n    }\n}\n```",
        "There's significant code duplication between this `importConfigFromPath` function and the existing `importSettings` function in `src/core/config/importExport.ts`. \n\nCould we refactor to share the common import logic? Perhaps extract a shared function that both can use, something like:\n\n```typescript\nexport async function importConfigFromData(\n  data: unknown,\n  options: ImportOptions\n): Promise<{ success: boolean; error?: string }>\n```\n\nThis would reduce maintenance burden and ensure consistent behavior between manual and automatic imports."
      ]
    },
    "profile": {
      "location": "Colombia",
      "blog": "",
      "site_admin": false,
      "followers": 34,
      "following": 3
    }
  },
  "ndeloof": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-ci-security-boundaries",
        "title": "CI security boundaries"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-follow-existing-naming-patterns",
        "title": "Follow existing naming patterns"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-maintain-documentation-consistency",
        "title": "Maintain documentation consistency"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-prefer-explicit-readability",
        "title": "prefer explicit readability"
      },
      {
        "slug": "compose-prevent-sensitive-data-exposure",
        "title": "Prevent sensitive data exposure"
      },
      {
        "slug": "compose-prevent-unintended-ci-behaviors",
        "title": "Prevent unintended CI behaviors"
      },
      {
        "slug": "compose-safe-collection-modification",
        "title": "Safe collection modification"
      },
      {
        "slug": "compose-schema-changes-upstream-first",
        "title": "Schema changes upstream first"
      },
      {
        "slug": "compose-scope-concurrency-control-precisely",
        "title": "Scope concurrency control precisely"
      },
      {
        "slug": "compose-use-api-options-pattern",
        "title": "Use API options pattern"
      },
      {
        "slug": "compose-use-structured-logging-framework",
        "title": "Use structured logging framework"
      },
      {
        "slug": "compose-wrap-and-check-errors",
        "title": "Wrap and check errors"
      },
      {
        "slug": "compose-write-deterministic-test-assertions",
        "title": "Write deterministic test assertions"
      }
    ],
    "comments": {
      "compose-scope-concurrency-control-precisely": [
        "We don't want providers to run sequentially, only the project mutation must be guarded by a mutex.",
        "could use an errorGroup (like we do in other places in compose) to capture first error and stop other goroutines by cancelable context.",
        "this isn't thread safe, would require a mutex (this is why we used a channel here)"
      ],
      "compose-use-api-options-pattern": [
        "Repository being mandatory parameter should not be part of the `Options` struct imho",
        "This is API version 1.44, but Moby v25 (typo here)\r\nMakes me wonder moby repo could offer a map for engine -> latest API version so we don't make such mistakes :)",
        "https://github.com/docker/compose/pull/11360",
        "could make it simpler by making API client an attribute in `composeService`, initialize with `dockerCli.Client()` and override here with `NewDryRunClient`.",
        "As this is not a generic Client proxy, but dedicated to dry-run usage, we could make it simpler and just have all API methods to directly invoke `client.XX` from dockerCli's client _BUT_ the few ones where we want to bypass actual actions on docker engine.\r\nOR rename this into generic `ClientProxy`, then we can configure dry-run by setting individual func to be overriden"
      ],
      "compose-use-structured-logging-framework": [
        "should be Errorf",
        "we've been using `logrus.Warning` in other places, we should do the same here"
      ],
      "compose-prevent-sensitive-data-exposure": [
        "service.environment may be set with a fixed value, not relying on any interpolation. Typically:\r\n```\r\ndb:\r\n    image: mysql\r\n    environment:\r\n      MYSQL_DATABASE: avatar\r\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db-password\r\n```\r\nthose should not prevent compose file to be published, right ?",
        "```suggestion\r\n\t\tfmt.Println(\"you are about to publish bind mounts declaration within your OCI artifact.\\n\" +\r\n\t\t\t\"only the bind mount declarations will be added to the OCI artifact (not content)\\n\" +\r\n\t\t\t\"please double check that you are not mounting potential user's sensitive directories or data\")\r\n```",
        "you MUST check each individual file in the compose project, not just the final model. Otherwise I _may_ publish:\r\ncompose.yaml\r\n```yaml\r\nservices:\r\n test\r\n   some: MY_SECRET_PASSWORD\r\n```\r\n\r\ncompose.yaml\r\n```yaml\r\nservices:\r\n test\r\n   some: !override ${ENTER_YOUR_OWN_SECRET}\r\n```\r\n\r\nPublisher expectation would be that secret is not exposed to consumer, but actually it is"
      ],
      "compose-follow-existing-naming-patterns": [
        "I'd suggest to use `-y` to align with https://github.com/docker/compose/blob/main/cmd/compose/create.go#L84",
        "As this also applies to Darwin, maybe rename `_posix` :trollface: ",
        "nit: AFAICT we don't `parse` things here, so better name this `toBuildContexts`",
        "this produces the `options` for the `ImageBuild` engine API, so ...",
        "sounds weird to me we use a distinct separator for fields, I'd prefer we use `:` everywhere\r\nThis allows an earlier version of compose to successfully parse the label, and ignore the last `:xxxx` parts (so we can add more in the future)",
        "Don't need to be exporte. Also name is bit confusing. Maybe `withSelectedServicesOnly` or something comparable?",
        "This is basically a duplicate for `projectFromLabels`, better use/adapt this existing function",
        "yes, move it to `compose.go`. Also, probably should be renamed `projectFromActualResources` or something comparable to be more explicit"
      ],
      "compose-keep-code-structure-flat": [
        "nested `if`s make this a bit hard to read, maybe just split this into simpler top-level `switch` statement?\r\n\r\n```go\r\nswitch {\r\n\tcase direction == fromService && index == 0: \r\n                containers = ...\r\n\tcase index != 0:\r\n                containers = ...\r\n        default:\r\n                containers = ...\r\n\t}\r\n``` \r\n\r\n",
        "`direction = from & index = 0` has been addressed by 1st case, so here we have `(from & index > 0) || (to & index > 0)` which can be simplified as `index > 0`"
      ],
      "compose-network-api-precision": [
        "This hack is also supported for long by compose (https://github.com/docker/compose/blob/v1/compose/config/config.py#L494-L498) and relies on `o=bind` and `device` option being set\r\n"
      ],
      "compose-ci-security-boundaries": [
        "Please don't set any kind of \"auto-fix\". CI role is to check everything is OK in contributor's commit, not to replace contributor doing things right."
      ],
      "compose-prefer-explicit-readability": [
        "took me time indeed to understand this syntax used in original code."
      ],
      "compose-maintain-documentation-consistency": [
        "rest of the documentation uses uppercase \"V1\" and \"V2\"\r\nplease place the compose-switch note as a separate paragraph to make it bit more visible to readers"
      ],
      "compose-wrap-and-check-errors": [
        "use `api.IsNotFoundError(err)`"
      ],
      "compose-prevent-unintended-ci-behaviors": [
        "we don't exec `docker buildx ...` for a standard build, but use the vendored buildx client, why not do the same here with `BuildOptions.PrintFunc`?"
      ],
      "compose-safe-collection-modification": [
        "> in general modifying a map during it's iteration is bad practice and can produce unpredictable results\r\n\r\nany reference ?\r\nI was looking for official reference regarding this and only found https://github.com/golang/go/issues/9926",
        "I'm not sure I remember the detail, but deletion here is expected afaict.\r\nMaybe better create a new `Volumes` before the loop, copy those that need to be kept while iterating, then override `s.Volumes`"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "watcher_naive relies on `SetRecursive` which doesn't exist in upstream"
      ],
      "compose-schema-changes-upstream-first": [
        "compose-spec.json",
        "in an ideal world, this whole file should be removed from docker/compose repo and replaced by a (build-time?) reference to https://github.com/compose-spec/compose-spec/blob/master/schema/config_schema.json",
        "did you checked if the current status of the compose-spec schema already includes all those changes or if there's anything missing ?"
      ],
      "compose-environment-variable-validation": [
        "a better place to manage this is `addProjectFlags` in cmd/compose.go, as you can set flag default value from env value (the same way we do for `--env-file`)",
        "we never used this mechanism in the past for optional/experimental docker compose features",
        "this is the way we used to load PWD/.env before we know the actual project directory. This logic is now covered by toProjectOptions"
      ],
      "compose-write-deterministic-test-assertions": [
        "This version string is very specific to Docker Desktop and will be outdated once we release v2.34, I fear this will bring some confusion. Better use a pure fake version for testing purpose: `v9.9.9-test` for example"
      ]
    },
    "profile": {
      "location": "Rennes, France",
      "company": "Docker",
      "blog": "http://blog.loof.fr",
      "twitter_username": "ndeloof",
      "site_admin": false,
      "followers": 443,
      "following": 0
    }
  },
  "DOsinga": {
    "repos": [
      "block/goose"
    ],
    "entries": [
      {
        "slug": "goose-ai-model-configuration-consistency",
        "title": "AI model configuration consistency"
      },
      {
        "slug": "goose-avoid-concurrency-anti-patterns",
        "title": "avoid concurrency anti-patterns"
      },
      {
        "slug": "goose-avoid-environment-variable-proliferation",
        "title": "avoid environment variable proliferation"
      },
      {
        "slug": "goose-avoid-panics-and-expects",
        "title": "avoid panics and expects"
      },
      {
        "slug": "goose-consolidate-api-parameters",
        "title": "consolidate API parameters"
      },
      {
        "slug": "goose-default-least-permissive",
        "title": "default least permissive"
      },
      {
        "slug": "goose-document-api-contracts",
        "title": "Document API contracts"
      },
      {
        "slug": "goose-document-non-obvious-code",
        "title": "Document non-obvious code"
      },
      {
        "slug": "goose-eliminate-code-duplication",
        "title": "eliminate code duplication"
      },
      {
        "slug": "goose-environment-aware-configuration-handling",
        "title": "Environment-aware configuration handling"
      },
      {
        "slug": "goose-extract-duplicate-constants",
        "title": "Extract duplicate constants"
      },
      {
        "slug": "goose-extract-duplicated-code",
        "title": "extract duplicated code"
      },
      {
        "slug": "goose-handle-errors-intentionally",
        "title": "Handle errors intentionally"
      },
      {
        "slug": "goose-leverage-typescript-nullability",
        "title": "leverage TypeScript nullability"
      },
      {
        "slug": "goose-optimize-algorithmic-efficiency",
        "title": "Optimize algorithmic efficiency"
      },
      {
        "slug": "goose-optimize-hook-dependencies",
        "title": "Optimize hook dependencies"
      },
      {
        "slug": "goose-optimize-memory-and-algorithms",
        "title": "Optimize memory and algorithms"
      },
      {
        "slug": "goose-use-configuration-enums",
        "title": "Use configuration enums"
      },
      {
        "slug": "goose-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "goose-use-established-logging-libraries",
        "title": "Use established logging libraries"
      },
      {
        "slug": "goose-use-idiomatic-optional-patterns",
        "title": "Use idiomatic optional patterns"
      },
      {
        "slug": "goose-use-self-documenting-names",
        "title": "Use self-documenting names"
      },
      {
        "slug": "goose-validate-ipc-inputs",
        "title": "validate IPC inputs"
      },
      {
        "slug": "goose-validate-object-availability",
        "title": "Validate object availability"
      }
    ],
    "comments": {
      "goose-leverage-typescript-nullability": [
        "in what case is chatContext undefined here? can we use the type system to make sure it is not?",
        "you mean we make the pid nullable and then have ts tell us where we are relying on this? that might be better",
        "done"
      ],
      "goose-avoid-panics-and-expects": [
        "this shouldn't happen of course, but I'd still rather do .ok_or or something so we don't panic",
        "the line above where we have .expect() will kill goosed and it is all over. not sure how we would have gotten here, but still, we should probably do another 500; I also think we should change the description of the 500 from internal server error here to something descriptive; could not update provider, make sure that you have the right config or something",
        "excellent. I wonder if we should avoid panicking here though - either return a default \"\", or bubble up?\r\n\r\nfn get_current_working_dir() -> Result<PathBuf, io::Error> {\r\n    std::env::current_dir().or_else(|_| Ok(get_home_dir()))\r\n}\r\n\r\n",
        "would be better to default here to the home_dir rather than die "
      ],
      "goose-validate-object-availability": [
        "hmm, we set the defaultValue here and then check a bunch of things where if we like the values, we set defaultValue to mostly the same value. \r\n\r\nin general we should probably set the validity of the recipe when we read it, not at this point. but here I think we can just leave out everything and just go\r\n```\r\nif (param.requirement === 'optional' && param.default) {\r\n    initialValues[param.key] = param.default\r\n}\r\n```\r\npossibly lowercasing it if param.input_type == 'boolean'"
      ],
      "goose-use-self-documenting-names": [
        "const hasConfiguration = hasEnvVars ?? step3Content !== null;\r\n\r\nalso prefer naming over comment",
        "this should probably be/use a constant seeing that we have 'New Chat' already 4 times"
      ],
      "goose-avoid-concurrency-anti-patterns": [
        "sounds good, but are you saying the message doesn't get there if we complete here?"
      ],
      "goose-avoid-environment-variable-proliferation": [
        "I agree. This should be done through settings",
        "two things:\r\n* why wouldn't we be able to ship whatever goose is run in the container with the right settings in the config.yaml?\r\n* even if we want to use environment variables, we should still read the values from the config; you can override those always with environment variables I think",
        "this with_var looks nice, but shouldn't this be at the other test that actually sets these? of course relying on these in tests is crazy to begin with\r\n\r\nceterum censeo variabiles ambientis esse delendas",
        "let's kill the environment variables - this function should just have the threshold passed in. some caller, somewhere may use the environment variable for now. but not this deep into things"
      ],
      "goose-eliminate-code-duplication": [
        "So I think here and it in the other handle, it would be better I think to move the work to custom_provider.rs, keep the routes light and have:\r\n\r\n`let custom_providers_dir = config_dir.join(\"custom_providers\");`\r\n\r\nappear only once type of thing. we can do this in a follow up though",
        "can we avoid the duplication here by just having another loop over (global_path, global_header) (local_path, local_header)?",
        "you can just loop through all the fields instead of repeating the code",
        "could be paranoid here and check the max line length of lines",
        "you are repeating this code 3 times (and not for scheduled tasks?) - can we just move this into agent.reply?"
      ],
      "goose-use-descriptive-identifiers": [
        "don't use time for unique ids",
        "Nice. rather than the comment (he said yelling at the clouds), can we just introduce a named constant that says this?"
      ],
      "goose-optimize-memory-and-algorithms": [
        "we could pre-allocate those since we know the number of files"
      ],
      "goose-document-non-obvious-code": [
        "should we also update the windows instructions. or better, make them as much as possible the same",
        "yeah, maybe to increase maintainability extract the commonalities so we will keep them in sync better going forward?"
      ],
      "goose-consolidate-api-parameters": [
        "can't we now just do this once in the api_client constructor?",
        "yeah, I get that, but if it is not provider specific, we can just always load it. then if we need something for providers that is not general, we can add it there.",
        "couldn't we just insert the json schema at this point? asking the llm to do it in this format probably works, but since the tool takes a json schema, why not use that?"
      ],
      "goose-use-established-logging-libraries": [
        "you should make a call on these prints - probably drop them and if not turn them into logs?",
        "why are we rolling our own log thing here and not just rely on tracing::debug!() for this? ",
        "tracing is the library meant for this as far as I know. writing a good logger is not trivial, so relying on something that exist and that we already use is probably the way to go\r\n\r\nwhere I think we should focus our attention is on tooling around it; we write json files to the current logs, can we have something that inspects them live? some light python tooling could be super useful"
      ],
      "goose-handle-errors-intentionally": [
        "if there is an error here, we show the dialog but never the secuirty warning, even if the scan failed?",
        "I don't think you want to generally catch errors here like this, just let it bubble up",
        "use safeJsonParse here, now in main",
        "maybe also check 503?"
      ],
      "goose-use-configuration-enums": [
        "let's have a separate flag for this or change the hasToolConfirmation thing into an enum"
      ],
      "goose-ai-model-configuration-consistency": [
        "maybe change the default model too?",
        "yeah, we're working on that. right now it is a bit of a mess which model a conversation will use. we're working on making this a real property of the session so that if you change the global settings, it won't change it for existing conversations.\r\n\r\neither way, changing the default here, should only change which model is suggested when you select the provider",
        "make that into a constant on top - ideally we'd do this in a more elegant way, but let's do that later.  don't make this mutable though, but have something like model = model.with_fast(CONSTANT) - possibly also check whether that is already set and don't overwrite it",
        "yeah, I think this goes back to the whole thinking around models <-> providers. in some ideal world I would say we have a models folder which defines all the models and their properties and then providers indicate which of those models they support. llama3-50b-9f has the same properties independent of the provider. I think",
        "make sure these models also have their token limits defined in models.rs",
        "how about:\r\n\r\n```\r\n    let (provider_to_use, model_to_use) = if provider == \"openrouter\" {\r\n        parse_model_id(model)\r\n            .map(|(real_provider, real_model)| (real_provider.as_str(), real_model.as_str()))\r\n            .unwrap_or((provider, model))\r\n    } else {\r\n        (provider, model)\r\n    };\r\n```"
      ],
      "goose-default-least-permissive": [
        "did you mean to delete this?",
        "this is probably fine for testing, but we should refactor this so we have a generic way to check whether we want to run a tool that says yes/no/ask the user with prompt and then take the least permissive version of that, i.e. if the counter says no, don't even run the security thing if that makes sense"
      ],
      "goose-optimize-hook-dependencies": [
        "what happened here? did we not get warnings about this?"
      ],
      "goose-optimize-algorithmic-efficiency": [
        "this is O(n^2) - consider using a map to do this more efficiently",
        "you can do this in one go: const lastAtIndex = text.lastIndexOf('@', cursorPosition - 1); and avoid the string creation"
      ],
      "goose-validate-ipc-inputs": [
        "> Since this is exposed as an IPC the path can be compromised to show any directory on the machine\r\n\r\ncan you help me understand the line of attack here? even if an outside attacker would break into goose (at which point they have full control over the host machine anyway), if they would this trigger this end point, all it would do is start the finder in a particular directory. only the end user would be able to see that, no? not the attacker?\r\n\r\nI'd love to understand security considerations with the electron app better as aI see some checks that I don't quite understand."
      ],
      "goose-use-idiomatic-optional-patterns": [
        "the AIs say you need an `as_ref()` in there still "
      ],
      "goose-extract-duplicate-constants": [
        "since these are the same, consider using a constant"
      ],
      "goose-environment-aware-configuration-handling": [
        "yes, but then you put the burden on the user to set that in their RustRover started backend to the same value - that thing currently sets it to test, so this is lower maintaince maybe?"
      ],
      "goose-extract-duplicated-code": [
        "why are hub and pair having the same diff & code here?",
        "we could extract this thing into a named function for clarity and code dedupe:\r\n\r\nconst closeMentionPopover = () => {\r\n  setMentionPopover(prev => ({ ...prev, isOpen: false }));\r\n};\r\n\r\nand then just return that here and below\r\n\r\n"
      ],
      "goose-document-api-contracts": [
        "I think if the configBase64 was not url encoded and it does contain a +, this will cause issues",
        "They are backwards compatible, but since we have both used url-encoded and not url-encoded versions, it doesn't quite fix it.\r\n\r\nis there a point in url-encoding the url_safe version?",
        "I am not sure I follow - the whole point of url safe base64 is that you can use it in, eh, urls. so url safe base64 with the = stripped seems like the most compact and portable way that we can do this\r\n\r\nfor decoding we'd have to put a bunch of safety measures in place probably, so still try to url-decode it if the base64 decode doesn't work etc",
        "great - do you want to do that in this, or merge this first"
      ]
    },
    "profile": {
      "location": "Miami Beach",
      "company": "Block",
      "blog": "https://neptyne.com",
      "twitter_username": "DOsinga",
      "site_admin": false,
      "followers": 164,
      "following": 0
    }
  },
  "Darksonn": {
    "repos": [
      "tokio-rs/tokio"
    ],
    "entries": [
      {
        "slug": "tokio-clear-command-documentation",
        "title": "Clear command documentation"
      },
      {
        "slug": "tokio-code-block-formatting-standards",
        "title": "Code block formatting standards"
      },
      {
        "slug": "tokio-design-error-handling-carefully",
        "title": "Design error handling carefully"
      },
      {
        "slug": "tokio-design-flexible-apis",
        "title": "Design flexible APIs"
      },
      {
        "slug": "tokio-document-null-safety-assumptions",
        "title": "Document null safety assumptions"
      },
      {
        "slug": "tokio-fast-deterministic-tests",
        "title": "Fast deterministic tests"
      },
      {
        "slug": "tokio-flexible-consistent-api-patterns",
        "title": "Flexible consistent API patterns"
      },
      {
        "slug": "tokio-follow-import-style",
        "title": "Follow import style"
      },
      {
        "slug": "tokio-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "tokio-graceful-error-handling",
        "title": "Graceful error handling"
      },
      {
        "slug": "tokio-granular-feature-flags",
        "title": "Granular feature flags"
      },
      {
        "slug": "tokio-memory-ordering-needs-justification",
        "title": "Memory ordering needs justification"
      },
      {
        "slug": "tokio-minimize-unsafe-code",
        "title": "Minimize unsafe code"
      },
      {
        "slug": "tokio-network-api-design-consistency",
        "title": "Network API design consistency"
      },
      {
        "slug": "tokio-optimize-algorithmic-complexity",
        "title": "Optimize algorithmic complexity"
      },
      {
        "slug": "tokio-optimize-algorithmic-efficiency",
        "title": "Optimize algorithmic efficiency"
      },
      {
        "slug": "tokio-optimize-ci-job-structure",
        "title": "Optimize CI job structure"
      },
      {
        "slug": "tokio-optimize-hot-paths",
        "title": "Optimize hot paths"
      },
      {
        "slug": "tokio-optimize-job-structure",
        "title": "Optimize job structure"
      },
      {
        "slug": "tokio-optimize-memory-allocation",
        "title": "Optimize memory allocation"
      },
      {
        "slug": "tokio-organize-code-logically",
        "title": "Organize code logically"
      },
      {
        "slug": "tokio-prefer-explicit-over-concise",
        "title": "Prefer explicit over concise"
      },
      {
        "slug": "tokio-release-locks-before-waking",
        "title": "Release locks before waking"
      },
      {
        "slug": "tokio-secure-unsafe-code",
        "title": "Secure unsafe code"
      },
      {
        "slug": "tokio-simplify-configuration-flags",
        "title": "Simplify configuration flags"
      },
      {
        "slug": "tokio-socket-configuration-guidance",
        "title": "Socket configuration guidance"
      },
      {
        "slug": "tokio-structural-configuration-approaches",
        "title": "Structural configuration approaches"
      },
      {
        "slug": "tokio-structure-api-doc-blocks",
        "title": "Structure API doc blocks"
      },
      {
        "slug": "tokio-structure-conditional-compilation",
        "title": "Structure conditional compilation"
      },
      {
        "slug": "tokio-structure-feature-flags-strategically",
        "title": "Structure feature flags strategically"
      },
      {
        "slug": "tokio-test-diverse-configurations",
        "title": "Test diverse configurations"
      },
      {
        "slug": "tokio-test-production-configurations-too",
        "title": "Test production configurations too"
      },
      {
        "slug": "tokio-use-option-methods-idiomatically",
        "title": "Use Option methods idiomatically"
      },
      {
        "slug": "tokio-write-focused-single-purpose-tests",
        "title": "Write focused single-purpose tests"
      }
    ],
    "comments": {
      "tokio-minimize-unsafe-code": [
        "Please include a short safety comment on why each unsafe block is okay.",
        "Please reduce the scope of this unsafe block. You only need it for the `&mut *tail` operation as far as I can tell.\r\n```Rust\r\nlet tail_block = unsafe { &mut *tail };\r\n```",
        "Generally, it is preferred to have a separate unsafe block for each unsafe operation, and to annotate each block with a `// SAFETY:` comment justifying its correctness."
      ],
      "tokio-test-diverse-configurations": [
        "That is fine. We may want to consider whether it makes sense to run with both?"
      ],
      "tokio-optimize-job-structure": [
        "Please don't install multiple Rust versions in a single CI run. It causes confusion about which version of Rust is actually being used in each call. Instead, create a new CI run for this check.",
        "The miri job is starting to take a rather long time. Could you add a new job for this, instead of adding additional work to the existing job? This will allow for parallelism.",
        "That's nice. The loom jobs are already much longer than that. We could consider gating it similarly to what we did for loom tests."
      ],
      "tokio-optimize-memory-allocation": [
        "The intent of this `unwrap_unchecked()` is to access the memory for free, but `get_mut` has a bunch of logic to lock the weak count and so on. If we actually want to access the memory without checks, we should go through `into_raw`.",
        "I don't think we should clear the buffer. That's up to the user. I think it's better to support appending to the vector if the user wants that.",
        "There are two options that make sense to me:\r\n\r\n * Add a third argument that specifies the maximum number of elements to add to the vector. In this case, the capacity is irrelevant, and we will resize the buffer if necessary.\r\n * Or use the capacity like this:\r\n\r\n```rs\r\nif buffer.len() == buffer.capacity() {\r\n    buffer.reserve(super::BLOCK_CAP);\r\n}\r\nlet max_number_added = buffer.capacity() - buffer.len();\r\n```\r\n\r\nIn either case, I prefer that we do not clear the buffer.",
        "I was looking at this paragraph, and I think it could be improved like this:\r\n```suggestion\r\n    /// If `buffer` has unused capacity, then this call will not reserve\r\n    /// additional space in `buffer`. This means that the maximum number of\r\n    /// received messages is `buffer.capacity() - buffer.len()`. However, if\r\n    /// the capacity is equal to the length, then this call will increase the\r\n    /// capacity to make space for additional elements.\r\n```\r\nThis actually raises a question: Perhaps it makes sense to only reserve space when we return a message? This way, we don't consume extra memory until a message arrives, and if the channel gets closed, we don't reserve any space.\r\n\r\nWhat do you think?",
        "How does this change the size of `Sleep`? Could `TimerEntry` be changed to reduce the change?",
        "What happens if you just don't call `clear_entry` in [this method](https://github.com/tokio-rs/tokio/blob/ef657d23fd0f73bc73d3cc872feaceb0f8bf36b7/tokio/src/runtime/time/entry.rs#L503-L527) when it hasn't yet been registered?\r\n\r\nBased on @conradludgate's comment [here](https://github.com/tokio-rs/tokio/issues/6504#issuecomment-2073343288), it sounds like this triggers a loom failure, but that's most likely due to some path where the timer is dropped concurrently with firing or something like that. If we've never been registered with the driver, then not calling `clear_entry` should be okay."
      ],
      "tokio-design-flexible-apis": [
        "Case 2 cannot happen since `SetOnce` isn't a channel that you clone. The return value can be a bare `&T`.",
        "Yeah ... I'm not sure what is the least surprising here.",
        "If the intent of this PR is a way to detect whether polling the channel will panic, then having a function that does exactly that sounds good to me.",
        "If the limit is zero, then I think it would be okay to just return 0 immediately.",
        "Hmm. I think immediately returning 0 is the least surprising behavior. After all, you asked to receive zero messages, and you got zero messages.",
        "If what is acceptable?\r\n\r\nI mainly see people be confused about length zero reads when they do this:\r\n```rs\r\nlet mut vec = Vec::with_capacity(1024);\r\nio.read(&mut vec);\r\n```\r\nand are surprised because `&mut vec` becomes a slice of length zero.\r\n\r\nBut that doesn't happen in our case.",
        "I think allowing a zero length is useful for cases where the length is computed dynamically. I've seen code along these lines:\r\n```rs\r\nlet len = io.read_u64().await? as usize;\r\nlet mut buffer = Vec::with_capacity(len);\r\nio.take(len).read_to_end(&mut buffer)?;\r\n```\r\nif there are length-zero messages, then the above makes use of `take(0)` behavior sometimes.",
        "If all of the `*acquire_many` methods already allow zero permits, then I see no harm in allowing it here. I would perhaps all the method `num_permits`?"
      ],
      "tokio-use-option-methods-idiomatically": [
        "Can you change this to only create a mutable reference if it is `None`?",
        "Yes, thanks for the suggestion."
      ],
      "tokio-code-block-formatting-standards": [
        "I'm happy to accept a PR that adds `--locked` to the command."
      ],
      "tokio-optimize-ci-job-structure": [
        "Please don't install multiple Rust versions in a single CI run. It causes confusion about which version of Rust is actually being used in each call. Instead, create a new CI run for this check.",
        "The miri job is starting to take a rather long time. Could you add a new job for this, instead of adding additional work to the existing job? This will allow for parallelism.",
        "That's nice. The loom jobs are already much longer than that. We could consider gating it similarly to what we did for loom tests."
      ],
      "tokio-optimize-hot-paths": [
        "```suggestion\r\n    #[doc(hidden)]\r\n    #[inline]\r\n    pub fn has_budget_remaining() -> bool {\r\n```"
      ],
      "tokio-document-null-safety-assumptions": [
        "This needs a safety comment to explain that this is okay because the caller guarantees that the written portion is reported correctly.\r\n\r\nAlso, it'd be nice to note that this is only correct because `self.buf.len() == 0` as asserted above.",
        "Can you add a safety comment?\r\n```\r\n// SAFETY: The memory may be uninitialized, but `rd.read` will only write to the buffer.\r\n```",
        "This unsafe block needs a safety comment. It should explain that the resulting pointer is in-bounds (or one after the end) of the `self.wakers` array.",
        "It seems like there's an extra call to `.cast()` here?",
        "Yes, using several `let` statements is good. It makes it clear what the types are."
      ],
      "tokio-test-production-configurations-too": [
        "That is fine. We may want to consider whether it makes sense to run with both?"
      ],
      "tokio-optimize-algorithmic-complexity": [
        "This iterates all of the buffers every time, even if we only write a few of them. If the buffers are very long and this is called in a loop, that gives quadratic performance.\r\n\r\nWe should be able to embed this logic inside the for loop instead to avoid that.",
        "No, `b.len()` is constant time. Instead, it's O(n) in the length of `bufs`, which you iterate over.",
        "I would expect that `saturating_pow` is rather expensive compared to a shift.\r\n```suggestion\r\n        let max_number = match 1.checked_shl(8 * self.length_field_len) {\r\n            Some(shl) => shl - 1,\r\n            None => u64::MAX,\r\n        };\r\n```",
        "If we're going to the effort of providing an implementation, then I think we should not go for a version that just calls `poll_next_entry` in a loop. At that point, it might as well be a generic piece of functionality that works for any `Stream`.\r\n\r\nEach call to that method generates a random number to pick a stream to start at, but I think it would make more sense to pick a random number only once, and then keep going around the loop until we have `limit` items, or until we've polled `self.entries.len()` times in a row without getting an item.",
        "Hmm. I imagine that when we get many items, we would want to get them from different streams (probably in the order they're stored). But with this strategy, whichever stream is at `start` will be preferred every time.",
        "Easiest is probably to either return the next index from `poll_one`, or to copy its implementation into `poll_next_many`.",
        "Seems simpler to just keep track of `added` from the beginning and change the while loop to `while added < limit`.",
        "Could something like this make more sense?\r\n```\r\nwheels_lock.0.iter_mut()\r\n    .filter_map(|wheel| wheel.get_mut().next_expiration_time())\r\n    .min();\r\n```\r\nThis way, we don't need to touch indexes at all.",
        "I would like to avoid having this variable at all when using biased. I don't think it can be optimized out because it gets stored as a field in the closure type.",
        "Yeah, I don't think we want the match. It duplicates the entire logic, which isn't nice. One option could be to define two helper types:\r\n```rs\r\n#[derive(Default)]\r\npub struct Rotater<const COUNT: usize> {\r\n    next: usize,\r\n}\r\n\r\nimpl<const COUNT: usize> Rotater<COUNT> {\r\n    #[inline]\r\n    pub fn num_skip(&mut self) -> usize {\r\n        let mut num_skip = self.next;\r\n        self.next += 1;\r\n        if self.next == COUNT {\r\n            self.next = 0;\r\n        }\r\n        num_skip\r\n    }\r\n}\r\n\r\n#[derive(Default)]\r\npub struct BiasedRotater {}\r\n\r\nimpl BiasedRotater {\r\n    #[inline]\r\n    pub fn num_skip(&mut self) -> usize {\r\n        0\r\n    }\r\n}\r\n```\r\nand then have the macro select which type to use:\r\n```text\r\n( biased; $($e:expr),+ $(,)?) => {\r\n    $crate::join!(@{ rotator=$crate::macros::support::BiasedRotator; () (0) } $($e,)*)\r\n};\r\n\r\n( $($e:expr),+ $(,)?) => {\r\n    $crate::join!(@{ rotator=$crate::macros::support::Rotator; () (0) } $($e,)*)\r\n};\r\n```"
      ],
      "tokio-network-api-design-consistency": [
        "This will incorrectly return `false` if stderr is a terminal but there is an ongoing IO operation. Can we instead do this?\r\n```suggestion\r\n    /// Returns true if the descriptor/handle refers to a terminal/tty.\r\n    pub fn is_terminal(&self) -> bool {\r\n        std::io::stderr().is_terminal()\r\n```",
        "Shouldn't this also take an interest? Otherwise we will eventually be asked to add an `with_epoll_flags_and_interest`.",
        "Please add to the documentation that `EPOLLONESHOT` must not be used, and that `EPOLLET` must be set. And please add debug asserts for this.",
        "Is Linux the only OS that needs this fix? Does macOS not have abstract path names? It would be worth to look at how mio did this prior to the v1 release.",
        "Ah, ok. Does `target_os = \"linux\"` also include Android? I see that it's also available there.",
        "Why the casts?",
        "Thanks."
      ],
      "tokio-granular-feature-flags": [
        "Hmm. I'll have to think about what implications this has for our MSRV.",
        "I'm okay with this, but I don't want to require hashbrown if you're just using the `TaskTracker`. Can you add a `join_map` feature?",
        "Would this be sufficient?\r\n```suggestion\r\njoin-map = [\"tokio/rt\", \"hashbrown\"]\r\n```",
        "We can always mark `mod task` with `#[cfg(any(rt, join_map))]`.",
        "Let me put that in its own PR so it gets a changelog entry. #6541"
      ],
      "tokio-structure-feature-flags-strategically": [
        "Some of these look redundant. Don't we always have libc on linux?",
        "Ok, that's fine.",
        "Hmm. I'll have to think about what implications this has for our MSRV.",
        "I'm okay with this, but I don't want to require hashbrown if you're just using the `TaskTracker`. Can you add a `join_map` feature?",
        "Would this be sufficient?\r\n```suggestion\r\njoin-map = [\"tokio/rt\", \"hashbrown\"]\r\n```",
        "We can always mark `mod task` with `#[cfg(any(rt, join_map))]`.",
        "We usually do it like this:\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/32970527633bb72fc4f01d02523484a9376ac26a/tokio/src/lib.rs#L1\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/32970527633bb72fc4f01d02523484a9376ac26a/.github/workflows/ci.yml#L1050-L1065\r\n\r\nIt may be worth to switch, but for now I would add a `#[allow(unexpected_cfgs)]` in the source code."
      ],
      "tokio-clear-command-documentation": [
        "I'm happy to accept a PR that adds `--locked` to the command."
      ],
      "tokio-release-locks-before-waking": [
        "Please add logic to `rt_multi_thread_available` to detect whether `--cfg tokio_unstable` is set and emit a hard error if it is not.",
        "The error should be similar to this error:\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/2506c9fa9916a1bdffbc762f7eb2ae5c2fd23836/tokio-macros/src/entry.rs#L193-L197",
        "I think both are okay.",
        "Doesn't that also trigger other unrelated errors when it fails?\r\n\r\nWhat we do today is essentially define the `main` macro two times to pass different values to [the `rt_multi_thread` boolean](https://github.com/tokio-rs/tokio/blob/master/tokio-macros/src/entry.rs#L488), and have `lib.rs` do this:\r\n```rs\r\n#[cfg(feature = \"rt-multi-thread\")]\r\npub use tokio_macros::main;\r\n#[cfg(not(feature = \"rt-multi-thread\"))]\r\npub use tokio_macros::main_rt as main;\r\n```\r\nThis way, the macro knows based on the cfgs that apply in the main Tokio crate. We could use the same approach and have four macros.",
        "I don't think that a shutdown future is an amazing example here. That makes sense for `select!`, but you wouldn't really have a shutdown future in `join!`. Not sure what a better example would be, though.",
        "Please make sure that the section names for `try_join!` match those we have for `join!`.",
        "```suggestion\r\n        /// place the shutdown future earlier in the `try_join!` list to ensure that it is\r\n```",
        "The current logic exists to reuse the thread parker logic when `block_on` is used many times. What is the reason for not reusing it?",
        "This is proposing to add them as stable despite the `LocalSet` question?",
        "The way we usually handle this kind of thing is to move _all_ entries in the list to another list (using the guarded linked list), and then we wake entries from the guarded list in batches. Can we not do the same here?",
        "I'm worried about panics here. What happens to entries still in the guarded list?\r\n\r\nI think some other examples of this wrap the guarded list in something that removes all entries from the list without waking their wakers."
      ],
      "tokio-graceful-error-handling": [
        "I think we probably want to just fall back to existing behavior if `stream_position` fails.\r\n```suggestion\r\n        let pos = std.stream_position().unwrap_or(0);\r\n```",
        "Using a debug assertion with a `--cfg` to disable it seems preferable to the `eprintln!`.",
        "You could implement the `source` method here.",
        "Let's not ignore errors.\r\n```suggestion\r\n///     compress_data(reader).await?;\r\n```",
        "Can you add `#[track_caller]` and a [test for the panic location](https://github.com/tokio-rs/tokio/blob/master/tokio/tests/rt_panic.rs)?"
      ],
      "tokio-write-focused-single-purpose-tests": [
        "It is easier for me to understand the tests if you split them into many small tests rather than one or two large ones. For example, these small tests could be useful:\r\n\r\n1. `is_closed` should return true after calling `close` but still has a sender\r\n2. `is_closed` should return true after dropping all senders\r\n3. `is_closed` should return true after dropping all senders except for a weak sender\r\n4. `is_closed` should return false when there is a sender\r\n5. `is_closed` should return false when there is a sender, even if enough messages have been sent to fill the channel\r\n6. `is_closed` should return false when there is a permit (but no senders)",
        "Thanks!\r\n\r\nThat could make sense. After all, they have different implementations of the semaphore, so we aren't *just* testing the same code twice.",
        "(No need to put everything you test related to thread ids in a single test. You can make more than one.)",
        "We like to avoid sleeps in tests to reduce the amount of time it takes to run the tests. If you make the runtime use mocked time via the `start_paused` parameter, then this test will run instantly no matter what the duration is.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/10e23d1c621ab38aadf2cefba1120494cff615f0/tokio/src/runtime/task/join.rs#L190-L205",
        "This will make the test run much much faster by using [simulated time](https://docs.rs/tokio/latest/tokio/time/fn.pause.html).\r\n```suggestion\r\n#[tokio::test(start_paused = true)]\r\n```",
        "We do not use sleeping in tests. We have 845 tests right now. Imagine how long it would take to run the tests if every single one had a 0.5 second sleep.",
        "Please include some tests for edge cases similar to the existing `empty_join` test at the bottom and the four `sync_*` tests at the top.",
        "Please add a test that uses this with `Vec<u8>` and `BytesMut` too.",
        "I'm concerned that we might need to supply the number of bytes as a separate parameter. I recall that resizable buffers are considered infinitely large by `BufMut`."
      ],
      "tokio-prefer-explicit-over-concise": [
        "If we're going to define a new type for this, then we should probably call it `AsyncFdTryNewError`.\r\n\r\nBut I would also be okay with `(T, io::Error)`.",
        "This name is okay, but I think there is precedent for the name `AbortHandle` elsewhere, so we might want to prefer that name.",
        "Actually, we have a type called `AbortHandle` in Tokio, and that does something else ... so that name doesn't work.\r\n\r\nBut another option is `AbortOnDropHandle`. I'll let you pick.",
        "I agree that `sender_strong_count` and `sender_weak_count` would be better. Otherwise, this PR looks good to me.",
        "Why not? I can't immediately see what would break if we just change `get` to `T: Clone`.",
        "This seems to compile locally:\r\n```diff\r\ndiff --git a/tokio/src/task/task_local.rs b/tokio/src/task/task_local.rs\r\nindex ba58ea6a..cb9d22c6 100644\r\n--- a/tokio/src/task/task_local.rs\r\n+++ b/tokio/src/task/task_local.rs\r\n@@ -264,16 +264,16 @@ impl<T: 'static> LocalKey<T> {\r\n     }\r\n }\r\n \r\n-impl<T: Copy + 'static> LocalKey<T> {\r\n+impl<T: Clone + 'static> LocalKey<T> {\r\n     /// Returns a copy of the task-local value\r\n-    /// if the task-local value implements `Copy`.\r\n+    /// if the task-local value implements `Clone`.\r\n     ///\r\n     /// # Panics\r\n     ///\r\n     /// This function will panic if the task local doesn't have a value set.\r\n     #[track_caller]\r\n     pub fn get(&'static self) -> T {\r\n-        self.with(|v| *v)\r\n+        self.with(|v| v.clone())\r\n     }\r\n }\r\n```",
        "Is it? If so, then you changed it before I got a chance to see it, because the thing I saw originally added a new method called `clone` instead of changing the existing `get` method.\r\n\r\nEither way, I have made up my mind. I would like to change `get` instead of introducing a new method.",
        "I'm sorry for the confusion. First, I am on the Tokio core team and am authoritative in this case (but it doesn't seem like @mox692 and I disagree?). As for your other questions:\r\n\r\n> Why remove get for Copy inner types?\r\n\r\nI am not suggesting that you remove `get` for `Copy` types. All `Copy` types are also `Clone` (it's a subtrait), so you will still be able to use `get` with any `Copy` type after changing `T: Copy` to `T: Clone`.\r\n\r\n> Why not just add a similar impl for `Clone`?\r\n\r\nWe could also do that, but then `Copy` types have two identical methods that do the same thing. To me, it seems simpler dev UX to have a single method that works for both `Copy` and `Clone`. That's why I suggested it.\r\n\r\n> I couldn't just extend on `get` for `Copy + Clone` simply because my type isn't `Copy` (it's an `Arc<_>`).\r\n\r\nIf you make `get` require `T: Clone` (rather than `T: Copy + Clone`), then it also works for `Arc<_>`."
      ],
      "tokio-structure-api-doc-blocks": [
        "Markdown docs generally start with one really short summary, followed by more text.\r\n```suggestion\r\n    /// Tries to convert a `WeakSender` into a [`Sender`]. \r\n    ///\r\n    /// This will return `Some`\r\n    /// if there are other `Sender` instances alive and the channel wasn't\r\n    /// previously dropped, otherwise `None` is returned.\r\n```\r\n\\+ reflow to line length",
        "We generally have empty lines between headers, text, and code blocks.\r\n```suggestion\r\n//! # Example encoding using `LinesCodec`\r\n//!\r\n//! The following example demonstrates how to use a codec such as [`LinesCodec`] to\r\n//! write a sink of framed data. [`FramedWrite`] can be used to achieve this. Data sent\r\n//! to [`FramedWrite`] are first framed according to a specific codec, and then sent to\r\n//! an implementor of [`AsyncWrite`].\r\n//!\r\n//! ```\r\n```",
        "This looks a bit weird with the sentence starting out of nowhere. Also, it's best to have extra information after a line break as that renders more nicely in the html docs.\r\n\r\n```suggestion\r\n    /// Returns the line number for where this symbol is currently executing.\r\n    ///\r\n    /// If debuginfo is missing, this is likely to return None.\r\n```",
        "This isn't publicly visible, so it's not a big deal here, but generally the convention is that documentation should start with one *short* line. If more explanation is needed, it can go in a separate paragraph. This is because when structs are shown in documentation, the first line is used as a summary in some cases, and the summary should be short."
      ],
      "tokio-optimize-algorithmic-efficiency": [
        "This iterates all of the buffers every time, even if we only write a few of them. If the buffers are very long and this is called in a loop, that gives quadratic performance.\r\n\r\nWe should be able to embed this logic inside the for loop instead to avoid that.",
        "No, `b.len()` is constant time. Instead, it's O(n) in the length of `bufs`, which you iterate over.",
        "I would expect that `saturating_pow` is rather expensive compared to a shift.\r\n```suggestion\r\n        let max_number = match 1.checked_shl(8 * self.length_field_len) {\r\n            Some(shl) => shl - 1,\r\n            None => u64::MAX,\r\n        };\r\n```",
        "I think the better way to check this is to use this:\r\n```rs\r\nnow.saturating_duration_since(timeout) > Duration::from_millis(5)\r\n```"
      ],
      "tokio-secure-unsafe-code": [
        "Please include a short safety comment on why each unsafe block is okay.",
        "We need to also mark `Blocking::new` as unsafe and require that `self.inner` satisfies the requirement. Otherwise this unsafe block isn't really correct.",
        "Please reduce the scope of this unsafe block. You only need it for the `&mut *tail` operation as far as I can tell.\r\n```Rust\r\nlet tail_block = unsafe { &mut *tail };\r\n```",
        "Generally, it is preferred to have a separate unsafe block for each unsafe operation, and to annotate each block with a `// SAFETY:` comment justifying its correctness."
      ],
      "tokio-flexible-consistent-api-patterns": [
        "I think that the `poll_proceed` / `made_progress` API we use internally is good. If we're going to expose more of coop, then I think we should expose that API instead of inventing a new one.\r\n\r\nBut I wouldn't do both \"expose coop API\" and \"make select coop aware\" in one PR. I'd like two PRs for the changelog.",
        "This provides a single object that is both the reader and writer, but in practice I think people will want those to be two different objects.",
        "I don't like this name. It sounds like something that would immediately make something abort.\r\n\r\nHow about `spawn_with_drop_handle` or `spawn_with_abort_handle`?",
        "Another possibility is to make this a constructor method. Then you would type `DropHandle::spawn`.",
        "If all of the `*acquire_many` methods already allow zero permits, then I see no harm in allowing it here. I would perhaps all the method `num_permits`?"
      ],
      "tokio-follow-import-style": [
        "Safety comments must be right before the unsafe block. Please move `let filled` before it.",
        "Which lint is this?",
        "I think this lint does not improve the code. Please add `#![allow(clippy::needless_lifetimes)]` to the top of the `lib.rs` files instead to silence it.",
        "We don't use this import style. Please split this into three `use` statements.",
        "In Tokio we usually split up imports from different modules like this:\r\n```suggestion\r\nuse std::future::Future;\r\nuse std::time::Duration;\r\n```\r\nThe brackets are only used for items in the same module.",
        "Tokio uses this convention for imports:\r\n```suggestion\r\n    os::fd::AsFd,\r\n    os::unix::io::{AsRawFd, RawFd},\r\n```"
      ],
      "tokio-socket-configuration-guidance": [
        "Can we improve this wording? I don't think it very clearly explains the situation. You could say something along the lines of \"Passing a listener in blocking mode is always errornous, and the behavior in that case may change in the future. For example, it could panic.\"",
        "Shouldn't this also take an interest? Otherwise we will eventually be asked to add an `with_epoll_flags_and_interest`.",
        "Please add to the documentation that `EPOLLONESHOT` must not be used, and that `EPOLLET` must be set. And please add debug asserts for this.",
        "Is Linux the only OS that needs this fix? Does macOS not have abstract path names? It would be worth to look at how mio did this prior to the v1 release.",
        "Ah, ok. Does `target_os = \"linux\"` also include Android? I see that it's also available there.",
        "Why the casts?",
        "Thanks."
      ],
      "tokio-organize-code-logically": [
        "Please put them in a sub-module so they don't clutter the front page of the crate documentation.",
        "Defining an inline module like you suggested in the beginning is fine. (But the other approaches would not be considered breaking.)",
        "No reason to expose a module with one item.\r\n```suggestion\r\nmod abort_on_drop;\r\npub use abort_on_drop::AbortOnDropHandle;\r\n```",
        "Please split this into a separate file. Rustfmt doesn't work inside the macros, so I don't want large codeblocks inside them."
      ],
      "tokio-fast-deterministic-tests": [
        "We like to avoid sleeps in tests to reduce the amount of time it takes to run the tests. If you make the runtime use mocked time via the `start_paused` parameter, then this test will run instantly no matter what the duration is.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/10e23d1c621ab38aadf2cefba1120494cff615f0/tokio/src/runtime/task/join.rs#L190-L205",
        "This will make the test run much much faster by using [simulated time](https://docs.rs/tokio/latest/tokio/time/fn.pause.html).\r\n```suggestion\r\n#[tokio::test(start_paused = true)]\r\n```",
        "We do not use sleeping in tests. We have 845 tests right now. Imagine how long it would take to run the tests if every single one had a 0.5 second sleep.",
        "Maybe this timeout should be larger?",
        "We expect tests to run with `cargo test` too, so we shouldn't use nextest specific features. Having a timeout sgtm."
      ],
      "tokio-simplify-configuration-flags": [
        "Could we just call it `--cfg tokio_uring`? We can have it require you to also pass `--cfg tokio_unstable`.",
        "Sorry, I meant how about we rename the flag to just `--cfg tokio_uring` instead of `--cfg tokio_unstable_uring`?",
        "Done."
      ],
      "tokio-structure-conditional-compilation": [
        "My main feedback is also this. It may make sense to merge this into a struct with the id, or to wrap the location in something that can be zero-sized on stable, or to otherwise avoid conditional compilation everywhere this information is passed around."
      ],
      "tokio-memory-ordering-needs-justification": [
        "Wait, does this PR not eliminate all usages of SeqCst?",
        "I don't know how your detector works, but mixing SeqCst and non-SeqCst orderings is never the right answer. Never.\r\n\r\nLooking over the code, I notice that there are atomics in both the `Notify`, but also the waiters. Before I can accept a PR relaxing any of them, I'm going to have to review the code for whether it currently relies on the SeqCst guarantees to synchronize between those different atomics. ",
        "Here you have exclusive access, so just perform a normal write."
      ],
      "tokio-follow-naming-conventions": [
        "We shouldn't add new `self` methods to types that implement `Deref`.\r\n```suggestion\r\n    pub fn rwlock(this: &Self) -> &Arc<RwLock<T>> {\r\n```",
        "This name is okay, but I think there is precedent for the name `AbortHandle` elsewhere, so we might want to prefer that name.",
        "Actually, we have a type called `AbortHandle` in Tokio, and that does something else ... so that name doesn't work.\r\n\r\nBut another option is `AbortOnDropHandle`. I'll let you pick.",
        "According to the API guidelines, this should be called `as_socket`. It's not an expensive conversion.\r\n```suggestion\r\n    fn as_socket(&self) -> socket2::SockRef<'_> {\r\n        socket2::SockRef::from(self)\r\n    }\r\n```",
        "Why not? I can't immediately see what would break if we just change `get` to `T: Clone`.",
        "This seems to compile locally:\r\n```diff\r\ndiff --git a/tokio/src/task/task_local.rs b/tokio/src/task/task_local.rs\r\nindex ba58ea6a..cb9d22c6 100644\r\n--- a/tokio/src/task/task_local.rs\r\n+++ b/tokio/src/task/task_local.rs\r\n@@ -264,16 +264,16 @@ impl<T: 'static> LocalKey<T> {\r\n     }\r\n }\r\n \r\n-impl<T: Copy + 'static> LocalKey<T> {\r\n+impl<T: Clone + 'static> LocalKey<T> {\r\n     /// Returns a copy of the task-local value\r\n-    /// if the task-local value implements `Copy`.\r\n+    /// if the task-local value implements `Clone`.\r\n     ///\r\n     /// # Panics\r\n     ///\r\n     /// This function will panic if the task local doesn't have a value set.\r\n     #[track_caller]\r\n     pub fn get(&'static self) -> T {\r\n-        self.with(|v| *v)\r\n+        self.with(|v| v.clone())\r\n     }\r\n }\r\n```",
        "Is it? If so, then you changed it before I got a chance to see it, because the thing I saw originally added a new method called `clone` instead of changing the existing `get` method.\r\n\r\nEither way, I have made up my mind. I would like to change `get` instead of introducing a new method.",
        "I'm sorry for the confusion. First, I am on the Tokio core team and am authoritative in this case (but it doesn't seem like @mox692 and I disagree?). As for your other questions:\r\n\r\n> Why remove get for Copy inner types?\r\n\r\nI am not suggesting that you remove `get` for `Copy` types. All `Copy` types are also `Clone` (it's a subtrait), so you will still be able to use `get` with any `Copy` type after changing `T: Copy` to `T: Clone`.\r\n\r\n> Why not just add a similar impl for `Clone`?\r\n\r\nWe could also do that, but then `Copy` types have two identical methods that do the same thing. To me, it seems simpler dev UX to have a single method that works for both `Copy` and `Clone`. That's why I suggested it.\r\n\r\n> I couldn't just extend on `get` for `Copy + Clone` simply because my type isn't `Copy` (it's an `Arc<_>`).\r\n\r\nIf you make `get` require `T: Clone` (rather than `T: Copy + Clone`), then it also works for `Arc<_>`.",
        "I agree that `sender_strong_count` and `sender_weak_count` would be better. Otherwise, this PR looks good to me."
      ],
      "tokio-design-error-handling-carefully": [
        "I think we probably want to just fall back to existing behavior if `stream_position` fails.\r\n```suggestion\r\n        let pos = std.stream_position().unwrap_or(0);\r\n```",
        "You could implement the `source` method here.",
        "I don't think we want two separate kinds of errors here. With `OnceCell` initialization can fail and can take a long time, so it's important to consider it to be a separate state, but with `SetOnce` the initialization state is always very short-lived and infallible, so I don't think we want to expose it to the end-user.",
        "Let's not ignore errors.\r\n```suggestion\r\n///     compress_data(reader).await?;\r\n```",
        "I like to avoid this error type in examples because it is not `Send` which means it only works in the `block_on` task.",
        "Avoiding unwrap is fine, I just prefer a different error type. E.g., it could be `Box<dyn Error+Send+Sync>`."
      ],
      "tokio-structural-configuration-approaches": [
        "My main feedback is also this. It may make sense to merge this into a struct with the id, or to wrap the location in something that can be zero-sized on stable, or to otherwise avoid conditional compilation everywhere this information is passed around.",
        "Does this need to be gated on `cfg_rt!`? Is the file not already gated on that?",
        "Can you add an `#[cfg_attr(not(feature = \"rt\"), allow(dead_code))]` on the module instead? The `cfg_rt!` blocks break rustfmt, so I try to avoid wrapping large blocks in them.",
        "Or maybe it'd be better to split the file into two and wrap the `mod` statement in `cfg_rt!`."
      ]
    },
    "profile": {
      "location": "Denmark",
      "company": "Google",
      "blog": "https://ryhl.io/",
      "site_admin": false,
      "followers": 2233,
      "following": 5
    }
  },
  "aryx": {
    "repos": [
      "semgrep/semgrep"
    ],
    "entries": [
      {
        "slug": "semgrep-add-ci-safety-checks",
        "title": "Add CI safety checks"
      },
      {
        "slug": "semgrep-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "semgrep-avoid-failwith-patterns",
        "title": "avoid failwith patterns"
      },
      {
        "slug": "semgrep-choose-efficient-algorithms",
        "title": "Choose efficient algorithms"
      },
      {
        "slug": "semgrep-ci-workflow-optimization",
        "title": "CI workflow optimization"
      },
      {
        "slug": "semgrep-conservative-security-assumptions",
        "title": "Conservative security assumptions"
      },
      {
        "slug": "semgrep-defensive-optional-handling",
        "title": "Defensive optional handling"
      },
      {
        "slug": "semgrep-design-consistent-tracing-interfaces",
        "title": "Design consistent tracing interfaces"
      },
      {
        "slug": "semgrep-document-code-purpose",
        "title": "Document code purpose"
      },
      {
        "slug": "semgrep-document-configuration-choices",
        "title": "Document configuration choices"
      },
      {
        "slug": "semgrep-document-configuration-troubleshooting",
        "title": "Document configuration troubleshooting"
      },
      {
        "slug": "semgrep-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "semgrep-justify-error-handling-design",
        "title": "justify error handling design"
      },
      {
        "slug": "semgrep-prefer-simple-readable-code",
        "title": "prefer simple readable code"
      },
      {
        "slug": "semgrep-require-module-documentation",
        "title": "require module documentation"
      },
      {
        "slug": "semgrep-use-appropriate-log-levels",
        "title": "Use appropriate log levels"
      },
      {
        "slug": "semgrep-use-appropriate-test-markers",
        "title": "use appropriate test markers"
      },
      {
        "slug": "semgrep-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "semgrep-use-typed-interfaces",
        "title": "Use typed interfaces"
      }
    ],
    "comments": {
      "semgrep-ensure-comprehensive-test-coverage": [
        "I think you've inverted the use of the .rule.yaml vs .yaml.\r\nLook at the other test file in the directory (I have to admit it's a but confusing, so some renaming could be useful or\r\na README).\r\nAlso the other tests have some     # ruleid: semgrep-metacheck-builtin\r\nin the .rule.yaml to actually test the functionality but none of your test have one.\r\n"
      ],
      "semgrep-use-descriptive-names": [
        "true, csv usually have multiple lines. I'll use comma_separated_strings",
        "do not use s, it's often used for string, src seems better (if we agree that source.ml is a good name for this module).\r\n",
        "code_of_origin is a bad name given you don't pass just an origin here but lots of arguments.\r\n",
        "Code is a very bad name, as this data structure does not contain code.\r\nMaybe call it RegularTarget, ",
        "maybe I would rename sort_by_mapped_key,\r\nto avoid ambiguity with the Assoc_.sort_by_key_xxx function\r\nand I would rename key as fkey, so it's clear we're passing a function here, not a key.\r\n",
        "remove the _match_data suffix in the field and the type\r\n",
        "a type with _info is not great.\r\n"
      ],
      "semgrep-justify-error-handling-design": [
        "Can we avoid yet another global ...? Is there no other way?\r\n"
      ],
      "semgrep-conservative-security-assumptions": [
        "you can actually pass all_caps here, but then restrict it for the callback, so for example don't pass the stdin/stdout capability to the callback so we're sure they can't interfere."
      ],
      "semgrep-document-configuration-troubleshooting": [
        "What happens if you remove one?\r\nWe get an error in CI?\r\nWhat happens it tomorrow we add a new language? Will the error be good enough to point to\r\nthis file?\r\n",
        "since the error mention src/main/dune\r\nyou can probably add a comment in it saying \"if you get a linking error like XXX, you probably need to modify .../flags.sh  and add the necessary library "
      ],
      "semgrep-choose-efficient-algorithms": [
        "simpler as cli_output.results |> List.map (fun x -> !!x.path) |> Set_.of_list |> Set_.elements |> List_.sort |>  String.concat \"\\n\") "
      ],
      "semgrep-require-module-documentation": [
        "Please add a high level overview of the goal of the module here, just after the prelude.",
        "now that this type has a few functions, please add a Xtarget.mli\r\n",
        "Please, add comments to each of those functions.\r\nSee Logs_.mli for example.\r\n",
        "please add a .mli\r\n\r\nWe usually use just a .ml when the file is mostly type definitions (because it's a bit annoying to copy paste all those type definitions), but for files having mostly functions, we usually prefer to have a .mli explaining how to use the module.\r\n",
        "Please add a .mli, with a toplevel comment explaining the goal of the module.\r\n",
        "please add comment explaining what this function is doing.\r\nIts name is very vague.\r\n",
        "please add a comment in the .mli about get_async and the fact it handles 307 errors and automatically redirect"
      ],
      "semgrep-design-consistent-tracing-interfaces": [
        "why not put the [@@trace] in rules_from_rule_source instead?\r\nso you don't need this wrapper.\r\n",
        "maybe @@tracing, to imitate @@profiling?\r\n"
      ],
      "semgrep-add-comprehensive-test-coverage": [
        "We don't have unit tests for that I think. The test plan is usually \"run -cfg_il and looks like it's ok\".\r\nMaybe we should start some test infra there (in a separate PR). Not super easy though to specify some tests and asserts that are readable.\r\n",
        "would be good to have a unit test for that. @mjambon wrote many of them in Unit_gitignore.ml\r\n"
      ],
      "semgrep-document-code-purpose": [
        "would be good to comment why this magic string comes from. Does it correspond to an internal test user?\r\n"
      ],
      "semgrep-use-typed-interfaces": [
        "please use the cli_output_extra field which is typed, so you can access it via cli_output_extra.dataflow_traces so mypy can detect typo.",
        "I mean that in python we should avoid to use code that can't be typed by mypy.\r\nIf you use dict[\"some_field\"], there is no way mypy can know whether this \"some_field\" is valid.\r\nThis is why we gradualy changed the python code to use the typed interface in semgrep_output_v1.py (generated from semgrep_output_v1.atd).\r\nWe migrated lots of the python code to use more types, but there's still a few places where we don't.\r\nBut in that case, we have a dataflow_traces field defined in the cli_match_extra so you should use it.\r\n"
      ],
      "semgrep-avoid-failwith-patterns": [
        "assert false is fine if we're sure this can never happen (because OCaml exhaustive check has limitations). If you're not sure, better failwith maybe\r\n",
        "maybe add in the error \"(this should not happen, FIXME)\""
      ],
      "semgrep-add-ci-safety-checks": [
        "if the commit already comes from pro, patch will fail later.\r\n",
        "ok, will add\r\n",
        "ok will add, thx",
        "when I have something working in GHA, even if it's not perfect, I want to commit it ASAP because you never know what little change can affect the thing and suddenly make the workflow fail ...\r\n",
        "Nice :)",
        "You're getting addicted!\r\n",
        "One of our CI job is checking whether semgrep is building on windows.",
        "build-test-windows-x86.yml, so if the workflow pass that means we're fine using git and git-unix\r\n"
      ],
      "semgrep-ci-workflow-optimization": [
        "please do 'make clean; make'\r\nbecause we don't handle dependencies right now so 'make' might be incorrect and forget\r\nto regenerate some files; make clean; make is a bit brute force, but for a CI check that's perfect.\r\n",
        "I think you can just use with: submodules: true\r\nso you don't need to do a separate checkout for semgrep-interfaces, and you can remove this comment about\r\nssh I don't really understand.\r\n",
        "maybe add a TODO saying we should find a way at some point to statically link semgrep for windows.\r\nThey managed to do it for the opam.exe binary for windows they distribute now."
      ],
      "semgrep-defensive-optional-handling": [
        "It's an operator I defined in Common.ml\r\n```\r\nlet ( ||| ) a b =\r\n  match a with\r\n  | Some x -> x\r\n  | None -> b\r\n```",
        "maybe add a (* TODO: we should not build such Tok.t in the first place without any filename, but let's be defensive here *)"
      ],
      "semgrep-document-configuration-choices": [
        "Same here, add a comment explaining why we need those special settings.",
        "maybe add some coupling: comment in this file and the related pypi file so if\r\none adds a python dependency, we know we also need to update this file.\r\n",
        "add a comment above before default_skip_libs explaining you can use regexps.\r\nIdeally you could even do let default_skip_libs : Pcre.regexp t = [\r\n ] |> List.map Pcre.regexp\r\n\r\n"
      ],
      "semgrep-use-appropriate-test-markers": [
        "I think you can remove those osemfail and no_semgrep_cli annotations; they are relevant just for the tests/e2e/ tests I think.\r\n",
        "maybe would be good to have some tests for that (the time will differ between runs, but we have some code\r\nin pytest to mask times.\r\nThat way we can also mark this tests as 'osemfail' so we know we need to port it.\r\n"
      ],
      "semgrep-use-appropriate-log-levels": [
        "Like I said in Slack, most of our logger#info should be converted in Logs.debug,\r\nbecause Logs.info can be seen by regular users when using --verbose.\r\n",
        "This should be Logs.err, like you did, but then that means every semgrep users will now see those errors ...\r\n\r\nI think that we should configure logging so that any Logs.err or Logs.info with a ~tags should be masked and shown only when you use --debug really.\r\n",
        "yes but then you should change this one to Logs.debug, because Logs.err will get displayed to the user I think.",
        "why we changed from Some Info to None now?\r\n"
      ],
      "semgrep-prefer-simple-readable-code": [
        "This is not readable. Avoid Option.map and Fun.xxx when you can and when the resulting simpler code is actually shorter.\r\n",
        "I usually find code more readable when they don't use Fun.const, or the $ operator, or curry, or Option.value, etc. KISS. In this case I think `match code_config with None -> false |  Some _ -> true' more readable.\r\n",
        "would probably have been better to define a wrapper my_alcotest_check_string()\r\nthat takes just 2 arguments and call Alcotest with the empty string, so we have\r\njust one place with this comment instead of 4.\r\n"
      ]
    },
    "profile": {
      "location": "Perugia, Italy",
      "blog": "https://twitter.com/yoann_padioleau",
      "site_admin": false,
      "followers": 454,
      "following": 26
    }
  },
  "brophdawg11": {
    "repos": [
      "remix-run/react-router"
    ],
    "entries": [
      {
        "slug": "react-router-api-backward-compatibility",
        "title": "API backward compatibility"
      },
      {
        "slug": "react-router-api-consistency-patterns",
        "title": "API consistency patterns"
      },
      {
        "slug": "react-router-api-naming-consistency",
        "title": "API naming consistency"
      },
      {
        "slug": "react-router-avoid-redundant-computations",
        "title": "avoid redundant computations"
      },
      {
        "slug": "react-router-avoid-timing-dependent-tests",
        "title": "avoid timing-dependent tests"
      },
      {
        "slug": "react-router-cancel-aborted-async-operations",
        "title": "Cancel aborted async operations"
      },
      {
        "slug": "react-router-complete-accurate-documentation",
        "title": "Complete accurate documentation"
      },
      {
        "slug": "react-router-configuration-compatibility-validation",
        "title": "configuration compatibility validation"
      },
      {
        "slug": "react-router-configuration-consistency-standards",
        "title": "configuration consistency standards"
      },
      {
        "slug": "react-router-configure-build-tools-properly",
        "title": "configure build tools properly"
      },
      {
        "slug": "react-router-configure-react-build-environments",
        "title": "Configure React build environments"
      },
      {
        "slug": "react-router-configure-rendering-modes-clearly",
        "title": "Configure rendering modes clearly"
      },
      {
        "slug": "react-router-dependency-version-ranges",
        "title": "dependency version ranges"
      },
      {
        "slug": "react-router-distinguish-falsy-vs-nullish",
        "title": "distinguish falsy vs nullish"
      },
      {
        "slug": "react-router-documentation-clarity-standards",
        "title": "documentation clarity standards"
      },
      {
        "slug": "react-router-documentation-generation-compatibility",
        "title": "documentation generation compatibility"
      },
      {
        "slug": "react-router-documentation-linking-standards",
        "title": "documentation linking standards"
      },
      {
        "slug": "react-router-extract-test-helpers",
        "title": "Extract test helpers"
      },
      {
        "slug": "react-router-graceful-error-handling",
        "title": "graceful error handling"
      },
      {
        "slug": "react-router-handle-ssr-hydration-mismatches",
        "title": "Handle SSR hydration mismatches"
      },
      {
        "slug": "react-router-hook-dependencies-stability",
        "title": "Hook dependencies stability"
      },
      {
        "slug": "react-router-http-protocol-compliance",
        "title": "HTTP protocol compliance"
      },
      {
        "slug": "react-router-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "react-router-organize-related-code-together",
        "title": "organize related code together"
      },
      {
        "slug": "react-router-organize-test-scripts-properly",
        "title": "organize test scripts properly"
      },
      {
        "slug": "react-router-pin-problematic-dependencies",
        "title": "Pin problematic dependencies"
      },
      {
        "slug": "react-router-precise-null-type-checking",
        "title": "precise null type checking"
      },
      {
        "slug": "react-router-prefer-built-in-react-types",
        "title": "prefer built-in React types"
      },
      {
        "slug": "react-router-prefer-explicit-readable-constructs",
        "title": "prefer explicit readable constructs"
      },
      {
        "slug": "react-router-provide-explicit-error-handling",
        "title": "Provide explicit error handling"
      },
      {
        "slug": "react-router-remove-obsolete-configuration-options",
        "title": "Remove obsolete configuration options"
      },
      {
        "slug": "react-router-simplify-configuration-setup",
        "title": "Simplify configuration setup"
      },
      {
        "slug": "react-router-structure-documentation-interfaces",
        "title": "Structure documentation interfaces"
      },
      {
        "slug": "react-router-use-descriptive-semantic-names",
        "title": "Use descriptive semantic names"
      }
    ],
    "comments": {
      "react-router-api-consistency-patterns": [
        "```suggestion\r\n  /** @deprecated Use `MetaArgs.loaderData` instead */\r\n```",
        "```suggestion\r\n   * @deprecated Use `UIMatch.loaderData` instead\r\n```",
        "Same as Remix was doing in 1.7.6: https://github.com/remix-run/remix/blob/remix%401.7.6/packages/remix-server-runtime/server.ts#L418",
        "For the moment we are not persisting `method:'POST'` as Remix did since we think that should be changed in Remix",
        "Instead of directly definig these here, have a base private `SharedSubmitOptions` that can be extended.  That way it's not easy to add something fetcher-only and accidentally leak it to non-fetcher cases.  Do this for `FetcherFormProps` as well",
        "`RedirectResult`'s contain the raw response now, so just grab location from there",
        "good call - yeah I think previously we would have handled that on the value in `result.location` and reduced it to a root relative path but that's probably missed now.  We can move the logic into a util and leverage here - search for the code comment:\r\n\r\n```\r\n// Strip off the protocol+origin for same-origin + same-basename absolute redirects\r\n```",
        "✅ ",
        "Could we build this around `redirect`?\r\n\r\n```suggestion\r\nexport const redirectWithReload: RedirectFunction = (url, init) => {\r\n  let response = redirect(url, init);\r\n  response.headers.set(\"X-Remix-Reload-Document\", \"true\");\r\n  return response;\r\n};\r\n```"
      ],
      "react-router-hook-dependencies-stability": [
        "I actually think this will work - the \"fix\" is removing `state.revalidation` as a dep of the revalidate function.  `dataRouterContext.router` should remain stable for the duration of the app ",
        "Previously, when you kicked off a revalidation and `state.revalidation` went into a `loading` state, you got a new instance of the `revalidate` function",
        "Can we throw existing return types on here to ensure we keep the contract the same?\r\n\r\n```suggestion\r\nexport function useRevalidator(): {\r\n  revalidate: () => Promise<void>;\r\n  state: DataRouter[\"state\"][\"revalidation\"];\r\n} {\r\n```",
        "Can we keep this like it was so we don't couple the return value of `useRevalidator().revalidate` and `router.revalidate`?\r\n\r\n```suggestion\r\n    async revalidate() {\r\n      await dataRouterContext.router.revalidate();\r\n    },\r\n```",
        "Tests needed to be updated so they don't return promises from `useEffect`",
        "The reason this was problematic in 6.11 is that we would call navigate twice and when in a `RouterProvider` we'd delay resolving the relative path until inside the router.  And thus the second execution would re-resolve `to` against the current location (which in a on-loader scenario would already be updated from the first effect).\r\n\r\nInstead, we resolve the absolute path here in `Navigate` so that duplicate calls to navigate via the data router go to the same path - just as they do in `BrowserRouter`.",
        "For future reference we tested the unit test setups from [93ccb2b](https://github.com/remix-run/react-router/pull/10435/commits/93ccb2b5d967142270a084cdd166bcc34943fefb) in demo apps using react 16.8 and 18 and confirmed the UI was the same (even if the underlying React.StrictMode execution approaches differed).  We may try to see if we can get our test suite running against both versions in a separate undertaking.",
        "Also, I think we're no worse off than we were previously where the effect had no second param since it would have re-run every time anyway?"
      ],
      "react-router-api-naming-consistency": [
        "```suggestion\r\nAdd `loaderData` arguments/properties alongside existing `data` arguments/properties to provide consistency and clarity between `loaderData` and `actionData` across the board\r\n - Updated types: `Route.MetaArgs`, `Route.MetaMatch`, `MetaArgs`, `MetaMatch`, `Route.ComponentProps.matches`, `UIMatch`\r\n - `@deprecated` warnings have been added to the existing `data` properties to point users to new `loaderData` properties, in preparation for removing the `data` properties in a future major release\r\n```",
        "lol yeah I should have probably grabbed my examples from those templates and not the playgrounds in our repo.  I'll merge this change and also go through and see if there's other stuff I should be bringing over too",
        "```suggestion\r\n- Log deprecation warnings for v7 flags\r\n- Add deprecation warnings to `json`/`defer` in favor of returning raw objects\r\n  - These methods will be removed in React Router v7\r\n```\r\n\r\n"
      ],
      "react-router-configure-rendering-modes-clearly": [
        "Never need to prerender a manifest anymore\r\n- `ssr: true` will handle the manifest via the server handler\r\n- `ssr: false` will not have fog of war enabled",
        "This went a bit too far - `ssr:false, prerender:['/']` is an explicit opt-into prerendering the `/` route and should trigger full SSG of the `/` route and prerender past the root.",
        "Write out this spa fallback file when the user chose `ssr:false + prerender:['/']` as a way for them to load/hydrate into non-prerendered paths without a runtime server",
        "Prerender `.data` files for any routes with at least one loader"
      ],
      "react-router-documentation-linking-standards": [
        "Same for typedocs - we when we don't find a `.md` file for the `@link` we look through the typedoc json (make sure you have run `pnpm run docs` locally before testing this out to generate the typedoc JSON file)\r\n\r\n```suggestion\r\nThe {@link Blocker} object returned by the hook has the following properties:\r\n```"
      ],
      "react-router-avoid-redundant-computations": [
        "Track up to 1000 paths (fifo) internally to avoid re-calling `patchRoutesOnMiss` on subsequent navigations to the same path"
      ],
      "react-router-cancel-aborted-async-operations": [
        "It will be up to users to abort their own promises going forward",
        "Cancel `defer()` instances created _after_ the `request` has already been aborted",
        "Cancel `defer()` instanced created _before_ the `request` is aborted"
      ],
      "react-router-configure-build-tools-properly": [
        "Needed to avoid TS incorrectly exporting a CJS module during the rollup esm build"
      ],
      "react-router-handle-ssr-hydration-mismatches": [
        "Wipe out any SSR'd 404s if we now match our new client routes - we will have a hydration error/flicker but it will recover with the client render",
        "```suggestion\r\n      // no-op - no changes if we can't construct a valid URL\r\n```"
      ],
      "react-router-prefer-built-in-react-types": [
        "Added a generic here so the remix layers could pass in their route type",
        "All these can go away and we just use the RR types - and users will add `context: appLoadContext` via module augmentation: https://reactrouter.com/dev/guides/upgrading/remix#step-7---update-types-for-apploadcontext"
      ],
      "react-router-remove-obsolete-configuration-options": [
        "yeah it's now a useless prop that was supposed to be removed in `7.0.0` but got missed.  I think you can view it as a types bug fix since the type allows you to pass a thing that does nothing.  FWIW it's also a build-time \"break\" not a runtime functional breaking change.",
        "Confirmed with  the tam we feel this is a types bug fix.\r\n\r\nAnother advantage is the error TS will provide after this change is a _good_ thing because if you are still passing `abortDelay` in RR v7 then it's highly likely you have a functional bug in your app because your streams are not going to timeout properly.  Surfacing this TS proper error will alert you to fix that bug.",
        "I think the css-bundle package is obsolete in a `vite`-only world?"
      ],
      "react-router-extract-test-helpers": [
        "Added some helpers for these things so we can add new fields in one spot and not have to go around and touch all tests that mock out a context",
        "Added the `decodedChar` to all existing setups - even though it's the same as the `char` since we now assert matched param values against `decodedChar`",
        "Moved to partial-hydration-test and these now also run against the `react-router` `RouterProvider` along with `createMemoryRouter`"
      ],
      "react-router-documentation-clarity-standards": [
        "```suggestion\r\nExport `DefineRouteFunction` type alongside `DefineRoutesFunction`\r\n```",
        "```suggestion\r\nDisable Lazy Route Discovery for all `ssr:false` apps and not just \"SPA Mode\" because there is no runtime server to serve the search-param-configured `__manifest` requests\r\n```",
        "```suggestion\r\nWe don't actually want the about page to be nested inside of the sidebar layout. Let's move the sidebar to a layout so we can avoid applying it to the about page. Additionally, we want to avoid loading all the contacts data on the about page.\r\n```",
        "```suggestion\r\nIn React Router v7 you define your routes using the [`app/routes.ts`][routing] file. For backwards-compatibility and for folks who prefer [file-based conventions][fs-routing], you can opt-into the same \"flat routes\" convention you are using in Remix v2 via the new `@react-router/fs-routes` package:\r\n```",
        "```suggestion\r\nWhen using `v7_relativeSplatPath`, properly resolve relative paths in splat routes that are children of pathless routes\r\n```\r\n\r\nThe pull request number will be added automatically during the release process, and the PR will link to the issue number :)",
        "```suggestion\r\n\"react-router-dom\": patch\r\n```\r\n\r\nThis can just be a patch - no new runtime functionality added, just exporting a missing existing type so it's more of a type bugfix IMO",
        "```suggestion\r\nExport `NavLinkRenderProps` type for easier typing of custom `NavLink` callback\r\n```",
        "```suggestion\r\nAllow falsy `location.state` values passed to `<StaticRouter>`\r\n```"
      ],
      "react-router-avoid-timing-dependent-tests": [
        "Are we sure this test exhibits the actual bug?  I see this failure when run against `dev`:\r\n\r\n```\r\n● navigation blocking › proceeds from blocked state using browser history › proceeds after quick block of back navigation\r\n\r\n    expect(received).toBe(expected) // Object.is equality\r\n\r\n    Expected: \"/about\"\r\n    Received: \"/\"\r\n```\r\n\r\nThe reproduction for the original issue leaves you on the original blocked page (`/three`) but this test, without the fix, actually ends up going back 2 history locations?",
        "Ah ok yeah this is probably a JSDOM issue.  I'm going to test this through an integration test in a real browser in https://github.com/remix-run/remix/pull/9914 instead of trying to hack JSDOM into behaving correctly"
      ],
      "react-router-structure-documentation-interfaces": [
        "yeah I was hoping the signature right above showing the rest/spread would make that apparent - but we could adjust the wording to make it more explicit?\r\n\r\n<img width=\"1640\" height=\"868\" alt=\"Screenshot 2025-07-21 at 5 18 05 PM\" src=\"https://github.com/user-attachments/assets/dc3f1ffe-caea-4de2-a430-5c5c860f58d3\" />\r\n\r\n\r\nI don't like [what we're doing today](https://reactrouter.com/api/components/PrefetchPageLinks) by trying to document each individual property since we're just being a worse MDN at that point and I'd rather just link out to them.",
        "Extracted to an interface so it gets it's own docs page and to align with `DOMRouterOpts`"
      ],
      "react-router-api-backward-compatibility": [
        "This internal `_renderMatches` was forked off as a private implementation to allow us to expand the function with data-router capabilities without changing the existing `renderMatches` public API.",
        "Because `middleware` is part of the default data strategy, we have to re-implement it here in our custom data strategy and can do so using the same `runMiddlewarePipeline` API we use internally.  I'm thinking we should make some form of this public API as well for userland `dataStrategy` implementation who want to use the normal middleware. \r\n\r\nThe current API is as follows - may be leaking some implementation details we could hide in the exported version though:\r\n\r\n```js\r\nrunMiddlewarePipeline(\r\n  // Passthrough of { request, matches, context } from dataStrategy\r\n  args, \r\n\r\n  // how deep?  I.e., what is the lowest handler to run\r\n  matchIndexToRunMiddlewareTo, \r\n\r\n  // Should I bubble up a returned Response?  SSR only - always `false` in user client-side implementations\r\n  false, \r\n\r\n  // callback to run the handlers and assign results to keyedResults\r\n  // async (keyedResults: Record<string, DataStrategyResult>) { ... },\r\n\r\n  // Error callback if a middleware throws an error - assign the error to keyedResults\r\n  async (e: MiddlewareError, keyedResults: Record<string, DataStrategyResult>) { ... }  \r\n)\r\n```\r\n\r\n\r\nMaybe we could pass it as an arg to `dataStrategy`?  We could remove the boolean and handle that for them internally, and then instead of using an index we could just let them hand us the matches which they could `.slice` if they didn't want to run all the way down:\r\n\r\n```js\r\nfunction dataStrategy({ request, params, context, matches, runMiddleware }) {\r\n  return runMiddleware(\r\n    { request, params, context }, \r\n    matches,\r\n    (results) => { /* run handlers, assign to results */ },\r\n    (e, results) => { /* handle error */ },\r\n  );    \r\n})\r\n```\r\n",
        "Ah ok yeah good catch - opened a PR to fix those https://github.com/remix-run/react-router/pull/13946.  I'll cut an experimental release if you want to try it out and see if it fixes the issue.  I'll tag you to move the convo over there",
        "Use the new `runMiddleware` API to wrap middleware around our existing `dataStrategy` - instead of calling middleware independently for each fork in the `dataStrategy`"
      ],
      "react-router-provide-explicit-error-handling": [
        "We can probably run this through the same utility we use elsewhere\r\n\r\n```suggestion\r\n      let error = new Error('Unhandled request')\r\n      return returnLastResortErrorResponse(error, serverMode);\r\n```",
        "All but 2 uses of `abortFetcher` were already defensive against this invariant and the 2 that weren't were the bug we're fixing, so now that they all want to be defensive we can just flatten the defensive check into this method."
      ],
      "react-router-organize-test-scripts-properly": [
        "This makes it easier to re-run the test without a build vis `pnpm test:integration:run` to iterate on test contents without changing source code."
      ],
      "react-router-pin-problematic-dependencies": [
        "```suggestion\r\n        # PLEASE KEEP THIS PINNED TO 1.4.10 to avoid a regression in 1.5.*\r\n        # See https://github.com/changesets/action/issues/465\r\n        uses: changesets/action@v1.4.10\r\n```"
      ],
      "react-router-documentation-generation-compatibility": [
        "Include this as a nested `dom` module underneath `react-router`",
        "Thanks!  Just got the `docs.ts` script updated to handle the new module",
        "We can lift the note above the example so it shows in the summary and then we shouldn't need to try to detect this - I updated that file in f96433f37",
        "Export this and `DOMRouterOpts` so they get picked up by typedoc"
      ],
      "react-router-use-descriptive-semantic-names": [
        "Rename for clarity:\r\n\r\n* `handlerContext`-> a `StaticHandlerContext` returned from `staticHandler.query()`\r\n* `requestContext` -> The `AppLoadContext` used passed to `staticHandler.query` by Remix SSR today\r\n* `routerContext` -> This new `DefaultRouterContext` used for SPAs since it's not tired to a request like the SSR use case\r\n\r\nIn shared code, `requestContext` becomes `routerContext`",
        "I wonder if we should rename this to `navigationType` in v7 to align with `useNavigationType`...?",
        "Could we rename this to something like `unblockBlockerHistoryUpdate` to align with the new approach?"
      ],
      "react-router-http-protocol-compliance": [
        "Lets drop a link in here for easier lookup\r\n\r\n```suggestion\r\n  // HTTP/2 doesn't support status messages\r\n  // https://datatracker.ietf.org/doc/html/rfc7540#section-8.1.2.4\r\n```",
        "When a prerendered document redirects we fall back on an http-equiv redirect"
      ],
      "react-router-graceful-error-handling": [
        "lol I was super confused for a bit - since we shouldn't be using this \"built-in\" hydration logic in Remix since we do our own hydration via `window.__remixContext`.  This is used to automatically hydrate from `StaticRouterProvider`'s `window.__staticRouterHydrationData` - but we specifically pass [`hydrate=false`](https://github.com/remix-run/react-router/blob/v7/packages/react-router-dom/ssr/server.tsx#L90) in the Remix SSR use case.\r\n\r\nTurns out we're just using the wrong implementation since we brought the Remix code over.  There's a dup version of this method in `ssr/errors.ts` that preserves the stack that we're not implementing since it found a local function with the same name.  \r\n\r\nThe original reason for automatically clearing stack traces is that it felt safer than assuming any DIY-SSR setups would always be stripping them on the server like Remix does so it prevented any accidental leakage of SSR stack traces.  Including the in dev-only mode felt like something advanced users could achieve via manual hydration.\r\n\r\nI think for now we can just de-dup them and maybe add a param to preserve the stack trace that we send from the Remix usage in `RouterProvider` and continue stripping in the RR case (`parseHydrationData`).\r\n\r\nI am also pretty sure the Remix usage can go away in v7 with single fetch but would need to look a bit deeper into that.  ",
        "Bubble a proper 404 to our UI if we're in a prerendered app and there is no static file on the server",
        "We have a built-in `warning` method we can use here:\r\n\r\n```suggestion\r\n      } catch (error) {\r\n          warning(\r\n            false,\r\n            \"Failed to save scroll positions in sessionStorage, <ScrollRestoration /> will not work properly.\"\r\n          );\r\n          console.error(error);\r\n      }\r\n```",
        "Yeah the idea there was to show the actual underlying error, but we could probably inline `error.message` into the warning as well if we wanted to keep just one console entry and avoid a `console.error`."
      ],
      "react-router-configure-react-build-environments": [
        "Set this correctly before calling the CLI so the proper version of React gets loaded (https://github.com/remix-run/react-router/issues/12078)"
      ],
      "react-router-configuration-consistency-standards": [
        "We have a build time `__DEV__` constant we can swap out for dev/prod builds to allow the library to remain runtime agnostic\r\n\r\n```suggestion\r\n  if (__DEV__ && !alreadyWarned[message]) {\r\n```",
        "Parse args and set `NODE_ENV` first",
        "\"SPA Mode\" is specifically no SSR and only generating a root `index.html` - this `isSpaMode` flag is what tells the server.client not to SSR/hydrate beyond the root route.  "
      ],
      "react-router-distinguish-falsy-vs-nullish": [
        "While the nullish coalescing operator is the \"proper\" way to do this, we have a lint rule that doesn't allow it because when it gets transpiled away it results in a fairly large bundle bloat so we stopped using it in the code until a major version bump when we can update our tranpsilation settings and minimum browser support.  Would you mind changing this to:\r\n\r\n```suggestion\r\n    state: locationProp.state != null ? locationProp.state : null,\r\n```"
      ],
      "react-router-dependency-version-ranges": [
        "I think in the root monorepo we are safe to put everything in `dependencies` since end users never use this"
      ],
      "react-router-precise-null-type-checking": [
        "Adjust types based on whether the param is required/optional",
        "No longer optional - still maintains a default type of `any` though",
        "This is a type definition bug - key is not optional in the implementations of `getFetcher`/`deleteFetcher`"
      ],
      "react-router-prefer-explicit-readable-constructs": [
        "I think I expected to see the `if (route.clientLoaderModule)` logic above moving down into `route.lazy.loader` as well?  I think it would be another split in this conditional (using if/else for readability now that it has 3 branches).  It might nicely colocate all of our loader initialization whereas we used to have 2 spots (outside/inside of `lazy`)\r\n\r\n```ts\r\n      // assume `dataRoute.lazy` is initialized as an empty object above - may not \r\n      // be the way we want to do it but it might read nicely as we build up the \r\n      // route object and let us colocate initialization logic per-field...\r\n      if (!route.hasClientLoader) {\r\n        // No `clientLoader` exists, use the `loader` to load styles and call the\r\n        // server `loader` (if it exists) in parallel with `route.lazy` execution\r\n        dataRoute.loader = (_: LoaderFunctionArgs, singleFetch?: unknown) =>\r\n          prefetchStylesAndCallHandler(() => {\r\n            return fetchServerLoader(singleFetch);\r\n          });\r\n      } else if (route.clientLoaderModule) {\r\n        // A `clientLoader` module exists, load it with route.lazy.loader\r\n        dataRoute.lazy.loader = async () => {\r\n          invariant(route.clientLoaderModule);\r\n          let { clientLoader } = await import(\r\n            /* @vite-ignore */\r\n            /* webpackIgnore: true */\r\n            route.clientLoaderModule\r\n          );\r\n          return (args: LoaderFunctionArgs, singleFetch?: unknown) =>\r\n            clientLoader({\r\n              ...args,\r\n              async serverLoader() {\r\n                preventInvalidServerHandlerCall(\"loader\", route);\r\n                return fetchServerLoader(singleFetch);\r\n              },\r\n            });\r\n        };\r\n      } else {\r\n        // No `clientLoader` module exists, load the `clientLoader` via the\r\n        // full route module\r\n        dataRoute.lazy.loader = async () => {\r\n          let { clientLoader } = await getLazyRoute();\r\n          invariant(clientLoader, \"No `clientLoader` export found\");\r\n          // This below is the same as above so may be able to be shared.  \r\n          // All that differs between this and the conditional branch above is \r\n          // where we get `clientLoader` from...\r\n          return (args: LoaderFunctionArgs, singleFetch?: unknown) =>\r\n            clientLoader({\r\n              ...args,\r\n              async serverLoader() {\r\n                preventInvalidServerHandlerCall(\"loader\", route);\r\n                return fetchServerLoader(singleFetch);\r\n              },\r\n            });\r\n        };\r\n      }\r\n```",
        "We can use `endsWith` here for a little more clarity:\r\n\r\n```suggestion\r\n    const endSlashPosition =\r\n      toPathname.length > 1 && toPathname.endsWith(\"/\")\r\n        ? toPathname.length - 1\r\n        : toPathname.length;\r\n```"
      ],
      "react-router-maintain-naming-consistency": [
        "This will now be auto-generated as just `HistoryRouter.md` so the URL stays stable even if we ever stabilize the API (which we won't in this case, but now it's consistent with how we handle `usePrompt` and others)",
        "Nit - we use the term \"browser\" in our code and repo playgrounds/templates.  I think the original motivation was to avoid any confusion of the SSR Server being a \"client\" of the RSC server.  I'm happy with either, but if we go with client it's probably worth updating our internal code to align.",
        "Is `pages` an explicit choice here?  Should we be consistent with https://github.com/remix-run/react-router-templates/tree/main/default/app/routes?"
      ],
      "react-router-configuration-compatibility-validation": [
        "```suggestion\r\nIf you need to prerender paths with dynamic/splat parameters, or you only want to prerender a subset of your static paths, you can provide an array of paths:\r\n```",
        "I might introduce `HydrateFallback` here since it can/should be used for this scenario where you have only a `clientLoader`, and no `loader`.  In that scenario, `clientLoader.hydrate` defaults to true because we _have_ to run it on hydration if we have no `loaderData` from the server.\r\n\r\nThen below you can mention:\r\n* When you provide a server `loader` in addition to a `clientLoader`, we _can_ render the during SSR using the server loader data\r\n* If if we do not want to render the route with only server loaderData, then set `clientLoader.hydrate=true` to run on hydration and provide a `HydrateFallback` to render until `clientLoader` completes"
      ],
      "react-router-simplify-configuration-setup": [
        "Externalize all `node_modules` deps like we did in Remix, instead of maintaining a manual list",
        "If it's true we only use this for experimentals now, we might be able to shed some more stuff in here:\r\n\r\n* This prompt stuff (step 2) isn't necessary since we're using `--skip-prompt`, so we could remove that CLI flag and this code\r\n* The manual version bump above (`if (version == null)`) I think is no longer needed since we'll _always_ provide a version as a CLI arg?\r\n* I think all the `isSnapshotVersion` conditionals could go away since it'll only be used for those",
        "it's probably a moot point too since the router will be collapsing into react-router too so everything will be versioned identically moving forward.  We can tackle that when we remove the router package 👍 "
      ],
      "react-router-organize-related-code-together": [
        "unrelated to this PR, but curious why this doesn't live in `__tests__`?",
        "Not strongly - just really consistency within the repo more than preferring one approach or the other",
        "nit - but this may make sense living in `utils.ts` next to `stripBasename`",
        "Anything directly exported from `internal-export.ts` now lives in the `types/internal-export/` folder",
        "Moved these to `UNSAFE_` exports instead of deep imports",
        "We have to export a bunch of stuff as `UNSAFE` here to consume from the subpath export file to avoid duplicating any implementations",
        "All router imports come from the source files, no more `lib/router/index.ts` barrel file to re-export"
      ],
      "react-router-complete-accurate-documentation": [
        "We've specifically avoided doing this so far because it causes an extra level of nesting that we don't want the UI - it now look like these APIs should be imported from `react-router/index` and `react-router/dom-export` - both of which are incorrect...\r\n\r\n<img width=\"576\" height=\"720\" alt=\"Screenshot 2025-07-21 at 5 39 15 PM\" src=\"https://github.com/user-attachments/assets/2a317692-eae8-44fa-a5bb-883cdbd99c55\" />\r\n\r\n\r\nI don't yet know the right solution but I think it's ok to omit the dom exports from typedoc for now and just document them in the MD files.",
        "👍 You can generate them just to make sure it passes but no need to include in this PR.  I can pull the branch and generate them locally to review them.  That will save us from needing to revert them before merging."
      ]
    },
    "profile": {
      "location": "Delaware (but Philly at heart)",
      "company": "Staff Developer - @remix-run @Shopify",
      "blog": "http://www.brophy.org",
      "twitter_username": "brophdawg11",
      "site_admin": false,
      "followers": 765,
      "following": 6
    }
  },
  "chris-olszewski": {
    "repos": [
      "vercel/turborepo"
    ],
    "entries": [
      {
        "slug": "turborepo-boundary-case-handling",
        "title": "Boundary case handling"
      },
      {
        "slug": "turborepo-choose-logging-levels-wisely",
        "title": "Choose logging levels wisely"
      },
      {
        "slug": "turborepo-configuration-precision-matters",
        "title": "Configuration precision matters"
      },
      {
        "slug": "turborepo-consider-config-generation-methods",
        "title": "Consider config generation methods"
      },
      {
        "slug": "turborepo-define-api-boundaries",
        "title": "Define API boundaries"
      },
      {
        "slug": "turborepo-descriptive-unambiguous-identifiers",
        "title": "Descriptive, unambiguous identifiers"
      },
      {
        "slug": "turborepo-design-ergonomic-apis",
        "title": "Design ergonomic APIs"
      },
      {
        "slug": "turborepo-document-cache-strategies",
        "title": "Document cache strategies"
      },
      {
        "slug": "turborepo-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "turborepo-graceful-error-recovery",
        "title": "Graceful error recovery"
      },
      {
        "slug": "turborepo-handle-errors-appropriately",
        "title": "Handle errors appropriately"
      },
      {
        "slug": "turborepo-keep-build-tooling-updated",
        "title": "Keep build tooling updated"
      },
      {
        "slug": "turborepo-know-your-implicit-configurations",
        "title": "Know your implicit configurations"
      },
      {
        "slug": "turborepo-minimize-lock-duration",
        "title": "Minimize lock duration"
      },
      {
        "slug": "turborepo-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "turborepo-semantic-naming-conventions",
        "title": "Semantic naming conventions"
      },
      {
        "slug": "turborepo-use-functional-null-handling",
        "title": "Use functional null handling"
      },
      {
        "slug": "turborepo-use-workspace-dependencies-consistently",
        "title": "Use workspace dependencies consistently"
      },
      {
        "slug": "turborepo-validate-configurations-comprehensively",
        "title": "Validate configurations comprehensively"
      },
      {
        "slug": "turborepo-validate-performance-impact-first",
        "title": "Validate performance impact first"
      }
    ],
    "comments": {
      "turborepo-configuration-precision-matters": [
        "For whatever reason, double `,` with trailing commas really creates a wild output that isn't useful when snapshot. The error could be better, but ITG as the first error still highlights the double `,,`\r\n```\r\nturbo_json_parse_error                                                                                                                                                        \r\n                                                                                                                                                                              \r\n  × Failed to parse turbo.json.                                                                                                                                               \r\n  ├─▶   × expected `}` but instead found `,`                                                                                                                                  \r\n  │      ╭─[turbo.json:2:50]                                                                                                                                                  \r\n  │    1 │ {                                                                                                                                                                  \r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,                                                                                                                 \r\n  │      ·                                                  ─                                                                                                                 \r\n  │    3 │   \"ui\": \"tui\",                                                                                                                                                     \r\n  │      ╰────                                                                                                                                                                \r\n  ├─▶   × End of file expected                                                                                                                                                \r\n  │      ╭─[turbo.json:3:3]                                                                                                                                                   \r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,                                                                                                                 \r\n  │    3 │   \"ui\": \"tui\",                                                                                                                                                     \r\n  │      ·   ────                                                                                                                                                             \r\n  │    4 │   \"tasks\": {                                                                                                                                                       \r\n  │      ╰────                                                                                                                                                                \r\n  ├─▶   × End of file expected                                                                                                                                                \r\n  │      ╭─[turbo.json:3:7]\r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  │    3 │   \"ui\": \"tui\",\r\n  │      ·       ─\r\n  │    4 │   \"tasks\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:3:9]\r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  │    3 │   \"ui\": \"tui\",\r\n  │      ·         ─────\r\n  │    4 │   \"tasks\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:3:14]\r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  │    3 │   \"ui\": \"tui\",\r\n  │      ·              ─\r\n  │    4 │   \"tasks\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:4:3]\r\n  │    3 │   \"ui\": \"tui\",\r\n  │    4 │   \"tasks\": {\r\n  │      ·   ───────\r\n  │    5 │     \"build\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:4:10]\r\n  │    3 │   \"ui\": \"tui\",\r\n  │    4 │   \"tasks\": {\r\n  │      ·          ─\r\n  │    5 │     \"build\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │       ╭─[turbo.json:4:12]\r\n  │     3 │       \"ui\": \"tui\",\r\n  │     4 │ ╭─▶   \"tasks\": {\r\n  │     5 │ │       \"build\": {\r\n  │     6 │ │         \"dependsOn\": [\"^build\"],\r\n  │     7 │ │         \"inputs\": [\"$TURBO_DEFAULT$\", \".env*\"],\r\n  │     8 │ │         \"outputs\": [\".next/**\", \"!.next/cache/**\"],\r\n  │     9 │ │       },\r\n  │    10 │ │       \"lint\": {\r\n  │    11 │ │         \"dependsOn\": [\"^lint\"]\r\n  │    12 │ │       },\r\n  │    13 │ │       \"check-types\": {\r\n  │    14 │ │         \"dependsOn\": [\"^check-types\"]\r\n  │    15 │ │       },\r\n  │    16 │ │       \"dev\": {\r\n  │    17 │ │         \"cache\": false,\r\n  │    18 │ │         \"persistent\": true\r\n  │    19 │ │       }\r\n  │    20 │ ╰─▶   }\r\n  │    21 │     }\r\n  │       ╰────\r\n  ├─▶   × End of file expected\r\n  │       ╭─[turbo.json:21:1]\r\n  │    20 │   }\r\n  │    21 │ }\r\n  │       · ─\r\n  │       ╰────\r\n  ╰─▶   × turbo.json has an incorrect type, expected an object, but received an array.\r\n```",
        "Can we make this an exact version to prevent this getting bumped and behaving unexpectedly.\r\n```suggestion\r\n    \"@types/d3-scale\": \"4.0.2\"\r\n```",
        "> I can't remember if our integration test fixtures generally use packageManager or not, but that might be useful since I know * has some weird behavior in different npm versions, especially with workspaces\r\n\r\nGood callout. With package manager requirements we now force usage of corepack for all test fixture defaulting to `npm` if one isn't specified in the fixture. Will open a ticket to make this explicit"
      ],
      "turborepo-graceful-error-recovery": [
        "Up to you since you're the one driving `query`, but returning information that we know is incorrect feels off to me. Do we need to worry about panics down the line in query around assumptions that every task has a valid package and has valid task dependencies?",
        "Unsure of why the actual diff isn't showing on the [prysk diff output](https://github.com/vercel/turborepo/actions/runs/13460494959/job/37619607217#step:9:1126), but it's missing the possible values\r\n```suggestion\r\n            Specify which tasks should continue running when an error occurs. Use \"none\" to cancel all tasks. Use \"independent-tasks-only\" to continue running independent tasks and cancel dependent ones. Use \"all\" to continue running all tasks [default: none] [possible values: none, independent-tasks-only, all]\r\n```"
      ],
      "turborepo-choose-logging-levels-wisely": [
        "Maybe add in the task id here\r\n```suggestion\r\n        debug!(\"{}: files to cache: {:?}\", self.task_id, files_to_be_cached.len());\r\n```"
      ],
      "turborepo-validate-configurations-comprehensively": [
        "We throw if you use legacy cache options with `TURBO_CACHE`:\r\n```\r\n[0 olszewski@chriss-mbp] /Users/olszewski/code/vercel/turborepo $ TURBO_REMOTE_CACHE_READ_ONLY=1 turbo_dev @turbo/types#lint --cache=remote:rw > /dev/null\r\n WARNING  No locally installed `turbo` found. Using version: 2.2.4-canary.9.\r\nturbo 2.2.4-canary.9\r\n\r\n WARNING  TURBO_REMOTE_CACHE_READ_ONLY is deprecated and will be removed in a future major version. Use TURBO_CACHE=remote:r\r\n  x Cannot set `cache` config and other cache options (`force`, `remoteOnly`,\r\n  | `remoteCacheReadOnly`) at the same time\r\n```",
        "```suggestion\r\n            // there can also be a TURBO_TEAM, so we'll use that as well\r\n            output.team_slug = input.TURBO_TEAM;\r\n```",
        "So something I missed: when we add new keys/fields to `turbo.json` we should always test both deserialization and serialization. When we don't we often create issues ([most recent issue caused by me forgetting to test this](https://github.com/vercel/turbo/issues/8311))",
        "This is necessary due to older versions of `pnpm` not having https://github.com/pnpm/npm-conf/pull/10 which results in `pnpm` crashing if `APPDATA` isn't defined.\r\n\r\nTechnically we could have users define this instead of baking it into `turbo` itself, but considering the popularity of `pnpm` and the amount of digging it took to find that PR, it seems nice to include it."
      ],
      "turborepo-use-workspace-dependencies-consistently": [
        "I was matching the version used in [`turborepo-lib`](https://github.com/vercel/turborepo/pull/9995/files#diff-217f728c3ab310388e85c7ed8a02edc124d9b28c82b9cf623a41b94204848d41L61). I can update both to use workspace version in a followup PR.",
        "```suggestion\r\ngit2 = { workspace = true, default-features = false }\r\n```"
      ],
      "turborepo-keep-build-tooling-updated": [
        "We should probably switch to `topo` so if users add tests to `@repo/ui` then `web#test` and `docs#test` don't block on those finishing:\r\n![graphviz(2)](https://github.com/user-attachments/assets/cc97dbfb-92dc-433b-bd23-f39a19d8f264)\r\n"
      ],
      "turborepo-validate-performance-impact-first": [
        "I think this is okay since https://github.com/vercel/turborepo/pull/9123 has been merged.\r\n\r\n`vt100` used to need to iterate through every scrollback row in order to render a visible row so 1024->2048 would be a noticeable perf hit. I will do a quick profile to check if there's a noticeable impact from this change.",
        "Did a test and this is a :shipit: no noticeable perf change",
        "```suggestion\r\n                let mut map: HashMap<&AnchoredSystemPath, (GitHashes, Vec<_>)> = HashMap::with_capacity({\r\n                    let (lower, upper) = package_roots.size_hint();\r\n                    upper.unwrap_or(lower)\r\n                });\r\n```",
        "~~Nit~~ Learning opportunity:\r\nGenerally one should prefer `&str` to `&String` as the latter has to follow jump through memory twice:\r\n - first to dereference the `&String` to `String`\r\n - then to read the pointer in the `String` object that points to the actual bytes\r\n\r\n`&str` on the other hand only has the pointer to the actual bytes\r\n\r\nExceptions to the rule: You need to mutate the string",
        "This change means the function now returns an iterator over all task names instead of an vector of them. Beneficial to us as it results in far fewer allocations (4 vector allocations + 1 allocation per task) to zero allocations."
      ],
      "turborepo-document-cache-strategies": [
        "Are you referring to the `Remote caching enabled` line in the run prelude? If so I can confirm that it becomes `Remote caching disabled` with `--cache=local:rw`."
      ],
      "turborepo-know-your-implicit-configurations": [
        "No, we'll always parse the lockfile and fold the packages into a task hash."
      ],
      "turborepo-minimize-lock-duration": [
        "We only acquire the guard in the block so we ensure we release the lock once we take the changed packages. This lets the `event_fut` make progress while `self.execute_run` is pending.",
        "`changed_packages_guard` is still held while run is being executed, this prevents any events from being handled. ",
        "We should drop the lock before we do a serial send."
      ],
      "turborepo-descriptive-unambiguous-identifiers": [
        "I find this name a little confusing. Does `clear_pinned_task` express the behavior better?\r\n```suggestion\r\n    fn update_task_selection_pinned_state(&mut self) {\r\n        // Preferences assume a pinned state when there is an active task.\r\n        // This `None` creates \"un-pinned-ness\" on the next TUI startup.\r\n        self.preferences.set_active_task(None);\r\n    }\r\n```",
        "Minor: can we rename to `create_config` instead of `config_init` if we're making this public? `config_init` naming only makes sense when we used it in `.get_or_init`",
        "Most of these variants seem more focused on why a package is included in a filter rather than \"what has changed\".  Is `PackagePresenceReason` maybe a more accurate name for this enum?",
        "Should this be `direct_dependencies`? Gives us the nice `direct_dependencies U indirect_dependencies = dependencies` relation"
      ],
      "turborepo-use-functional-null-handling": [
        "We can get rid of the unwrap by converting these `Result<T,E>` to `Option<T>` since we're already not doing anything with the error type\r\n```suggestion\r\n    let package_manager = PackageJson::load(&base.repo_root.join_component(\"package.json\"))\r\n        .ok()\r\n        .and_then(|package_json| {\r\n            PackageManager::read_or_detect_package_manager(&package_json, &base.repo_root).ok()\r\n        })\r\n        .map_or_else(|| \"Not found\".to_owned(), |pm| pm.to_string());\r\n```",
        "Maybe keep this in case anyone is parsing summary/dry JSON and expecting the user to be present?"
      ],
      "turborepo-handle-errors-appropriately": [
        "I think we would want to abort if we are missing/can't read the root `package.json`"
      ],
      "turborepo-propagate-errors-with-context": [
        "Bubble up this error so it reaches the caller and they can decide what to do with it. In our case when this error is eventually returned in `change_detector.rs` we should catch it and behave as if every package changed since we cannot identify which exact packages changed. The warning you added should be moved there with an updated message.",
        "I do not want a lossy conversion as this will cause silent failures upstream. Please report up any non-UTF8 output as an error.",
        "Generally we should prefer methods on the `turbopath`s types since we have nicer error messages. (By default Rust io errors don't include paths in them which can lead to frustrating error messages).\r\n\r\n```suggestion\r\n        preferences_file.ensure_dir()?;\r\n```",
        "We generally want to avoid `panic` here and if we do panic, we should make sure to include the underlying error.\r\n\r\nYou can use `?` operator here to bubble up the error if you add a new variant to the cli error type [here](https://github.com/vercel/turborepo/blob/main/crates/turborepo-lib/src/cli/error.rs#L21):\r\n```\r\n#[error(transparent)]\r\nInfo(#[from] info::Error),\r\n```\r\n```suggestion\r\n            info::run(base).await?;\r\n```",
        "Lets keep around the error as it could indicate some underlying issues with the system\r\n\r\n```suggestion\r\n        |e| format!(\"Cannot determine current binary: {e}\"),\r\n```",
        "Can we keep the error messaging from `map_environment` in `env.rs`? The `unwrap`s will just report `tried to unwrap None` which isn't helpful for end users.",
        "There was an existing bug where we would ignore any errors that came from resolving config options from a source. In reality since these were (mostly) eagerly calculated it only affected and validation errors from extracting config options from a `turbo.json`.\r\n\r\nWe now bubble up any errors generated by resolving config options."
      ],
      "turborepo-boundary-case-handling": [
        "This one is totally up to you if you think explicitly handling the underflow case is easier to reason about than leveraging modulos.\r\n```suggestion\r\n            self.selected_task_index = self.selected_task_index.checked_sub(1).unwrap_or(num_rows - 1);\r\n```",
        "I'm an `Option` addict, I love how clean the control flow that methods provide is."
      ],
      "turborepo-design-ergonomic-apis": [
        "Just using `strip_prefix` from the jump lets us skip the `unwrap`\r\n```suggestion\r\n        if let Some(catalog_name) = specifier.strip_prefix(\"catalog:\") {\r\n            if let Some(catalogs) = &self.catalogs {\r\n                if let Some(catalog) = catalogs.get(catalog_name) {\r\n                    if let Some(dep) = catalog.get(name) {\r\n                        return Ok(Some(&dep.version));\r\n                    }\r\n                }\r\n            }\r\n            return Ok(None);\r\n```",
        "If you follow the above suggestion you can change this to also just take `&str`\r\n```suggestion\r\n        fn token(mut self, value: &str) -> Self {\r\n            self.output.token = Some(value.into());\r\n            self\r\n        }\r\n```",
        "Only when building the engine do we view a non-existent `turbo.json` as a non-error. I'm not sure requiring the other callers to construct the `NoTurboJSON` error type manually is worth the better ergonomics in the engine builder.",
        "I would highly suggest leveraging `#[serde(into)]` [docs](https://serde.rs/container-attrs.html#into) here instead of manually implementing this. The general pattern is create a new type like `RepoDetailsDisplay` and then create a `impl<'a> From<&'a RepositoryDetails<'a>> for RepoDetailsDisplay<'a>` which does any of the desired conversions. e.g. `(&PackageName, &AnchoredSystemPath) -> PackageDetails { name: String, path: String }`",
        "We don't need to specify an exact underlying writer type here, we can render an app regardless of what sort of stdin handles it has.\r\n\r\n```suggestion\r\nfn view<W>(app: &mut App<W>, f: &mut Frame, rows: u16, cols: u16) {\r\n```"
      ],
      "turborepo-consider-config-generation-methods": [
        "The go yaml package explicitly doesn't support controlling whitespace: https://github.com/go-yaml/yaml/issues/627\r\nHaving encoding/decoding to preserve the file was really helpful for development when diffing the original and pruned lockfile. If sacrificing that for a straightforward yaml serialization is desired we can do that. I'll quick double check that pnpm doesn't throw if parsing these formatting differences.",
        "From reading through pnpm I don't think these can collide. If a dep is marked optional then it won't appear in `dependencies`, peer deps are handled in the same manner. In the case that there's an optional peer it appears under peer with the `optional` attribute set true in `peerDependenciesMeta`"
      ],
      "turborepo-eliminate-code-duplication": [
        "Can you remove this line? This is already done by the `..Self::default()` on the following line.",
        "Not blocking, but would love to dedupe this logic with `Config::root_turbo_json_path` instead of copy-pasta. Would need to break out the behavior to a new static function.",
        "We can just borrow here instead of cloning\r\n```suggestion\r\n        let repo_root = &self.repo_root;\r\n```",
        "Up to you, but you can dedupe the spacing logic so it's only contained in `build_message_vec`:\r\n```\r\nlet build_message_vec = |footer_texts: &[&str]| -> Line {\r\n    let mut messages = Vec::new();\r\n    messages.extend_from_slice(footer_texts);\r\n```\r\n\r\n```suggestion\r\n            LayoutSections::Pane => build_message_vec(&[EXIT_INTERACTIVE_HINT]),\r\n            LayoutSections::TaskList => {\r\n                // Spaces are used to pad the footer text for aesthetics\r\n                build_message_vec(&[ENTER_INTERACTIVE_HINT, SCROLL_LOGS])\r\n            }\r\n```",
        "I don't see this used anywhere so I think it can get removed? In general I think we don't want to be changing the counts on the `ExecutionSummary` and have all modifications happen to `SummaryState`"
      ],
      "turborepo-semantic-naming-conventions": [
        "Not blocking, but I feel like `from` or `base` is less confusing name.\r\nDefer to Git wizard @NicholasLYang, if you're finding affected packages between `HEAD` and another commit, is there a standard name for what to call that other commit?"
      ],
      "turborepo-define-api-boundaries": [
        "Do we want to clarify that we do not consider unstructured terminal output as an API? For example https://github.com/vercel/turborepo/pull/10079 should not be considered a breaking change IMO. Now if it was a removal of a field from `turbo ls --output=json` it would be.",
        "> Do other CLI tools consider their stdout/stderr as semver-protected if its meant for human readability?\r\n\r\nIt depends on the output, if it's meant to be human readable, then usually no. All for omitting this since this is usually not assumed to be an API."
      ]
    },
    "profile": {
      "location": "Cleveland",
      "company": "@vercel ",
      "blog": "",
      "site_admin": false,
      "followers": 71,
      "following": 0
    }
  },
  "logseq-cldwalker": {
    "repos": [
      "logseq/logseq"
    ],
    "entries": [
      {
        "slug": "logseq-add-explanatory-documentation",
        "title": "Add explanatory documentation"
      },
      {
        "slug": "logseq-api-input-validation",
        "title": "API input validation"
      },
      {
        "slug": "logseq-classify-configuration-properties-appropriately",
        "title": "Classify configuration properties appropriately"
      },
      {
        "slug": "logseq-configuration-option-lifecycle",
        "title": "Configuration option lifecycle"
      },
      {
        "slug": "logseq-defer-expensive-operations",
        "title": "Defer expensive operations"
      },
      {
        "slug": "logseq-ensure-semantic-naming-accuracy",
        "title": "Ensure semantic naming accuracy"
      },
      {
        "slug": "logseq-extract-reusable-hooks",
        "title": "Extract reusable hooks"
      },
      {
        "slug": "logseq-fail-fast-explicitly",
        "title": "Fail fast explicitly"
      },
      {
        "slug": "logseq-filter-nil-values-defensively",
        "title": "Filter nil values defensively"
      },
      {
        "slug": "logseq-maintain-documentation-consistency",
        "title": "Maintain documentation consistency"
      },
      {
        "slug": "logseq-multi-arity-backward-compatibility",
        "title": "Multi-arity backward compatibility"
      },
      {
        "slug": "logseq-optimize-algorithm-performance",
        "title": "optimize algorithm performance"
      },
      {
        "slug": "logseq-prevent-command-injection-vulnerabilities",
        "title": "Prevent command injection vulnerabilities"
      },
      {
        "slug": "logseq-respect-existing-formatting",
        "title": "Respect existing formatting"
      },
      {
        "slug": "logseq-separate-user-system-data",
        "title": "Separate user system data"
      },
      {
        "slug": "logseq-simplify-code-readability",
        "title": "Simplify code readability"
      },
      {
        "slug": "logseq-simplify-naming-conventions",
        "title": "Simplify naming conventions"
      }
    ],
    "comments": {
      "logseq-api-input-validation": [
        "I see a couple `fill`s were changed to `type`. Is the latter more useful for stability or something else ? Asking so I know when to use which api call"
      ],
      "logseq-fail-fast-explicitly": [
        "Almost everywhere we call a worker fn we use a when-let. Is this actually necessary in most places or is it only one or two places that needed it before worker was defined and then it got copied everywhere? If it is needed in most places, would it be helpful to provide a more stable api fn that wraps the atom?",
        "Thanks for making this more consistent to use and adding explicit error handling!",
        "A comment for this check could be helpful. Unclear why we pass invalid calls through silently rather than fail explicitly",
        "Gotcha. Good to know it was useful for refactoring. Hopefully we can delete it later when there aren't any more missed deletions",
        "This isn't a change in this PR but if sqlite is missing, we should fail explicitly to the user so users are aware their data isn't being written and they could lose a lot of data silently. I can address this tomorrow if you'd like",
        "A catch here has the downsides that it gives users a misleading impression that something works when it doesn't and we don't get a sentry issue if something fails. What motivated adding a catch here?",
        "Thanks for addressing"
      ],
      "logseq-maintain-documentation-consistency": [
        "Looks like renaming files in this PR has broken documentation links. Worth searching and updating broken linnks"
      ],
      "logseq-respect-existing-formatting": [
        "> we recommend that you do so for code that you change/add\r\n\r\nWe would rather that contributions respect existing whitespace conventions of the files they modify. We'll eventually put this advice in a CONTRIBUTING.md, hopefully soon"
      ],
      "logseq-simplify-naming-conventions": [
        "I have not seen most of this section consistently adhered to and I've never seen it enforced on a PR. Personally I'd be ok with removing this section or just putting a brief mention of the header which is the only part of the commit style that is intermittently followed. Curious to hear what the rest of the team thinks"
      ],
      "logseq-ensure-semantic-naming-accuracy": [
        "I found this name confusing as I thought they were going to be unique per view but instead it's acting like another type. Some possible suggestions: ui-type, feature-type, feature",
        "Are there any other types that won't have ref default values? Maybe :datetime? If there are other non-ref values should we generalize the name and :type e.g. scalar-default-value and :any so we don't have to keep modifying all the places this is used",
        "Can we rename this to before-scripts or scripts-before? At some point someone will also want to inject scripts at the end to override logseq js",
        "Since we're fixing a bug from #9563, I tweaked the name here since that PR also enhanced the command so that it can be used outside of query tables",
        "I think you wanted `validate-namespaces` but that doesn't usually mean modification. How about `make-valid-namespaces`?"
      ],
      "logseq-add-explanatory-documentation": [
        "Could be useful to document or comment somewhere when to use this option and why. I saw it in the PR but having dev docs in PRs aren't easy to find",
        "Perhaps a brief comment summarizing why there's a condition here would be helpful as I wouldn't know if I hadn't reviewed",
        "@cnrpman @andelf Would one of you be interested in adding ns docstrings for `src/electron` in a followup PR? If so, add the path here to enable linting for it"
      ],
      "logseq-prevent-command-injection-vulnerabilities": [
        "Is there any way we could make this available to plugins? By special casing for alda here, we encourage building an alda feature in logseq, instead of as a plugin",
        "I'm concerned this list doesn't protect enough. Have we considered something like the shellquote npm lib - https://auth0.com/blog/preventing-command-injection-attacks-in-node-js-apps/#Preventing-Command-Injection?"
      ],
      "logseq-extract-reusable-hooks": [
        "@tiensonqin When I disable the two local state atoms for simple queries, it has no effect on preserving sorting state as that is handled by the block property. Do we know what local state is needed for here? The `block/*custom-query` component is used in so many contexts that I left the local atoms in order to not accidentally break something",
        "Thanks. Makes sense. Will remove"
      ],
      "logseq-optimize-algorithm-performance": [
        "For perf reasons I wish we weren't using postwalk but without a good understanding of every ast node combination, this is what we have to do",
        "`map` is lazy so this isn't being run. Would recommend `mapv` or `doseq`"
      ],
      "logseq-classify-configuration-properties-appropriately": [
        "Great to have all these property values documented. Would be helpful to add these to `gp-property/hidden-built-in-properties` so that the graph-parser can treat them as built in properties instead of user properties. Happy to add autocompletion for property values if that's useful some time",
        "Looks like these were added to `hidden-built-in-properties`. I'm guessing that's a mistake as they wouldn't be editable",
        "When using the query builder, I noticed that the property filter now displays the new built-in properties. My guess based on previous issues is that users are not going to want to see this but we can also wait and see:\r\n<img width=\"225\" alt=\"Screen Shot 2023-04-03 at 5 57 41 PM\" src=\"https://user-images.githubusercontent.com/97210743/229636887-fad93ceb-2995-4c35-93d0-9920d1493748.png\">\r\n\r\nThis happens because these properties haven't been added to `gp-property/hidden-built-in-properties`",
        "Agree this shouldn't block the PR. I'm ok either way on addressing this later"
      ],
      "logseq-defer-expensive-operations": [
        "Ah yep. Fixed",
        "Given how walks aren't that performant, we should remove extra ones like this if we don't need em. I'll do this"
      ],
      "logseq-multi-arity-backward-compatibility": [
        "`eval-string` is used in other places besides `<src` so this would be adding this configuration in other features that would be confusing. Let's only pass them as args from `eval-result` for now",
        "My concern wasn't arguments being passed but that other features have additional configuration that are specific to the src feature. I've addressed this",
        "Optional: It could be nice to keep this fn simpler and have it just take an entity id (eid) like db/pull or https://cljdoc.org/d/datascript/datascript/1.4.2/api/datascript.core#entity. This would mean passing in the `db/entity` arg",
        "The logseq.api also uses `resolve-input`. Since `current-block-uuid` is nil for it, these next two `cond` conditions would fail and possibly confuse plugin authors. Could you update them to also check for `current-block-uuid`?"
      ],
      "logseq-filter-nil-values-defensively": [
        "Some food for thought on \"safe\" fns (a pattern that I've seen rarely outside of this codebase). Two downsides I see:\r\n- It forces all callers to handle a nil state where they previously may not have had to. This additional complexity is one reason we have `(remove nil?)` throughout our codebase\r\n- It propagates and creates secondary error states by returning nils instead of letting the error happen\r\n   For example, see the two calls of `(-> (state/get-block-id dom) uuid ...` in [the editor handler](https://github.com/logseq/logseq/pull/8816/files?diff=unified&w=1#diff-2f055ca9c790c38c4cbe601146ca7c19ad359f36a73554039581c2e5fae428d6). If el was `nil` we'd get the original error of `Cannot read properties of null (reading 'getAttribute')` but because we forgot to handle nil here we get `Assert failed: (string? s)`.  We've made it harder to determine the root cause of failure by propagating a nil. Thankfully this one is close to the original but there's no guarantee other secondary errors will be as easy to diagnose. We can of course fix these with `some->` but it's an ongoing issue we've forced on ourselves as described in the previous point\r\n                     \r\nIf wanting to stick with safe usage would recommend renaming to `get-safe-block-id` so callers know they should handle nils accordingly"
      ],
      "logseq-separate-user-system-data": [
        "While I'm glad this PR allows for workflows that weren't possible before e.g. classes as properties, I don't think we should be encouraging it as common practice. Doing so muddles two important concepts in the DB version and encourages poor modeling practices i.e. one identity that behaves both as a class and a property. For most users who won't use this feature, it also makes an irreversible mistake easier i.e. turning a class into a property with no ability to undo. I also noticed for large class graphs like schema.org graphs, the property dropdown is laggy because it fetches all classes.\r\n\r\nIf we still want this classes in properties behavior, I could put it behind a config option if there's no objection",
        "Using this in the builder autocompletes to meaningless strings for users. This feels buggy\r\n\r\nI haven't looked at the rest of these attributes but there is probably a lot more to be said about them...",
        "> I haven't looked at the rest of these attributes but there is probably a lot more to be said about them...\r\n\r\nI looked at the rest of these and disabled more that are implementation details that could change. I would've disabled :block/type as it's still a work in progress e.g. :block/type \"journal\" nodes tagged with `#Journal`  but left it alone since you wanted to make it queryable with https://github.com/logseq/logseq/commit/94773db6f3e17a3e4f0ca42d8e9dcf0193d9c7f5",
        "I don't think ignoring whiteboard properties by feature is a sustainable approach. Currently I'm seeing unwanted results in a query for the `type` property, see below. Even if we could find every feature that uses `:block/properties`, fix it and ensure future features filter whiteboard properties, this would break user queries if they use a whiteboard property. From lambda, I'm seeing the following whiteboard properties that could potentially break user queries (except :collapsed) :\r\n\r\n```\r\n$ bb '(->> (fs/glob \"whiteboards\" \"*.edn\") (map #(-> % str slurp edn/read-string)) (mapcat #(-> % :blocks first :block/properties keys)) distinct sort)'\r\n(:blockType :collapsed :collapsedHeight :compact :fill :id :index :isAutoResizing :ls-type :noFill :nonce :opacity :pageId :parentId :point :scale :scaleLevel :size :stroke :strokeType :strokeWidth :type)\r\n```\r\n\r\nI see a couple solutions to the problem:\r\n1. If the all the whiteboard properties are known, put them under a namespace like we did with [macros](https://github.com/logseq/logseq/pull/6105#discussion_r934914562). Then add those whiteboard properties in `gp-property/hidden-built-in-properties`\r\n2. Introduce a new block attribute just for this purpose for whiteboards e.g. `:block/whiteboard-properties`\r\n3. Introduce a new block attribute that can be used by whiteboards and third parties that have the same need to store feature-generated properties. Perhaps `:block/{non-user-properties,feature-properties,data-properties}`. Over time we could migrate some of our built-in properties to use this block attribute instead\r\n\r\nI'd suggest solution 3 or 1 depending on if all the whiteboard properties are known, as they both seem like longer term solutions.\r\n@tiensonqin I'd be interested to hear what you think of this and what solutions you see and prefer\r\n\r\n<img width=\"612\" alt=\"Screen Shot 2022-09-24 at 2 27 03 AM\" src=\"https://user-images.githubusercontent.com/97210743/192083425-6502f48c-f864-4ad7-8197-e61532050bf8.png\">\r\n",
        "Also prefer solution 1 as it made it easy to hide it from autocompletion",
        "No longer seeing query issue for new whiteboards 😄 "
      ],
      "logseq-configuration-option-lifecycle": [
        "Would be good to remove these additions of default config values. We have a tech debt item to move all these out to `frontend.state`. Also, as @andelf pointed out, hardcoding default values would make global config unusable",
        "I can address in the followup commit. Breaking the basic usability of a feature is not as important as a small workflow enhancement",
        "If we only set default-config the user loses all of their graph config which would cause annoying, silent bugs e.g. certain properties aren't rendered correctly or their journals start saving in a different format",
        "Unfortunately we don't have access to both valid configs here. Since global config doesn't have validation, it's probably the global config being invalid that causes the failure. We could add the current graph config as a temporal fix but it doesn't guarantee that there still isn't an issue and leads to another issue - a poor user notification experience. These catch's are occurring in a low level fn that can be called a number of times during render, which leads to notifying the user a number of times, at a time potentially much later than when they edited the invalid config. If we don't want illegal global config we should just validate and tell the user when they have entered it"
      ],
      "logseq-simplify-code-readability": [
        "Let's simplify this fn as it only needs 2 args, the filtered-count and total-count, as we can pass nil for filtered-count if filters? isn't present in the calling fn. Since most translators aren't coders would be good to show examples in comments of different strings one can see",
        "```suggestion\r\n                                       (if (= total 1) \" Linked Reference\" \" Linked References\")\r\n```\r\nLet's simplify logic to `if` and `str` for the non-coders where possible"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 48,
      "following": 0
    }
  },
  "benluelo": {
    "repos": [
      "unionlabs/union"
    ],
    "entries": [
      {
        "slug": "union-avoid-error-display-duplication",
        "title": "avoid error display duplication"
      },
      {
        "slug": "union-avoid-hardcoded-configuration",
        "title": "avoid hardcoded configuration"
      },
      {
        "slug": "union-avoid-unnecessary-memory-operations",
        "title": "avoid unnecessary memory operations"
      },
      {
        "slug": "union-choose-appropriate-async-primitives",
        "title": "Choose appropriate async primitives"
      },
      {
        "slug": "union-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "union-clean-documentation-links",
        "title": "Clean documentation links"
      },
      {
        "slug": "union-configuration-file-completeness",
        "title": "Configuration file completeness"
      },
      {
        "slug": "union-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "union-environment-specific-configuration-values",
        "title": "Environment-specific configuration values"
      },
      {
        "slug": "union-maintain-naming-consistency",
        "title": "Maintain naming consistency"
      },
      {
        "slug": "union-prioritize-code-clarity",
        "title": "prioritize code clarity"
      },
      {
        "slug": "union-reuse-existing-api-utilities",
        "title": "Reuse existing API utilities"
      },
      {
        "slug": "union-sql-query-best-practices",
        "title": "SQL query best practices"
      },
      {
        "slug": "union-use-descriptive-semantic-names",
        "title": "Use descriptive semantic names"
      },
      {
        "slug": "union-use-structured-logging-fields",
        "title": "Use structured logging fields"
      },
      {
        "slug": "union-validate-before-type-conversions",
        "title": "Validate before type conversions"
      },
      {
        "slug": "union-validate-network-addresses",
        "title": "Validate network addresses"
      },
      {
        "slug": "union-validate-production-configurations",
        "title": "Validate production configurations"
      },
      {
        "slug": "union-version-serializable-structures",
        "title": "Version serializable structures"
      },
      {
        "slug": "union-workspace-dependency-consistency",
        "title": "workspace dependency consistency"
      }
    ],
    "comments": {
      "union-choose-appropriate-async-primitives": [
        "this can be https://docs.rs/tokio-util/latest/tokio_util/sync/struct.CancellationToken.html#method.run_until_cancelled",
        "unless you hold this lock across an await point, you should use the std locks or the ones from [`parking_lot`](https://docs.rs/parking_lot/latest/parking_lot/)"
      ],
      "union-ensure-comprehensive-test-coverage": [
        "this needs to be tested to ensure it works correctly",
        "this should technically fail, since the vector length is incorrect, so:\r\n\r\n- add a test that ensures the sync committee bits vector length is checked correctly\r\n- use https://doc.rust-lang.org/stable/std/vec/struct.Vec.html#method.clear here instead"
      ],
      "union-clean-documentation-links": [
        "in general, prefer to put only the type name in the link name, and use the full path in the link. it makes the generated docs a lot less noisy\r\n\r\n```suggestion\r\n/// The returned [`Op`] ***MUST*** resolve to an [`OrderedHeaders`][crate::data::OrderedHeaders] data.\r\n```",
        "```suggestion\r\n/// A subset of [`FullEvent`][ibc_union_spec::event::FullEvent], containing only events that cause an action on the counterparty chain.\r\n```"
      ],
      "union-environment-specific-configuration-values": [
        "note that we need to add a branch for testnet vs mainnet here before we upgrade mainnet",
        "this allows us to dump the depinject graph on startup"
      ],
      "union-avoid-error-display-duplication": [
        "don't use both from and display otherwise the error will be double printed",
        "don't print an error in the display if it's also a source, otherwise you'll get duplicated messages when printing the full trace",
        "you either need to set the source or print it in display, but not both. i have [this type](https://github.com/unionlabs/union/blob/752d7221d5725012a215e6795f5d89957212d257/lib/unionlabs/src/lib.rs#L243) that can be used to print the entire error trace in display.",
        "minor nit, non blocking: prefer to use `#[source]` instead of displaying the error (`sqlx::Error` in this case) as you will lose all sources other than the top level error"
      ],
      "union-configuration-file-completeness": [
        "heights shouldn't update, we need to fix the script I guess?"
      ],
      "union-reuse-existing-api-utilities": [
        "we have this in serde_utils already if you want to use that crate",
        "https://github.com/unionlabs/union/blob/3fc7119b562776d14f1718705197758259d7a31e/lib/serde-utils/src/lib.rs#L626",
        "you can't use the rpc attr in cosmwasm contracts",
        "use `MsgTransfer::type_url()`, provided by this trait: https://docs.rs/prost/latest/prost/trait.Name.html",
        "we should probably also add this type to unionlabs, but don't worry about that for this PR"
      ],
      "union-avoid-hardcoded-configuration": [
        "please confirm this is correct @aeryz, it was hardcoded to 18 previously",
        "i know we have union hardcoded right now, but just leaving a note that this can be fetched from the voyager client (`voyager_client.client_info()`)"
      ],
      "union-validate-production-configurations": [
        "why do we check against the configured gov token, rather than just use the configured value?",
        "default = null (default false)",
        "required BC the default is intended for local devnet (minimal config) scenarios, and is not viable for production. just removing a potential footgun by making it not optional",
        "its not keccak(wasm) right? just wasm?",
        "we're sure we can safely update nixpkgs-unstable?"
      ],
      "union-avoid-unnecessary-memory-operations": [
        "what is this used for?",
        "why not just store the original state? is the `initial_sync_committee` large?",
        "oof yeah lets zero that out indeed lol",
        "maybe leave a comment as to why we remove it"
      ],
      "union-sql-query-best-practices": [
        "general tip - you can use the `query!` macro to have compile time verification of your queryies: https://docs.rs/sqlx/latest/sqlx/macro.query.html"
      ],
      "union-version-serializable-structures": [
        "version this (`enum ClientState { V1(ClientStateV1) }`)"
      ],
      "union-validate-before-type-conversions": [
        "don't arbitrarily cast to an integer width. you need to consider all possible values this may take. additionally, since we control this error type, we don't need to cast to a specific integer width - we can just change the field type (as i mentioned in another comment)."
      ],
      "union-prioritize-code-clarity": [
        "we should export these as consts from the lib crate",
        "nit: either move the E bound to the params list or all bounds to the where clause",
        "should probably extract this to a constant",
        "small nit: a helper fn to extract the u32 at an index would drastically help readability",
        "instead of .and_then with Some(), just use .map",
        "clippy will probably tell you to use `.is_ok()` here",
        "writing to an atomic bool in an if guard is interesting, i would recommend pulling this out into the arm expression for clarity/ least surprise"
      ],
      "union-use-structured-logging-fields": [
        "nit: in general, tracing logs should be single-line and not contain newlines. i would recommend putting the stdout and stderr as log fields (i.e. something like `debug!(%output.stdout, %output.stderr, \"abi build log\", log);`)",
        "in general, prefer to use tracing fields as opposed to formatting strings: https://docs.rs/tracing/latest/tracing/index.html#recording-fields\r\n\r\nthis function is a good example: https://github.com/unionlabs/union/blob/main/lib/relay-message/src/event.rs#L59",
        "general comment about all logs: prefer to keep all logs lowercased for consistency",
        "you should also include chain id here as a field, and make latest_block a field as well",
        "include the balance and requested amount as fields in the log"
      ],
      "union-maintain-naming-consistency": [
        "The channel version of ucs01 relay is `ucs01-relay-1`, same as `ics20-1` is the version for ICS20 channels"
      ],
      "union-validate-network-addresses": [
        "the reason we did this in the solidity contract is that we re-encode the port (contract address) as hex when indexing into the capabilities table, and if you create the channel with a checksummed address as the port id then it doesn't match and you can't actually send any packets on the channel. i'm not sure how this will work in move, but do keep it in mind"
      ],
      "union-use-descriptive-semantic-names": [
        "deployer should be an address, salt should be b256, and this should return an address\r\n\r\nhttps://docs.rs/alloy-primitives/latest/alloy_primitives/struct.Address.html\r\nhttps://docs.rs/alloy-primitives/latest/alloy_primitives/aliases/type.B256.html",
        "new* typically means constructor - if you want this method, it should be `into_beacon_light_client_update`",
        "`race_client` instead of `raceclient`",
        "you may want to use https://docs.rs/dashmap/latest/dashmap/",
        "also, what are these maps to/from? `SharedMap` could probably use a better name and documentation in general would be good",
        "very minor nit as well, but prefer to follow parent > child ordering, put the top-most type first, then the types contained in that, etc\r\n\r\n```rs\r\npub type SharedMap = Arc<Mutex<HashMap<String, InnerMap>>>;\r\ntype InnerMap = HashMap<i32, InnerInnerMap>;\r\ntype InnerInnerMap = HashMap<i32, (bool, Option<chrono::DateTime<chrono::Utc>>)>;\r\n```",
        "also, if these type aliases aren't used anywhere else, its fine to inline them here"
      ],
      "union-workspace-dependency-consistency": [
        "default-features = false here doesn't do anything, and i think cargo may complain about it",
        "It should be workspace but I forgot, will address in a followup PR",
        "ended up fixing this in this PR to trigger garnix"
      ],
      "union-choose-efficient-data-structures": [
        "perhaps I'm missing some context but why is this impossible?",
        "ah ok i missed that line. maybe good to do the comparison as the checked_sub call then? then we can match on the result, instead of doing the check twice?",
        "this needs to be non-zero (i.e. starting at 1) such that it can be reliably differentiated from an empty storage value",
        "a commit is a fixed 20 byte hash",
        "we should use a newtype or a map structure (btreemap), some way to enforce no duplicate keys",
        "I highly recommend peg for this usecase, it makes it very trivial to define recursive grammars: https://docs.rs/peg/latest/peg/",
        "this is invalid, shouldn't you do [`i.count_ones()`](https://doc.rust-lang.org/stable/std/primitive.u8.html#method.count_ones)?"
      ]
    },
    "profile": {
      "location": "Canada",
      "company": "@unionlabs",
      "blog": "",
      "twitter_username": "0xbonlulu",
      "site_admin": false,
      "followers": 488,
      "following": 37
    }
  },
  "sapphi-red": {
    "repos": [
      "vitejs/vite"
    ],
    "entries": [
      {
        "slug": "vite-break-down-complex-functions",
        "title": "Break down complex functions"
      },
      {
        "slug": "vite-clean-configuration-organization",
        "title": "Clean configuration organization"
      },
      {
        "slug": "vite-clean-network-resources",
        "title": "Clean network resources"
      },
      {
        "slug": "vite-complete-deployment-commands",
        "title": "Complete deployment commands"
      },
      {
        "slug": "vite-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "vite-document-code-purposefully",
        "title": "Document code purposefully"
      },
      {
        "slug": "vite-document-protocol-configurations-clearly",
        "title": "Document protocol configurations clearly"
      },
      {
        "slug": "vite-ensure-documentation-accuracy",
        "title": "Ensure documentation accuracy"
      },
      {
        "slug": "vite-environment-variable-management",
        "title": "Environment variable management"
      },
      {
        "slug": "vite-escape-html-content-properly",
        "title": "Escape HTML content properly"
      },
      {
        "slug": "vite-evolve-apis-with-compatibility",
        "title": "Evolve APIs with compatibility"
      },
      {
        "slug": "vite-explicit-version-requirements",
        "title": "Explicit version requirements"
      },
      {
        "slug": "vite-manage-configuration-inheritance-carefully",
        "title": "Manage configuration inheritance carefully"
      },
      {
        "slug": "vite-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "vite-optimize-glob-operations",
        "title": "Optimize glob operations"
      },
      {
        "slug": "vite-permission-hierarchy-awareness",
        "title": "Permission hierarchy awareness"
      },
      {
        "slug": "vite-precise-documentation-language",
        "title": "Precise documentation language"
      },
      {
        "slug": "vite-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "vite-react-transformation-tool-clarity",
        "title": "React transformation tool clarity"
      },
      {
        "slug": "vite-restrict-server-access",
        "title": "Restrict server access"
      },
      {
        "slug": "vite-runtime-agnostic-api-design",
        "title": "Runtime-agnostic API design"
      },
      {
        "slug": "vite-secure-workflow-permissions",
        "title": "Secure workflow permissions"
      },
      {
        "slug": "vite-separate-configuration-responsibilities",
        "title": "Separate configuration responsibilities"
      },
      {
        "slug": "vite-vue-component-import-handling",
        "title": "Vue component import handling"
      }
    ],
    "comments": {
      "vite-ensure-documentation-accuracy": [
        "```suggestion\r\n- Package manager lockfile content, e.g. `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml` or `bun.lock`\r\n```\r\nSince this list is just an example, I think it's fine to remove `bun.lockb` because `bun.lock` has been added instead.\r\n",
        "Having both feels too verbose. Since `bun.lock` is the new default for bun, I think `bun.lock` should be listed instead of `bun.lockb`.",
        "I believe this \"can also be\" was confusing as the other possibility (`true`) doesn't actually exist now.",
        "Since `root` is deprecated and does not need to be set anymore, I think we can remove `root` from all the examples and types in the docs."
      ],
      "vite-evolve-apis-with-compatibility": [
        "I think we can deprecate these two methods as well. When migrated from `ssrLoadModule` to `ModuleRunner`, these APIs won't need to be called anymore as the `ModuleRunner` will interpret sourcemaps and rewrite the error stacks."
      ],
      "vite-document-protocol-configurations-clearly": [
        "> Is there more information of what are the sufficient values for TLS?\r\n\r\nThe only thing I found on the Node's document is this line.\r\n\r\n> [For servers, the identity options (`pfx` or `key`/`cert`) are usually required.](https://nodejs.org/api/http2.html#http2createserveroptions-onrequesthandler:~:text=For%20servers%2C%20the%20identity%20options%20(pfx%20or%20key/cert)%20are%20usually%20required.)\r\n\r\n> Does this also mean that it's possible to only enable TLS without HTTP/2 if the values are not sufficiently provided? (regardless of proxy)\r\n\r\nNo, Vite always tries to start a HTTP/2 server with TLS enabled if `server.https` is an object if `server.proxy` is not passed. If the options are not sufficiently provided, Node.js starts the server (probably) without any cert and reject any connections.\r\n",
        "Makes sense. I'll change to the previous one."
      ],
      "vite-manage-configuration-inheritance-carefully": [
        "Probably needs a helper function that merges recursively but replaces arrays, slightly different from `mergeConfig`.",
        "Added `mergeWithDefaults` function 👍 ",
        "There's many inferred options. Is it fine to simply omit it?",
        "I'm fine with trimming down the values for the exposed object. I think the object form makes it easier for users to find out the value for each options. But given that we already need to expose `mainFields` and `conditions` separately, maybe that advantage is gone.",
        "I pushed a commit that removes the new exports so that we can separate the discussion about exposing the default values. 👍 "
      ],
      "vite-react-transformation-tool-clarity": [
        "```suggestion\r\n### `@vitejs/plugin-react-oxc`\r\n\r\nWhen using `@vitejs/plugin-react` or `@vitejs/plugin-react-swc`, you can switch to the `@vitejs/plugin-react-oxc` plugin, which uses Oxc for JSX/TSX transformation instead of esbuild. It is designed to be a drop-in replacement, providing better build performance and aligning with the underlying architecture of `rolldown-vite`.\r\n\r\nBe aware that you can only switch to `@vitejs/plugin-react-oxc` if you are not using any Babel or SWC plugins (including the React compiler), or mutate the SWC options.\r\n```",
        "> which uses Oxc for JSX/TSX transformation instead of esbuild.\r\n\r\nThis isn't correct. These two plugins use transformers as shown in the table below. The difference is what handles the react-refresh transformation.\r\n\r\n- plugin-react (without plugins)\r\n  - dev (rollup-vite): uses babel for react-refresh transform + uses esbuild for JSX/TSX transform\r\n  - dev (rolldown-vite): uses babel for react-refresh transform + uses Oxc for JSX/TSX transform\r\n  - build (rollup-vite): uses esbuild for JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform\r\n- plugin-react-swc (without plugins)\r\n  - dev (rollup-vite): uses SWC for react-refresh and JSX/TSX transform\r\n  - dev (rolldown-vite): uses SWC for react-refresh and JSX/TSX transform\r\n  - build (rollup-vite): uses esbuild for JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform\r\n- plugin-react-oxc\r\n  - dev (rolldown-vite): uses Oxc for react-refresh and JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform"
      ],
      "vite-restrict-server-access": [
        "Added a similar warning to `server.cors` for `server.allowedHosts` as well. Technically, it is safe to set `server.allowedHosts: true` if the dev server runs behind a reverse proxy (the reverse proxy needs to check the host in that case though). But I didn't mention it here as I guess that usage isn't common and setting `allowedHosts` doesn't hurt."
      ],
      "vite-clean-configuration-organization": [
        "I think this should be in the \"Linting\" section rather than \"Bundler mode\" section as it's not required for Vite. Maybe also good to add a comment that it can be disabled if desired.",
        "Ah, didn't know that. I'll remove this."
      ],
      "vite-clean-network-resources": [
        "I guess we should call `socket.close` and `socket.removeEventListener` to avoid memory leak.",
        "I confirmed that `Promise.allSettled` is also supported by our default modern browser target.\r\nhttps://caniuse.com/mdn-javascript_builtins_promise_allsettled"
      ],
      "vite-optimize-glob-operations": [
        "We need the returned `files` to be absolute. I thought we can utilize the `cwd` option like:\r\n```ts\r\nconst files = globSync(globPattern, {\r\n  cwd: path.resolve(path.dirname(id), dir),\r\n  absolute: true,\r\n  expandDirectories: false,\r\n  ignore: ['**/node_modules/**'],\r\n})\r\n```\r\nI think this is what @SuperchupuDev suggested.\r\n",
        "Given that the problem is that `toAbsoluteGlob` escaping the characters unnecessary, how about not using that function?\r\nChecking the `toAbsoluteGlob` function, most of the code is handling about globs and escaping the paths. I think it would be trimmed down to:\r\n```js\r\n// replace `normalizeGlobPattern` + `toAbsoluteGlob` with\r\nconst dir = importer ? dirname(importer) : root\r\nconst normalized = rawPattern[0] === '/' ? posix.join(root, rawPattern.slice(1)) : posix.join(dir, rawPattern)\r\n\r\n// pass it to `newRawPattern`\r\nlet newRawPattern = posix.relative(posix.dirname(importer), normalized)\r\n```\r\n"
      ],
      "vite-separate-configuration-responsibilities": [
        "```suggestion\r\n      \"fileMatch\": [\"packages\\/create-vite\\/src\\/index\\\\.ts$\"],\r\n```",
        "Updated 👍 "
      ],
      "vite-precise-documentation-language": [
        "I replaced \"files generated by create-vite\" with \"files generated from those files\" so that we don't say anything about files generated by the redirected CLIs (e.g. create-vue, create svelte)."
      ],
      "vite-complete-deployment-commands": [
        "```suggestion\r\n   - **Build Command**: `npm install && npm run build`\r\n```",
        "Note for others: `npm install` is required\r\nhttps://docs.render.com/deploy-sveltekit#manual-deploy\r\n"
      ],
      "vite-explicit-version-requirements": [
        "`require(ESM)` is only supported (without a flag) for Node 20.19.0+, 22.12.0.\r\nhttps://nodejs.org/docs/latest-v20.x/api/modules.html#loading-ecmascript-modules-using-require\r\nDo we want to bump the version to that version? Or would it be fine to say \"if you want to use CJS, update the Node version\"?\r\n",
        "Sounds good 👍 ",
        "`packages/vite/src/node/__tests__/package.json` was picked up for `packages/vite/src/node/__tests__/build.spec.ts` by eslint-plugin-n and because that package.json does not have `engines` field, `>=16.0.0` was used and an lint error happened for `util.stripVTCharacters`.\r\nhttps://github.com/eslint-community/eslint-plugin-n?tab=readme-ov-file#configured-nodejs-version-range\r\nI set this to make `^18.0.0 || ^20.0.0 || >=22.0.0` to be applied across the repo.\r\n"
      ],
      "vite-runtime-agnostic-api-design": [
        "I've pushed a commit that replaces them with `CustomDevEnvironment` and `.runEntrypoint` so that it doesn't look like it can satisfy the `FetchableDevEnvironment` interface."
      ],
      "vite-environment-variable-management": [
        "```suggestion\r\nThe directory from which `.env` files are loaded. Can be an absolute path, or a path relative to the project root. `false` will disable the `.env` file loading.\r\n\r\nSee [here](/guide/env-and-mode#env-files) for more about environment files.\r\n```\r\nI think it doesn't have to be a warning.\r\n",
        "The current change would make this example broken (`env.APP_ENV` will be `undefined`).\r\n\r\n```suggestion\r\n  // Set the third parameter to 'APP_' to load envs with the `APP_` prefix.\r\n  // If necessary, you can set the optional third parameter to '' to load all env regardless of the `VITE_` prefix.\r\n  const env = loadEnv(mode, process.cwd(), 'APP_')\r\n```\r\n",
        "```suggestion\r\nThe variables declared in an env file for a specific mode (e.g. `.env.production`) will take higher priority than the ones in a generic one (e.g. `.env`).\r\n```\r\nThe fact that `.env` is always loaded is written above in the code block. Does changing the sentence to clarify that the priority is talking about the variables rather than the files make things clear?\r\n",
        "> When running with a specific mode, Vite will always load `.env` and `.env.local` in addition to the mode-specific `.env.[mode]` file. Variables declared in mode-specific files will take precedence over those in generic files, but variables defined only in .env or .env.local will still be available in the environment.\r\n\r\nLooks good to me. I think we can remove the `When running with a specific mode, ` as Vite always have a mode set (it's `development` in dev and `production` in build by default).\r\n"
      ],
      "vite-permission-hierarchy-awareness": [
        "I guess this diff is not needed. `permissions.triage` is always true for the ones that has `permissions.write`/`permissions.admin`."
      ],
      "vite-document-code-purposefully": [
        "```suggestion\r\n  /** @internal */\r\n  hostname: Hostname\r\n```\r\nI'd like to mark this as a internal type.",
        "```suggestion\r\n              // only limit to these extensions because:\r\n              // - for the `@import`/`@use`s written in file loaded by `load` function,\r\n              //   the `canonicalize` function of that `importer` is called first\r\n              // - the `load` function of an importer is only called for the importer\r\n              //   that returned a non-null result from its `canonicalize` function\r\n              (resolved.endsWith('.css') ||\r\n                resolved.endsWith('.scss') ||\r\n                resolved.endsWith('.sass'))\r\n```\r\nLet's add a comment that explains why we need to limit the extension here."
      ],
      "vite-break-down-complex-functions": [
        "I think splitting the function into 3 like this will make the function more easier to understand\r\n```ts\r\nconst toStyleSheetLinkTag = (\r\n  file: string,\r\n  toOutputPath: (filename: string) => string,\r\n): HtmlTagDescriptor => ({\r\n  tag: 'link',\r\n  attrs: {\r\n    rel: 'stylesheet',\r\n    crossorigin: true,\r\n    href: toOutputPath(file),\r\n  },\r\n})\r\n\r\nconst getCssFilesForChunk = (\r\n  chunk: OutputChunk,\r\n  seenChunks: Set<string> = new Set(),\r\n  seenCss: Set<string> = new Set(),\r\n): string[] => {\r\n  if (seenChunks.has(chunk.fileName)) {\r\n    return []\r\n  }\r\n  seenChunks.add(chunk.fileName)\r\n\r\n  if (analyzedChunk.has(chunk)) {\r\n    return analyzedChunk.get(chunk)!\r\n  }\r\n\r\n  const files: string[] = []\r\n  chunk.imports.forEach((file) => {\r\n    const importee = bundle[file]\r\n    if (importee?.type === 'chunk') {\r\n      files.push(...getCssFilesForChunk(importee, seenChunks, seenCss))\r\n    }\r\n  })\r\n  analyzedChunk.set(chunk, files)\r\n\r\n  chunk.viteMetadata!.importedCss.forEach((file) => {\r\n    if (!seenCss.has(file)) {\r\n      seenCss.add(file)\r\n      files.push(file)\r\n    }\r\n  })\r\n\r\n  return files\r\n}\r\n\r\nconst getCssTagsForChunk = (\r\n  chunk: OutputChunk,\r\n  toOutputPath: (filename: string) => string,\r\n) =>\r\n  getCssFilesForChunk(chunk).map((file) =>\r\n    toStyleSheetLinkTag(file, toOutputPath),\r\n  )\r\n```\r\nI think `analyzedChunk` should be renamed to `analyzedImportedCssFiles` in this case.",
        "Ah, yeah, `getCssFilesForChunk` should be\r\n```diff\r\nconst getCssFilesForChunk = (\r\n  chunk: OutputChunk,\r\n  seenChunks: Set<string> = new Set(),\r\n  seenCss: Set<string> = new Set(),\r\n): string[] => {\r\n  if (seenChunks.has(chunk.fileName)) {\r\n    return []\r\n  }\r\n  seenChunks.add(chunk.fileName)\r\n\r\n  if (analyzedChunk.has(chunk)) {\r\n+    const files = analyzedChunk.get(chunk)!\r\n+    const additionals = files.filter(file => !seenCss.has(file))\r\n+    additionals.forEach(file => seenCss.add(file))\r\n+    return additionals\r\n-    return analyzedChunk.get(chunk)!\r\n  }\r\n\r\n  const files: string[] = []\r\n  chunk.imports.forEach((file) => {\r\n    const importee = bundle[file]\r\n    if (importee?.type === 'chunk') {\r\n      files.push(...getCssFilesForChunk(importee, seenChunks, seenCss))\r\n    }\r\n  })\r\n  analyzedChunk.set(chunk, files)\r\n\r\n  chunk.viteMetadata!.importedCss.forEach((file) => {\r\n    if (!seenCss.has(file)) {\r\n      seenCss.add(file)\r\n      files.push(file)\r\n    }\r\n  })\r\n\r\n  return files\r\n}\r\n```"
      ],
      "vite-minimize-memory-allocations": [
        "```suggestion\r\n                      size: Buffer.byteLength(chunk.code),\r\n```\r\n`Buffer.byteLength` should be performant than `Buffer.from().length` as it doesn't require the whole converted value to be held in the memory.\r\n",
        "Good catch! Instead of doing that, I made the `deepClone` to be call only once before merging with defaults (ffb92fdf743026c98edd8830e0868878db36f060, 2d6bc9cc73f15e4b7e551af649bc69c3265e4e0d). This way, the properties of defaults that would be overridden with the values will be cloned unnecessarily compared to your suggestion, but I think that's negligible.\r\n\r\n\r\n\r\n",
        "It was mainly for `RegExp`. I put the `structuredClone` in a condition 👍 (38162a0a1603da5de06d683d97a1b175289caeeb, e4364e59fbd760c2f7f6e63a2cc137720cd2690d)."
      ],
      "vite-escape-html-content-properly": [
        "Checking [the example](https://github.com/component/escape-html#:~:text=console.dir(%27%3Cinput%20name%3D%22full_name%22%20value%3D%22%27%20%2B%20escapeHtml(fullName)%20%2B%20%27%22%3E%27)), I think it should be\r\n```suggestion\r\n      res += ` ${key}=\"${escapeHtml(attrs[key])}\"`\r\n```\r\notherwise, `JSON.stringify` will do unnecessary escapes."
      ],
      "vite-secure-workflow-permissions": [
        "Good catch 🙏 ",
        "Let's move this step and the \"React Based on Permissions\" step to the top so that nothing other than the reaction would happen for users without permission."
      ],
      "vite-vue-component-import-handling": [
        "yes\r\nhttps://vite.dev/changes/hotupdate-hook.html#:~:text=handle%20additional%20watch%20events%20with%20create%20and%20delete."
      ],
      "vite-descriptive-consistent-naming": [
        "```suggestion\r\n      // normalize and rewrite accepted deps\r\n      const resolvedAcceptedDeps = new Set<string>()\r\n```\r\nWould you rename this variable now that we push ids?",
        "I think we should rename this variable as this no longer only matches `context=*` but also matches `module`. For example, `svelteScriptModuleRE`."
      ],
      "vite-propagate-errors-with-context": [
        "I think this should be\r\n```suggestion\r\n    if (!fs.existsSync(srcFile)) {\r\n```\r\nAlso I think it'd nice to avoid this `existsSync` call and add an `try-catch` to `fs.statSync` instead. `fs.statSync` throwed `ENOENT` on my machine (both Windows and WSL), did `fs.statSync` return a result on your machine?\r\n",
        "We need to wrap the whole handler with `try-catch` and call `next(error)` in the `catch`, so that `next` is called  when an error happens asynchronously.",
        "I think that is the expected behavior.",
        "For `send`s that were called while connecting the error would be voided and won't be able to catch by the user. In Vite 5, the error happened for the `send` call later that was called (which is confusing I guess).\r\nhttps://github.com/vitejs/vite/blob/c54c860f9d90e4074e5321648f9c5ee9fbda7038/packages/vite/src/shared/hmr.ts#L180-L190\r\n\r\n",
        "I think the order itself is natural as this is how the cause property works. But you have a point. Most users would want to know where the plugin error happened. I'll put it in a different property.",
        "Updated in edb34684dd3f6aefa215d663df11fb7b86c024dc\r\nI passed the whole error to the property, otherwise the console.log will output with quotes and escapes.\r\n```\r\nError: foo\r\n    at file:///D:/documents/GitHub/vite/foo.mjs:2:22\r\n    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5) {\r\n  runnerStack: 'Error: RunnerError\\n' +\r\n    '    at file:///D:/documents/GitHub/vite/foo.mjs:3:47\\n' +\r\n    '    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\\n' +\r\n    '    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\\n' +\r\n    '    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5)'\r\n}\r\n```\r\n```\r\nError: foo\r\n    at file:///D:/documents/GitHub/vite/foo.mjs:2:22\r\n    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5) {\r\n  runnerStack: Error: RunnerError\r\n      at file:///D:/documents/GitHub/vite/foo.mjs:3:47\r\n      at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n      at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n      at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5)\r\n}\r\n```\r\n",
        "I think we should keep the error message (`Parse failure: `...) so that `console.log(err)` shows the position and contents (#5192, #12060).",
        "I don't know when `err.loc` exsits (when it passes the previous code path) though.",
        "Thanks for checking! That makes sense."
      ]
    },
    "profile": {
      "location": "Japan",
      "company": "void (0)",
      "blog": "https://green.sapphi.red/",
      "twitter_username": "sapphi_red",
      "site_admin": false,
      "followers": 1188,
      "following": 21
    }
  },
  "pirate": {
    "repos": [
      "browser-use/browser-use"
    ],
    "entries": [
      {
        "slug": "browser-use-avoid-external-test-dependencies",
        "title": "avoid external test dependencies"
      },
      {
        "slug": "browser-use-avoid-hardcoded-api-assumptions",
        "title": "Avoid hardcoded API assumptions"
      },
      {
        "slug": "browser-use-avoid-logging-sensitive-content",
        "title": "Avoid logging sensitive content"
      },
      {
        "slug": "browser-use-configuration-documentation-completeness",
        "title": "Configuration documentation completeness"
      },
      {
        "slug": "browser-use-consistent-terminology-usage",
        "title": "Consistent terminology usage"
      },
      {
        "slug": "browser-use-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "browser-use-environment-variable-configuration",
        "title": "Environment variable configuration"
      },
      {
        "slug": "browser-use-maintain-configuration-consistency",
        "title": "Maintain configuration consistency"
      },
      {
        "slug": "browser-use-model-initialization-formatting",
        "title": "Model initialization formatting"
      },
      {
        "slug": "browser-use-optimize-ci-performance",
        "title": "optimize CI performance"
      },
      {
        "slug": "browser-use-optimize-docker-layers",
        "title": "optimize Docker layers"
      },
      {
        "slug": "browser-use-organize-tests-by-category",
        "title": "organize tests by category"
      },
      {
        "slug": "browser-use-pin-testing-dependency-versions",
        "title": "Pin testing dependency versions"
      },
      {
        "slug": "browser-use-preserve-exception-context",
        "title": "preserve exception context"
      },
      {
        "slug": "browser-use-replace-polling-with-events",
        "title": "Replace polling with events"
      },
      {
        "slug": "browser-use-simplify-algorithmic-approaches",
        "title": "Simplify algorithmic approaches"
      },
      {
        "slug": "browser-use-simplify-complex-implementations",
        "title": "Simplify complex implementations"
      },
      {
        "slug": "browser-use-validate-connection-timeouts",
        "title": "validate connection timeouts"
      },
      {
        "slug": "browser-use-validate-file-inputs",
        "title": "validate file inputs"
      },
      {
        "slug": "browser-use-validate-llm-reliability",
        "title": "validate LLM reliability"
      }
    ],
    "comments": {
      "browser-use-avoid-hardcoded-api-assumptions": [
        "```suggestion\r\n\t\t\tf'Search the query in {self.search_engine.title()}, the query should be a search query like humans search on Google, concrete and not vague or super long.',\r\n```",
        "the point is we're guiding it to use 'google-ish' search behavior and not put full sentences into the search box, the instruction applies even if the underlying engine used isn't google",
        "FWIW anti-bot-detection is more important to us right now than firefox and safari support. The google haters still have other options with chromium-based browsers like Brave, Edge, Ungoogled Chromium, Opera, etc.\r\n\r\n@Vinyzu does patchright still allow connecting to firefox/Safari normally without any patches? It would be annoying to have to import a different library everywhere depending on which browser we connect to."
      ],
      "browser-use-validate-connection-timeouts": [
        "this is still needed, there are cases where we get a zombie `browser_context` that looks connected that only `browser.is_connected()` can rule out (when we're lucky enough to have a `browser` object at all)",
        "```suggestion\r\n\t\t\tawait page.goto(params.url, timeout=int((params.timeout or 30)*1000))\r\n```"
      ],
      "browser-use-pin-testing-dependency-versions": [
        "```suggestion\r\nRUN pip install --upgrade playwright==$PLAYWRIGHT_VERSION\r\n```",
        "```suggestion\r\nRUN pip install playwright==$PLAYWRIGHT_VERSION\r\n```",
        "```suggestion\r\n# 6. Install playwright (version from pyproject.toml)\r\nRUN PLAYWRIGHT_VERSION=$(grep -oP '(p....right>=\\K[0-9.]+' pyproject.toml)\\\r\n\t&& pip install playwright==$PLAYWRIGHT_VERSION\r\n```"
      ],
      "browser-use-simplify-complex-implementations": [
        "personal preference but I kinda like the new python walrus operator feature for stuff like this\r\n```suggestion\r\n\t\t\t'final_result_response': (\r\n\t\t\t\tlast_action['content']\r\n\t\t\t\tif (last_action := self.step_results[-1]['actions'][-1])['is_done']\r\n\t\t\t\telse None\r\n\t\t\t),\r\n```"
      ],
      "browser-use-avoid-external-test-dependencies": [
        "dont use live URLs in tests, use pytest-httpserver like in `test_controller.py` or just visit `chrome://version` or `about:blank`",
        "same here, no live URLs in tests",
        "should use pytest-httpserver (see `tests/ci/test_controller.py`)"
      ],
      "browser-use-avoid-logging-sensitive-content": [
        "Yeah we should probably check the content a bit before doing this:\r\n\r\n- it could be a binary file like an image or PDF\r\n- it could be extremely long, blowing out the LLM context and printing a wall of text to the terminal\r\n- it could contain PII that shouldn't appear in logs (even if they're using a self-hosted LLM they're ok sending it to)\r\n\r\nLogging just the name, mimetype, and size/character count are probably fine for the `logger.info(...)`."
      ],
      "browser-use-simplify-algorithmic-approaches": [
        "this overwrites both of these, are you sure you dont want to set-append?\r\n```suggestion\r\n\t\tself.settings.available_file_paths = list({*self.settings.available_file_paths, *file_paths})\r\n\t\tself._message_manager.settings.available_file_paths = list({*self._message_manager.settings.available_file_paths, *file_paths})\r\n```",
        "sorry to nit but I think this would be clearer/simpler as a list of regexes. Makes your function a bit simpler too as you can just check `re.match` for each pattern.\r\n\r\n```suggestion\r\nMODELS_WITHOUT_TOOL_SUPPORT = [\r\n\t\"deepseek-reasoner\",\r\n\t\"deepseek-r1\",\r\n\t\"gemma.*-it\",\r\n]\r\n```"
      ],
      "browser-use-optimize-docker-layers": [
        "this should be removed too, trust the image to provide `pip` otherwise we have two copies in the layers",
        "```suggestion\r\nRUN pip install .\r\n```",
        "to avoid installing a duplicate copy of the code in this layer, install it in linked/editable mode instead:\r\n```suggestion\r\nRUN pip install -e .\r\n```"
      ],
      "browser-use-model-initialization-formatting": [
        "```suggestion\r\nagent = Agent(\r\n\ttask=\"Your task here\",\r\n\tllm=llm,\r\n\tplanner_llm=planner_llm,\r\n\textend_planner_system_message=extend_planner_system_message\r\n)\r\n```",
        "```suggestion\r\nllm = ChatOpenAI(base_url='https://api.novita.ai/v3/openai', model='deepseek/deepseek-v3-0324', api_key=SecretStr(api_key))\r\n```"
      ],
      "browser-use-replace-polling-with-events": [
        "do we even need `self.state.paused` anymore if we are using `self._external_pause_event` as the semaphore? I think we should remove it if it's not needed"
      ],
      "browser-use-maintain-configuration-consistency": [
        "FYI if both window_size and viewport are set, then viewport should be smaller than the window size or content will be cut off, I usually take ~40px off the height and ~5px off the width to account for the window frame."
      ],
      "browser-use-document-configuration-decisions": [
        "```suggestion\r\n    \"anyio>=4.9.0\",\r\n```",
        "```suggestion\r\nignore = [\"ASYNC109\", \"E101\", \"E402\", \"E501\", \"F841\", \"E731\"]  # TODO: determine if adding timeouts to all the unbounded async functions is needed / worth-it so we can un-ignore ASYNC109\r\n```"
      ],
      "browser-use-environment-variable-configuration": [
        "I don't want the Controller to manage action-specific configuration at the moment.\r\n\r\nWe haven't decided how we want to design configuration for actions yet. Could you change it to use an environment variable instead for now? then we could merge it faster.\r\n\r\ne.g. in side your action just check for a `search_engine = os.getenv('BROWSERUSE_SEARCH_ENGINE', 'google').lower().strip().split('.')[0]` and `assert search_engine in search_engines`",
        "```suggestion\r\n\t\t\t\tf'--remote-debugging-port={self.config.chrome_remote_debugging_port}',\r\n```",
        "```suggestion\r\n\t\t\tif os.getenv('ANONYMIZED_TELEMETRY', 'true').lower()[0] in 'fn0':\r\n```",
        "```suggestion\r\nCAMOUFOX = True\r\n\r\n```\r\n\r\nthis should definbitely be an env var instead of hardcoded.\r\nSwitching the entire to project to Firefox instead of chromium is a massive change to just force everyone into.",
        "ah ok no worries. Support for non-chromium browsers did just get merged recently https://github.com/browser-use/browser-use/pull/950, so there may be some interest in this again. just needs to be added here `browser/browser.py:_setup_standard_browser`",
        "```suggestion\r\ndef check_env_variables(keys: list[str], any_or_all=all) -> bool:\r\n    \"\"\"top-level helper function to check env var values\"\"\"\r\n    return any_or_all(os.getenv(key).strip() for key in keys)\r\n```\r\n\r\n- can be a pure util function so we can import it elsewhere too (e.g. from the examples)\r\n- then at startup we can call and check for the keys needed for that run based on config e.g. `check_env_variables(['OPENAI_API_KEY', 'CHROME_USER_DATA_DIR'])`",
        "yes, moving it to `utils.py` is what I had in mind 👍 "
      ],
      "browser-use-preserve-exception-context": [
        "As a rule I never use a bare `str(e)` or `f'{e}'`, it's hard to know what the actual error is from those because it doesn't including the error class name, this is usually better: `f'{type(e).__name__}: {e}'`\r\n\r\n```suggestion\r\n\t\t\treturn {'task_id': task_folder.name, 'judgement': None, 'success': False, 'error': f'{type(err).__name__}: {err}', 'score': 0.0}\r\n```",
        "```suggestion\r\n\t\t\t\traise LLMException(401, 'LLM API call failed') from e\r\n```\r\nalways preserve the original error in the traceback with `from e`"
      ],
      "browser-use-validate-file-inputs": [
        "yeah this is critical, this needs a bunch more validation to be safe:\r\n\r\n- only allow certain file extensions (probably just `.txt` right?)\r\n- don't allow `'..'`, `/`, or `\\` in filenames\r\n- strip all non-`[a-Z0-9 _-]` characters from filenames entirely, replace with `_`\r\n- only allow filenames less than 64 chars long\r\n- don't allow files to start with `#!` to prevent LLM writing scripts that could be executed + check for `+x` permission bits after writing and raise a hard error + erase the file if they are present (SMB, NFS, etc. filesystems can force permissions that would make the file dangerously executable)\r\n\r\nI would even consider `chroot`ing it somewhow. Giving an LLM direct filesystem access to save web content requires a ton of security hardening to do safely.",
        "at a minimum you should check that the passed `dir_path` is not any of these:\r\n\r\n- `/`\r\n- `~`\r\n- `~/Desktop`\r\n- `~/Documents`\r\n\r\netc. basically require that this be a dedicated directory that can be safely handed to an LLM with no oversight. Otherwise someone will for-sure call `.delete_folder()` and nuke their home folder or accidentally upload their tax return.\r\n",
        "Personally, I think we can remove params entirely to not give the LLM decision fatigue and also avoid potential security issues of allowing it to come up with arbitrary (possibly conflicting/invalid/undescriptive) filenames.\r\n\r\nThere are improvements to download saving coming soon in this branch: https://github.com/browser-use/browser-use/tree/gregor/lib-48-better-file-uploads\r\nso this will be revisited and improved soon anyway",
        "yes that is intentional. the LLM should not be able to write to arbitrary paths on the filesystem for security reasons, we should definitely restrict it to the current working directory or a dedicated temp directory + sanitize & truncate filenames to prevent `OSError`s. Otherwise you could convince an LLM to overwrite/write to `~/.bashrc`, `/etc/password`, `~/Downloads/invoice_to_be_paid.pdf` etc. by hiding some malicious jailbreak prompt in the page."
      ],
      "browser-use-consistent-terminology-usage": [
        "same here lets keep it named `playwright` everywhere for now.\r\n\r\nI don't want to use the word driver at all in this code because we plan on introducing some new concepts to browser-use eventually related to \"drivers\" and I want to keep that terminology separate. I think it's fine to call it playwright even if it's another driver because it has to be 100% playwright-API-compatible anyways.",
        "```python\r\n# add the type signature somewhere above in the file\r\nAgentHookFunc = Callable[['Agent'], None]\r\n```\r\n\r\nCan you rename these and add the type ^\r\n```suggestion\r\n\t\ton_step_start: AgentHookFunc | None = None,\r\n\t\ton_step_end: AgentHookFunc | None = None\r\n```"
      ],
      "browser-use-validate-llm-reliability": [
        "the test for `raw` should probably ask the model to respond with some JSON manually in the same way we do in the actual system prompt in `raw` mode.\r\n\r\nI think for the same reason we check if the model knows the capital of France, basic sanity checks are very useful to weed out low power models that struggle to follow directions + return JSON",
        "can you try and keep the `what is the capital of France?` question + answer check here as part of the tool call prompt & validation logic? we've found it very useful in practice to set a minimum intelligence bar for models, it really reduces the support load from people trying to run very low-power or broken models",
        "```suggestion\r\n\t\t\t\t\t\taction_instance = self.ActionModel(done={\r\n\t\t\t\t\t\t\t'success': False,\r\n\t\t\t\t\t\t\t'text': 'No next action returned by LLM!',\r\n\t\t\t\t\t\t})\r\n```"
      ],
      "browser-use-organize-tests-by-category": [
        "lets split this into like 6 top-level jobs next:\r\n\r\n- `tests/browser/builtin_patchright`\r\n- `tests/browser/user_binary`\r\n- `tests/browser/cdp`\r\n- `tests/models/openai`\r\n- `tests/models/google`\r\n- `tests/models/anthropic`\r\n\r\nand we can keep adding more of the existing tests as jobs + implementing more in the future:\r\n\r\n- `tests/models/deepseek`\r\n- `tests/models/grok`\r\n- ...\r\n- `tests/functionality/click`\r\n- `tests/functionality/tabs`\r\n- `tests/functionality/input`\r\n- ...",
        "matrix is ok for this one but do one line per test like this:\r\n\r\n```suggestion\r\n        test:\r\n            - browser/patchright\r\n            - browser/user_binary\r\n            - browser/remote_cdp\r\n            - models/openai\r\n            - models/google\r\n            - models/anthropic\r\n            # TODO: keep adding more in the future\r\n            # - models/azure         # (requires AZURE_OPENAI_API_KEY set)\r\n            # - models/deepseek      # (requires DEEPSEEK_API_KEY set)\r\n            # - models/grok          # (requires GROK_API_KEY set)\r\n            # - functionality/click\r\n            # - functionality/tabs\r\n            # - functionality/input\r\n```\r\nMost of these don't exist yet, that's ok for now."
      ],
      "browser-use-configuration-documentation-completeness": [
        "```suggestion\r\nThe browser will use this directory to store all profile data, which will persist between browser sessions.  \r\nOnly one browser at a time can be running with this `user_data_dir`, make sure to close any browsers using it before starting `browser-use`.\r\n```"
      ],
      "browser-use-optimize-ci-performance": [
        "```suggestion\r\n```\r\nnone of this is needed we already intall it directly above this block.\r\n\r\nespecially dont install `chrome`, that's old-headless which is no longer used by any code path.",
        "we should use an off-the-shelf action for this with proper caching instead of manually apt-installing, otherwise it will add a lot of time to every test",
        "lets split this into 3 separate jobs (up a level from here) so they get separate rows in the Github PR UI and so they can be parallelized (it saves a lot of clicks having to dig into CI errors every time to see what failed).\r\n\r\n- cheap and fast check for basic syntax errors w/ PLE\r\n- check for ruff style/formatting + pre-commit hook errors\r\n- check for errors from type checker (pyright)\r\n\r\nall 3 can share the same uv cache so they should each run for <20sec in theory, and if they run in parallel then all PR checks can be ready in <30sec.",
        "so the `build` should actually happen first without any matrix, just setup python 3.12 and uv and build, and push the build output as an artifact. in theory building the package should never change across python versions since it's not actually interpreting any library code during build, it's just bundling according to uv/hatch logic.\r\n\r\nThen the `build_test` test should re-use the previously built artifact and import it across the whole OS/python-version matrix + test `python -c 'from browser_use import Agent, Browser, Controller, ActionModel, ActionResult'` on each os/version combo."
      ]
    },
    "profile": {
      "location": "Oakland, California",
      "company": "@browser-use",
      "blog": "https://sweeting.me",
      "twitter_username": "theSquashSH",
      "site_admin": false,
      "followers": 1967,
      "following": 908
    }
  },
  "ggerganov": {
    "repos": [
      "ggml-org/llama.cpp"
    ],
    "entries": [
      {
        "slug": "llama.cpp-ai-parameter-organization",
        "title": "AI parameter organization"
      },
      {
        "slug": "llama.cpp-api-minimalism-principle",
        "title": "API minimalism principle"
      },
      {
        "slug": "llama.cpp-choose-appropriate-error-mechanism",
        "title": "Choose appropriate error mechanism"
      },
      {
        "slug": "llama.cpp-eliminate-code-duplication",
        "title": "eliminate code duplication"
      },
      {
        "slug": "llama.cpp-enable-callback-chaining",
        "title": "Enable callback chaining"
      },
      {
        "slug": "llama.cpp-explicit-control-flow-logic",
        "title": "explicit control flow logic"
      },
      {
        "slug": "llama.cpp-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "llama.cpp-maintain-code-consistency",
        "title": "maintain code consistency"
      },
      {
        "slug": "llama.cpp-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "llama.cpp-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "llama.cpp-measure-algorithm-performance-impact",
        "title": "measure algorithm performance impact"
      },
      {
        "slug": "llama.cpp-measure-before-optimizing",
        "title": "measure before optimizing"
      },
      {
        "slug": "llama.cpp-metal-shared-memory-sizing",
        "title": "Metal shared memory sizing"
      },
      {
        "slug": "llama.cpp-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "llama.cpp-use-environment-variables",
        "title": "Use environment variables"
      },
      {
        "slug": "llama.cpp-use-model-metadata",
        "title": "use model metadata"
      },
      {
        "slug": "llama.cpp-validate-bounds-before-access",
        "title": "validate bounds before access"
      }
    ],
    "comments": {
      "llama.cpp-maintain-naming-consistency": [
        "```suggestion\r\n        params.send_progress = json_value(data, \"send_progress\", false);\r\n```",
        "For consistency, all of these should either be byte offsets or element offsets, but not both.",
        "For consistency with the other arch names, use a dash instead of underscore:\r\n\r\n```suggestion\r\n    { LLM_ARCH_FALCON_H1,        \"falcon-h1\"        },\r\n```"
      ],
      "llama.cpp-use-environment-variables": [
        "Avoid this by checking an environment variable instead. See `GGML_SCHED_DEBUG` for an example."
      ],
      "llama.cpp-maintain-code-consistency": [
        "For long argument lists, list them on new lines to improve readibility.",
        "This method is analogous to the `build_attn_inp_` methods, so we have to model it in a similar way.\r\n\r\nReplace this method with:\r\n\r\n```c++\r\n    // similar to build_attn_inp_kv_unified()\r\n    llm_graph_input_rs * build_rs_inp() const;\r\n```\r\n\r\nIntroduce new input class:\r\n\r\n```c++\r\n// similar to llm_graph_input_attn_kv_unified\r\n// put the `s_copy` tensor in this class (similar to `kq_mask`)\r\nclass llm_graph_input_rs : public llm_graph_input_i;\r\n```\r\n\r\nIn the future, this input class could be extended with additional input tensors that are needed by the recurrent cache if necessary (similar to the attention input classes).\r\n\r\nReplace `build_recurrent_state()` and `build_rwkv_shift_load()` with overloads:\r\n\r\n```c++\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rs(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n             ggml_tensor * s,             \r\n                 int32_t   state_size,\r\n                 int32_t   n_seqs,\r\n                    bool   avoid_copies = false) const;\r\n\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rwkv_token_shift_load(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n      const llama_ubatch & ubatch,\r\n                     int   il) const;\r\n```\r\n\r\n",
        "When this change is applied, we have to do a similar addition for the hybrid implementation. The basic pattern is that you need to introduce a new input class similar to `llm_graph_input_attn_kv_unified` and `llm_graph_input_rs`, but this one will contain inputs for both the attention and for the recurrent state.\r\n\r\nSo probably something like:\r\n\r\n```c++\r\n// this input class will have both the input tensors needed for the attention and for\r\n// the recurrent state. see llm_graph_input_attn_kv_unified_iswa for example\r\nclass llm_graph_input_mem_hybrid : public llm_graph_input_i;\r\n```\r\n\r\nWe then add overloads for `build_attn()`, `build_rs()` and `build_rwkv_token_shift_load()`.",
        "For now it's important to follow the existing patterns even if there is some extra code duplication. We can rework the implementation if needed in separate refactor PRs.\r\n\r\nThe recommended way to avoid large duplications is how the `build_attn()` is implemented to use a helper, memory-agnostic method `build_attn_mha()`. This way, the memory-specific logic is implemented in the `build_attn()` overloads and the bulk of the remaining logic is reused by calling `build_attn_mha()`. You can apply this pattern both for `build_rs()` and `build_mambaX_layer()`.",
        "Looking at the `build_mamba_layer()` function, it appears complex, but actually it boils down to something like this:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...>(mstate);\r\n\r\n    conv = rec_state->update_conv(inp); // analogous to unified get_k() + cpy_k()\r\n    ssm  = rec_state->update_ssm (inp); // analogous to unified get_v() + cpy_v()\r\n\r\n    common = get_common(model);\r\n\r\n    do_something_with_conv(model, common, conv);\r\n    do_something_with_ssm (model, common, ssm);\r\n}\r\n```\r\n\r\nThe `get_common(model)` and `do_something_with_...(model, ...)` parts have to be extracted out of this function and remain implemented in `llama-model.cpp` because they use model tensors and do not depend on the memory module. The `update_conv()` and `update_ssm()` function have to be implemented only once in the `class llama_kv_cache_recurrent`. Internally, they can use a helper `build_recurrent_state()` function to deduplicate the code. Note that the hybrid cache will call these methods from the recurrent cache instance and won't have to implement them a second time.\r\n\r\nWhen you decompose the function like this, the implementation becomes much simpler:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...based_on_inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp);\r\n    ssm  = rec_state->update_ssm (inp);\r\n\r\n    return { conv, ssm };\r\n}\r\n```\r\n\r\nThis is much easier to overload multiple times for different `inp` types. I would even split it one more time:\r\n\r\n```c++\r\n// llama-graph:\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_conv(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp->get_s_copy());\r\n    \r\n    return conv;\r\n}\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_ssm(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    ssm = rec_state->update_ssm(inp->get_s_copy());\r\n    \r\n    return ssm;\r\n}\r\n```\r\n\r\nI hope I'm not missing something, but I think this should work and should be clean.\r\n\r\nAlternatively, we can also just bring the PR to a working state any way you think makes sense and then I will try to do a refactoring pass and see how it goes. Would just need some sample test commands to experiment with."
      ],
      "llama.cpp-metal-shared-memory-sizing": [
        "You can always allocate `32*sizeof(float)` bytes of shared memory for simplicity:\r\n\r\n```suggestion\r\n                    const int64_t shmem_size = 32;\r\n```\r\n\r\nNote that shared memory buffers in metal require to have size multiple of 16:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/e2b7621e7c265a6739225125cf9c534f471b3472/ggml/src/ggml-metal/ggml-metal.m#L4912-L4916\r\n\r\nBy forcing `shmem_size = 32;` you handle this requirement."
      ],
      "llama.cpp-explicit-control-flow-logic": [
        "> For example, a sequence set containing multiple seq_ids cannot be mixed with one having a seq_id in the multi-sequence set.\r\n\r\nYes, this logic here at the beginning of the function determines the unique non-overlapping sequence sets that will be contained in this ubatch:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/034b0557987c31d624854e923b505d4541399c7b/src/llama-batch.cpp#L421-L446"
      ],
      "llama.cpp-api-minimalism-principle": [
        "What is the reason to implement this inside the `llama_context` as opposed to in the diffusion example itself? If we do so, the user would have much more control over the inference and we won't have to design around this new API.",
        "It should be moved to the example. The `libllama` API should be as minimal as possible without redundant interfaces. Wrappers are implemented in user code (e.g. `libcommon`/`examples`/`tools`).",
        "If this is the only use case of `llama_sampler_accept_str()` maybe we can simply extend `llama_sampler_init_grammar()` to accept an initial string (i.e. the trigger) and avoid extending the API.",
        "> Oh, you mean to move the trigger logic in the grammar itself?\r\n\r\nNo, I had in mind something more like this:\r\n\r\n```c\r\n// llama.h\r\n    LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\r\n            const struct llama_model * model,\r\n                          const char * grammar_str,\r\n                          const char * grammar_root,\r\n                          const char * init_str); // optionally used to feed initial data to the grammar state\r\n\r\n// common/sampling.cpp\r\n    gsmpl->grmr  = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\", trigger.c_str());\r\n    \r\n    return true;\r\n```\r\n\r\nThe logic for triggering remains in user code."
      ],
      "llama.cpp-measure-before-optimizing": [
        "I think it's OK to enable by default. It requires the `LLAMA_SET_ROWS` anyway for the majority of models (except cacheless embedding models such as BERT), so it will take some time before it gets actually enabled. Should be enough to spot any potential problems until then.",
        "Yes, the time for reset is microscopic. I will simplify as suggested.",
        "It's done once per completion request, at the beginning, upon processing the input json parameters.",
        "I fixed this anyway: https://github.com/ggml-org/llama.cpp/pull/14721"
      ],
      "llama.cpp-maintain-consistent-naming-patterns": [
        "Using `shortconv` prefix would be more consistent with the existing naming pattern:\n\n```suggestion\n    MODEL_TENSOR.SHORTCONV_CONV:            \"shortconv.{bid}.shortconv.conv\",\n    MODEL_TENSOR.SHORTCONV_INPROJ:          \"shortconv.{bid}.shortconv.in_proj\",\n    MODEL_TENSOR.SHORTCONV_OUTPROJ:         \"shortconv.{bid}.shortconv.out_proj\",\n```"
      ],
      "llama.cpp-validate-bounds-before-access": [
        "I used the `.at()` method: [11ee725](https://github.com/ggml-org/llama.cpp/pull/14363/commits/11ee725a373f8a3ec8f9c8bd94cdd99e72fcd501)",
        "I missed that. Fixing.",
        "This is slightly more future-proof version:\r\n\r\n```suggestion\r\n            GGML_ASSERT(src1->type == GGML_TYPE_I32);\r\n            int64_t row_idx = ((const int32_t *)src1->data)[i];\r\n            GGML_ASSERT(row_idx >= 0 && row_idx < src0->ne[1]);\r\n```\r\n\r\nAt some point in the future we might consider changing the indices of `ggml_get_rows()` to become I64 so this assert will be helpful."
      ],
      "llama.cpp-use-model-metadata": [
        "This should be handled by the meta data in the GGUF model. There is a boolean field for when BOS is needed or not."
      ],
      "llama.cpp-measure-algorithm-performance-impact": [
        "I am not convinced that we have to add this new implementation.\r\n\r\nCan you provide a reference for the PlaMo-2 tokenizer so we can understand how it differs from the existing tokenizer algorithms?",
        "Regarding the implementation here - it has to follow the existing pattern for all other tokenizers. No need to create a new `class llama_vocab_plamo2`.",
        "> ... handle multiple stop words efficiently - with grammar trigger words we may have many\r\n\r\nCan you clarify why is that and how many stop words we could have in typical use cases?",
        "I see. I don't have a good sense of the computation complexity for finding the trigger words in typical use cases. If we can show that the algorithm improves the performance in a measurable way, then it's ok. If not, we might want to fallback to some simpler brute-force approach."
      ],
      "llama.cpp-eliminate-code-duplication": [
        "After adding the broadcast support to `ggml_set_rows()` this is not really needed anymore, but I think it's nice to have either way.",
        "Ok, will add template in a follow-up PR. For now, removed the i64 support and added TODO."
      ],
      "llama.cpp-follow-naming-conventions": [
        "The convention is to match the enum name with the prefix of the values:\r\n\r\n```suggestion\r\nenum diffusion_alg {\r\n    DIFFUSION_ALG_ORIGIN       = 0,\r\n    DIFFUSION_ALG_MASKGIT_PLUS = 1,\r\n    DIFFUSION_ALG_TOPK_MARGIN  = 2,\r\n    DIFFUSION_ALG_ENTROPY      = 3,\r\n};\r\n```",
        "Add a `DEPRECATED` overload of this call to make it easier for developers to adjust to the change.",
        "Ah, we break the `llama_sampler_init_...` pattern if we do it like this. Maybe like this instead:\r\n\r\n```c\r\nllama_sampler_init_grammar(...); // same as master\r\nllama_sampler_init_grammar_lazy(...); // same as PR but without `bool lazy`?\r\n```\r\n\r\np.s. huh, naming things is difficult 😄 ",
        "Can this be simply `LLM_KV_MAMBA_RMS_NORM`? If there isn't anything very specific for Falcon H1, it's better to keep the names generic.",
        "Should this be just `ssm_d_ssm`?\r\n\r\nGenerally, I think the `mamba` prefix is not needed here. For example:\r\n\r\n```\r\nmamba_rms_norm -> ssm_rms_norm\r\nmamba_expand -> ssm_expand\r\n```\r\n\r\n@compilade Do you agree?",
        "For consistency, rename the var:\r\n\r\n```suggestion\r\n    std::string api_prefix = \"\";                                                                         // NOLINT\r\n```"
      ],
      "llama.cpp-ai-parameter-organization": [
        "This is already available from the `hparams` - no need to duplicate it here.",
        "`max_length` should be removed and the existing `n_ubatch` parameter should be used instead.",
        "Normally, we don't put the `vocab_size` as `hparam`. Instead, we pick it from the `llama_vocab`. So this is likely not needed.",
        "This parameter should can be avoided.\r\n\r\nSee the logic here:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L529-L538\r\n\r\nAnd as an example how we apply it for the Gemma model which can have a custom attention head size like in your case:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L1021-L1025"
      ],
      "llama.cpp-enable-callback-chaining": [
        "I wonder if instead of extending the `llama_sampler` API, it would be better to pass necessary callbacks (such as `is_empty`, `accept_str`, etc.) through the `llama_sampler_init_grammar()` call."
      ],
      "llama.cpp-optimize-algorithmic-complexity": [
        "```suggestion\r\n#ifdef __ARM_FEATURE_MATMUL_INT8\r\n    assert((nrc == 2) || (nrc == 1));\r\n#else\r\n    assert(nrc == 1);\r\n#endif\r\n```"
      ],
      "llama.cpp-choose-appropriate-error-mechanism": [
        "I don't understand the change - if `repack()` returns -1, we will immediately assert on line 405:\r\n\r\n```c++\r\n    auto OK            = tensor_traits->repack(tensor, data, size);\r\n\r\n    GGML_ASSERT(OK == 0);\r\n```",
        "In the compute calls, it's better to keep the `GGML_ASSERT`s. These would never get called if `repack()` has already bailed."
      ]
    },
    "profile": {
      "location": "Sofia, Bulgaria",
      "company": "@ggml-org ",
      "blog": "https://ggerganov.com",
      "twitter_username": "ggerganov",
      "site_admin": false,
      "followers": 17691,
      "following": 13
    }
  },
  "SeanCassiere": {
    "repos": [
      "TanStack/router"
    ],
    "entries": [
      {
        "slug": "router-algorithm-implementation-trade-offs",
        "title": "algorithm implementation trade-offs"
      },
      {
        "slug": "router-api-backward-compatibility",
        "title": "API backward compatibility"
      },
      {
        "slug": "router-api-parameter-design",
        "title": "API parameter design"
      },
      {
        "slug": "router-avoid-repeated-object-creation",
        "title": "Avoid repeated object creation"
      },
      {
        "slug": "router-avoid-unnecessary-complexity",
        "title": "avoid unnecessary complexity"
      },
      {
        "slug": "router-avoid-unnecessary-workflow-restrictions",
        "title": "avoid unnecessary workflow restrictions"
      },
      {
        "slug": "router-break-complex-expressions",
        "title": "break complex expressions"
      },
      {
        "slug": "router-choose-efficient-algorithms",
        "title": "Choose efficient algorithms"
      },
      {
        "slug": "router-clear-contextual-error-messages",
        "title": "Clear contextual error messages"
      },
      {
        "slug": "router-comprehensive-test-validation",
        "title": "comprehensive test validation"
      },
      {
        "slug": "router-configuration-option-consistency",
        "title": "Configuration option consistency"
      },
      {
        "slug": "router-defensive-null-checking",
        "title": "defensive null checking"
      },
      {
        "slug": "router-dependency-version-specification",
        "title": "Dependency version specification"
      },
      {
        "slug": "router-enhance-code-clarity",
        "title": "Enhance code clarity"
      },
      {
        "slug": "router-ensure-documentation-clarity",
        "title": "Ensure documentation clarity"
      },
      {
        "slug": "router-environment-aware-api-usage",
        "title": "environment-aware API usage"
      },
      {
        "slug": "router-environment-aware-configuration-management",
        "title": "Environment-aware configuration management"
      },
      {
        "slug": "router-environment-file-management",
        "title": "Environment file management"
      },
      {
        "slug": "router-explicit-error-type-handling",
        "title": "explicit error type handling"
      },
      {
        "slug": "router-maintain-backward-compatibility",
        "title": "Maintain backward compatibility"
      },
      {
        "slug": "router-maintain-comprehensive-documentation",
        "title": "Maintain comprehensive documentation"
      },
      {
        "slug": "router-memoize-for-render-optimization",
        "title": "memoize for render optimization"
      },
      {
        "slug": "router-optimize-file-based-routing",
        "title": "optimize file-based routing"
      },
      {
        "slug": "router-optimize-react-patterns",
        "title": "Optimize React patterns"
      },
      {
        "slug": "router-prioritize-code-legibility",
        "title": "Prioritize code legibility"
      },
      {
        "slug": "router-test-real-user-interactions",
        "title": "test real user interactions"
      },
      {
        "slug": "router-use-descriptive-conflict-free-names",
        "title": "Use descriptive, conflict-free names"
      },
      {
        "slug": "router-use-descriptive-scoped-names",
        "title": "Use descriptive scoped names"
      },
      {
        "slug": "router-use-parameterized-testing",
        "title": "Use parameterized testing"
      },
      {
        "slug": "router-validate-before-accessing",
        "title": "validate before accessing"
      },
      {
        "slug": "router-validate-client-inputs",
        "title": "validate client inputs"
      },
      {
        "slug": "router-validate-configuration-schemas",
        "title": "validate configuration schemas"
      }
    ],
    "comments": {
      "router-break-complex-expressions": [
        "Better to underscore prefix it to silence any eslint warning, since having easy access to `console.log`-ing this value during dev would be nice.\n\n```suggestion\n        } catch (_err) {\n```",
        "Can we break up one-liner into a couple of booleans? with variables like `isError` and `isElapsed`."
      ],
      "router-optimize-file-based-routing": [
        "I think its worth adding a comment here for future adventurers stating that the code-splitting we're exiting early here since the `rootRoute` is never code-split.\n\nPerhaps the exit condition should just check for `createRootRoute` or `createRootRouteWithContext`."
      ],
      "router-validate-client-inputs": [
        "I'd add a callout on how client-side fields like this should not be \"trusted\". Purely for security due-dilligence."
      ],
      "router-environment-file-management": [
        "Please add a general Node `.gitignore` and have `.env` added in, whilst still leaving `.env.example` as visible.",
        "Please add `.env` but exclude `.env.example`. Not sure if this is already available further up the tree for the monorepo, but since this example needs to be clonable, it'd be best to add it in.",
        "Please add a comment that says pretty something like this:\r\n\r\n```\r\n# copy this file over as `.env.local` and fill in these values from the Firebase console.\r\n```\r\n\r\nOr even update it to say copy the files and follow the instructions in the README to get the values."
      ],
      "router-validate-before-accessing": [
        "I assume you mean only for object properties yes? Not to strip out `undefined` as an array value, only as an object value, when comparing `[undefined, 'foo', { a: undefined }]` with `['foo', undefined, { a: undefined }]`.\r\n\r\nI was hesitant to make such changes to the `deepEqual` function itself since that'd have an impact on the `router.matchRoute` and `router.commitLocation` methods, especially without understanding the conditions in which `undefined` would get either pass or fail the function.\r\n\r\nIt's why I introduced that function since it explicitly does a single job and has a narrow impact area (only the link component).",
        "@schiller-manuel thoughts on having the `deepEqual` function internally call the `deepRemoveUndefinedFromObject` function on both its inputs `a` and `b`?",
        "Yup, that's what'll work best here. I'll beef up those tests for the `deepRemoveUndefinedFromObject` function and push this.",
        "This `Boolean` filtering was causing falsy number values (like `0`, `-1`, etc...) to be excluded from the joined path.\r\n\r\nThis ripples out into the router logic, where all the right routes don't get matched since the pathname it's using for checking will be incomplete or in some cases even outright incorrect."
      ],
      "router-avoid-repeated-object-creation": [
        "Would the definition of `makeMaybe` outside of the for loop have any performance gains? (still being left within the scope of `loadMatches` though)"
      ],
      "router-avoid-unnecessary-workflow-restrictions": [
        "```suggestion\r\n          repo-token: ${{ secrets.GITHUB_TOKEN }}\r\n```\r\n\r\nThis should be fine:\r\n- https://docs.github.com/en/actions/security-for-github-actions/security-guides/automatic-token-authentication\r\n- https://docs.github.com/en/actions/security-for-github-actions/security-guides/automatic-token-authentication#permissions-for-the-github_token",
        "If a member opens a PR, this means that another member would have to go in and approve the PR correct?"
      ],
      "router-maintain-backward-compatibility": [
        "There should be separate Panel export as well yes?",
        "Not too sure, I think it was something about portalling it or something 🤷🏼‍♂️.\r\n\r\nJust don't want to break anything for anyone who upgrades.",
        "Just checked the source, it's to do with portalling the devtools into a shadowDOM target.\r\n\r\nRelated issues:\r\n- https://github.com/TanStack/router/issues/1065\r\n- https://github.com/TanStack/router/issues/1246\r\n- https://github.com/TanStack/router/issues/1247",
        "Did a bit more investigation into the Panel component.\r\n\r\nIt's essentially, the devtools component itself, without the floating component aspect to it. So the panel component won't have resize functionality or the concept of being anchored to a position (\"bottom-right\" | \"top-left\" | \"...\").\r\n\r\nThe idea is to be able to portal/attach the devtools component onto a ShadowDOM node.",
        "🚨 Switching to object syntax, makes this a breaking change. I think we may have to stick with the existing API surfaces.",
        "We follow semver and this'd be a breaking change to the user.\r\n\r\nConsidering we don't want to introduce compexity with function overloads, I think we'd have to go with the alternatives here (like `useBlocker(() => void, value)`), or simply export a new useBlocker hook altogether (with a new name) and preserve the old hook as-is but mark it as deprecated.\r\n\r\n@schiller-manuel thoughts on this?",
        "Rebasing with main should fix this now.",
        "Maybe this needs to be a function overload?\r\n\r\n```tsx\r\nexport function useBlocker(blockerFn: BlockerFn, condition?: boolean | any): BlockerResolver;\r\nexport function useBlocker(blockerOpts: BlockerOpts): BlockerResolver {\r\n  // ...\r\n}\r\n```\r\n\r\nAnd probably should depreciate the old syntax.",
        "Is there a reason for this api surface to change?\r\nThis'll be a breaking change for anyone currently calling `notFound({ global: true })` yes?",
        "Not sure if that'd help this though, as this is a user input type.\r\nSince the type `NotFoundError` is the same user input used when throwing a `notFound()`, changing it makes it so now you'd have to `throw notFound({ _global:true })` instead of just `throw notFound({ global: true })`."
      ],
      "router-maintain-comprehensive-documentation": [
        "Similarly, please have the expected value be listed 'in full' as a string. It'd be better for our tests on this to show us exactly what's expected for a certain transformation.",
        "Couple of small changes here please:\n\n- Rename to `handleTransitionerHashScroll`\n- Annotate the function with some JSDoc comments marking it as `@internal` with a short description saying that this is to be setup in the `<Transitioner>` component and what it actually does.",
        "Optionally, the rename is not necessary if just the JSDoc comment is there. ",
        "The MDN link should also be placed in the markdown files.\r\n\r\n- RouterOptionsType.md\r\n- RouterType.md",
        "I'd decorate this with a comment like \"actually fire off the `beforeLoad` callback\", purely for readability for others in the future.",
        "Resolved in https://github.com/TanStack/router/pull/1908"
      ],
      "router-choose-efficient-algorithms": [
        "Had to make a few modifications, but the essential gist of your suggestion has been applied."
      ],
      "router-prioritize-code-legibility": [
        "Please switch this to using explicit returns with their brackets.\r\nAlso, don't restructure the pets array, instead just join with a comma.\r\n\r\n```js\r\n.handler(async ({ data: { name, age, pets } }) => {\r\n\treturn `Hello, ${name}! You are ${age + testValues.__adder} years old, and your favourite pets are ${pets.join(',')}.`\r\n})\r\n```",
        "> can do, but why, out of curiosity?\r\n\r\nLegibility mostly. For a lib, the more legible the better."
      ],
      "router-use-descriptive-scoped-names": [
        "Might be better to align and use a get function here (i.e `getIsClosedStatus()`)",
        "If possible, please keep the `MyRouterContext` with auth as a key.\r\n\r\nEssentially, similar to what we already have going. Currently, this would make it difficult to inject Query into the context.\r\n\r\ni.e.\r\n```ts\r\nexport type MyRouterContext = {\r\n\tauth: AuthContextType\r\n}\r\n```",
        "Rename this to something that's more scoped to what it is like `BlockerFnResolver`, `BlockerResolver`, or something to that effect."
      ],
      "router-ensure-documentation-clarity": [
        "Maybe instead of a warning, this could be a small heading like \"What is a no-op?\", that'd show the user the type of a no-op.\r\n\r\nPerhaps something like this:\r\n\r\n```ts\r\ntype NoOpFunction = () => undefined // add a comment here\r\n```\r\n\r\n",
        "With the title being set up, the top-level heading can be removed, since the website takes this `title` and renders a top-level `h1`.",
        "Your approach was correct.\r\n\r\nThe frontmatter should be used.\r\n\r\n```md\r\n---\r\ntitle: Xyx\r\n---\r\n\r\nThis guide...\r\n```\r\n\r\nIt's the top-level heading markdown heading that shouldn't be used.",
        "I think you are mixing the two separate things here.\r\n\r\nThe title (using the front matter) has nothing to do with the navigation links. Aside from the navigation links, the title stuff was also wrong.\r\n\r\nSince, this PR's content has now been changed to remove the changes regarding the titles, I'll merge this as-is for just fixing the navigation."
      ],
      "router-comprehensive-test-validation": [
        "Two reasons on my part:\r\n1. There already was a virtual routes example using the config directly supplied by the user, so this would also validate that the file (`route.ts`) reading was working.\r\n2. I wanted a test, without the `routeTree.gen.ts` committed into git so that on every CI run, it generated the `routeTree.gen.ts` from the `routes.ts` and ensured the navigations were successful."
      ],
      "router-validate-configuration-schemas": [
        "You shouldn't need to do this, the vite plugin should automatically pickup the file.\r\n\r\nOr just simply inline the config with the file plugin here."
      ],
      "router-configuration-option-consistency": [
        "```suggestion\r\nThis option turns off the formatting function on the generated route tree file, which can be time-consuming for large projects.\r\n```",
        "```suggestion\r\nSet the `server.preset` value to `cloudflare-pages` and the `server.unenv` value to the `cloudflare` from `unenv` in your `app.config.ts` file.\r\n```"
      ],
      "router-use-parameterized-testing": [
        "Please break this one large test function into individual ones.\r\n\r\n```ts\r\ndescribe('createHashHistory', () => {\r\n\tdescribe('parseLocation',  () => {\r\n\t\ttest('...', () => {})\r\n\r\n\t\ttest('...', () => {})\r\n\r\n\t\t// ...\r\n\t})\r\n})\r\n```\r\n\r\nMaybe add some into a matrix where it checks both \"on load's `parseLocation` call\" and \"after pushState's `parseLocation`\" values.",
        "Maybe the word matrix wasn't the right word here. Essentially, what I'm talking about is checking the behaviours in both \"on load\" and \"on navigate\" scenarios.\r\n\r\nSomething like this for example.\r\n\r\n```ts\r\ndescribe.each([\r\n\t['/', { pathname: '', search: '' }],\r\n\t// ...\r\n])('check for %i', ([path, exp]) => {\r\n\t// check if parseLocation works in initial load\r\n\ttest(`onLoad with ${path}`, () => {\r\n\t\twindow.location.href=path\r\n\t\tconst history = createHashHistory()\r\n\t\texpect(...).toBe(exp['...'])\r\n\t})\r\n\r\n\t// check if parseLocation works after navigate\r\n\ttest(`onNavigate with ${path}`, () => {\r\n\t\tconst history = createHashHistory()\r\n\t\twindow.history.pushState({}, null, path)\r\n\t\texpect(...).toBe(exp['...'])\r\n\t})\r\n})\r\n```",
        "It might just be missing the correct setup for the testing env.\r\n\r\nCould you refactor to using `window.location` and then ping me? I'll fix up the deps and get the testing environment working.\r\n\r\nEdit: the correct testing environment should already be available in `packages/react-router`.",
        "You are right, my mistake. I was thinking about this https://github.com/TanStack/router/blob/35c7c495f38f89675a5570a9b4fa35e63f0a3525/packages/react-router/tests/link.test.tsx#L51\r\n\r\nIn that case, don't mock `window`, just do a `window.(push/replace)State` before calling `createHashHistory` for the \"on-load\" tests.",
        "I'd go for the first option since this is currently just testing the `generator` function and this could change should we decide to test the `config` related stuff as well.\r\n\r\nAlso, didn't know that `it.each` was a thing...\r\nPS: Literally my first time ever writing tests... Pretty much yolo'd till now.",
        "I'll dump all the dirs inside a folder and get the foldernames from there."
      ],
      "router-clear-contextual-error-messages": [
        "Please use different error messages both in the `errorComponent` and the `component` so that anyone reading the test knows exactly what the expected outcome is.\r\n\r\nSomething like:\r\n- `errorComponent` - \"Rendering errorComp message\"\r\n- `component` - \"Throwing from route component\"\r\n\r\nFeel free to change the exact verbiage as you please. Just please make them very distinctly unique."
      ],
      "router-enhance-code-clarity": [
        "```suggestion\r\n├── -posts-table.tsx // 👈🏼 ignored\r\n```",
        "```suggestion\r\n│   ├── header.tsx // 👈🏼 ignored\r\n```",
        "```suggestion\r\n│   ├── footer.tsx // 👈🏼 ignored\r\n```",
        "```suggestion\r\n  - Note that `ReactDOM.createRoot` is required.\r\n  - The legacy `.render()` function is not supported.\r\n```",
        "```suggestion\r\nIf you don't have ready access to your route object (i.e. you're deep in the component tree for the current route), you can use `getRouteApi` to access the same hook (as well as the other hooks on the Route object). This should be preferred over importing the Route object, which is likely to create circular dependencies.\r\n```"
      ],
      "router-explicit-error-type-handling": [
        "Maybe make this a more specific? \r\n```\r\nstartsWith(`${RECOVERABLE_ERROR}: `)\r\n```"
      ],
      "router-optimize-react-patterns": [
        "This could just be a derived value yes? instead of being stored in state.",
        "I'm not too familiar with this hook, I guess it should be fine leaving out the deps array.",
        "Are you satisfied with the PR? If so, I'm happy to merge."
      ],
      "router-memoize-for-render-optimization": [
        "Could this also be memoed? Or maybe pulled into `useMemo` below?\r\n\r\nSince these values don't change very often, there is no need to recompute this."
      ],
      "router-dependency-version-specification": [
        "This shouldn't be needed, since Vitest gets inherited from the monorepo. Plus we don't have vitest as deps or devDeps in any of the other packages.\r\n\r\nIts probably whats causing the lockfile failure in CI.",
        "Oh, I see!!!\r\n\r\nThis tends to happen for me locally as well when I haven't kept up to date with the **lockfile** on _main_ or if my pnpm version isn't in sync with `packageManager` field in the `package.json` file.\r\n\r\nSome common indicators of happening which I've noted are:\r\n- local tests suddenly thinks vitest is missing\r\n  - for which \"fixing\" it causes CI installs to fail\r\n- local formatting using prettier causes unrelated files or areas of affected code to change their code-style.\r\n- local builds failings with nx being unable to build one or more packages.\r\n- local builds start getting interrupted by unrelated/unexpected eslint violations.\r\n\r\nTo fix this, I often switch to _main_ and pull down all the latest changes. Then make sure the pnpm version is correct and run `pnpm install`. Then for good measure, I run `pnpm dedupe`.",
        "Yea, something like `>=2 <=3` should suffice for the time being.",
        "Just set it to whatever is the latest version ATM. You can get these from the package.json fields. So don't refer to them as __\"workspace:*\"__.\r\n\r\npnpm locally uses the overrides to use the local version and when deployed our CI automatically changes these versions.",
        "The packages shouldn't be workspace prefixed.\r\n\r\n`\"^1.46.3\"`"
      ],
      "router-environment-aware-api-usage": [
        "Looking at the failures with the unit tests, we probably should perform a check for `window` or something like `typeof window !== 'undefined' && 'CSS' in window` and then do the check as `window.CSS.supports(...)`.",
        "`\"url\"` => `\"node:url\"`"
      ],
      "router-algorithm-implementation-trade-offs": [
        "Is there anything that can be done here where all keys aren't by default an array? I'd rather switch it to an array **only** if multiple instances of the same `name` are encountered.\r\n\r\nIf you've got better reasoning for sticking with this approach, I'm open to pushback on this one."
      ],
      "router-avoid-unnecessary-complexity": [
        "```suggestion\nThe [`@typescript-eslint/only-throw-error`](https://typescript-eslint.io/rules/only-throw-error/) rule, enabled by default in the `recommended-type-checked` and `strict-type-checked` rulesets, disallows the throwing of non-Error values as exceptions, which is considered a good practice.\n\nTo make sure it that it does not conflict with TanStack Router, you should add `redirect` to the allowed as a throwable objects.\n```",
        "> Yeah... I was wondering about this because both existing prettier options (quoteStyle and semicolons) are documented as settings for the generated route tree file, but from what I can tell they are only used to format the route files.\r\n\r\nThat's probably a mistake in the docs. Those settings (semicolons and quotes) are used for both the `routeTree` and generated route files.\r\n\r\nIt's generally not an issue for the route files, since as soon as user content comes in, the router-generator should be pretty hands-off at that point. @schiller-manuel, I hate to say it, but a lot of this could be avoided, if these transformations in the route file, were done using AST transformations.",
        "> Just using AST transformations would still run into formatting issues. If the AST transformation updates the route id to one that exceeds the line's character limit, then the user would still have to run their formatter over the transformed code to ensure compliance.\r\n\r\nEither way, it falls on the user, even if a `formatter='none'` option was introduced, that too would require the user to run their format on the output.\r\n\r\nSince the goal of `formatter='none'`, would be to save the file without making any **stylistic** changes (i.e what we are doing now with prettier), it'd make more sense (in my point of view) to make those changes via AST transformations and let the user's formatted do the necessary work on that side. This way, we really are guaranteeing that we aren't saving the route files with any opinions on code style.",
        "> The goal of formatter='none' is to allow the user to opt out of the stylistic changes currently made to route files by TanStack Router, without introducing any breaking changes.\r\n\r\nIn agreement with you here 👍🏼\r\n\r\n> Maybe I'm missing something, but wouldn't this just add extra dependencies/complexity, and require extra parsing options (e.g. babel config) to be configurable in the tsr config? \r\n\r\nDefinitely not simpler but it would be the safer method of performing these changes. We've just be taking the existing babel utils be have in `router-utils`.\r\n\r\n> How is this better than a simple string replacement?\r\n\r\nCurrently, the string replacement is using `Regex`. This is a bottleneck of sorts since it inherently expects a certain level of formatting. Like the `createFileRoute('/posts/')` being on one line and not broken up like this due to a formatting change.\r\n\r\n```js\r\n// We could have parsing issues with this.\r\nexport const Route = createFileRoute(\r\n\t'/posts' // 🧠 imagine a longer route ID here\r\n)\r\n```\r\n\r\n---\r\n\r\n@benjamesfleming Overall, I'm in agreement and support the goal of this PR 💪🏼.\r\n\r\nI just want to make sure we correctly approaching the fix for it. \r\ncc @schiller-manuel ",
        "@benjamesfleming speaking with @schiller-manuel, we've decided to go ahead with this PR, with one minor exception. The removal of the `formatter` option entirety.\r\n\r\nThe behavior would be as follows:\r\n\r\n- When the generator is saving the route file during an \"update\" event, then the formatter shouldn't be used.\r\n- However, on first write of the route file and any operations on the routeTree file, the formatter should still be in control.\r\n\r\nThis would make this the new behavior for all and would be classified as fix."
      ],
      "router-api-backward-compatibility": [
        "Don't make changes to the `RouterMatch` type, since that is more of an internal thing. We keep the public surface on the docs quite sparse on purpose.\r\n\r\nAt most, I'd keep the `status`, `isFetching`, and `ssr` changes.",
        "You've already defined the `ReturnType` for `head` right?\r\n\r\nWhat I'm talking about is the type for the `RouteMatch` which is being publicly shown. \r\n\r\nIdeally, the user _should_ have very few interactions with the actual route match itself.",
        "The actual underlying type there is something similar to what we’ve got going on here https://tanstack.com/router/latest/docs/framework/react/api/router/LinkPropsType\r\n\r\nor perhaps make up something like FrameworkMetaTagAttributes\r\n\r\nEither way, when it comes to RouteMatch type, that isn’t some we want to commit to especially since we change that quite often."
      ],
      "router-api-parameter-design": [
        "In its current form, this change to how arrays are represented in stringified format, would break any existing bookmarks. This'd be a problem and a breaking change.\n\nPlus, the general accepted spec for having the same key (i.e Array values) be repeated multiple times in the query parameters are as such.\n\n```\n?genre=Pop&genre=RNB&year=2005\n```",
        "What concerns me, is what gets printed out to the end-user's URL bar and what our \"sensible defaults\" are.\r\n\r\nSo, at runtime, will inputting `search = { genres=[Pop,Rock] }` change from `?genres=Pop&genres=Rock` to something else (i.e `?genres=Pop%2CRock`)?\r\n\r\n- **If so**, then this is a **breaking change**.\r\n- **If not,** then my bad. The way I interpreted the test case, gave me the impression that this had an effect on the defaults and what get's output into the URL bar.",
        "Alrighty 👍🏼\r\n\r\nAs long as the user sees no changes to any of their existing bookmarks, then all good from my side!",
        "Could avoid the spread here yea?\r\n\r\n`navigate: (opts: any) => this.navigate(opts as any)`",
        "What's the purpose of removing `from` in the route preload?\r\n\r\nOr is this automatically being resolved in `buildLocation`?"
      ],
      "router-environment-aware-configuration-management": [
        "Perhaps this bit should be optionally checked since Start isn't guaranteed to be there?\r\n```suggestion\r\n        __html: `(${restoreScroll.toString()})(${JSON.stringify(storageKey)},${JSON.stringify(resolvedKey)});__TSR__?.cleanScripts?.()`,\r\n```",
        "```suggestion\r\n// TODO: Bump `v1` to `v2` when performing the library major bump.\r\nexport const storageKey = 'tsr-scroll-restoration-v1-3'\r\n```\r\n\r\nAnd then in v2, change it to `tsr-scroll-restoration-v2-1`.",
        "Has the implementation details (components, utils, etc.) for `@tanstack/router-devtools` not been nuked yet?\r\n\r\n---\r\n\r\nTo retain the same \"available in production\" behaviour, maybe just export a legacy option from `@tanstack/react-router-devtools` and have that be the primary export for `@tanstack/router-devtools`.\r\n\r\nSo, that'd be.\r\n\r\n1. Setup an export as `@tanstack/react-router-devtools/legacy`.\r\n1.1 `packages/react-router-devtools/src/legacy.ts` where the legacy export would retain the same behaviour in deployment as we have now.\r\n1.2 `packages/react-router-devtools/vite.config.ts` to have a separate entry for the legacy entry.\r\n1.3 `packages/react-router-devtools/package.json` to have a separate entry for the legacy entry.\r\n2. `@tanstack/router-devtools` exports the components from `@tanstack/react-router-devtools/legacy`.",
        "We just need to maintain the package export for `@tanstack/router-devtools`. The package itself would basically just be proxying out `@tanstack/react-router-devtool/legacy`.\r\n\r\nEssentially, the same as what we are doing for `@tanstack/router-vite-plugin`.",
        "Awesome!\r\n\r\nI just don't want to maintain any implementation in `@tanstack/router-devtools`. It should all just live in `@tanstack/react-router-devtools`.\r\n\r\nThe `@tanstack/router-devtools` would just be a proxy. No implementation."
      ],
      "router-use-descriptive-conflict-free-names": [
        "While it makes sense for the `matches` to have keys be shortened (i.e. `i`, `b`, `l`, etc.), I think these single callers may benefit from just being left with having descriptive names to represent their purpose (i.e. `router/dehydratedRouter`, `close`, `whatever_v_is`).",
        "Could you apply that change while doing the type-safe `routeId` stuff?",
        "cc @chorobin @schiller-manuel ",
        "Please rename this util so it doesn't clash with the naming of React hooks. Maybe `getSupabaseServer`?",
        "I believe this should follow the naming above and should be `disableManifestGeneration` or something to that effect.",
        "Better naming and to not compete with `fetch`."
      ],
      "router-test-real-user-interactions": [
        "I think it'd be better if we beefed up this test, either by amending this or adding a new one, to-use testing library to make sure the component gets rendered on the screen (like how `routeContext.test.tsx` does).",
        "Resolved in https://github.com/TanStack/router/pull/1908",
        "In addition to this test, I think we should also be testing the user hover event from testing-library.\r\n\r\nSetting `defaultPreload: 'intent'` and doing `element.focus()` would also simulate a \"real\" interaction.",
        "Resolved in https://github.com/TanStack/router/pull/1908",
        "Could a test also be added in, testing the href value when a `basePath` value is set on the router.\r\n\r\n```tsx\r\nconst router = createRouter({\r\n  basePath: '/app'\r\n})\r\n```\r\n\r\nSeems like a good runtime check to guarantee."
      ],
      "router-defensive-null-checking": [
        "```suggestion\r\n        if (!context) return null\r\n```",
        "```suggestion\r\n        if (!context.getTitle) return null\r\n```"
      ]
    },
    "profile": {
      "location": "Invercargill, New Zealand",
      "blog": "seancassiere.com",
      "twitter_username": "SeanCassiere",
      "site_admin": false,
      "followers": 95,
      "following": 11
    }
  },
  "ReneWerner87": {
    "repos": [
      "gofiber/fiber"
    ],
    "entries": [
      {
        "slug": "fiber-api-design-clarity",
        "title": "API design clarity"
      },
      {
        "slug": "fiber-avoid-count1-flag",
        "title": "avoid `-count=1` flag"
      },
      {
        "slug": "fiber-check-all-error-returns",
        "title": "check all error returns"
      },
      {
        "slug": "fiber-choose-descriptive-property-names",
        "title": "Choose descriptive property names"
      },
      {
        "slug": "fiber-clear-network-api-documentation",
        "title": "Clear network API documentation"
      },
      {
        "slug": "fiber-document-error-conditions-clearly",
        "title": "Document error conditions clearly"
      },
      {
        "slug": "fiber-document-mutex-usage",
        "title": "Document mutex usage"
      },
      {
        "slug": "fiber-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "fiber-explicit-cicd-configuration",
        "title": "explicit CI/CD configuration"
      },
      {
        "slug": "fiber-extract-duplicate-logic",
        "title": "Extract duplicate logic"
      },
      {
        "slug": "fiber-follow-naming-patterns",
        "title": "Follow naming patterns"
      },
      {
        "slug": "fiber-include-practical-examples",
        "title": "Include practical examples"
      },
      {
        "slug": "fiber-maintain-clean-linter-configs",
        "title": "maintain clean linter configs"
      },
      {
        "slug": "fiber-minimize-memory-allocations",
        "title": "minimize memory allocations"
      },
      {
        "slug": "fiber-prefer-existing-apis",
        "title": "prefer existing APIs"
      },
      {
        "slug": "fiber-prefer-standard-library-functions",
        "title": "Prefer standard library functions"
      },
      {
        "slug": "fiber-simplify-logging-integrations",
        "title": "simplify logging integrations"
      },
      {
        "slug": "fiber-use-context-for-configuration",
        "title": "Use context for configuration"
      },
      {
        "slug": "fiber-validate-configuration-defaults",
        "title": "Validate configuration defaults"
      }
    ],
    "comments": {
      "fiber-choose-descriptive-property-names": [
        "`PredefinedFormat` sound better\r\nas it is not a fixed list of formats and cannot be customised\r\nagreed",
        "@gaby ok with this ?",
        "@edvardsanta we had consulted internally and think an extra property is not necessary, as it can also be set directly via the format\r\nwe should just point this out in the documentation \r\n```go\r\napp.Use(logger.New(logger.Config{\r\n    Format: logger.FormatCommon,\r\n}))\r\n```\r\n\r\npls change this"
      ],
      "fiber-simplify-logging-integrations": [
        "@haochunchang can we put that or a better generic wrapper in the code so that not every dev has to create this struct? or would that be too undynamic? \r\n\r\nwhat do you think? @gaby @efectn ",
        "maybe we should recommend something like this ?\r\n\r\n```go\r\nfunc LoggerToWriter(customLogger fiberlog.AllLogger, level fiberlog.Level) io.Writer {\r\n\treturn &struct {\r\n\t\tWrite func(p []byte) (n int, err error)\r\n\t}{\r\n\t\tWrite: func(p []byte) (n int, err error) {\r\n\t\t\tswitch level {\r\n\t\t\tcase fiberlog.LevelDebug:\r\n\t\t\t\tcustomLogger.Debug(string(p))\r\n\t\t\tcase fiberlog.LevelError:\r\n\t\t\t\tcustomLogger.Error(string(p))\r\n\t\t\t}\r\n\t\t\treturn len(p), nil\r\n\t\t},\r\n\t}\r\n}\r\n```\r\nthen we really don´t need extra code in the codebase",
        "ok @haochunchang pls adjust it a little bit\r\n",
        "@haochunchang yeah, but in my example its there\r\nso can you update the PR with this code snippet ?",
        "@haochunchang \r\nyeah add it in the readme in a new sub section",
        "@coderabbitai pls prepare this change"
      ],
      "fiber-avoid-count1-flag": [
        "this prefix was used to disable the golang cache for testing, because in the past several tests went through it\r\n\r\nfor now, would get this construct",
        "> for now, would get this construct\r\n\r\nwith \"-count=1\""
      ],
      "fiber-include-practical-examples": [
        "examples for the use of some methods (everywhere) would be cool ",
        "@negrel \r\n> Consider adding a practical example or a more detailed explanation of the DialDualStack option's impact on proxy behavior to aid user understanding.\r\n\r\nis a good hint, could you do this\r\n\r\nonly a feature that is understandable and well documented will be found and used"
      ],
      "fiber-prefer-standard-library-functions": [
        "can you try https://github.com/gofiber/utils/blob/master/strings.go#L8",
        "standard lib is faster if it is already lower case, which should be the case in most cases",
        "can you try https://github.com/gofiber/utils/blob/master/strings.go#L8",
        "standard lib is faster if it is already lower case, which should be the case in most cases"
      ],
      "fiber-extract-duplicate-logic": [
        "since the part with the configurations has become larger, we can move this to a private function for creating the configuration, so that it becomes more usable again",
        "Can we do the process in the `init` function ?\r\nAnd can you outsource this whole block in a function called\r\n`initServices`\r\ni like short functions (CleanCode disciple)  and we blow up the `New` or `init` function with this whole block\r\n\r\nLater\r\n\r\n```go\r\nfunc (app *App) init() *App {\r\n\t// lock application\r\n\tapp.mutex.Lock()\r\n\t\r\n\tapp.initServices()\r\n\t...\r\n```",
        "can we add a reset method\r\nits better than handling every option separat",
        "this part looks almost identical to the other square bracket parsings, if it were possible it would be cool if we could move this into another function and use it in all three places\r\n\r\nquery params\r\nhttps://github.com/gofiber/fiber/blob/775e0a73f3fc0eba0940488e2accd3b87ab245ee/binder/query.go#L34-L45\r\nform params urlencoded\r\nhttps://github.com/gofiber/fiber/blob/775e0a73f3fc0eba0940488e2accd3b87ab245ee/binder/form.go#L41-L52",
        "this part is always the same, can you outsource it, e.g. make a break if there is an error and handle this part under the switch or outsource it to another methodic\r\n\r\nplease check the benchmark again afterwards",
        "![image](https://user-images.githubusercontent.com/7063188/218414669-630eefa6-f831-4227-8569-d09922bb21b9.png)\r\nlooks like it is a complete copy of the other method\r\n\r\ncan we improve this by swapping the core of these methods into a new one and only ever swap the action method ?",
        "```go\r\nfunc Do(c *fiber.Ctx, addr string, clients ...*fasthttp.Client) error {\r\n\treturn do(c, addr, func(cli *fasthttp.Client, req *fasthttp.Request, resp *fasthttp.Response) error {\r\n\t\treturn cli.Do(req, resp)\r\n\t}, clients)\r\n}\r\n\r\nfunc DoRedirects(c *fiber.Ctx, addr string, maxRedirectsCount int, clients ...*fasthttp.Client) error {\r\n\treturn do(c, addr, func(cli *fasthttp.Client, req *fasthttp.Request, resp *fasthttp.Response) error {\r\n\t\treturn cli.DoRedirects(req, resp, maxRedirectsCount)\r\n\t}, clients...)\r\n}\r\n\r\nfunc DoDeadline(c *fiber.Ctx, addr string, deadline time.Time, clients ...*fasthttp.Client) error {\r\n\treturn do(c, addr, func(cli *fasthttp.Client, req *fasthttp.Request, resp *fasthttp.Response) error {\r\n\t\treturn cli.DoDeadline(req, resp, deadline)\r\n\t}, clients...)\r\n}\r\n\r\nfunc DoTimeout(c *fiber.Ctx, addr string,timeout time.Duration, clients ...*fasthttp.Client) error {\r\n\treturn do(c, addr, func(cli *fasthttp.Client, req *fasthttp.Request, resp *fasthttp.Response) error {\r\n\t\treturn cli.DoTimeout(req, resp, timeout)\r\n\t}, clients...)\r\n}\r\n\r\nfunc do(\r\n\tc *fiber.Ctx,\r\n\taddr string,\r\n\taction func(cli *fasthttp.Client, req *fasthttp.Request, resp *fasthttp.Response) error,\r\n\tclients ...*fasthttp.Client,\r\n) error {\r\n\tvar client *fasthttp.Client\r\n\tif len(clients) != 0 {\r\n\t\t// Set local client\r\n\t\tclient = clients[0]\r\n\t} else {\r\n\t\t// Set global client\r\n\t\tlock.RLock()\r\n\t\tclient = client\r\n\t\tlock.RUnlock()\r\n\t}\r\n\r\n\treq := c.Request()\r\n\tres := c.Response()\r\n\toriginalURL := utils.CopyString(c.OriginalURL())\r\n\tdefer req.SetRequestURI(originalURL)\r\n\r\n\tcopiedURL := utils.CopyString(addr)\r\n\treq.SetRequestURI(copiedURL)\r\n\t// NOTE: if req.isTLS is true, SetRequestURI keeps the scheme as https.\r\n\t// issue reference:\r\n\t// https://github.com/gofiber/fiber/issues/1762\r\n\tif scheme := getScheme(utils.UnsafeBytes(copiedURL)); len(scheme) > 0 {\r\n\t\treq.URI().SetSchemeBytes(scheme)\r\n\t}\r\n\r\n\treq.Header.Del(fiber.HeaderConnection)\r\n\tif err := action(client, req, res); err != nil {\r\n\t\treturn err\r\n\t}\r\n\tres.Header.Del(fiber.HeaderConnection)\r\n\treturn nil\r\n}\r\n```\r\nsomething like this, but better",
        "@gaby \r\nit is not absolutely necessary to offer the other methods as well\r\nfor me it was only important that we do not duplicate the code although the core is the same"
      ],
      "fiber-clear-network-api-documentation": [
        "![image](https://github.com/user-attachments/assets/13284f63-b320-4eed-b10f-5161aef0e8ee)\r\n```suggestion\r\nA boolean property that is `true` if the request’s [X-Requested-With](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers) header field is [XMLHttpRequest](https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest), indicating that the request was issued by a client library (such as [jQuery](https://api.jquery.com/jQuery.ajax/)).\r\n```"
      ],
      "fiber-maintain-clean-linter-configs": [
        "the performance and speed of fiber and fasthttp relies on some trick with the unsafe packet\r\ni am not sure if we should limit this ? or is the plan to make exceptions where this is used ?",
        "ok"
      ],
      "fiber-document-mutex-usage": [
        "why do we close the mutext so late? after writing the status and the json? \r\nbesides, in previous benchmarks it had turned out that the defer calls are slower",
        "ok it can stay",
        "no this method is provided by all adapters\r\nhttps://github.com/search?q=repo%3Agofiber%2Fstorage%20conn&type=code",
        "sync.Map is ok, but slower or ?"
      ],
      "fiber-api-design-clarity": [
        "not sure if we should allow this feature\r\nsince the middleware hangs on a certain route, the route is already handled with a timeout config\r\n\r\nto have further configurations of the routes in there I see as wrong\r\n\r\nalso people will want the routes to work with wildcard and other route specific matchings\r\n\r\ndon't know if we should allow this feature in the middleware config which itself is bound to a route if it already works\r\n\r\nf.e.\r\n```go\r\napp.Get(\"/reports\", timeout.New(handler, timeout.Config{\r\n    Timeout: 5 * time.Second,\r\n    Routes: map[string]time.Duration{\r\n        \"/reports\": 30 * time.Second,\r\n    },\r\n}))\r\n```\r\nwhat added value does the timeout value have if the actual value is in the routes map\r\n\r\n\r\nsame with the skip feature\r\nfor this we have the skip middleware \r\nhttps://docs.gofiber.io/next/middleware/skip",
        "would like to separate route and middleware logic\r\nthat didn't work well with the health middleware back then and caused countless bug reports\r\n\r\nwhere the route is defined, the timeout middleware can be defined with a specific time or globally\r\n\r\nyou can specify a route in app.use and also operate handler chaining\r\n\r\n```go\r\napp.Get(\"/api/reports\", timeout.NewWithConfig(timeout.Config{\r\n    Timeout: 30 * time.Second,\r\n}), myHanlderLogic)\r\n```\r\n\r\nor \r\n\r\n```go\r\napp.Use(\"/api/reports\", timeout.NewWithConfig(timeout.Config{\r\n    Timeout: 30 * time.Second,\r\n})\r\napp.Get(\"/api/reports\", myHanlderLogic)\r\n```\r\n\r\nor\r\n\r\n```go\r\napp.Route(\"/api/reports\").Use(timeout.NewWithConfig(timeout.Config{\r\n    Timeout: 30 * time.Second,\r\n}).Get(myHanlderLogic)\r\n```",
        "the one with the routes will not work well, people will then think they can also define dynamic routes not just completely static ones like `“/api/user/:userId”`",
        "remove ParamsInt documentation and function",
        "the aim was to replace these functions with the generics\r\nso let's stick to it "
      ],
      "fiber-prefer-existing-apis": [
        "can we have an interface with the basic functions of a binder instead of any\r\n\r\n`Name`, `Reset`, `Bind` if possible",
        "maybe 2 different binders, then you could make a unified binder interface and the formMultipart binder could have a `Bind` method",
        "why not do something like this ?\r\n\r\n```go\r\nfunc (a *Agent) CustomJSON(v interface{}, ctype string) *Agent {\r\n     a.JSON(v)\r\n     a.req.Header.SetContentType(ctype)\r\n     return a\r\n}\r\n```\r\n",
        "why not do something like this ?\r\n\r\n```go\r\nfunc (c *Ctx) CustomJSON(data interface{}, ctype string) error {\r\n    err := c.JSON(data)\r\n    if err != nil {\r\n\treturn err\r\n    }\r\n    c.fasthttp.Response.Header.SetContentType(ctype)\r\n    return nil\r\n}\r\n```",
        "can you also test a signature with variadic params\r\n\r\n```go\r\nfunc (c *Ctx) JSON(data interface{}, ctype ...string) error {\r\n    raw, err := c.app.config.JSONEncoder(data)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    c.fasthttp.Response.SetBodyRaw(raw)\r\n\t\r\n    if len(ctype) > 0 {\r\n        c.fasthttp.Response.Header.SetContentType(ctype[0])\r\n    } else {\r\n        c.fasthttp.Response.Header.SetContentType(MIMEApplicationJSON)\r\n    }\r\n    return nil\r\n}\r\n```\r\nif this does not affect the performance we prefer this",
        "@rhburt "
      ],
      "fiber-follow-naming-patterns": [
        "ok, then lets keep it"
      ],
      "fiber-validate-configuration-defaults": [
        "@LaptopCat what do you thing?",
        "yeah makes sense, lets have 10s as default\r\n@ksw2000 can you do this",
        "should be fine, check this https://github.com/gofiber/fiber/pull/2731#issuecomment-1823236098"
      ],
      "fiber-check-all-error-returns": [
        "add early error return, otherwise it do the rest "
      ],
      "fiber-minimize-memory-allocations": [
        "can we use syncPool to optimize the initialization?",
        "can you test this\r\n\r\nhttps://github.com/valyala/fasthttp/blob/f0865d4aabbbea51a81d56ab31a3de2dfc5a9b05/bytesconv.go#LL201C15-L201C15\r\n\r\nmaybe its faster",
        "Can you try https://github.com/gofiber/fiber/blob/master/utils/convert_s2b_new.go#L11\n\nUnsafeBytes method instead of the other convert before you use the fasthttp method "
      ],
      "fiber-use-context-for-configuration": [
        "can you use https://docs.gofiber.io/api/ctx#app\r\nin the examples \r\n\r\nthink in a flat file this state management makes no sense, only if you have a deep structure and separate server initialization and controller with handler where you no longer have the app\r\nthat's why we should structure the examples so that you recognize the need to use this feature\r\nmaybe simulate several files in the example itself",
        "```suggestion\r\n    app.Get(\"/config\", func(c fiber.Ctx) error {\r\n        config := map[string]any{\r\n            \"environment\": environment,\r\n            \"apiUrl\":      fiber.GetStateWithDefault(c.App().State(), \"apiUrl\", \"\"),\r\n            \"debug\":       fiber.GetStateWithDefault(c.App().State(), \"debug\", false),\r\n```",
        "```suggestion\r\n        rdb, ok := fiber.GetState[*redis.Client](c.App().State(), \"redis\")\r\n```",
        "```suggestion\r\n        rdb, ok := fiber.GetState[*redis.Client](c.App().State(), \"redis\")\r\n```"
      ],
      "fiber-ensure-comprehensive-test-coverage": [
        "@efectn can you add this test\r\n\r\n```go\r\n// Verify that no flash messages are processed (should be empty) \r\nif len(r.c.flashMessages) != 0 {\r\nt.Errorf(\"Expected no flash messages to be processed, but got %d\", len(r.c.flashMessages))\r\n}\r\n```\r\n\r\nmaybe you can use https://pkg.go.dev/github.com/stretchr/testify/assert#Len instead",
        "please add a second test where you inject another logger and show that it receives the data"
      ],
      "fiber-explicit-cicd-configuration": [
        "yeah would be better"
      ],
      "fiber-document-error-conditions-clearly": [
        "```suggestion\r\n:::warning\r\nAfter calling `Close`, any attempt to use the request or response may result in data races or undefined behavior. Ensure all processing is complete before closing.\r\n:::\r\n```",
        "```suggestion\r\n:::info\r\nIf any request hook returns an error, the request is interrupted and the error is returned immediately.\r\n:::\r\n```"
      ]
    },
    "profile": {
      "location": "Germany",
      "blog": "",
      "site_admin": false,
      "followers": 233,
      "following": 21
    }
  },
  "sindresorhus": {
    "repos": [
      "vadimdemedes/ink"
    ],
    "entries": [
      {
        "slug": "ink-avoid-any-type-usage",
        "title": "avoid `any` type usage"
      },
      {
        "slug": "ink-clear-api-interfaces",
        "title": "Clear API interfaces"
      },
      {
        "slug": "ink-consistent-code-formatting",
        "title": "Consistent code formatting"
      },
      {
        "slug": "ink-documentation-code-quality",
        "title": "Documentation code quality"
      },
      {
        "slug": "ink-documentation-quality-standards",
        "title": "Documentation quality standards"
      },
      {
        "slug": "ink-ensure-documentation-usability",
        "title": "Ensure documentation usability"
      },
      {
        "slug": "ink-explicit-type-checking",
        "title": "explicit type checking"
      },
      {
        "slug": "ink-fix-linting-root-causes",
        "title": "Fix linting root causes"
      },
      {
        "slug": "ink-optimize-iteration-patterns",
        "title": "optimize iteration patterns"
      },
      {
        "slug": "ink-packagejson-configuration-standards",
        "title": "Package.json configuration standards"
      },
      {
        "slug": "ink-prefer-idiomatic-patterns",
        "title": "prefer idiomatic patterns"
      },
      {
        "slug": "ink-provide-clear-documentation-context",
        "title": "Provide clear documentation context"
      },
      {
        "slug": "ink-react-interface-clarity",
        "title": "React interface clarity"
      },
      {
        "slug": "ink-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "ink-use-descriptive-parameter-names",
        "title": "Use descriptive parameter names"
      },
      {
        "slug": "ink-use-descriptive-variable-names",
        "title": "Use descriptive variable names"
      },
      {
        "slug": "ink-useeffect-for-measurements",
        "title": "useEffect for measurements"
      }
    ],
    "comments": {
      "ink-packagejson-configuration-standards": [
        "```suggestion\r\n\t\"exports\": {\r\n\t\t\"types\": \"./build/index.d.ts\",\r\n\t\t\"default\": \"./build/index.js\"\r\n\t},\r\n```",
        "I would prefer to have the CLI flags as package.json `babel` config properties instead where possible.",
        "Why are we even using Babel? We could just use the TypeScript compiler? TS supports class properties now.",
        "Why are you locking this dependency and others?"
      ],
      "ink-use-descriptive-parameter-names": [
        "```suggestion\r\n\tinputHandler: (input: string, metadata: InputMetadata) => void\r\n```",
        "`meta` can be confused as `meta` keys, which is something different.",
        "Or maybe just call it `key` as that's what it really is. Not worth optimizing for a potential future where you add other things there."
      ],
      "ink-react-interface-clarity": [
        "```suggestion\r\n\t * Assign an ID to this component, so it can be programmatically focused with `.focus(id)`.\r\n```"
      ],
      "ink-explicit-type-checking": [
        "Should it really be height 1 when empty?\r\n\r\n```suggestion\r\n\t\t\tconst outputHeight = output === '' ? 0 : output.split('\\n').length;\r\n```"
      ],
      "ink-optimize-iteration-patterns": [
        "```suggestion\r\n\t\t\t\tfor (let [index, line] of lines.entries()) {\r\n```"
      ],
      "ink-prefer-idiomatic-patterns": [
        "```suggestion\r\n\t\treturn () => {\r\n\t\t\tstdin.off('data', handleData);\r\n\t\t};\r\n```\r\n\r\nYou don't want to return the event emitter to the user, which `stdin.off` returns. That's why I only use inline arrow functions when I actually want to use the return value, not as a tiny syntactic shortcut.",
        "Seems like a switch statement would be better here.",
        "```js\r\nchar.repeat(max);\r\n```"
      ],
      "ink-useeffect-for-measurements": [
        "```suggestion\r\n**Note:** `measureElement()` returns correct results only after the initial render, when layout has been calculated. Until then, `width` and `height` equal to zero. It's recommended to call `measureElement()` in a `useEffect` hook, which fires after the component has rendered.\r\n```",
        "```suggestion\r\n**Note:** `measureElement()` returns correct results only after the initial render, when layout has been calculated.\r\n```"
      ],
      "ink-avoid-any-type-usage": [
        "Don't use `any`. Use `unknown`.",
        "Don't use `any`. Also applies to all the other places `any` is used. Try to use a more explicit type.",
        "Use `unknown`."
      ],
      "ink-use-descriptive-variable-names": [
        "Don't use one character variable names. Use descriptive names.",
        "Don't use abbreviations for variable names.\r\n```suggestion\r\n\t\t\t<Box unstable__transformChildren={(string: string) => `{${string}}`}>\r\n```",
        "```suggestion\r\n\t\t\t\tpreviousState.focusables.find(event => event?.id === id)?.id ||\r\n```",
        "Ah, yes."
      ],
      "ink-consistent-code-formatting": [
        "```suggestion\r\n\t * Set this property to `hidden` to hide content overflowing the box.\r\n\t *\r\n\t * @default 'visible'\r\n```",
        "```suggestion\r\nexport type Styles = PaddingStyles &\r\n\tMarginStyles &\r\n\tFlexStyles &\r\n\tDimensionStyles &\r\n\tPositionStyles &\r\n\tWrapTextStyles;\r\n```"
      ],
      "ink-documentation-code-quality": [
        "Yeah, I have caved and started to use `sh` too because of the copy feature."
      ],
      "ink-documentation-quality-standards": [
        "`@default` should only receive an actual TS value, not a sentence. Just use `Default: ...` instead.",
        "Include a usage example like in the readme. I would recommend just using all the text from the readme here. It makes it much easier to keep both in sync."
      ],
      "ink-ensure-documentation-usability": [
        "The example is not runnable. See the other examples.",
        "This needs to be documented."
      ],
      "ink-fix-linting-root-causes": [
        "Don't disable things that can be fixed. Here, the linting is correct.",
        "Can you open an issue on https://github.com/yannickcr/eslint-plugin-react if there isn't already one?",
        "```suggestion\r\n\treadonly justifyContent?:\r\n\t\t| 'flex-start'\r\n\t\t| 'center'\r\n\t\t| 'flex-end'\r\n\t\t| 'space-between'\r\n\t\t| 'space-around';\r\n```",
        "Definitely a bug in ESLint-plugin-react or our config for it. Most likely the former."
      ],
      "ink-provide-clear-documentation-context": [
        "Maybe include your use-case as it might not be clear to the user what they would use this capability for.",
        "I think you should spend some more time on your readme there. The point of this list is to show off Ink usage, and you don't even have a screenshot (or better yet, animated GIF) of how it looks."
      ],
      "ink-clear-api-interfaces": [
        "```suggestion\r\n\toptions?: {\r\n\t\tcolumns?: number\r\n\t}\r\n```",
        "```suggestion\r\n * Hook that calls the `inputHandler` callback with the input that the program received.\r\n```",
        "I would clarify when the inputHandler is called. It's not clear whether it's for each keypress, each line, or something else. Also mention the paste thing."
      ],
      "ink-use-descriptive-names": [
        "> previousCounter\r\n\r\n👍 ",
        "Just `Bar` is too ambiguous. `ProgressBar` would be better."
      ]
    },
    "profile": {
      "blog": "https://sindresorhus.com/apps",
      "twitter_username": "sindresorhus",
      "site_admin": false,
      "followers": 75259,
      "following": 31
    }
  },
  "micalevisk": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-avoid-testing-anti-patterns",
        "title": "Avoid testing anti-patterns"
      },
      {
        "slug": "nest-choose-meaningful-identifier-names",
        "title": "Choose meaningful identifier names"
      },
      {
        "slug": "nest-comprehensive-dependency-security-checks",
        "title": "Comprehensive dependency security checks"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "nest-document-configuration-behaviors",
        "title": "Document configuration behaviors"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-modern-null-safety-patterns",
        "title": "Modern null safety patterns"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-proactive-dependency-security",
        "title": "Proactive dependency security"
      },
      {
        "slug": "nest-proper-asynchronous-error-handling",
        "title": "Proper asynchronous error handling"
      },
      {
        "slug": "nest-standardize-logger-configuration-patterns",
        "title": "Standardize logger configuration patterns"
      },
      {
        "slug": "nest-standardize-null-safety-patterns",
        "title": "Standardize null safety patterns"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      }
    ],
    "comments": {
      "nest-configurable-log-formatting": [
        "I think having this interface wouldn't be that easy to write the output that that Issue need because `pidMessage` is too tied with the default formatting and uses that `color` function\r\n\r\nCan you try to rewrite this `formatMessage` like this:\r\n\r\n```ts\r\nprotected formatMessage(\r\n  pid: number,\r\n  logLevel: string,\r\n  context: string,\r\n  timestampDiff: number,\r\n  output: string,\r\n): string {\r\n  return `` ...\r\n}\r\n```\r\n\r\nbut then we'll need to change the `updateAndGetTimestampDiff` method to extract the coloring stuff from it and make it return a number instead of string.\r\n\r\nAnd then make `formatMessage` return an string with the color applied instead of applying it on `printMessages`. But yeah, that could be a bit harsh\r\n"
      ],
      "nest-document-configuration-behaviors": [
        "@IlliaHalchun you can find the docs here: https://docs.nestjs.com/techniques/caching#use-module-globally\r\n\r\nfeel free do change it if needed",
        "```suggestion\r\n   * Defines if file parameter is optional.\r\n   * @default false\r\n   */\r\n```",
        "we can't use `@default` tag here because `KafkaOptions['options']` is being used by both client and server"
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "```suggestion\r\n      await expect(catsController.findAll()).resolves.toStrictEqual([]);\r\n```\r\n\r\ncan we use this instead? https://jestjs.io/docs/tutorial-async#asyncawait",
        "```suggestion\r\n```\r\n\r\nif we're testing `CatsService#findAll`, we shouldn't mock its implementation, otherwise we end up testing nothing.\r\n\r\nInstead, you somehow should do `catService.cats = result`. I guess it's fine do write it like this:\r\n\r\n```ts\r\n// @ts-ignore\r\ncatService.cats = result\r\n```\r\n"
      ],
      "nest-modern-null-safety-patterns": [
        "```suggestion\r\n    return pattern?.test(str);\r\n```",
        "I guess this would be better for readability:\r\n\r\n```suggestion\r\n    if (!isNil(this.min) && float < this.min) {\r\n```\r\n\r\n`import { isNil } from '../utils/shared.utils';`"
      ],
      "nest-follow-protocol-standards": [
        "Note that since the return of `response.getHeader('Content-Type')` depends on the arguments provided to `response#setHeader` then this condition could be mislead(?). For instance, if we pass any falsy value to `response.setHeader` in some controller's method, like `response.setHeader('Content-Type', '')`, we'll not receive that content type.\r\n\r\nI know this scenario is weird but since I _(as a Nest user)_ have explicity called `response.setHeader` in my code, it would be strange to receive the default value instead. What do you think?",
        "I was thinking more in HTTP client usage and where the content type value is defined dynamically (or sort of). Instead of identifying quickly, by looking into the headers sent, that the value was wrong for something that you've implemented, the dev will see another value -- maybe this could be documented, idk.\r\n\r\nBut I agree with you now. Since we're in dealing with a framework is better to apply some restrictions. ty!",
        "oh I just read how fastify handles that here: https://github.com/fastify/fastify/blob/7e18edcf76fb58dc33b842b1dba14a425dd6feba/lib/reply.js#L136-L142 looks like they **do allow** falsy values.\r\n\r\nSo the `AbstractHttpAdapter#reply` will not behave in the same way for both adapters in those edge cases. Do you guys think this could be an issue somehow? "
      ],
      "nest-avoid-testing-anti-patterns": [
        "```suggestion\r\n```\r\n\r\nif we're testing `CatsService#findAll`, we shouldn't mock its implementation, otherwise we end up testing nothing.\r\n\r\nInstead, you somehow should do `catService.cats = result`. I guess it's fine do write it like this:\r\n\r\n```ts\r\n// @ts-ignore\r\ncatService.cats = result\r\n```\r\n"
      ],
      "nest-proper-asynchronous-error-handling": [
        "```suggestion\r\n        body.getStream().once('error', (err: Error) => {\r\n```\r\n\r\nto prevent the error 'Cannot set headers after they are sent to the client' if for whatever reason the _error_ event is emitted multiple times (not sure if this is possible tho)"
      ],
      "nest-use-consistent-control-structures": [
        "I prefer your way tbh. I thought `npm run lint:fix` would fix that :p"
      ],
      "nest-pin-dependency-versions": [
        "nothing much https://github.com/actions/checkout/compare/v2...v3\r\n"
      ],
      "nest-http-header-management": [
        "Note that since the return of `response.getHeader('Content-Type')` depends on the arguments provided to `response#setHeader` then this condition could be mislead(?). For instance, if we pass any falsy value to `response.setHeader` in some controller's method, like `response.setHeader('Content-Type', '')`, we'll not receive that content type.\r\n\r\nI know this scenario is weird but since I _(as a Nest user)_ have explicity called `response.setHeader` in my code, it would be strange to receive the default value instead. What do you think?",
        "I was thinking more in HTTP client usage and where the content type value is defined dynamically (or sort of). Instead of identifying quickly, by looking into the headers sent, that the value was wrong for something that you've implemented, the dev will see another value -- maybe this could be documented, idk.\r\n\r\nBut I agree with you now. Since we're in dealing with a framework is better to apply some restrictions. ty!",
        "oh I just read how fastify handles that here: https://github.com/fastify/fastify/blob/7e18edcf76fb58dc33b842b1dba14a425dd6feba/lib/reply.js#L136-L142 looks like they **do allow** falsy values.\r\n\r\nSo the `AbstractHttpAdapter#reply` will not behave in the same way for both adapters in those edge cases. Do you guys think this could be an issue somehow? "
      ],
      "nest-use-consistent-curly-braces": [
        "I prefer your way tbh. I thought `npm run lint:fix` would fix that :p"
      ],
      "nest-choose-meaningful-identifier-names": [
        "I prefer the `RouteSchema` name (`route-schema.decorator.ts`). Kinda following the same convention as the others decorators ",
        "what do you think on renaming `value` to `fileOrFiles` or `filesOrFile`",
        "oh right.\r\n\r\nCan we have `shouldFlushLogsOnOverride: boolean`, and `flushLogsOnOverride(): void` method instead? Otherwise I'll add `setFlushLogsOnOverride(value: boolean)`\r\n"
      ],
      "nest-use-factory-providers": [
        "I notice another good side-effect on changing `ExternalContextCreator` and `SerializedGraph` providers to factory:\r\n\r\n`SerializedGraph#toJSON` was called 6x in a very simple nestjs app before these changes, which I think it was useless because it was only invoked due to the name `toJSON` being known as a special method for `JSON.stringify` (used by `stringify` from `fast-safe-stringify`)\r\n\r\n:partying_face: ",
        "I guess we can also suggest them to use factory providers over value providers",
        "and I'm not sure if the word _object_ here would help the end user. But yeah, this is not a log to the app, it's an internal one"
      ],
      "nest-comprehensive-dependency-security-checks": [
        "is there any link where we can see that? because there were no reports on `npm audit` \r\n\r\n![image](https://github.com/user-attachments/assets/2243ed54-2069-4f3c-89b2-68160da1e913)\r\n"
      ],
      "nest-standardize-null-safety-patterns": [
        "```suggestion\r\n    return pattern?.test(str);\r\n```",
        "I guess this would be better for readability:\r\n\r\n```suggestion\r\n    if (!isNil(this.min) && float < this.min) {\r\n```\r\n\r\n`import { isNil } from '../utils/shared.utils';`"
      ],
      "nest-descriptive-identifier-names": [
        "to me, `isEmpty` sounds that it could be used on non-array values. But I saw that it is only being used on arrays\r\n\r\npeharps we could rename this utility to `isEmptyArray` and drop the following:\r\n\r\nhttps://github.com/nestjs/nest/blob/e1b91d02a601c03cb8d0438b32badfaae5403447/packages/common/pipes/file/parse-file.pipe.ts#L63",
        "what do you think on renaming `value` to `fileOrFiles` or `filesOrFile`",
        "oh right.\r\n\r\nCan we have `shouldFlushLogsOnOverride: boolean`, and `flushLogsOnOverride(): void` method instead? Otherwise I'll add `setFlushLogsOnOverride(value: boolean)`\r\n"
      ],
      "nest-explicit-default-configurations": [
        "```suggestion\r\n    this.rawOutputPackets = this.getOptionsProp(options, 'rawOutputPackets', false);\r\n```\r\n\r\nhttps://github.com/nestjs/nest/blob/8617ee9952f4961841c8609329de9627cd8087f9/packages/microservices/server/server.ts#L146-L151",
        "```suggestion\r\n   * Defines if file parameter is optional.\r\n   * @default false\r\n   */\r\n```",
        "we can't use `@default` tag here because `KafkaOptions['options']` is being used by both client and server"
      ],
      "nest-proactive-dependency-security": [
        "is there any link where we can see that? because there were no reports on `npm audit` \r\n\r\n![image](https://github.com/user-attachments/assets/2243ed54-2069-4f3c-89b2-68160da1e913)\r\n"
      ],
      "nest-standardize-logger-configuration-patterns": [
        "instead of `console.error` we could use REPL's logger that was supplied to `NestFactory.createApplicationContext` above",
        "I think having this interface wouldn't be that easy to write the output that that Issue need because `pidMessage` is too tied with the default formatting and uses that `color` function\r\n\r\nCan you try to rewrite this `formatMessage` like this:\r\n\r\n```ts\r\nprotected formatMessage(\r\n  pid: number,\r\n  logLevel: string,\r\n  context: string,\r\n  timestampDiff: number,\r\n  output: string,\r\n): string {\r\n  return `` ...\r\n}\r\n```\r\n\r\nbut then we'll need to change the `updateAndGetTimestampDiff` method to extract the coloring stuff from it and make it return a number instead of string.\r\n\r\nAnd then make `formatMessage` return an string with the color applied instead of applying it on `printMessages`. But yeah, that could be a bit harsh\r\n"
      ]
    },
    "profile": {
      "location": "Manaus, Amazonas (Brazil)",
      "company": "Sr. Solutions Architect @TarkenAg",
      "blog": "https://www.linkedin.com/in/micalevisk",
      "site_admin": false,
      "followers": 337,
      "following": 54
    }
  },
  "martin-brennan": {
    "repos": [
      "discourse/discourse"
    ],
    "entries": [
      {
        "slug": "discourse-api-parameter-handling",
        "title": "API parameter handling"
      },
      {
        "slug": "discourse-avoid-n1-queries",
        "title": "Avoid N+1 queries"
      },
      {
        "slug": "discourse-consistent-null-safety-patterns",
        "title": "Consistent null safety patterns"
      },
      {
        "slug": "discourse-css-modifier-naming",
        "title": "CSS modifier naming"
      },
      {
        "slug": "discourse-extract-duplicate-code",
        "title": "Extract duplicate code"
      },
      {
        "slug": "discourse-follow-established-naming-conventions",
        "title": "Follow established naming conventions"
      },
      {
        "slug": "discourse-improve-code-readability",
        "title": "Improve code readability"
      },
      {
        "slug": "discourse-include-contextual-error-information",
        "title": "Include contextual error information"
      },
      {
        "slug": "discourse-include-contextual-log-information",
        "title": "Include contextual log information"
      },
      {
        "slug": "discourse-optimize-test-fixtures",
        "title": "optimize test fixtures"
      },
      {
        "slug": "discourse-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "discourse-self-documenting-identifiers",
        "title": "Self-documenting identifiers"
      },
      {
        "slug": "discourse-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "discourse-use-dynamic-configuration-access",
        "title": "Use dynamic configuration access"
      },
      {
        "slug": "discourse-use-guardian-authorization-classes",
        "title": "use Guardian authorization classes"
      },
      {
        "slug": "discourse-use-modern-null-handling",
        "title": "Use modern null handling"
      },
      {
        "slug": "discourse-use-modern-syntax",
        "title": "Use modern syntax"
      },
      {
        "slug": "discourse-validate-configuration-dependencies",
        "title": "Validate configuration dependencies"
      }
    ],
    "comments": {
      "discourse-prevent-async-race-conditions": [
        "I'm not sure why this request ID stuff is needed, the debounce should handle this, or at least that was my understanding of it",
        "I'm just not sure because I haven't seen this pattern elsewhere in the codebase. I guess leave it for this PR, and we can ask CVX for a followup...I'm pretty sure the best way would be to use `AbortController` instead, and pass an instance of it to the suggester"
      ],
      "discourse-avoid-n1-queries": [
        "IK it's probably unlikely to be a lot of these, but this will cause an N1. Maybe we can do it in a single query like this?\r\n\r\n```ruby\r\nresults = DB.query(<<~SQL, names: names)\r\n  SELECT 'user' AS type, id\r\n  FROM users\r\n  WHERE username IN (:names)\r\n  UNION ALL\r\n  SELECT 'group' AS type, id\r\n  FROM groups\r\n  WHERE name IN (:names)\r\nSQL\r\n\r\nresults.each do |row|\r\n  if row.type == \"user\"\r\n    user_ids << row.id\r\n  else\r\n    group_ids << row.id\r\n  end\r\nend\r\n```"
      ],
      "discourse-include-contextual-log-information": [
        "```suggestion\n          Rails.logger.info(\"Completed video conversion for upload ID #{upload.id} and job ID #{args[:job_id]}\")\n```",
        "```suggestion\n          Rails.logger.error(\"Upload #{upload.id} URL remained blank after #{MAX_RETRIES} retries when optimizing video\")\n```"
      ],
      "discourse-use-modern-syntax": [
        "```suggestion\r\n  #buildEventObject(from, to) {\r\n```\r\n\r\nShould use native private, dev-xp wants this now not the fake `_` one",
        "```suggestion\r\n  #buildStandaloneEvent(detail) {\r\n```\r\n\r\nShould use native private, dev-xp wants this now not the fake `_` one",
        "In JS land you can now do this fanciness like ruby :) \r\n\r\n```suggestion\r\n    const lastWord = words.at(-1).toLowerCase();\r\n```"
      ],
      "discourse-css-modifier-naming": [
        "```suggestion\r\n      &.--selected {\r\n```\r\n\r\nModifiers need to have the `--`"
      ],
      "discourse-consistent-null-safety-patterns": [
        "```suggestion\r\n    const crossAxis = options.crossAxisShift ?? true;\r\n```",
        "Or even just move this down to the `shift` now",
        "We do some initial `response?.` safe navigation here but then not below, could we just return if response is null?",
        "OK all good"
      ],
      "discourse-include-contextual-error-information": [
        "```suggestion\n          Rails.logger.error(\"Failed to handle video conversion completion for upload ID #{upload.id} and job ID #{args[:job_id]}\")\n```",
        "Should we requeue some amount of times on error? Or will it never pass if this happens?",
        "```suggestion\n        \"Failed to create optimized video for upload ID #{upload.id}: #{optimized_video.errors.full_messages.join(\", \")}\",\n```",
        "Is it possible this could fail with an error? If so we should handle it.\n\nIt would be good if we could do this as soon as you enabled the site setting, but I don't think we have a good mechanism for that 🤔 Otherwise this could silently fail and the conversion would never work, and admins wouldn't know.\n\nOne possible thing to do is  make an admin problem check that checks if this exists with the current settings, and if not tell admins they have to fix up the keys/permissions and so on"
      ],
      "discourse-improve-code-readability": [
        "Think we should bring back `MAX_RESULTS` const for the 20 then it can be used throughout this file and in the tests"
      ],
      "discourse-use-modern-null-handling": [
        "```suggestion\r\n      return d.hours() || d.minutes() || d.seconds();\r\n```\r\n\r\nIf you don't want to do this fine, but I think you could do this since if any of them are not 0 this will be true...would be nice if moment had a `hasTime()` function or something",
        "```suggestion\r\n    const text = detail.message.split(\"\\n\").filter(Boolean);\r\n```\r\n\r\n?",
        "```suggestion\r\n    return this.args.onChange || () => {};\r\n```",
        "```suggestion\r\n    value ??= \"\"\r\n```"
      ],
      "discourse-validate-configuration-dependencies": [
        "We should also note here that `discourse_reactions_enabled_reactions` is no longer used when this is enabled.",
        "Also will confirm with product team internally.",
        "Ah yes I see; IMO it's still a bit misleading though without extra explanation for admins...hold tight on this though still discussing internally",
        "IMO this should be a `enum` type setting, with only a single option of `aws_mediaconvert`:\n\nhttps://github.com/discourse/discourse/blob/3f666262cda2c269b11aa846d5cf7bdd453274ec/config/site_settings.yml#L207-L213\n\nI think we should unhide it as well, since the mediaconvert role ARN one is unhidden.",
        "We need some site setting validation for this.\n\nIf it is set to true, and `video_conversion_service` is `aws_mediaconvert`, and `mediaconvert_role_arn` is blank, we should show an error.",
        "Oh also needs to validate that S3 access key and secret key are present"
      ],
      "discourse-self-documenting-identifiers": [
        "```suggestion\r\n          label: Allow editing options after posting (dynamic poll)\r\n```\r\n\r\nI think the second half is more relevant to people than the \"dynamic\" wording"
      ],
      "discourse-extract-duplicate-code": [
        "I think this `values.each` code could be split out into a `calculate_all_or_any_scope` method or something, since it does the exact same thing for groups and users",
        "We are repeating this same poll Hash/JSON boilerplate here...can we add something like `Poll.dynamic_map(post)` that just gives the right thing for here and the `PollUpdater`?",
        "Wouldn't this not get hit if `!valid_settings?` when calling `convert`?\n\nAlso this does feel a little repetitive, can we just declare the `client ` once with ` Aws::MediaConvert::Client.new`, then after the endpoint is fetched can we do something like `client.endpoint = SiteSetting.mediaconvert_endpoint` ? Or is that not possible?\n\nIf not then maybe a `build_client` method called from here would cut down on repetition:\n\n```ruby\nbuild_client(endpoint: nil)\n  Aws::MediaConvert::Client.new(\n    region: SiteSetting.s3_region,\n     credentials:\n              Aws::Credentials.new(SiteSetting.s3_access_key_id, SiteSetting.s3_secret_access_key),\n      endpoint: endpoint\n  )\nend\n```",
        "These lines are repeated a ton in this test, maybe could move them into `prepare_image_node` method?"
      ],
      "discourse-use-dynamic-configuration-access": [
        "You should look up this order configuration with the `site` service:\r\n\r\nhttps://github.com/discourse/discourse/blob/1c97488d207c68f70c67a1adaf16af22b639114b/app/assets/javascripts/discourse/app/components/composer-editor.gjs#L507-L507",
        "Instead of hardcoding this chat stuff in the client, you should update `ChannelHashtagDataSource` to set the `style_type` to `icon`\r\n\r\nhttps://github.com/discourse/discourse/blob/1c97488d207c68f70c67a1adaf16af22b639114b/plugins/chat/lib/chat/channel_hashtag_data_source.rb#L26-L26",
        "IMO it's no big deal to just do it in the same PR :) It won't affect any other hashtag stuff by adding it, and the chat data source reflects reality better too. You could also set `style_type` for the tag hastag data source in the same way"
      ],
      "discourse-api-parameter-handling": [
        "I think you need to tweak this VALID_HASHTAGS storage a bit to handle hashtags of multiple different contexts. A context is the topic composer or the chat composer, in topic composer you can do `#general` by itself and it will refer to the category, in chat you can do `#general` by itself and it will refer to the channel. In the old composer, you can do this:\r\n\r\n![image](https://github.com/user-attachments/assets/62896ee2-c7c5-4505-b270-83606c952e1a)\r\n\r\nBut in the new one we get this:\r\n\r\n![image](https://github.com/user-attachments/assets/2aac9efc-6ef5-4a84-b084-ad53a0b03b17)\r\n\r\nHashtags in the format `general::channel` are called `refs`, since they contain the type and the slug of the hashtag. In the old `hashtag-cooked` elements in the composer we store both the slug and the type of the hashtag:\r\n\r\n```\r\n<a class=\"hashtag-cooked\" href=\"/chat/c/dev/300\" data-type=\"channel\" data-id=\"300\" data-slug=\"dev\" data-style-type=\"null\" tabindex=\"-1\"><svg class=\"fa d-icon d-icon-comment svg-icon hashtag-color--channel-300 svg-string\" aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\"><use href=\"#comment\"></use></svg><span>dev</span></a>\r\n```\r\n\r\nSo I think for `VALID_HASHTAGS` you will need to do the same and store the name + type, and make sure these specific refs can be rendered correctly."
      ],
      "discourse-use-guardian-authorization-classes": [
        "```suggestion\r\n- Always use Guardian classes for authorization checks, the Guardian class defined in lib/guardian.rb\r\n- There are other Guardian classes defined in lib/guardian\r\n```"
      ],
      "discourse-optimize-test-fixtures": [
        "Ah yes thanks will fix it up quickly"
      ],
      "discourse-follow-established-naming-conventions": [
        "```suggestion\r\n  #buildGroupedEvents(detail) {\r\n```\r\n\r\nShould use native private, dev-xp wants this now not the fake `_` one",
        "```suggestion\r\n  #modifyDatesForTimezoneOffset(from, to, timezoneOffset) {\r\n```\r\n\r\nShould use native private, dev-xp wants this now not the fake `_` one",
        "```suggestion\r\n  #findAverageTimezone(eventTimezones) {\r\n```\r\n\r\nShould use native private, dev-xp wants this now not the fake `_` one"
      ],
      "discourse-use-descriptive-identifiers": [
        "Using the `?` placeholders and arrays for the params feels like of dangerous here...if things were rearranged a bit this could introduce some subtle bugs. IMO change this and the above to use `:group_ids` and `:user_ids` to make this clearer and more intentional"
      ]
    },
    "profile": {
      "location": "Brisbane, Queensland",
      "company": "@discourse ",
      "blog": "http://writing.martin-brennan.com",
      "site_admin": false,
      "followers": 29,
      "following": 1
    }
  },
  "Viicos": {
    "repos": [
      "langgenius/dify",
      "pydantic/pydantic"
    ],
    "entries": [
      {
        "slug": "dify-safe-null-handling",
        "title": "Safe null handling"
      },
      {
        "slug": "pydantic-avoid-shared-structure-mutation",
        "title": "Avoid shared structure mutation"
      },
      {
        "slug": "pydantic-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "pydantic-balance-documentation-thoroughness",
        "title": "Balance documentation thoroughness"
      },
      {
        "slug": "pydantic-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "pydantic-categorize-error-types",
        "title": "Categorize error types"
      },
      {
        "slug": "pydantic-consistent-configuration-patterns",
        "title": "Consistent configuration patterns"
      },
      {
        "slug": "pydantic-data-structure-correctness",
        "title": "Data structure correctness"
      },
      {
        "slug": "pydantic-document-configuration-relationships",
        "title": "Document configuration relationships"
      },
      {
        "slug": "pydantic-documentation-formatting-standards",
        "title": "Documentation formatting standards"
      },
      {
        "slug": "pydantic-eliminate-redundant-computation",
        "title": "Eliminate redundant computation"
      },
      {
        "slug": "pydantic-enforce-style-with-linters",
        "title": "Enforce style with linters"
      },
      {
        "slug": "pydantic-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "pydantic-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "pydantic-preserve-language-conventions",
        "title": "Preserve language conventions"
      },
      {
        "slug": "pydantic-robust-error-messaging",
        "title": "Robust error messaging"
      },
      {
        "slug": "pydantic-safe-attribute-access-pattern",
        "title": "Safe attribute access pattern"
      },
      {
        "slug": "pydantic-semantic-over-syntactic",
        "title": "Semantic over syntactic"
      },
      {
        "slug": "pydantic-simple-defaults-flexible-overrides",
        "title": "Simple defaults, flexible overrides"
      },
      {
        "slug": "pydantic-specific-types-for-performance",
        "title": "Specific types for performance"
      },
      {
        "slug": "pydantic-standardize-dependency-management",
        "title": "Standardize dependency management"
      },
      {
        "slug": "pydantic-structured-configuration-management",
        "title": "Structured configuration management"
      },
      {
        "slug": "pydantic-write-targeted-specific-tests",
        "title": "Write targeted, specific tests"
      }
    ],
    "comments": {
      "pydantic-cache-expensive-computations": [
        "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."
      ],
      "pydantic-data-structure-correctness": [
        "```suggestion\r\nThis error is raised when an unhashable value is validated against a [`set`][] or a [`frozenset`][]```",
        "Actually this example is invalid from a type checking perspective, and it does makes sense in some way. I think the logic applied is \"can we end up with a stop when trying to match a value to the type?\".\r\n\r\nFor instance, with\r\n\r\n```python\r\ntype B = list[C]\r\ntype C = B | None\r\n```\r\n\r\nthe value `[None, [[None]], []]` successfully matches and we came to stop, thanks to the `| None` part. without it, there's no such value that can match the type alias without dealing with infinite recursion (hence my suggestion above).\r\n\r\nLet's go with:\r\n\r\n```suggestion\r\nFor example, this is a valid type alias:\r\n\r\n```py test=\"skip\" lint=\"skip\" upgrade=\"skip\"\r\ntype A = list[A] | None\r\n```\r\n```"
      ],
      "pydantic-simple-defaults-flexible-overrides": [
        "This is for validated function calls (see `test_unsupported_field_attribute_nested_with_function()`). We don't want to raise a warning for:\n\n```python\n@validate_call\ndef func(a: Annotated[int, Field(alias='b')]): ...\n```",
        "```suggestion\r\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch', 'clickhouse+http', 'clickhouse', 'clickhouses', 'clickhousedb'],\r\n```"
      ],
      "pydantic-maintain-code-consistency": [
        "Both are equivalent, but indeed passing all the metadata directly to `_construct()` is cleaner. Applied.",
        "```suggestion\r\n    def __init_subclass__(cls) -> None:\r\n```\r\nWe can safely omit the kwargs here",
        "I inlined the logic as I couldn't find a good way to keep it in a single method as I need to raise it differently in `__delattr__`. A bit unfortunate, but at least this removes the `_check_frozen` method on the `BaseModel` class, so it avoids polluting the namespace.",
        "Why was it moved?"
      ],
      "pydantic-structured-configuration-management": [
        "The Ruff target version needs to be updated as well (`target-version = 'py39'`), I believe you'll then get new errors asking to update `typing.*` to `collections.abc.*` (e.g. for `Iterable`, etc).",
        "> Move dependency-groups section under the project one, as it is part of the [packaging specifications](https://packaging.python.org/en/latest/specifications/)."
      ],
      "pydantic-safe-attribute-access-pattern": [
        "This check did not really make sense in this `get_function_type_hints()`, but rather in `get_callable_return_type()` (this PR moved the check in this one), which has a smaller scope (i.e. analyzing callables used for validators/serializers functions)",
        "```suggestion\r\n    if getattr(email_validator, '__version__', '').partition('.')[0] == '2':\r\n```\r\nWith `''` as a default value, `''.partition('.')` returns `('', '', '')`",
        "Yes, what I meant is that the `hasattr` check is not necessary because of the `getattr` fallback to `''`. But I think it's fine to keep it for clarity"
      ],
      "pydantic-balance-documentation-thoroughness": [
        "Probably for the serialization docs rewrite, but I don't think we should have this workaround documented. Having models serialized as something else than `dict` is uncommon, and the proposed solution isn't future proof (like in this case, where we make changes to the signature, this would break type checking for users using this workaround).",
        "I mention it in:\r\n\r\n> - `'allow'`: Providing extra data is allowed and stored in the `__pydantic_extra__` dictionary attribute.\r\n  The `__pydantic_extra__` can explicitly be annotated to provide validation for extra fields.\r\n\r\nI was trying to avoid duplication of documentation, and so the `__pydantic_extra__` example is present in the API docs. Maybe a broader discussion could be _what should live in the concepts/API docs_",
        "Can we simplify this section and the following one with a single _Similarly to Pydantic models, nested dataclasses and generics are supported_ (and we refer to the relevant model documentation)? Imo the added value is quite low here, we would expect the examples to work anyway",
        "They can just click on the `[init_typed](#init_typed)` reference, where I added a description for the setting (and same for others). Having things centralized avoids duplication, and having a heading makes it easier to link to when responding on github issues/discussions etc\r\n\r\nTo be clear, the explanation is not removed, just centralized in the configuration options section"
      ],
      "pydantic-robust-error-messaging": [
        "Yes I can probably move the check below, but I prefer enforcing the data to be provided as external users of this method don't know whether the default factory requires the argument or not, and I think it's best to unconditionally raise here (and enforcing the argument in the overload).",
        "You will have to add a `try..finally` block in case any exception happens, as otherwise the original function will have its attributes mutated.\r\n\r\nAlso, I'll have to think about it more, but are we certain that every callable that can be used with `validate_call` can have the `__qualname__`/`__annotations__` etc arguments mutated?",
        "```suggestion\r\n                       \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\r\n```\r\n\r\nI don't think mentioning \"this is a bug\" is correct, as users could really just call `get_default` in the wrong way.",
        "The `TypeAdapter[...]` form seems a bit weird, especially because `type_repr` seems to be `str(type)` when called from `TypeAdapter`. Probably fine as is, just wanted to note that this may lead to weird string representations (maybe `_display.display_as_type` could be used, although it is costly to compute)."
      ],
      "pydantic-document-configuration-relationships": [
        "This makes me think the literal pattern would really fit better here.. If having this boolean pattern on two configuration values only introduced the inconsistency when setting both `validate_by_alias=False, validate_by_name=False`, it would be fine (I don't see why users would do so), but I won't be surprised if many users find it counter-intuitive that you also need to set `validate_by_name=True` here.\r\n\r\nI think it's worth reconsidering, cc @samuelcolvin ",
        "Also, what should happen if you set `validate_by_alias=False`, but explicitly set `by_alias=True` or `by_name=True` during validation?",
        "Yes, as discussed on Slack, thanks for summing things up here, this might be useful as a reference in case we get questions about the current API.\r\n\r\nAs we discussed as well, defaulting `validate_by_name` to `True` if `validate_by_alias` is set to `False` is postponed after this PR, and should be tackled either before 2.11 or after. Leaving this conversation unresolved so that it's easier to find it later.",
        "I think you'll have to update the `collect_config()` logic to handle both `validate_by_name` and `populate_by_name`. Mypy does static analysis so it can't be aware of the added logic in `ConfigWrapper.core_config()`.\r\n\r\nAlso, I'm not sure if the plugin already handled this, but previously if `populate_by_name` was set, the following calls were allowed:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    field: int = Field(alias='alias')\r\n\r\n    model_config = {'populate_by_name': True}\r\n\r\nModel(field=1)  # OK\r\nModel(alias=1)  # OK\r\n```\r\n\r\nIf the plugin accepted both these calls, it will probably need to be updated to disallow the following:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    field: int = Field(alias='alias')\r\n\r\n    model_config = {'validate_by_alias': True, 'validate_by_name': False}\r\n\r\nModel(field=1)  # Type checker error\r\nModel(alias=1)  # OK\r\n```"
      ],
      "pydantic-avoid-shared-structure-mutation": [
        "This is related to what I mentioned:\r\n\r\n> the first one I could find: https://github.com/pydantic/pydantic/issues/7102. The https://github.com/pydantic/pydantic/issues/7102#issuecomment-1682288722 isn't really compelling, the user was doing things not the intended way. It also led to [a scary and hacky fix](https://github.com/pydantic/pydantic/pull/8066/files) that I would really like not having.\r\n\r\nThese removed lines had the effect of moving the ref from the inner schema of a `function-*` schema to the the `function-*` schema itself. With the following simplified test code added in the mentioned issue above:\r\n\r\n```python\r\nclass Numeric(BaseModel):\r\n    value: float\r\n\r\n    @classmethod\r\n    def __get_pydantic_core_schema__(cls, source_type, handler):\r\n        return core_schema.no_info_before_validator_function(cls.validate, handler(source_type))\r\n\r\n    @classmethod\r\n    def validate(cls, v):\r\n        ...\r\n\r\nclass OuterModel(BaseModel):\r\n    x: Numeric\r\n    y: Numeric\r\n```\r\n\r\nOn `main`, the schema of `OuterModel` would look like:\r\n\r\n<details>\r\n\r\n```python\r\n{\r\n│   'type': 'definitions',\r\n│   'schema': {\r\n│   │   'type': 'model',\r\n│   │   'cls': <class '__main__.OuterModel'>,\r\n│   │   'schema': {\r\n│   │   │   'type': 'model-fields',\r\n│   │   │   'fields': {\r\n│   │   │   │   'x': {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:109238506193520'}, 'metadata': {}},\r\n│   │   │   │   'y': {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:109238506193520'}, 'metadata': {}}\r\n│   │   │   },\r\n│   │   │   'model_name': 'OuterModel',\r\n│   │   │   'computed_fields': []\r\n│   │   },\r\n│   │   'config': {'title': 'OuterModel'},\r\n│   │   'ref': '__main__.OuterModel:109238503123056',\r\n│   │   'metadata': {'<stripped>'}\r\n│   },\r\n│   'definitions': [\r\n│   │   {\r\n│   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   'schema': {\r\n│   │   │   │   'type': 'model',\r\n│   │   │   │   'cls': <class '__main__.Numeric'>,\r\n│   │   │   │   'schema': {\r\n│   │   │   │   │   'type': 'model-fields',\r\n│   │   │   │   │   'fields': {'value': {'type': 'model-field', 'schema': {'type': 'float'}, 'metadata': {}}},\r\n│   │   │   │   │   'model_name': 'Numeric',\r\n│   │   │   │   │   'computed_fields': []\r\n│   │   │   │   },\r\n│   │   │   │   'config': {'title': 'Numeric'}\r\n│   │   │   },\r\n│   │   │   'ref': '__main__.Numeric:109238506193520',\r\n│   │   │   'metadata': {'<stripped>'},\r\n│   │   │   'type': 'function-before'\r\n│   │   }\r\n│   ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\nOn this PR, it looks like:\r\n\r\n<details>\r\n\r\n```python\r\n{\r\n│   'type': 'definitions',\r\n│   'schema': {\r\n│   │   'type': 'model',\r\n│   │   'cls': <class '__main__.OuterModel'>,\r\n│   │   'schema': {\r\n│   │   │   'type': 'model-fields',\r\n│   │   │   'fields': {\r\n│   │   │   │   'x': {\r\n│   │   │   │   │   'type': 'model-field',\r\n│   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   │   'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:105945921898336'},\r\n│   │   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   │   },\r\n│   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   },\r\n│   │   │   │   │   'metadata': {}\r\n│   │   │   │   },\r\n│   │   │   │   'y': {\r\n│   │   │   │   │   'type': 'model-field',\r\n│   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   │   'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:105945921898336'},\r\n│   │   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   │   },\r\n│   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   },\r\n│   │   │   │   │   'metadata': {}\r\n│   │   │   │   }\r\n│   │   │   },\r\n│   │   │   'model_name': 'OuterModel',\r\n│   │   │   'computed_fields': []\r\n│   │   },\r\n│   │   'config': {'title': 'OuterModel'},\r\n│   │   'ref': '__main__.OuterModel:105945922827312',\r\n│   │   'metadata': {'<stripped>'}\r\n│   },\r\n│   'definitions': [\r\n│   │   {\r\n│   │   │   'type': 'model',\r\n│   │   │   'cls': <class '__main__.Numeric'>,\r\n│   │   │   'schema': {\r\n│   │   │   │   'type': 'model-fields',\r\n│   │   │   │   'fields': {'value': {'type': 'model-field', 'schema': {'type': 'float'}, 'metadata': {}}},\r\n│   │   │   │   'model_name': 'Numeric',\r\n│   │   │   │   'computed_fields': []\r\n│   │   │   },\r\n│   │   │   'config': {'title': 'Numeric'},\r\n│   │   │   'ref': '__main__.Numeric:105945921898336'\r\n│   │   }\r\n│   ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\nEssentially, the difference in these two schemas is that we don't \"move\" the ref from the inner schema to the `function-before` schemas.\r\n\r\nThe changes in this PR + removing this reference moving coincidentally make it work still.\r\n\r\nHowever, doing so was a dangerous game: on _L793_, `schema` directly comes from another model. The `pop` calls removes the reference to the schema, and mutating schemas from other models has been a known issue. \r\n\r\nYou may be wondering: why this doesn't break things in the example I gave? Surely the `pop` call should have mutated the core schema of `Numeric`. Turns out it doesn't, because `Numeric.__get_pydantic_core_schema__` does not cache the schema, so calling it will generate a new one every time (and this is what happens during the schema gen of `OuterModel`). But on a similar issue, [I mentioned](https://github.com/pydantic/pydantic/issues/10160#issuecomment-2298257506) that explicitly caching the core schema in the `__get_pydantic_core_schema__` method would resolve the user issue (as the use case was slightly different)! \r\n\r\nSo to conclude, overriding `BaseModel.__get_pydantic_core_schema__` is full of unexpected behaviors, but that's fine as officially supporting them would be a huge pain.\r\n\r\n",
        "iirc (but I'm not sure), I was able to remove it only thanks to the other changes. This won't clutter the git diff though, because it's just a removal. Probably by having a proper commit description when merging, I can add a note about this?",
        "This was present inside `_generate_schema_from_property` before, but actually I think it should come first. Whenever you call `generate_schema`, if we pass in `typing(_extensions).Self`, we need to resolve the type before trying to build the schema.\r\n\r\nI moved it at the top of `GenerateSchema.generate_schema`",
        "Oops, seems like moving it breaks things, it needs to be right after the `__get_pydantic_core_schema__` check, so I'll leave it here"
      ],
      "pydantic-categorize-error-types": [
        "It would make sense to do so if these kind of errors happen at runtime, _after_ initial module imports and application setup (like validation errors, where it makes sense to know how to handle them).\r\n\r\nPydantic errors are just usage exceptions and it doesn't really make sense to try..catch on these ones.",
        "```suggestion\r\nWhile classes are callables themselves, `validate_call` can't be applied on them, as it needs to know about which method to use (`__init__` or `__new__`) to fetch type annotations. If you want to validate the constructor of a class, you should put `validate_call` on top of the appropriate method instead.\r\n```",
        "```suggestion\r\nAlthough you can create custom callable types in Python by implementing a `__call__` method, currently the instances of these types cannot be validated with `validate_call`. This may change in the future, but for now, you should use `validate_call` explicitly on `__call__` instead.\r\n```"
      ],
      "pydantic-preserve-language-conventions": [
        "You mean `instanciate_by_*` only take effect on direct instantiation (i.e. `Model(...)`)?\r\n\r\nThis would really complicate the API. Using `__init__` directly is better suited when you provide the arguments directly (e.g. `Model(a=1, b='test')`). In that case, the user can simply provide the aliases (and this is what static type checkers will enforce, we have no control over it).\r\n\r\nIf you want to validate data where you don't control the provided keys, then `model_validate()` is better suited anyway: `Model.model_validate({'a': 1, 'b': 'test'})`, and you can provide `by_name=True` there.",
        "> This is intuitive and aligned with dataclass and other frameworks in statically typed languages.\r\n\r\nDataclasses don't make use of aliases, but this is something supported by the `@dataclass_transform` spec, and as per the [fields specifiers](https://typing.readthedocs.io/en/latest/spec/dataclasses.html#field-specifier-parameters) section:\r\n\r\n> `alias` is an optional str parameter that provides an alternative name for the field. This alternative name is used in the synthesized `__init__` method.\r\n\r\nBut I get your point, `Model(SomeRandomValue=147, Env_Global='test')` feels weird in Python code. The fact that type checkers will enforce aliases in `__init__` is unfortunate though.\r\n\r\nThis merits a broader discussion, currently we don't have a proper distinction between direct instantiation (`__init__`) and the `model_validate(_*)` methods when it comes to validation behavior. ",
        "```suggestion\r\nwhich is available on the [`model_dump()`][pydantic.main.BaseModel.model_dump] and\r\n[`model_dump_json()`][pydantic.main.BaseModel.model_dump_json] methods, as well as\r\nthe [`TypeAdapter`][pydantic.type_adapter.TypeAdapter] ones.\r\n```"
      ],
      "pydantic-eliminate-redundant-computation": [
        "`is_generic_alias` does an `isinstance()` check against `typing._GenericAlias` (e.g. `List[int]` is an instance of such a class), which isn't documented and technically private (although I don't think it will change). So it's best to avoid relying on it.\r\n\r\nIt is also a footgun as while `is_generic_alias()` works for all parameterized typing objects, it doesn't check for new unions (`is_generic_alias(int | str) == False`, but `is_generic_alias(Union[int, str]) == True`). For instance, I'm not sure we expected new unions to be skipped here:\r\n\r\nhttps://github.com/pydantic/pydantic/blob/acb0f10fda1c78441e052c57b4288bc91431f852/pydantic/_internal/_core_utils.py#L66-L74\r\n\r\nSimilarly, I've used this function here as a way to check for `type[list[int]]` forms (here `type_param` is `list[int]`):\r\n\r\nhttps://github.com/pydantic/pydantic/blob/acb0f10fda1c78441e052c57b4288bc91431f852/pydantic/_internal/_generate_schema.py#L1711-L1715\r\n\r\nThis would also match `type[Union[int, str]]`, which we actually want to support! Thankfully there's a specific check for unions just before, but this could easily be missed.\r\n\r\n---\r\n\r\nI think there are still valid use cases where you want to check if something is a generic alias (and by that I don't mean `isinstance(obj, (types.GenericAlias, typing._GenericAlias)`, but if the `obj` is a parameterized generic class -- excluding unions, typing special forms like `Literal`, `Annotated`, etc), but it's probably best to rely on `get_origin()` and the `typing_objects` check functions.\r\n",
        "```suggestion\r\n            which may not be types and thus do not have a `__module__` available\r\n```\r\n\r\nmaybe? I think the most common example is `SomeType = list[...]`, and is more common that PEP 695 type aliases. I think it's best to emphasize on the fact that most objects passed to type adapters are _instances_ (e.g. `type A = int`, `A` is instance of a `TypeAliasType`).",
        "This `display_as_type` function needs to be refactored, as it is relatively expensive to recursively check for `get_origin`, `get_args`, etc. It is currently used:\r\n- In `FieldInfo.__repr_args__`, to make a string representation of the `annotation` attribute. This can be kept.\r\n- In `get_type_ref`, called for each arg of a parametrized type. We should find a simpler way to generate a core schema reference."
      ],
      "pydantic-specific-types-for-performance": [
        "This is added by me, as I think a common misconception is to use abstract containers as types thinking this will allow (in the case of `collections.abc.Sequence`) both lists and tuples to validate, while in fact this is already supported by Pydantic.",
        "I added the link. Regarding the second bullet point, not sure what you mean. Do you have a suggestion?"
      ],
      "pydantic-avoid-unnecessary-operations": [
        "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."
      ],
      "pydantic-explicit-over-implicit": [
        "Previously, this was a bit weird because even though we did not install any extra, `tzdata` (which is installed through the `timezone` extra) was still included in the `dev` dependency group. I've tried changing the pytest skip marker for these tests to check for the presence of the `tzdata` library but it's tricky as at the module level, some `ZoneInfo` instances are created so it still fails.\r\n\r\nIn the future, if we include new extras, we should change the name to `Test only with 'timezone' extra`",
        "So the thing is `uv` will prefer the specified version in the `.python-version` file if present, no matter the previously installed version. We currently don't have such a file, but it could be pretty bad if we end up creating one at some point, especially for the jobs with a Python version matrix: CI will only run on the version from `.python-version`, and we won't notice anything.\r\n\r\nIt's a bit unfortunate, probably using tox (and tox-uv) could help. "
      ],
      "pydantic-semantic-over-syntactic": [
        "I wanted to avoid having the name depending on the capitalization of the object, and simply have `is_<name>`, but both make sense I think",
        "```suggestion\r\n    def defined_constraints(self) -> dict[str, Any]:\r\n```\r\n\r\nmaybe? set_constraints can be confusing at first, feels like this is representing an action of _setting_ something"
      ],
      "pydantic-consistent-configuration-patterns": [
        "The benefit of using the class arguments is that it is recognized by type checkers. I think this is only relevant for `frozen`, so actually I'll change the example and add an annotation about it"
      ],
      "dify-safe-null-handling": [
        "`NonNegativeInt` is defined as an annotated form:\r\n\r\n```python\r\nNonNegativeInt = Annotated[int, annotated_types.Ge(0)]\r\n```\r\n\r\nSo in theory this shouldn't be necessary?"
      ],
      "pydantic-enforce-style-with-linters": [
        "Actually let's include `PIE790`, I think it is worth being included. I've checked `PIE804`, and I'm not sure why we get so many violations in the tests files, so fine to exclude it for now.",
        "Yes tried to find something without success, I'll look a bit more",
        "There's https://github.com/tox-dev/toml-fmt but way too much diff generated with our current file. For now let's just be careful when making edits to the pyproject.toml"
      ],
      "pydantic-standardize-dependency-management": [
        "```suggestion\r\n      - name: Install UV\r\n        uses: astral-sh/setup-uv@v5\r\n        with:\r\n            python-version: ${{ matrix.python-version }}\r\n```\r\n\r\nThe reason I had to use the setup-python action in the previous third-party test is because of the comment I added regarding the uv action. In normal circumstances (i.e. when the project isn't nested under a specific repository folder) you can just let uv setup the Python version.",
        "```suggestion\r\n    - uses: actions/setup-python@v5\r\n      with:\r\n        python-version: ${{ matrix.python-version }}\r\n```\r\n\r\nBest to be as close to the project's CI."
      ],
      "pydantic-documentation-formatting-standards": [
        "Was there a reason to change these? These examples are not tested (marked as `test=\"skip\"`) so I think this isn't right now, the actual comment showing the output is most likely different.",
        "Ah then let's unify all of them using `print(<inst>)` instead of `print(repr(<inst>))`",
        "Hum I still see the `repr()` in the examples?",
        "The added newline broke the rendering. Was it added by the linter? It seems the extra level of nesting below did not have the same newline added.\r\n\r\n| Before                                                                                    | After                                                                                     |\r\n|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\r\n| ![image](https://github.com/user-attachments/assets/643d5681-a822-48c8-85a9-445f25f16156) | ![image](https://github.com/user-attachments/assets/21435ae0-5eb9-47c6-9c4f-419e82811729) |",
        "These new lines are required, as otherwise the code block isn't assumed to be part of the list element.\r\n\r\n| Before                                                                                    | After                                                                                     |\r\n|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\r\n| ![image](https://github.com/user-attachments/assets/0a282db3-571a-4f0c-81a7-9b8cb74beb75) | ![image](https://github.com/user-attachments/assets/0011b415-a802-4311-869a-a5c00d9df459) |\r\n\r\n(it also breaks admonitions/annotations defined after the code block)\r\n\r\nSeems like indenting the code block with two spaces (and keeping the newline, which should make the linter happy) works, could we apply this pattern?\r\n"
      ],
      "pydantic-write-targeted-specific-tests": [
        "The test you added is unrelated to your change, which makes me believe your contribution is AI generated. If so, please state it explicitly in the PR description.\r\n\r\nYou can replace by the following test:\r\n\r\n```python\r\ndef test_private_attribute_not_skipped_during_ns_inspection() -> None:\r\n    # It is important for the enum name to start with the class name\r\n    # (it previously caused issues as we were comparing qualnames without\r\n    # taking this into account):\r\n    class Fullname(str, Enum):\r\n        pass\r\n\r\n    class Full(BaseModel):\r\n        _priv: object = Fullname\r\n\r\n    assert isinstance(Full._priv, ModelPrivateAttr)\r\n```",
        "> 1. Can we include the `SchemaError` information in this result?\r\n\r\nPytest will display the exception by default. This is the output you would get for a failing test:\r\n\r\n<details>\r\n\r\n```\r\ntests/test_json_schema.py F                                                                                                                       [100%]\r\ntests/test_json_schema.py:6476 test_fails - Failed: Failed to validate the JSON Schema against the Draft 2020-12 spec…                            [100%]\r\n======================================================================= FAILURES ========================================================================\r\n______________________________________________________________________ test_fails _______________________________________________________________________\r\n\r\nargs = (<pydantic.json_schema.GenerateJsonSchema object at 0x7b80723f5820>, {'metadata': {'pydantic_js_annotation_functions':...onSchema.__get_pydantic_json_schema__ of WithJsonSchema(json_schema={'type': 'invalid'}, mode=None)>]}, 'type': 'int'})\r\nkwargs = {'mode': 'validation'}, json_schema = {'type': 'invalid'}\r\n\r\n    def generate(*args: Any, **kwargs: Any) -> Any:\r\n        json_schema = orig_generate(*args, **kwargs)\r\n        if not request.node.get_closest_marker('skip_json_schema_validation'):\r\n            try:\r\n>               Draft202012Validator.check_schema(json_schema)\r\n\r\ntests/conftest.py:166: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ncls = <class 'jsonschema.validators.Draft202012Validator'>, schema = {'type': 'invalid'}\r\nformat_checker = <FormatChecker checkers=['date', 'email', 'idn-email', 'idn-hostname', 'ipv4', 'ipv6', 'regex', 'uuid']>\r\n\r\n    @classmethod\r\n    def check_schema(cls, schema, format_checker=_UNSET):\r\n        Validator = validator_for(cls.META_SCHEMA, default=cls)\r\n        if format_checker is _UNSET:\r\n            format_checker = Validator.FORMAT_CHECKER\r\n        validator = Validator(\r\n            schema=cls.META_SCHEMA,\r\n            format_checker=format_checker,\r\n        )\r\n        for error in validator.iter_errors(schema):\r\n>           raise exceptions.SchemaError.create_from(error)\r\nE           jsonschema.exceptions.SchemaError: 'invalid' is not valid under any of the given schemas\r\nE           \r\nE           Failed validating 'anyOf' in metaschema['allOf'][3]['properties']['type']:\r\nE               {'anyOf': [{'$ref': '#/$defs/simpleTypes'},\r\nE                          {'type': 'array',\r\nE                           'items': {'$ref': '#/$defs/simpleTypes'},\r\nE                           'minItems': 1,\r\nE                           'uniqueItems': True}]}\r\nE           \r\nE           On schema['type']:\r\nE               'invalid'\r\n\r\n../../.pyenv/versions/3.12.4/envs/pydanticdev/lib/python3.12/site-packages/jsonschema/validators.py:317: SchemaError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test_fails():\r\n>       TypeAdapter(Annotated[int, WithJsonSchema({'type': 'invalid'})]).json_schema()\r\n\r\ntests/test_json_schema.py:6478: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npydantic/type_adapter.py:142: in wrapped\r\n    return func(self, *args, **kwargs)\r\npydantic/type_adapter.py:549: in json_schema\r\n    return schema_generator_instance.generate(self.core_schema, mode=mode)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nargs = (<pydantic.json_schema.GenerateJsonSchema object at 0x7b80723f5820>, {'metadata': {'pydantic_js_annotation_functions':...onSchema.__get_pydantic_json_schema__ of WithJsonSchema(json_schema={'type': 'invalid'}, mode=None)>]}, 'type': 'int'})\r\nkwargs = {'mode': 'validation'}, json_schema = {'type': 'invalid'}\r\n\r\n    def generate(*args: Any, **kwargs: Any) -> Any:\r\n        json_schema = orig_generate(*args, **kwargs)\r\n        if not request.node.get_closest_marker('skip_json_schema_validation'):\r\n            try:\r\n                Draft202012Validator.check_schema(json_schema)\r\n            except SchemaError:\r\n>               pytest.fail('Failed to validate the JSON Schema against the Draft 2020-12 spec')\r\nE               Failed: Failed to validate the JSON Schema against the Draft 2020-12 spec\r\n\r\ntests/conftest.py:168: Failed\r\n                                   Summary of Failures                                    \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━┓\r\n┃  File                       ┃  Function    ┃  Function Line  ┃  Error Line  ┃  Error   ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━┩\r\n│  tests/test_json_schema.py  │  test_fails  │  6477           │  6478        │  Failed  │\r\n└─────────────────────────────┴──────────────┴─────────────────┴──────────────┴──────────┘\r\nResults (1.65s):\r\n         1 failed\r\n      5846 deselected\r\n\r\n```\r\n\r\n</details>\r\n\r\n> 2\\. Could we add a note about the fact that this is purely a testing feature, not a runtime `pydantic` check (at this point)?\r\n\r\nAdded.\r\n\r\n> Perhaps this is a bit excessive, but can we test this test?\r\n\r\nSeems like there are [ways to do so](https://stackoverflow.com/a/56635224), but they are pretty involved. I added a test with an expected failure.\r\n",
        "Yes can be removed after looking at the git blame."
      ]
    },
    "profile": {
      "location": "Amsterdam",
      "company": "@pydantic",
      "blog": "",
      "site_admin": false,
      "followers": 76,
      "following": 3
    }
  },
  "chrisduerr": {
    "repos": [
      "alacritty/alacritty"
    ],
    "entries": [
      {
        "slug": "alacritty-assess-security-trade-offs",
        "title": "Assess security trade-offs"
      },
      {
        "slug": "alacritty-avoid-unnecessary-operations",
        "title": "avoid unnecessary operations"
      },
      {
        "slug": "alacritty-avoid-unwrap-on-nullables",
        "title": "Avoid unwrap on nullables"
      },
      {
        "slug": "alacritty-centralize-workspace-dependencies",
        "title": "centralize workspace dependencies"
      },
      {
        "slug": "alacritty-choose-familiar-intuitive-names",
        "title": "Choose familiar, intuitive names"
      },
      {
        "slug": "alacritty-configuration-documentation-accuracy",
        "title": "Configuration documentation accuracy"
      },
      {
        "slug": "alacritty-configuration-validation-feedback",
        "title": "Configuration validation feedback"
      },
      {
        "slug": "alacritty-consistent-error-handling",
        "title": "consistent error handling"
      },
      {
        "slug": "alacritty-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "alacritty-document-configuration-specifics",
        "title": "Document configuration specifics"
      },
      {
        "slug": "alacritty-explain-code-intent",
        "title": "Explain code intent"
      },
      {
        "slug": "alacritty-follow-metadata-specifications",
        "title": "Follow metadata specifications"
      },
      {
        "slug": "alacritty-keep-documentation-together",
        "title": "Keep documentation together"
      },
      {
        "slug": "alacritty-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "alacritty-platform-specific-api-documentation",
        "title": "Platform-specific API documentation"
      },
      {
        "slug": "alacritty-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "alacritty-synchronize-platform-configurations",
        "title": "synchronize platform configurations"
      },
      {
        "slug": "alacritty-unsafe-code-practices",
        "title": "unsafe code practices"
      },
      {
        "slug": "alacritty-use-constraining-types",
        "title": "Use constraining types"
      },
      {
        "slug": "alacritty-use-descriptive-contextual-names",
        "title": "Use descriptive contextual names"
      },
      {
        "slug": "alacritty-user-friendly-network-descriptions",
        "title": "User-friendly network descriptions"
      },
      {
        "slug": "alacritty-write-audience-appropriate-documentation",
        "title": "Write audience-appropriate documentation"
      }
    ],
    "comments": {
      "alacritty-write-audience-appropriate-documentation": [
        "This is the user-facing changelog, so we should explain things a little bit. I'd suggest the following:\r\n\r\n```suggestion\r\n- Hide login message if `~/.hushlogin` is present\r\n```\r\n\r\nThis way we're properly documenting the effect it has on the user, rather than some internal details.",
        "```suggestion\r\n- Replaced `Options::hold` with `Options::drain_on_exit` that drains, but doesn't hold, since holding can be done outside of alacritty_terminal\r\n```\r\n\r\n\"could\" and \"easily\" are two words I'd rather not use in a changelog since they're vague and superfluous. Additionally \"user\" is kinda confusing because this is about the library consumer not the terminal user.",
        "This is a user-facing document, however this is worded in a developer-centric fashion.\r\n\r\nThis would probably be better:\r\n\r\n```suggestion\r\nNotable changes to the `alacritty_terminal` crate are documented in its\r\n[CHANGELOG](./alacritty_terminal/CHANGELOG.md).\r\n```\r\n\r\nArguably the same could be said about the documentation about the sections, but that is a bit older.",
        "I wonder if we should somehow indicate breaking changes? Do you have any opinion on this?\r\n\r\nMaybe a migration guide between versions?",
        "> you can look at the changelog we have in winit now\r\n\r\nI mean that's exactly the issue right there. A ton of changes but no clear indication which changes require my attention.\r\n\r\nI don't care if a bunch of new stuff was added or something was fixed when updating to a new version, but I **do** care if something needs manual intervention like this focus change to keep the behavior consistent.\r\n\r\nSome changelogs have ugly prefixes like `[BREAKING]` for these kind of entries, but I don't think that's particularly nice. Maybe we can add a line at the top explaining that breaking changes are bold, then use that to indicate them?",
        "> It has a big chunk of code and explanation for what you need. So it's just 2 changes where you need attention.\r\n\r\nIt doesn't though. It states nowhere that these are the only changes that require my attention. It just tells me what I need to do for those.\r\n\r\n> We can do a separate section with breaking changes.\r\n\r\nDo you suggest adding an extra section with its own Fixed/Added/etc?",
        "I like bold for breaking, it's unintrusive but still pretty clear.",
        "There's numerous issues with the default ConPTY. This section should be way more generic.\r\n\r\nIt's also pointlessly detailed, nobody cares what it's called or how it works, they just want a fix.",
        "I think this is still significantly too verbose. It should be easy to explain this in a single paragraph. It's not necessary to mention potential issues at all, if people are running into problems we just need to point them at things they can try. We also don't need to document in Alacritty how other terminal emulators behave.\r\n\r\nWhen it comes to how to get this to work, we don't need to really mention anything in the FAQ at all. I'd just add a section to the install.md."
      ],
      "alacritty-centralize-workspace-dependencies": [
        "Didn't want to migrate everything, but I think for most stuff (especially shared deps) it makes sense to just define them in the root.",
        "Going with just dirs 2 seems reasonable to me for now."
      ],
      "alacritty-avoid-unwrap-on-nullables": [
        "This unwrap does not sound right to me. This is deprecated, is there plan to remove it potentially which could cause this to fail? We shouldn't crash here under any circumstances.",
        "Seems easy enough to handle, so I don't see why we wouldn't. Unwrap's not the right choice.",
        "Just falling back to the `else` branch if anything goes wrong is fine, should be the simplest solution. I'd just early return on success probably.",
        "That's a good point. Falling back to en_US.UTF-8 is also fine or whatever you think is a reasonable default. I think the assumption here is this is almost never hit anyway, so we just don't want to crash really. Bugs would be easier to work around for users than crashes.",
        "```suggestion\r\n                // Fall back to en_US in case the country code is not available.\r\n```",
        "This would mean that an empty string is a c0 or c1 codepoint. Which I'd say is inaccurate.\r\n\r\nI also think that looking at chars/u32s is unnecessary here. Just getting `bytes()` should be fine since no valid unicode character (and `&str` is always valid unicode) can start with a C0/C1 character?",
        "This should be cleaner as `map_or`.",
        "Instead of doing `match padding.y { Some(y) => { ... }, None => {} }` it would be more ergonomic to use `if let Some(y) => { ... }`. Like this the `None => {}` part which does nothing can just be left out."
      ],
      "alacritty-keep-documentation-together": [
        "The different window levels should be documented, see `startup_mode` for reference.\r\n\r\nHaving the variants in parenthesis after the description is not beneficial, so that should be removed.",
        "Not a fan of having all the defaults documented in a massive blog so far from its definition.\r\n\r\nI don't want to scroll through half a manpage just to get the documentation for that one field I'm looking for. Especially because I don't even know if that documentation is still going to come. If I'm just looking at that one field, how am I supposed to know there's more?\r\n\r\nThis obviously applies to all the toml blocks.",
        "I **really** don't like that which is why I didn't write the manpage like this to begin with.\r\n\r\nI think it's a bloody mess and makes the entire manpage unreadable."
      ],
      "alacritty-optimize-algorithmic-efficiency": [
        "No need for saturating here, point.line cannot be bigger than end.line. At that point it's probably also fine to just put it all on a single line.",
        "If it's a hyperlink escape, we ALWAYS highlight regardless of the bounds. If we don't check for `is_some`, we'd only highlight within bounds.",
        "```suggestion\r\n    for (p1, p2) in f_x.zip(g_x) {\r\n```\r\n\r\nI think this should just work since `LineEquation` implements iterator and it's shorter?",
        "Cloning the padding seems like a waste. It should be perfectly fine for padding to implement `Copy`.",
        "Currently you're iterating twice to get to the point where things need to be split off. However the `char_indices` iterator is already an iterator which provides everything necessary to get to the split-off point.\r\n\r\nI'd propose something like this:\r\n```\r\n        // Remove non-alphabetical characters before the scheme\r\n        // https://tools.ietf.org/html/rfc3986#section-3.1\r\n        if let Some(index) = self.state.find(\"://\") {\r\n            let iter = self\r\n                .state\r\n                .char_indices()\r\n                .rev()\r\n                .skip_while(|(byte_index, _)| byte_index >= &index);\r\n            for (byte_index, c) in iter {\r\n                match c {\r\n                    'a'...'z' | 'A'...'Z' => (),\r\n                    _ => {\r\n                        self.state = self.state.split_off(byte_index + c.len_utf8());\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n```\r\n\r\nThis should work fine, though I haven't manually tested it. But the automatic tests don't seem to complain.",
        "```suggestion\r\n            if !uniq_hyperlinks.contains(&hyperlink) {\r\n                uniq_hyperlinks.insert(hyperlink.clone());\r\n                Some((cell, hyperlink))\r\n            } else {\r\n                None\r\n            }\r\n```",
        "This definitely needs a comment, seems like you use this to skip over cells that contain already existing hyperlinks, but doesn't `if uniq_hyperlinks.contains(&hyperlink) {` already do this? Seems like it's kinda duplicate work?",
        "Why use a dynamic Vec for something that will have at most two elements? Can't we just use a fixed-size slice with two elements for it?",
        "The `draw_string` method now takes an iterator, right? So do we need a `Vec` here?",
        "What's the point of this division? Wouldn't it be easier to just do `1_000_000_000 / 60_000` rather than `1_000_000 / (60_000 / 1000)`?"
      ],
      "alacritty-platform-specific-api-documentation": [
        "Is it necessary to quote thhe `--working-directory` flag here? And does `\\\"` work or does it need to be `&quot;`?",
        "Quoting the path is fine, as long as it is required and it doesn't work with a space in the path otherwise (that should be simple to test).\r\n\r\nSo if quotes around the path is required, then 2 would be the best option. There's no reason to quote something if it works without it. That just makes things unreadable.\r\n\r\nAlso single quotes might work just fine?",
        "```suggestion\r\n\tRequest compositor to blur content behind transparent windows.\r\n```\r\n\r\nWindow manager technically doesn't have the capability to do this, it's the compositor that can."
      ],
      "alacritty-follow-metadata-specifications": [
        "Yeah let's just fix it all in one PR. I'm sure @AsciiWolf doesn't mind.",
        "You can't just change the metadata license. The existing text is not available under CC0.",
        "> given that we don't use copyleft, requiring permission of the author is not needed\r\n\r\nI'm not sure what you mean by this. No matter what, we'd require permission by the author if we're changing the license.",
        "Yes, so the only option is rewriting the manifest from scratch, which I cannot do because I've been tainted. So we cannot change the license."
      ],
      "alacritty-use-descriptive-contextual-names": [
        "The variable name is confusing, since you're storing a column count not a line number.",
        "How frequently does winit log? I could see this easily going out of control.",
        "Could go for `alacritty_winit_event` just to be precise that this isn't actually logged from winit?\r\n\r\nThere's zero reason to be brief here, since it's always used through the constant anyway.",
        "Might want to change the name because of `0x7f`? Maybe just `is_control_character` with a comment which specific control characters this function detects?",
        "`x` is not a good variable name.",
        "`x` is not a good variable name.",
        "\"normal\" is a bad variable name because there's no reference for what is normal about it.\r\n\r\nIt would probably be easier to name this variable if it is inverted.\r\n\r\nThe comment size can also probably be reduced by replacing the list with an unnumbered one and removing the extra whitespace.",
        "Honestly thinking about it `f_x` and `g_x` are pretty terrible names. Would be much easier to understand all of this by just naming them after what they actually are. Something like `top_line` and `bottom_line` maybe.\r\n\r\nThe whole `_x` thing is just confusing because it's basically referring to a variable that isn't actually passed to the line function (we just call `.next()`). It makes sense mathematically but the concept just doesn't translate.",
        "This is a bad name. It's not an equation it's an iterator. An equation would map to a function in programming so it would just be `fn f(x: i32) -> i32` or whatever.",
        "Not really a fan of naming a variable after what it is, rather than *why* it is.\r\n\r\nCalling it just `ViMotion` would be fine. If you wanted to change the name a more descriptive one would be `SerdeViMotion` which communicates *why* it is wrapped.",
        "Shouldn't use the full path here. Just import it as `WinitOptionsAsAlt` or something.",
        "Please refrain from single-character variables. There's no reason not to just use `user` here. Same applies to the other match blocks."
      ],
      "alacritty-configuration-validation-feedback": [
        "What is the difference between `None` and `WindowLevel::Normal`? Either `WindowLevel::Normal` should be removed, or the option. Otherwise this seems redundant.",
        "Switched this from `cfg(unix)` to `allow(unused)` because otherwise you'd get warnings on Windows with this in your config. We don't need to change the old one since that will warn for deprecation anyway. Realistically this should have never gotten added with a `cfg` attribute.",
        "Shouldn't be a thing, but switching to a config option easily solves this too.",
        "Add it where?",
        "```suggestion\r\n                // Require at least one of hyperlink or regex to trigger hint matches.\r\n```"
      ],
      "alacritty-use-constraining-types": [
        "This function's API doesn't really make sense, the arguments here are seemingly arbitrary. This makes it look like any parameter is accepted for these parameters but that's just incorrect. The API should represent the accepted values and use enums where appropriate to prevent incorrect usage.",
        "Taking a `&[u8]` here definitely sucks, but I think constants are the best option with events that don't have any payload. Should there be a necessity for a payload at some point, pulling in a crate like `bincode` that does the translation is likely the correct choice."
      ],
      "alacritty-assess-security-trade-offs": [
        "I mean yaml is just a security nightmare anyway. I don't see how keeping an unmaintained version would be more secure.\r\n\r\nAnd yaml probably isn't going away soon.",
        "```suggestion\r\n\tControls the ability to write to the system clipboard with the _OSC 52_\r\n\tescape sequence. While this escape sequence is useful to copy contents\r\n\tfrom the remote server, allowing any application to read from the clipboard can be easily abused while not providing significant benefits over explicitly pasting text.\r\n```\r\n\r\nMy main gripe with this comment was the \"not that necessary\" part sounding kinda imprecise. I think the new suggestion should be a little more direct."
      ],
      "alacritty-avoid-unnecessary-operations": [
        "To avoid unnecessary clones.",
        "This change isn't strictly necessary, but it seems to just work so why not?",
        "But it was relying on the same thing before, just in a more verbose manner?",
        "Yeah honestly I was kinda worried that this patch would only change the mouse cursor when on a hyperlink escape, but that doesn't seem to be the case.",
        "Actually I found why this is necessary and my changes broke things:\r\n\r\nIf you're in mouse mode and hover over a URL, this will change mouse shape without shift.",
        "```suggestion\r\n        if self.frame().damage_all || selection == self.old_selection {\r\n            return;\r\n        }\r\n\r\n```",
        "If the `bg_alpha` is zero, then it won't be rendered regardless of what color it is. So when changing the background color you also need to make sure you're setting alpha again.\r\n\r\nWhy is the bg alpha 0 when it's the same as the cell background? Performance. It's unnecessary to draw a background for every cell when the background is the same as the terminal background anyway.",
        "If it's the same as the background color, we're not drawing anything anyway. So it doesn't matter what you set your `window_opacity` to. Making it *less* transparent than 0 will just cause it to be rendered, which is a waste of time since you can't see it anyway."
      ],
      "alacritty-consistent-naming-conventions": [
        "```suggestion\r\n\t\tNumpad keys are prefixed by _Numpad*_: \"NumpadEnter\" | \"NumpadAdd\" |\r\n\t\t\"NumpadComma\" | \"NumpadDivide\" | \"NumpadEquals\" | \"NumpadSubtract\" |\r\n\t\t\"NumpadMultiply\" | \"Numpad[0-9]\".\r\n```"
      ],
      "alacritty-document-configuration-specifics": [
        "```suggestion\r\n- Add `/etc/alacritty/alacritty.toml` fallback for system wide configuration\r\n```",
        "```suggestion\r\n- Pass `-q` to `login` on macOS if `~/.hushlogin` is present\r\n```",
        "```suggestion\r\n- Config option `window.level = \"AlwaysOnTop\"` to force Alacritty to always be the toplevel window\r\n```",
        "We don't document things for developers, we document them for users.\r\n\r\nSo the message you should be adding is that config keys are now all available under their proper name.",
        "```suggestion\r\n- Support for keybindings with dead keys\r\n```",
        "```suggestion\r\n**Alacritty versions before 0.13.0 are using YAML configuration files\r\nand all their settings were documented in the `alacritty.yml`. The example\r\n`alacritty.yml` file for each release can be found on [GitHub releases page](https://github.com/alacritty/alacritty/releases).**\r\n```"
      ],
      "alacritty-choose-familiar-intuitive-names": [
        "At this point I do not yet want this to be the default way to open new windows, maybe never.\r\n\r\nAlso I'm not sure it's appropriate to add a binding for spawning new windows on Linux, it seems like something users likely wouldn't be that familiar with anyway so they might as well just configure it themselves.",
        "While doing so in the future might make sense, I'm not convinced it's the best option *right now*. I think this is a feature that generally would benefit from a lot of testing and even then reliability will always be worse than separate instances. So it's difficult for me to recommend it as a default to anyone. Though I suppose on macOS this is expected, so I'm not opposed to changing the default binding in the future. But I see no reason to do it now.",
        "```suggestion\r\n  # By default, these will use the inverse primary color.\r\n```\r\n\r\nSince \"inverse text\" is the usual way terminal emulators refer to swapping foreground/background, I think it's good to use the well known language here."
      ],
      "alacritty-user-friendly-network-descriptions": [
        "This is a very technical description of what has been fixed (which is great for devs), but doesn't really help the user understand what has been fixed.\r\n\r\nI'd propose something more like this:\r\n`- Unicode characters at the beginning of URLs are now ignored`\r\n\r\nOf course this is just a suggestion, feel free to improve on it if you have any ideas.",
        "```suggestion\r\n- Escape sequence to set hyperlinks (`OSC 8 ; params ; URI ST`)\r\n```"
      ],
      "alacritty-synchronize-platform-configurations": [
        "You couldn't. Until I wrote this patch to make it possible. :D \r\n\r\nCheck out the changes in `src/input.rs`, it was fairly simple so I thought that this would be the best solution.",
        "With the previous configuration double bindings would just silently be ignored, like this they're both used. If it would have caused errors previously I think this would be a far more severe change, but like this it should actually make things more transparent and less error prone.\r\n\r\nOr that was my train of thought at least. But I was trying to justify the change after the fact. :D",
        "Yeah completely agree with that sentiment. I tested it on Linux by changing it to `Control`, but it's always better to be safe than sorry. :D"
      ],
      "alacritty-configuration-documentation-accuracy": [
        "We should reuse the \"looks for\"/search verbs here. There shouldn't be a config file there, it's all optional.",
        "```suggestion\r\ncan be found at _https://toml.io/en/v1.0.0_. Every _Default:_ and _Example:_\r\nentry is valid TOML and can be copied directly into the configuration file.\r\n```"
      ],
      "alacritty-unsafe-code-practices": [
        "I'd assume that reregister has mostly the same properties as register. Is this safety note not also true for reregister?",
        "This really doesn't seem like something Alacritty should be doing by hand. That seems like a rather poor Winit API? Especially as an unsafe interface, that's kinda unacceptable…",
        "> You'd be surprised, but we already do that. It's unsafe because it touches env variables, as in mutates the global state. You can't do that transparently, because it's again, global variables and it's all racy.\r\n\r\nYou mean without using `std::env`? Also which env is this removed from? If it's Alacritty, can't we do this in Rust?",
        "> I mean, the function simply calls std::env::remove_var that's all it does and that's the reason it's unsafe, because it removes the env which you could use for launching, so you should take care.\r\n\r\nHow is that unsafe?",
        "Mutating global state that is behind a lock is not unsafe…\r\n\r\nThat's just not how this works.",
        "Yeah that's not how the unsafe keyword is supposed to be used.",
        "There are no safety-relevant contracts that could be violated here."
      ],
      "alacritty-consistent-error-handling": [
        "Seems like the easiest solution to me?",
        "Well it's a completely separate execution path.",
        "I want to keep it this way because it gives us more control over the error messages.\r\n\r\nWe're not just trying to print errors and bail here, but instead we're trying to have nice human-readable CLI output.\r\n\r\nThat's why I did it this way, letting Rust's `?` handle error printing is usually worse.",
        "You should return an error with span information, not panic. See `src/config_deserialize/mod.rs` for reference.",
        "I don't like the idea of sidestepping error handling to cause an explicit panic, doesn't really make sense.\r\n\r\nEspecially not in here, considering there's other places this function could error out from. If anything, our error handling should be consistent?",
        "That just sounds like the `FreeConsole` code should be reworked."
      ],
      "alacritty-prefer-early-returns": [
        "Early return for a lil less indentation?",
        "I mean it's 4 levels of indentation already. With kernel indentation rules that would be 32 columns of indentation alone for the deepest level. I wouldn't call that \"low\".\r\n\r\nIf there were plans to handle different causes in the future, it might be different. But I don't see any immediate reason why we **ever** would handle different causes, and if we did, we'd probably switch to a match anyway. So I fail to see how an early return isn't the best option here.",
        "This is going to be **far** cleaner if you early return on error. Just to deal with the already ridiculous amount of indentation.",
        "The else branch is just `false`, so this can probably be written as a single conditional?\r\n\r\nYou could still keep the variables extracted for documentation purposes, but they're also pretty self-documenting.",
        "```suggestion\r\n        if drop_hyperlink {\r\n            self.extra = None;\r\n        } else {\r\n            let extra = self.extra.get_or_insert(Default::default());\r\n            Arc::make_mut(extra).hyperlink = hyperlink;\r\n        }\r\n```\r\n\r\nShould avoid the extra return, just complicates control flow.",
        "I recommend putting this in an `else` block rather than `break`ing, it's way too big of a conditional block with the early return to put an early return just to skip 5 lines."
      ],
      "alacritty-explain-code-intent": [
        "```suggestion\r\n        // Attempt to make the context current, if it is not.\r\n        let mut was_context_reset = if is_current {\r\n            false\r\n        } else {\r\n            match self.context.make_current(&self.surface) {\r\n                Err(err) if err.error_kind() == ErrorKind::ContextLost => {\r\n                    info!(\"Context lost for window {:?}\", self.window.id());\r\n                    true\r\n                },\r\n                _ => false,\r\n            }\r\n        };\r\n```\r\n\r\nI think a comment here is nice to explain that our goal here is not just checking if the context was reset, but actually performing the make_current call.",
        "Maybe we should add a comment *why* one would want to do this?\r\n\r\nSo something like \"to ensure toml compatibility\" or something.",
        "This method is getting very long, the least we should be doing is adding comments on all the individual code blocks for what they do.",
        "This took me a second to figure out, I think it might be nice to be a little more verbose:\r\n\r\n```\r\n// If we can't fit the entire arrow in the cell, we cut off the tip of the arrow by drawing a rectangle between the two lines.\r\n```",
        "It's not immediately clear what this does, should be documented, probably extracted in a separate variable with a nice name.",
        "It's not immediately clear what this does, should be documented, probably extracted in a separate variable with a nice name.",
        "This should be made more clear, probably by adding a comment and newline as separation. It's not just some error handling, this is how hyperlinks are stopped."
      ]
    },
    "profile": {
      "blog": "https://christianduerr.com",
      "site_admin": false,
      "followers": 264,
      "following": 7
    }
  },
  "jasnell": {
    "repos": [
      "cloudflare/workerd",
      "expressjs/express",
      "nodejs/node"
    ],
    "entries": [
      {
        "slug": "express-structured-release-workflows",
        "title": "Structured release workflows"
      },
      {
        "slug": "node-await-all-promises",
        "title": "Await all promises"
      },
      {
        "slug": "node-behavior-focused-test-design",
        "title": "Behavior-focused test design"
      },
      {
        "slug": "node-benchmark-before-optimizing-code",
        "title": "Benchmark before optimizing code"
      },
      {
        "slug": "node-descriptive-function-names",
        "title": "Descriptive function names"
      },
      {
        "slug": "node-document-non-intuitive-code",
        "title": "Document non-intuitive code"
      },
      {
        "slug": "node-document-with-precise-accuracy",
        "title": "Document with precise accuracy"
      },
      {
        "slug": "node-evolve-return-values",
        "title": "Evolve return values"
      },
      {
        "slug": "node-follow-consistent-naming-patterns",
        "title": "Follow consistent naming patterns"
      },
      {
        "slug": "node-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "node-format-docs-for-readability",
        "title": "Format docs for readability"
      },
      {
        "slug": "node-idempotent-error-safe-disposers",
        "title": "Idempotent error-safe disposers"
      },
      {
        "slug": "node-informative-error-messages",
        "title": "Informative error messages"
      },
      {
        "slug": "node-limit-environment-variable-scope",
        "title": "Limit environment variable scope"
      },
      {
        "slug": "node-minimize-configuration-dependencies",
        "title": "Minimize configuration dependencies"
      },
      {
        "slug": "node-prefer-clarity-over-cleverness",
        "title": "Prefer clarity over cleverness"
      },
      {
        "slug": "node-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "node-public-over-internal-apis",
        "title": "Public over internal APIs"
      },
      {
        "slug": "node-resource-aware-programming-patterns",
        "title": "Resource-aware programming patterns"
      },
      {
        "slug": "node-reuse-computed-values-efficiently",
        "title": "Reuse computed values efficiently"
      },
      {
        "slug": "node-scope-security-settings",
        "title": "Scope security settings"
      },
      {
        "slug": "node-standardize-null-pointer-checks",
        "title": "Standardize null pointer checks"
      },
      {
        "slug": "node-thread-safe-resource-management-patterns",
        "title": "Thread-safe resource management patterns"
      },
      {
        "slug": "node-use-appropriate-metric-types",
        "title": "Use appropriate metric types"
      },
      {
        "slug": "node-use-modern-c-features",
        "title": "Use modern C++ features"
      },
      {
        "slug": "node-validate-network-request-parameters",
        "title": "Validate network request parameters"
      },
      {
        "slug": "node-version-apis-with-care",
        "title": "Version APIs with care"
      },
      {
        "slug": "workerd-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "workerd-avoid-unnecessary-allocations",
        "title": "avoid unnecessary allocations"
      },
      {
        "slug": "workerd-choose-appropriate-logging-functions",
        "title": "Choose appropriate logging functions"
      },
      {
        "slug": "workerd-choose-efficient-algorithms",
        "title": "Choose efficient algorithms"
      },
      {
        "slug": "workerd-clear-descriptive-naming",
        "title": "Clear descriptive naming"
      },
      {
        "slug": "workerd-compatibility-flag-consistency",
        "title": "compatibility flag consistency"
      },
      {
        "slug": "workerd-comprehensive-assertion-testing",
        "title": "comprehensive assertion testing"
      },
      {
        "slug": "workerd-configure-secret-scanning-exceptions",
        "title": "Configure secret scanning exceptions"
      },
      {
        "slug": "workerd-connection-reuse-safety",
        "title": "Connection reuse safety"
      },
      {
        "slug": "workerd-defer-async-callbacks",
        "title": "defer async callbacks"
      },
      {
        "slug": "workerd-document-compatibility-flags-comprehensively",
        "title": "Document compatibility flags comprehensively"
      },
      {
        "slug": "workerd-document-security-sensitive-features",
        "title": "document security-sensitive features"
      },
      {
        "slug": "workerd-ensure-test-cleanup",
        "title": "ensure test cleanup"
      },
      {
        "slug": "workerd-extract-repeated-expressions",
        "title": "extract repeated expressions"
      },
      {
        "slug": "workerd-http-protocol-compliance",
        "title": "HTTP protocol compliance"
      },
      {
        "slug": "workerd-isolate-lock-safety",
        "title": "isolate lock safety"
      },
      {
        "slug": "workerd-maintain-consistent-patterns",
        "title": "maintain consistent patterns"
      },
      {
        "slug": "workerd-minimize-memory-operations",
        "title": "minimize memory operations"
      },
      {
        "slug": "workerd-network-resource-state-validation",
        "title": "Network resource state validation"
      },
      {
        "slug": "workerd-nodejs-api-compatibility",
        "title": "Node.js API compatibility"
      },
      {
        "slug": "workerd-optimize-algorithm-performance",
        "title": "Optimize algorithm performance"
      },
      {
        "slug": "workerd-prefer-higher-level-apis",
        "title": "prefer higher-level APIs"
      },
      {
        "slug": "workerd-prefer-nullish-coalescing-operators",
        "title": "prefer nullish coalescing operators"
      },
      {
        "slug": "workerd-prioritize-code-clarity",
        "title": "prioritize code clarity"
      },
      {
        "slug": "workerd-prioritize-descriptive-naming",
        "title": "Prioritize descriptive naming"
      },
      {
        "slug": "workerd-simplify-async-patterns",
        "title": "Simplify async patterns"
      },
      {
        "slug": "workerd-test-comprehensive-error-scenarios",
        "title": "Test comprehensive error scenarios"
      },
      {
        "slug": "workerd-use-appropriate-exception-types",
        "title": "Use appropriate exception types"
      },
      {
        "slug": "workerd-use-kj-unwrap-or-pattern",
        "title": "Use KJ_UNWRAP_OR pattern"
      },
      {
        "slug": "workerd-validate-post-manipulation-state",
        "title": "validate post-manipulation state"
      },
      {
        "slug": "workerd-wrap-throwing-operations",
        "title": "Wrap throwing operations"
      }
    ],
    "comments": {
      "workerd-validate-post-manipulation-state": [
        "```suggestion\r\nassert.strictEqual(globalThis.process, process);\r\n// -------------------------------------------------------\r\n```",
        "Perhaps include an escaped char (e.g. `\\\"') in the quoted portion of these to ensure that those are being handled correctly"
      ],
      "node-await-all-promises": [
        "```suggestion\r\n  const { promise, resolve, reject } = Promise.withResolvers();\r\n  fs.stat(filePath, { signal }, (err, stats) => {\r\n    if (err) {\r\n      return reject(err);\r\n    }\r\n    resolve(stats);\r\n  });\r\n\r\n  await assert.rejects(promise, { name: 'AbortError' });\r\n```",
        "should the read at least be awaited?",
        "It doesn't necessarily need to be done in this PR (but would need to be done before this graduates from experimental), but we should make sure that this plays well with AsyncLocalStorage. Specifically, something like the following should work:\r\n\r\n```\r\nconst als = new AsyncLocalStorage();\r\nals.run(123, () => {\r\n  navigator.locks.request('exclusive-test', async (lock) => {\r\n    assert.strictEqual(als.getStore(), 123);\r\n  });\r\n});\r\n```\r\n\r\nThis would suggest that the lock request is capable of capturing the current async context frame and restoring it when the lock has been acquired and the callback is invoked."
      ],
      "workerd-add-explanatory-comments": [
        "We likely want to stress in the documentation that `newHttpClient` is not a part of the \"standard\" `Socket` API that we're proposing in WinterTC.",
        "Hm... a comment in here indicating that the `#socket` is not actually used for anything other than making the property available would be ideal. ",
        "Just noting... we need to make sure we adequately document the fact that an `http.Server` only supports the `listen(...)` variant where we pass in a port to listen. Specifying the hostname, family, etc won't be supported.\r\n\r\nWe also need to make sure we document that `address().address` will always be hardcoded to `127.0.0.1`",
        "Which addition would trigger the deprecated warning?",
        "Might benefit from a comment about it. Not critical tho."
      ],
      "node-version-apis-with-care": [
        "The case will need to be wrapped in the `#ifdef NAPI_EXPERIMENTAL` as well",
        "Ah right, I forgot that this file defines `NAPI_EXPERIMENTAL` unconditionally at the top of the file. ",
        "Likewise here. Wrap the else if here in an `#ifdef NAPI_EXPERIMENTAL`",
        "My fear here is that `apiSurface`, even as a generated file, is likely to become quite massive and difficult to manage. As is we cannot even use the github UI to review it since it says, \"29,926 additions... not shown because the diff is too large...\". Is there any way to break it up further? Maybe generate a set of files rather than one single massive one?"
      ],
      "workerd-prefer-higher-level-apis": [
        "Well, we need to think through whether this makes sense for every case we use `jsg::Serializer` for, including storage... since it does change the actual produced serialization format for errors without changing the format version.\r\n\r\n~~That said tho, since `Serializer` is in jsg it does not have direct access to the compat flags and routing a compat flag through just to eliminate the option here just feels like, overkill?~~ Actually, I think we'll need this anyway if we use a compat flag to control it. ",
        "If you keep `constructor` as a `JsValue`, you can just do `auto name = constructor.get(js, \"name\"_kj);`",
        "It's better to leave this as a `JsObject` and to use the APIs there for getting the proto and the constructor name for doing these checks.",
        "You can use `js.global()` here to get the `Global` as a `jsg::JsObject` that'll be a bit nicer to use in here.",
        "Nit.. for later... we should probably have a version of `js.parseJson` that just returns a `jsg::JsValue`",
        "You're parsing the path above into a url, then again as a path here. The below here should probably work off the url's `pathname` instead for consistency with the rest of the fs apis."
      ],
      "workerd-choose-efficient-algorithms": [
        "If my bitshifting skills aren't too rusty, I *think* you can simplify this even further with: \r\n\r\n```\r\nMath.random() * 0x8000 | 0x8000\r\n```",
        "While it is beyond exceedingly unlikely that this will ever recurse more than once it would be nice if we could eliminate the recursion entirely and just use a loop.",
        "Something that most folks miss here is that `slice(...)` on a TypedArray actually creates a copy of the data for the slice. Is that what you're intending here or do you want a view (in which case you want `subarray(...)` not `slice(...)`."
      ],
      "workerd-extract-repeated-expressions": [
        "You're performing this operation (`event.invocationId + event.spanId`) multiple times in this block. Might be easier to maintain to do it once before entering the switch."
      ],
      "workerd-compatibility-flag-consistency": [
        "I disagree. If we are going to go down this route of tracking eols by version at all, then we should accurately follow which major version an API was eol'd in. ",
        "Let's do this as a path forward. We have a general `EOL` flag, and version specific flags. The even numbered versioned `EOL` flags will be implied on by the general one, the odd numbered will be implied on by the next highest even numbered.\r\n\r\n* `EOL22` implied by `EOL` at 22 LTS end date\r\n* `EOL23` implied by `EOL24` at 23 support end date\r\n* `EOL24` implied by `EOL` at 24 LTS end date\r\n* etc\r\n\r\nExample: Suppose my worker is running with `EOL22` and I'm using API `foo` ... that `foo` is removed in 23. I update my worker later to EOL24 simply because I updated my compat date, suddenly `foo` is gone. I don't care about any other removed functions but I need `foo`, at least for now, I can turn off the EOL23 flag and get `foo` back but still otherwise be on EOL24.\r\n",
        "Side note... in the docs for the compat flag and the examples, please be sure to mention that the `disallow_importable_env` compat flag should *not* be set if you are using this and need access to the `env` since that will prevent access.",
        "I'd prefer that we find a way of auto-generating the list from the source capnp file. Let's explore that first rather than adding another thing to manually update. I'm not in a hurry to land this.",
        "Revert this to use `Cloudflare.compatibilityFlags`",
        "It doesn't appear functionality related to the `node:http` stuff, isn't require to enable `node:http` server, and is changing it in the wrong direction (we should be moving to `Cloudflare.compatibilityFlags` and not away from it."
      ],
      "node-document-with-precise-accuracy": [
        "As I mentioned previously this test is only actually testing the behavior when `fs.stat` is called with an already aborted `AbortSignal`... The test description above should be updated to reflect that and I would add a comment about it in here. Not sure exactly how we can reliably test aborting the call while it is in flight since file systems have such broadly different performance characteristics and often these conclude so quickly that it rarely can be caught and canceled in time.",
        "Small typo in that, otherwise looks good\r\n\r\n```\r\n// This test verifies that fs.stat immediately throws an AbortError if the provided AbortSignal\r\n// has already been canceled. This approach is used because trying to abort an fs.stat call in flight\r\n// is unreliable given that file system operations tend to complete very quickly on many platforms.\r\n```",
        "```suggestion\r\n  * @param {string[]} data\r\n```",
        "This is such an unusual pattern to see that it likely warrants some code comments in here to explain why we're setting the prototype this way (as opposed to `class DOMException extends Error`."
      ],
      "workerd-wrap-throwing-operations": [
        "I'd move the `getReader()` call into the `try` here. getReader() can throw and that should not be ignored and should lead to `this` being destroyed.\r\n\r\n```suggestion\r\n      this.#reader ??= this._stream.getReader();\r\n      const data = await this.#reader.read();\r\n```",
        "Currently, if any errors are thrown synchronously here they will bubble up to the regular fetch handler, erroring the request, which is fine if that's what we want. However, I think we should consider wrapping this in a try/catch and erroring the server.",
        "Not sure why this was marked resolved. I think we still need to answer this question about whether the error should bubble up or should just error the server."
      ],
      "node-format-docs-for-readability": [
        "Check out how we handle links in the other docs in this folder. We collect them at the end of the doc and use in-doc references rather than inlining the URLs. Much more readable that way."
      ],
      "workerd-nodejs-api-compatibility": [
        "Yes, they were and they weren't. They were eol'd in 22 but the exports were never actually removed. They were actually exported *as* undefined.",
        "In Node.js the default export of `node:module` is `Module`. If we are adding `Module`, even if as a non-op, we should match Node.js' expectation here.",
        "Should we at least stub out the `Module` prototype with non-ops?\r\n\r\n```js\r\n> require('module').Module.prototype\r\n{\r\n  load: [Function (anonymous)],\r\n  require: [Function (anonymous)],\r\n  _compile: [Function (anonymous)]\r\n}\r\n```",
        "Not blocking but We know that we don't support trailers so I'm not sure there's much value in implementing this at all.",
        "the concern is that this is a fair amount of code that just isn't going to be exercised at all. I think it's better to remove the implementation and just replace with non-ops to simplify things overall.",
        "If the `sentHeaders` is already an iterable of [header, value] type pairs, I believe you might be able to do this more efficiently by passing the `sentHeaders` directly to the constructor? Would need to verify... `new Headers(sentHeaders)`"
      ],
      "node-evolve-return-values": [
        "I don't think the `enableHelpPrinting` here is a good idea. Instead, the return value of `parseArgs` should just be capable of returning the serialized help text. Allowing for something like,\r\n\r\n```\r\nconst result = parseArgs(...);\r\nconsole.log(result.printUsage());\r\n```",
        "We can have something, yes, but having these kinds of side effects on the constructor can be problematic",
        "Hmm.. good question. I really don't know.",
        "```suggestion\r\n   return type of an existing API is a breaking change.\r\n4. When an existing API signature does not lend itself easily to supporting making\r\n    the return value disposable and a new API needs to be introduced, it is worth\r\n    considering whether the existing API should be deprecated in favor of the new.\r\n    Deprecation is never a decision to be taken lightly, however, as it can have major\r\n    ecosystem impact.\r\n```",
        "This is likely something that can be best handled in documentation than code. For instance, in code we can have it be a fully anonymous object:\r\n\r\n```js\r\nreturn {\r\n  dispose() { ... },\r\n  [Symbol.dispose]() { this.dispose(); }\r\n}\r\n```\r\n\r\nWhile in documentation is can at least *appear* to be a named interface:\r\n\r\n```markdown\r\n### `foo()`\r\n\r\n* Returns {Disposable}\r\n\r\n### `Disposable`\r\n\r\n#### `disposable.dispose()`\r\n\r\n#### `disposable[Symbol.dispose]`\r\n```\r\n\r\nSo I'd suggest that we're talking about a documentation difference here, not necessarily a coding difference.",
        "I'm not suggesting that we would leave things undocumented. I'm saying that we don't always need formal classes in the actual implementation. For some return values we can safely rely on documentation-only and leave the actual implementation as anonymous objects. We even have existing precedence for this in the current documentation. See, for instance, all of the \"Class\" documentations for Web Crypto operations here: https://nodejs.org/docs/latest/api/webcrypto.html#algorithm-parameters ... the things like `AesDerivedKeyParams` is documented as a class but, in reality, there is no actual class named `AesDerivedKeyParams` in the source.",
        "I've expanded the document to include coverage of documentation of anonymous dispoables."
      ],
      "workerd-choose-appropriate-logging-functions": [
        "Might make sense to make this `LOG_ONCE` or `LOG_PERIODICALLY`"
      ],
      "workerd-optimize-algorithm-performance": [
        "Does `chars.first(endPos)` work here?",
        "Also, you should be able to do `js.str(chars.slice(0, endPos))` I believe, and avoid the extra allocation (`kj::str` heap allocates)",
        "Since we only actually care about `Array` and `Object`, it would likely be more efficient to filter out non-Array, and non-Object values when adding to the queue, that way we can avoid unnecessary additional allocations via the queue reservations."
      ],
      "workerd-avoid-unnecessary-allocations": [
        "Actually... do we need to be creating a new `Reader` on every call to `_read(...)`? Couldn't we create one and cache/reuse it?",
        "This is going to copy twice, once to create the `Buffer` and again to copy into the `Uint8Array`.\r\n\r\n```suggestion\r\n    const buf = Buffer.from(chunk, encoding);\r\n    return new Uint8Array(buf.buffer, buf.byteOffset, buf.byteLength);\r\n```",
        "This is going to become quite expensive as the buffers get bigger. If nodejs_compat was enabled then we could use `Buffer.concat` to be far more efficient."
      ],
      "workerd-simplify-async-patterns": [
        "nit: I believe you could simplify this down to...\r\n```suggestion\r\n    waitUntil(scheduler.wait(100).then(resolve));\r\n```",
        "bit more compact and readable ...\r\n\r\n```suggestion\r\n      const { promise, resolve } = Promise.withResolvers();\r\n      waitUntil(scheduler.wait(100).then(resolve));\r\n      await promise;\r\n```"
      ],
      "workerd-isolate-lock-safety": [
        "Should this use a `kj::MutexGuarded` instead?",
        "Also, is a mutex actually needed in this? Operations on this should be guarded by the isolate lock already?",
        "ah, nevermind, just noticed this is a static... I'm not sure that's a great approach here. the registry would end up being shared across *all* workers in the process but I don't think that's actually what we want, is it? Could easily end up having conflicts across workers from different customers, etc."
      ],
      "workerd-defer-async-callbacks": [
        "```suggestion\r\n      queueMicrotask(() => response.emit('close'));\r\n```\r\n?",
        "the queueMicrotask might not be needed here, just want to confirm.",
        "Does Node.js call this immediately or should this be defered with a `nextTick()`?",
        "Even in this case the callback is expected to be run async.\r\n\r\n```suggestion\r\n      queueMicrotask(() => callback?.());\r\n```\r\n\r\nFor example, in Node.js:\r\n```js\r\nconst { Writable } = require('stream');\r\nconst w = new Writable({ write(d, e, cb) { console.log('.'); cb(); }});\r\nw.write('hello', () => console.log('!'));\r\nconsole.log('written');\r\n```\r\n\r\nOutputs:\r\n\r\n```\r\n.\r\nwritten\r\n!\r\n```"
      ],
      "node-document-non-intuitive-code": [
        "This could use a code comment to describe the differences between `hashDigest(...)` and `xosHashDigest(...)`"
      ],
      "node-use-modern-c-features": [
        "Nit... since we're finally up on c++20, we can start using the more condensed `namespace node::inspector::protocol {` syntax where appropriate.",
        "```suggestion\r\n  enum class Mode { Shared, Exclusive };\r\n```\r\n\r\nOptional style nit.",
        "Since we are standardized on c++20 now, this can be condensed to `namespace node::worker::locks {`",
        "We really shouldn't use `new` with `v8::Global`. These can just be `v8::Global` and we make use of move semantics. Instead of using `delete` with `v8::Global`, the correct way to clear them is to use their `reset()` method"
      ],
      "node-use-appropriate-metric-types": [
        "One additional type of Guage that might be useful is a `HighwaterMarkGauge` whose value only every actually increases. For instance, suppose we are tracking deltas like:\r\n\r\n```js\r\ngauge.applyDelta(10);  // Records the stored highwater mark value as 10\r\ngauge.applyDelta(-5);  // Stored highwater mark value is still 10\r\ngauge.applyDelta(10);  // Stored highwater mark value is 15\r\ngauge.applyDelta(-15);  // Stored highwater mark value is still 15\r\ngauge.applyDelta(10);  // Stored highwater mark value is still 15\r\ngauge.applyDelta(10);  // Stored highwater mark value is 20\r\n```"
      ],
      "workerd-document-security-sensitive-features": [
        "We might also set a new record for most number of new compat flags added in a single week lol"
      ],
      "node-descriptive-function-names": [
        "Let's name this method `toJSON` to that it works seamlessly with `JSON.stringify(...)` ",
        "For the `IntervalHistogram` (https://nodejs.org/docs/latest/api/perf_hooks.html#class-intervalhistogram-extends-histogram) we use `enable()` and `disable()`. Not critical but I'd prefer to keep the API names consistent."
      ],
      "workerd-prioritize-code-clarity": [
        "The `!!` is harmless here but I'll remove it in the follow up PR I'm working on that builds on this.",
        "Reviewer Note: The first block installs the new module registry on the v8::Context. The second installs the original module registry. Regardless of which one is used, the `kj::Own` holding the instance is stored in the `ContextGlobal`. Previously these were being stored in two separate places which didn't make sense.",
        "I think I'm the only person in the world who finds that less readable 😆 ",
        "You could likely get away with just passing `levelStr` conditionally. If the value is passed as `undefined`, then assume `structuredLogging` is false, otherwise we assume `structuredLogging` is true.",
        "Why odd? It's fairly idiomatic in JavaScript.\r\n\r\n```c++\r\nargs[length + 1] = js.undefined();\r\nif (structuredLogging) {\r\n  args[length + 1] = js.strIntern(levelStr);\r\n}\r\n```\r\n\r\nThen in JS\r\n\r\n```js\r\nif (level !== undefined) {\r\n  // structured logging enabled\r\n} else {\r\n  // structured logging disabled\r\n}\r\n```",
        "Just to simplify things slightly, we can probably get by with just a single `IoContext::hasCurrent()` check and do both writes within it/",
        "Review note: the original implementation had `getTarget` simply deferring to `getCurrentTarget`, the new impl moves the original impl into `getTarget` here so having `currentTarget` point to `getTarget` preserves the original behavior.",
        "```suggestion\r\n      } else {\r\n        // The original implementation had getTarget simply deferring to\r\n        // getCurrentTarget, the new impl moves the original impl into\r\n        // getTarget here so having currentTarget point to getTarget\r\n        // preserves the original behavior.\r\n```",
        "You can likely get away with using an `.attach(kj::defer([this, self = JSG_THIS]() { running = false; monitoring = false; }))` to the promise rather than explicitly handling it in both the then and error handler to eliminate the duplication here."
      ],
      "node-prefer-clarity-over-cleverness": [
        "this feels almost a bit too clever. Why not simply have `_guessHandleType(fd)` return null explicitly? then your return value below could be `return handletypes[type] || type`.",
        "```suggestion\r\n\r\n  loadFromFile(savePath=\"./blocklist.json\") {\r\n```\r\n\r\nWhy make this an arrow function?\r\n\r\nInstead of building the file loading directly into `BlockList`, consider having `BlockList` instead generate a `Buffer` or be created from a `Buffer` and let the application do the file handling itself.\r\n\r\n```\r\nconst blockList = createBlockListSomehow();\r\nconst buf = blockList.save();\r\nfs.writeFileSync('blocklist.json', buf);\r\n```\r\n",
        "JSON or buffer doesn't matter so much. I'd prefer to avoid adding the node:fs uses into BlockList and would rather separate those concerns. We can greatly simplify this, for instance, by having `toJSON` and `fromJSON` methods on BlockList and handing the fs operations externally to BlockList."
      ],
      "node-follow-naming-conventions": [
        "For consistency, these should use lower-case, snake-case\r\n```suggestion\r\n  V(client_id_string, \"clientId\")                                              \\\r\n```",
        "For concepts like this, I'd prefer if we adopted a naming scheme like `IsCallable`"
      ],
      "workerd-use-appropriate-exception-types": [
        "It's better to keep this as a `JsValue` and you can just do...\r\n\r\n```cpp\r\nJSG_REQUIRE(constructor.isFunction(), TypeError, \"...\");\r\n```",
        "`DOMDataCloneError` *is* a `DOMException`",
        "Probably not a great idea to use `KJ_ASSERT` here. The user code might override `console.log` or `console.error` and this would result in an \"internal error\". I mean, it's exceedingly unlikely but possible. Let's make this a JSG_REQUIRE instead?",
        "Super nit: since this is an internal only error message, it seems unfortunate to allocate the string (kj::str(...)) entirely. This could just be...\r\n\r\n```suggestion\r\n  KJ_ASSERT(res.statusCode == 200,\r\n            \"Request for Pyodide bundle failed\",\r\n            url, res.statusCode);\r\n```\r\n\r\nAnd we'd avoid the additional string allocation.\r\n\r\nObviously extremely minor but still ;-)",
        "For these, use `KJ_ASSERT` rather than `KJ_REQUIRE`"
      ],
      "node-informative-error-messages": [
        "Here I would create the error outside the `process.nextTick(...)` so that it has a useful stack trace attached to it.",
        "It would likely be worthwhile to assign a warning code to this warning so that it can be suppressed with the `--dsable-warning` CLI flag.\r\n\r\n![image](https://github.com/user-attachments/assets/ec94215e-e7f7-4729-9c02-69bf1c433d2f)",
        "That's reasonable. Updated",
        "Likely yes but I think it's reasonable to throw a more specific error here."
      ],
      "workerd-configure-secret-scanning-exceptions": [
        "Might need to add this to https://github.com/cloudflare/workerd/blob/main/.github/secret_scanning.yml to keep the bot from complaining about a potentially leaked secret"
      ],
      "workerd-comprehensive-assertion-testing": [
        "`assert.throws(...)` really ought to have a second argument to test the error type and message (here and in the other cases below).",
        "Per the other comments, if `remotePort` is expected to be within a specific range we should likely test for that. Likewise, there are a number of other branches in the relevant code that aren't tested here (such as the values being undefined when the connection is closed). Can be done in a separate PR tho.",
        "This test is confusing. Are you expecting this to throw or succeed? If you are expecting a throw, using the `node:assert` `rejects(...)` API would be a better way to construct this block",
        "Need to make sure to include tests that involve passing the `port` option on listen as `0` (auto-assign the port from what's available), invalid port values (like `NaN`, `Infinity`, etc), and port values that are out-of-range.",
        "A variation of this test that honors backpressure signaling would be good to have as well. Specifically, it would help to exercise the backpressure signaling as well ensuring the logic around buffering and multiple reads is working correctly.",
        "It's not clear if there were any changes made in response to this. I see the added tests below. When resolving these, could I ask that you leave a comment indicating the change that was made. There are a number of these resolved without changes and it's difficult to differentiate without digging through the whole change set.",
        "There should also be an example where `req.destroy()` is called without an error as that would mean no `'error'` event is emitted. Also, Is there a test that verifies what happens if `res.destroy()` and `res.destroy(error)` are called?",
        "Perhaps have a test that includes multiple `\\n` instances in a single write?"
      ],
      "workerd-maintain-consistent-patterns": [
        "Elsewhere you use `Object.create(null)`. Don't have a preference but likely best to stay consistent.",
        "I know this already merged but as a follow up, the next time someone is in this code, since the `_header !== null || this._headerSent` check is duplicated several times it might make sense to separate out into a utility function for reuse.",
        "done",
        "+1... we should not freeze these. There is code in the ecosystem that mutates this."
      ],
      "workerd-prefer-nullish-coalescing-operators": [
        "Nit: I believe you could simplify this a bit with...\r\n\r\n```suggestion\r\n  return this._eventsCount > 0 \r\n    ? Reflect.ownKeys(this._events || {})\r\n    : [];\r\n```"
      ],
      "node-propagate-errors-with-context": [
        "These might need to be handled separately. If the first call succeeds, then the first created function will take ownership over the `fulfill_holder`. If the second call then fails for whatever reason, we are deleting the `fulfill_holder` while it's external is still holding the reference to it that it assumes it owns. It would be best to separate this into two separate calls rather than aggregating them together like this. Create one, create it's external and it's function, then create the second...\r\n\r\nOr, can we at least be certain that we won't end up with a free-after-free type error when deleting these while the External is still holding them?",
        "Not something to do here, but using `Check()` here has the same issue as using `ToLocalChecked()` in that it will just crash the process rather than propagate the error. This is a common issue throughout the code, however, so not something I would block this PR on. We need to handle these better in general. \r\n\r\nIf you did want to handle this here, then changing these to check if the return value is empty then doing some proper error propagation similar to the way the ToLocal(...) results are handled would be ideal.",
        "We should avoid using `USE` for the same error propagation reasons. Calling `Then(...)` can cause a JavaScript error to be scheduled. USE would cause that to be ignored when we ought to propagate it."
      ],
      "workerd-document-compatibility-flags-comprehensively": [
        "These need some documentation with them. We also should document the strategy here under the `removeNodejsCompatEOL` flag just to provide more context.",
        "```suggestion\r\n      $impliedByAfterDate(name = \"removeNodejsCompatEOL\", date = \"2027-04-30\");\r\n  # Removes APIs that reached end-of-life in Node.js 22.x. When using the\r\n  # removeNodejsCompatEOL flag, this will default enable on/after 2027-04-30.\r\n```",
        "```suggestion\r\n      $impliedByAfterDate(name = \"removeNodejsCompatEOLv24\", date = \"2025-09-01\");\r\n  # Removes APIs that reached end-of-life in Node.js 23.x. This will default\r\n  # enable when the removeNodejsCompatEOLv24 flag is enabled after 2025-09-01.\r\n```",
        "```suggestion\r\n      $impliedByAfterDate(name = \"removeNodejsCompatEOL\", date = \"2028-04-30\");\r\n  # Removes APIs that reached end-of-life in Node.js 24.x. When using the\r\n\t# removeNodejsCompatEOL flag, this will default enable on/after 2028-04-30.\r\n```",
        "Let's also document that the type stripping flag is mutually exclusive and will take precedence if both happen to be defined.",
        "Let's expand this comment a bit...\r\n\r\n```suggestion\r\n  # Removes the Node.js compatibility layer for EOL versions of Node.js.\r\n  # When the flag is enabled, APIs that have reached End-of-Life in Node.js\r\n  # will be removed for workers. When disabled, the APIs are present (but\r\n  # might still be non-functional stubs)\r\n  # This flag is intended to be a roll-up flag. That is, as additional APIs\r\n  # reach EOL, new compat flags will be added for those that will have\r\n  # `impliedByAfterDate(name = \"removeNodeJsCompatEOL\" , ...` annotations.\r\n```",
        "Eventially both of these would have `impliedByAfter` annotations such that they would just be on when `nodejs_compat` is enabled... but yeah, some expanded docs about the relationship of these flags is helpful."
      ],
      "node-minimize-configuration-dependencies": [
        "Given that this is Windows only, I would generally prefer that this entire internal binding only be compiled on windows, with the non-functional stubs implemented solely in javascript on the other platforms."
      ],
      "workerd-clear-descriptive-naming": [
        "Bikeshed: not a big fan of this name as it's not super clear what it means.\r\n\r\nPerhaps `registerPortHandler(...)` ?",
        "Add features like what? I'd rather not try to predict the future too much. If we need a new API in the future we can just add it.",
        "We've generally tried to avoid designing APIs around hypothetical futures. Pinging @kentonv because I know he's got opinions on public API surface like this.",
        "that name works for me."
      ],
      "workerd-minimize-memory-operations": [
        "I've added a comment. Specifically, it's not strictly necessary but I wanted it to be clear that the value is being moved and not copied. If it's bothersome then it's perfectly fine to remove it.",
        "It would be ideal if we could avoid the additional allocation (`kj::str(...)`) and wrapping it in `jsg::ByteString` here.",
        "I was referring to the additional allocation caused by the `kj::str(...)` call, which copies the `kj::StringPtr`. It would be awesome if we could eliminate that allocation unless we end up in the else block.",
        "Nit... we know how many `filenames` there are, we could reserve the space in advance here on the vector.",
        "Nit: `const tracing::TraceId&` to prevent the extra copy? I know it's a small class but still. Could also get by with having the `trace->traceId = traceId` in the impl be a `trace->traceId = kj::mv(traceId)`"
      ],
      "node-validate-network-request-parameters": [
        "I might have missed it elsewhere, but we need to make sure that `requestHost`, `reqOptions.port`, `reqOptions.host`, `auth` etc are validated here to not include and invalid characters (like `\\r` and \\n`) in order to prevent any kind of request smuggling.",
        "If these are checked somewhere else, then a comment in here indicating where would be helpful for people coming into this code later."
      ],
      "workerd-ensure-test-cleanup": [
        "If the tests fail these files are going to accumulate. Likely best to structure this so that the files are always removed whether the tests pass or fail?"
      ],
      "workerd-test-comprehensive-error-scenarios": [
        "Adding the abc 123 bit here demonstrates that the enhanced handling is working, and the unmodified stack check demonstrates that we're seeing local stacks, not the remote stack.",
        "How about -1, -Infinity, 1.1 (non-integer values), 9999999 (out of range values)"
      ],
      "node-scope-security-settings": [
        "I think I'd much prefer this to be set as an option on an individual connection rather than programmatically impacting all connections. A cli flag is one thing because it's set by the individual running the app. This API could be set by dependencies impacting global state without the application being aware of it. Setting it per connection seems the safest. ",
        "I won't block but I'm still unconvinced this is a good thing to add."
      ],
      "node-follow-consistent-naming-patterns": [
        "Since these aren't static methods, the `b` should be lower-case and likely needs `make format-md` or `make lint` run\r\n\r\n```suggestion\r\n### `blockList.fromJSON(value)`\r\n```",
        "I'd prefer the constructor methods here to follow a naming pattern like `createCounter`, `createTimer`, etc, both for ergonomics and for consistency (see `createHistogram(...)` in `perf_hooks`.\r\n\r\n```js\r\nconst counter = createCounter('api.calls', { service: 'web' });\r\nconst timer = createTimer('api.request.duration', { service: 'web' });\r\n```",
        "Yeah I kind of picked `use` here intentionally because of that ;-) ... wanted to prompt a discussion about it."
      ],
      "node-behavior-focused-test-design": [
        "Nit: you can use `deepStrictEqual` to compare the full `result` to an expectation",
        "The callback passed to the `then` should be wrapped with a `common.mustCall(...)`",
        "Wrap the callback in `common.mustCall(...)`"
      ],
      "node-standardize-null-pointer-checks": [
        "Consider having this return a `MaybeLocal<Object>` to better facilitate error propagation"
      ],
      "workerd-prioritize-descriptive-naming": [
        "Nit: The use of `$class` here is obscure enough that a comment here would likely be helpful for future folks looking into this code.",
        "Yep :-) ... *I* know that but it's one of those odd looking ones that I'm sure trips newcomers up when they happen to stumble across it. :-)",
        "How about just `allow_eval`? The `experimentalAllowEvalAlways` name here, of course, is only used internally so we can rename that any time. It's the actual enable flag name that matters here.",
        "Chatted with @kentonv a bit on the naming here... he asked for a scarier sounding name... I agreed. updated! PTAL",
        "I would call this `typescriptErasableSyntax`, which is the more common, familiar name to folks who use this.\r\n\r\n```suggestion\r\n  typescriptErasableSyntax @109 :Bool\r\n    $compatEnableFlag(\"typescript_erasable_syntax\")\r\n    $experimental;\r\n  # Support typescript erasable syntax support.\r\n  # This flag is expected to always be opt-in and never have\r\n  # a default on date.\r\n```",
        "That's fine. Get's the point across :-)",
        "Yeah, renaming makes sense.. additional commit shortly",
        "Fair.. will change in the fixup commit soon",
        "I prefer the `Node` prefix on this as it is more descriptive.",
        "I still prefer the descriptiveness of `NodeExceptionCode` here.",
        "Nit: While it's more verbose, I prefer avoiding the abbreviation here. `s/toNEHex/toNetworkOrderHex`"
      ],
      "node-benchmark-before-optimizing-code": [
        "The results are mixed and it's just not clear if the difference is worth being concerned about... some benchmarks are faster, others are slower.\r\n\r\n```\r\nstreams/compose.js n=1000                                                                       ***    -19.41 %       ±1.17%  ±1.57%  ±2.06%\r\nstreams/creation.js kind='duplex' n=50000000                                                    ***      4.39 %       ±1.62%  ±2.16%  ±2.81%\r\nstreams/creation.js kind='readable' n=50000000                                                           2.46 %       ±4.12%  ±5.48%  ±7.14%\r\nstreams/creation.js kind='transform' n=50000000                                                   *      2.21 %       ±1.69%  ±2.25%  ±2.94%\r\nstreams/creation.js kind='writable' n=50000000                                                  ***      6.72 %       ±2.15%  ±2.85%  ±3.71%\r\nstreams/destroy.js kind='duplex' n=1000000                                                              -0.50 %       ±3.36%  ±4.48%  ±5.84%\r\nstreams/destroy.js kind='readable' n=1000000                                                             1.07 %       ±2.43%  ±3.24%  ±4.22%\r\nstreams/destroy.js kind='transform' n=1000000                                                           -0.55 %       ±2.33%  ±3.10%  ±4.03%\r\nstreams/destroy.js kind='writable' n=1000000                                                             0.16 %       ±2.75%  ±3.66%  ±4.77%\r\nstreams/pipe-object-mode.js n=5000000                                                           ***      4.68 %       ±2.00%  ±2.67%  ±3.50%\r\nstreams/pipe.js n=5000000                                                                       ***     10.30 %       ±1.19%  ±1.59%  ±2.07%\r\nstreams/readable-async-iterator.js sync='no' n=100000                                                    1.07 %       ±1.92%  ±2.55%  ±3.32%\r\nstreams/readable-async-iterator.js sync='yes' n=100000                                          ***      8.45 %       ±3.80%  ±5.06%  ±6.59%\r\nstreams/readable-bigread.js n=1000                                                                       0.55 %       ±2.27%  ±3.02%  ±3.94%\r\nstreams/readable-bigunevenread.js n=1000                                                        ***     -2.12 %       ±1.13%  ±1.52%  ±1.99%\r\nstreams/readable-boundaryread.js type='buffer' n=2000                                             *      1.03 %       ±0.84%  ±1.12%  ±1.46%\r\nstreams/readable-boundaryread.js type='string' n=2000                                             *      2.08 %       ±1.69%  ±2.26%  ±2.97%\r\nstreams/readable-from.js type='array' n=10000000                                                ***     -5.20 %       ±2.30%  ±3.06%  ±3.99%\r\nstreams/readable-from.js type='async-generator' n=10000000                                               1.73 %       ±2.22%  ±2.96%  ±3.86%\r\nstreams/readable-from.js type='sync-generator-with-async-values' n=10000000                              1.88 %       ±2.05%  ±2.73%  ±3.56%\r\nstreams/readable-from.js type='sync-generator-with-sync-values' n=10000000                              -1.75 %       ±2.41%  ±3.24%  ±4.28%\r\nstreams/readable-readall.js n=5000                                                               **     -4.53 %       ±3.18%  ±4.24%  ±5.51%\r\nstreams/readable-uint8array.js kind='encoding' n=1000000                                         **      2.40 %       ±1.52%  ±2.02%  ±2.63%\r\nstreams/readable-uint8array.js kind='read' n=1000000                                            ***     -2.98 %       ±1.59%  ±2.11%  ±2.75%\r\nstreams/readable-unevenread.js n=1000                                                             *     -1.81 %       ±1.45%  ±1.93%  ±2.51%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='no' sync='no' n=100000                     1.10 %       ±3.76%  ±5.00%  ±6.51%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='no' sync='yes' n=100000           ***     27.55 %       ±9.28% ±12.36% ±16.09%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='yes' sync='no' n=100000                    2.88 %       ±5.18%  ±6.90%  ±8.98%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='yes' sync='yes' n=100000            *     11.47 %       ±9.08% ±12.08% ±15.73%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='no' sync='no' n=100000                   -1.45 %       ±3.28%  ±4.36%  ±5.68%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='no' sync='yes' n=100000          ***     23.32 %       ±8.66% ±11.53% ±15.01%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='yes' sync='no' n=100000                   3.83 %       ±4.48%  ±5.96%  ±7.76%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='yes' sync='yes' n=100000                  1.32 %       ±6.89%  ±9.17% ±11.94%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='no' sync='no' n=100000                   -1.30 %       ±3.51%  ±4.68%  ±6.09%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='no' sync='yes' n=100000          ***     30.32 %      ±10.19% ±13.57% ±17.67%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='yes' sync='no' n=100000                   0.12 %       ±3.88%  ±5.17%  ±6.73%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='yes' sync='yes' n=100000          **     13.40 %       ±9.34% ±12.47% ±16.32%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='no' sync='no' n=100000                  -1.46 %       ±4.38%  ±5.83%  ±7.61%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='no' sync='yes' n=100000         ***     16.67 %       ±8.21% ±10.94% ±14.28%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='yes' sync='no' n=100000                  1.19 %       ±4.05%  ±5.39%  ±7.01%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='yes' sync='yes' n=100000        ***     21.43 %       ±9.84% ±13.14% ±17.21%\r\nstreams/writable-uint8array.js kind='object-mode' n=50000000                                             0.64 %       ±3.66%  ±4.87%  ±6.34%\r\nstreams/writable-uint8array.js kind='write' n=50000000                                            *      3.85 %       ±3.32%  ±4.42%  ±5.76%\r\nstreams/writable-uint8array.js kind='writev' n=50000000                                           *      3.15 %       ±2.99%  ±3.98%  ±5.18%\r\n```"
      ],
      "node-resource-aware-programming-patterns": [
        "Yeah, I'll be expanding the documentation before moving this PR out of draft. But feel free to offer suggestions ;-)",
        "```suggestion\r\nAn attempt was made to read a file larger than the supported 2 GiB limit for\r\n`fs.readFile()`. This is not a limitation of `Buffer`, but an internal I/O constraint.\r\nFor handling larger files, consider using `fs.createReadStream()` to read the\r\nfile in chunks.\r\n```"
      ],
      "node-public-over-internal-apis": [
        "These aren't really considered internal modules, fwiw",
        "My concern would be whether or not anyone in userland could be using this, as unlikely as that may be. Should we export this through the regular `node:http` API to provide an alternative path to them?"
      ],
      "node-idempotent-error-safe-disposers": [
        "```suggestion\r\n   closed. If there is no difference in disposing in success or exception contexts,\r\n   then separate disposal methods are unnecessary.\r\n```",
        "We can definitely soften this but I think the guideline is correct. Most of the time disposal should be sync as much as possible. There are likely exceptions to that rule, of course. Will think about how to soften this a bit to say it's ok to use but should be intentional?",
        "Consider the following two cases:\r\n\r\n```\r\n{\r\n  using foo = new MyDisposable();\r\n}\r\n\r\n{\r\n  using foo = new MyDisposable();\r\n  throw new Error('boom');\r\n}\r\n```\r\n\r\nThe disposer in each case is going to be called. Unfortunately, however, the disposer has no idea if there is an exception pending or not. Let's say our `MyDisposable` has two possible modes: one is a clean, graceful async shutdown, the second is a dirty, abrupt shutdown in case of an error. Since the disposer does not know whether it is being called with a pending exception or not we cannot safely assume within the disposer that a graceful async shutdown should be used. Instead, we have to assume it is an exception case.\r\n\r\n```js\r\nclass MyDisposer {\r\n  #closed = false;\r\n  #aborted = false;\r\n\r\n  close() {\r\n    if (this.#closed) return;\r\n    this.#closed = true;\r\n  }\r\n\r\n  abort() {\r\n    this.#aborted = true;\r\n    this.close();\r\n  }\r\n\r\n  [Symbol.dispose]() {\r\n    this.abort();\r\n  }\r\n}\r\n```\r\n\r\n",
        "> ... But for these I question if ERM is really the right way to go...\r\n\r\nGenerally I think we should be leaving the decision up to users whether to use ERM or not. These guidelines are more about what we need to do for enablement. ",
        "Updated the language on this in the doc, please take another look!",
        "```suggestion\r\n   be synchronous and immediate. Avoiding async disposal is not always possible,\r\n   however, as some types of disposable objects require asynchronous cleanup.\r\n```",
        "````suggestion\r\n```\r\n\r\nThis is because of the fact that, when the disposer is called, it has no way\r\nof knowing if there is a pending exception or not and it is generally safest\r\nto assume that it is being called in an exceptional state. While some types\r\nof disposable objects make no differentiation between dispose in success\r\nand dispose in exception cases, those that do otherwise have no way of\r\ndifferentiating the conditions from within the disposer itself.\r\n````",
        "```suggestion\r\n3. It is recommended to avoid throwing errors within disposers.\r\n   If a disposer throws an exception while there is another pending\r\n   exception, then both exceptions will be wrapped in a `SupressedError`\r\n   that masks both. This makes it difficult to understand the context\r\n   in which the exceptions were thrown.\r\n```",
        "```suggestion\r\n  let disposed = false;\r\n  return {\r\n    dispose() {\r\n      if (disposed) return;\r\n      diposed = true;\r\n      console.log('Resource disposed');\r\n    }\r\n    [Symbol.dispose]() {\r\n      this.dispose();\r\n    },\r\n  };\r\n```",
        "Hmm.. I'm not sure how best to change this to address your comment here. Would you mind if we push any update on this one to another edit PR?"
      ],
      "node-limit-environment-variable-scope": [
        "Just a nit... I'd be more comfortable with this extracting just the env vars that are specifically relevant to `proxyEnv` rather than passing the entire `process.env`"
      ],
      "node-thread-safe-resource-management-patterns": [
        "This pattern looks... odd... and likely a bit error prone. We generally prefer the use of smart pointers to explicit use of `delete` and this could use some comments around it so it's clear what is happening here.",
        "These's aren't correct here. You really should not be holding a `v8::Global<T>` inside a `v8::External`, then deleting it like this. It's fine to use indirection through another type... e.g. having the `External` hold an instance of a struct that is holding the `Global<T>`.",
        "I ought to be able to take another look this afternoon. Just catching up after two week of being out of the office ;-) ... just mentioning because I didn't want you to feel I was ignoring your pings on this ;-) "
      ],
      "workerd-network-resource-state-validation": [
        "This should check to make sure the `socket`'s `readable` or `writable` are not locked, and possibly not disturbed. It especially needs to check to ensure the socket is not closed, and probably needs to wait for the opened promise to be resolved.\r\n\r\nLocked indicates that there is `Reader` or `Writer` associated, in which case wrapping the socket should fail. Disturbed indicates that the socket has been written to or read from, indicating that it may not be in the correct state to be wrapped correctly.",
        "We should probably also arrange it such that the returned `Fetcher` completely takes ownership over the `Socket` such that the `readable` and `writable` streams get closed/detached or locked so that nothing on the JS side mistakenly tries to access them while the socket it bound... e.g.\r\n\r\n```js\r\nconst socket = connect('...');\r\nconst fetcher = getHttpClient(socket);\r\nsocket.readable.getReader();  // should fail!\r\n```",
        "As an additional bit of fun, since `fetch(...)` is cancelable using an AbortSignal, the tests need to verify that canceling a fetch via the returned Fetcher works as expected here. \r\n\r\nAlso, is the `Socket` expected to support multiple http requests or should it be closed when the one http request is finished? Would it be expected to support pipelining (I'd expect no). This would need to ensure that we don't end up with multiple concurrent http requests going over the socket at a time... e.g. \r\n\r\n```js\r\nconst socket = connect('...');\r\nconst fetcher = getHttpClient(socket);\r\nawait Promise.all([ fetcher.fetch('...'), fetcher.fetch('...') ]);\r\n```\r\n",
        "Both of these require that there aren't pending reads or writes in flight and will throw if there are. Just want to make sure that's accounted for here. When a socket is in the middle of a starttls upgrade, for instance, we make sure things are drained before proceeding with the upgrade and detaching the original streams. This is to ensure that the streams are in a good state to actually be detached.\r\n\r\nThis likely also needs to account for whether a starttls operation is underway, the socket is already closed, etc... basically just some additional guards around whether the socket is in a good state overall to have the underlying connection removed.",
        "Let's also add a comment in here so folks coming into the code later can have a heads up about the requirements.",
        "That shouldn't ever happen. If anything, this should just assert if the port is already entangled, but in every use of this currently there's no way for the ports to already be entangled."
      ],
      "express-structured-release-workflows": [
        "Yeah not sure an npm org would work but a shared ID for publishing releases for express related stuff would be good. The shared secrets can be managed in a private repo the way we do shared secrets for core stuff. (There may\nalready be some tool for this somewhere)\n"
      ],
      "workerd-use-kj-unwrap-or-pattern": [
        "If believe this can just be... which would avoid the double cast.\r\n\r\n```suggestion\r\n        if (!proto.isObject()) {\r\n```"
      ],
      "node-reuse-computed-values-efficiently": [
        "You might consider moving the declaration for the `Local<Object> lock_info` to outside of the for loop and just reset it here so that we're reusing the same declaration on each iteration rather than creating a new one.",
        "Same here... if the `Local<Object> lock_info;` is moved outside of the for loops then it can just be reused here."
      ],
      "workerd-http-protocol-compliance": [
        "the `value.split(', ')` here will not work correctly with some of these. `content-type` in particular allows parameters to be `quoted-string` which can contain commas. The split algorithm needs to ignore any commas that are contained within quoted-string constructions. The `authorization` and `proxy-authorization` fields also allow for quoted-string parameters. The `value.split(', ')` worked for `host` since valid values for host don't include `quoted-string`",
        "example for test case: `content-type: text/plain; f=\"a, b, c\", text/foo; a=\"1, 2, 3\"`",
        "Also, I'm not sure if the space after the comma is actually required. It's defined as `OWS` which is \"optional white space\", which is to say that this is not going to be correct with headers that omit the OWS... oh! and it's worth pointing out that `OWS` also includes horizontal-tab, `OWS = *( SP / HTAB )` so the split here needs to take spaces and tabs around both sides of the splitting comma into consideration.",
        "See https://www.rfc-editor.org/rfc/rfc7230.html#appendix-B for complete details on the header syntax.",
        "In Node.js, if `port` is passed as a `0`, then it is automatically assigned a random available port number. We should support that here. Likely the easiest approach would be to have the `portMapper` be able to assign the port for us.",
        "Actually, thinking about this further, this is actually supposed to be registered as a handler for the `listening` event emitted by the server and we should probably do the same. It's not super common but there is code in the wild that does `server.on('listening', ...); server.listen()` and that should work here also. So instead of calling the `callback` directly in the queueMicrotask, this should register the event handler and `emit('listening')` in the queueMicrotask.",
        "Oh, wait, you're already adding the `'listening'` event handler above... so yeah, you're already halfway there."
      ],
      "workerd-connection-reuse-safety": [
        "Additional test cases here should include:\r\n\r\n1. What happens if the socket writable/readable are already locked to a `Writer` or `Reader`\r\n2. What happens if the socket has already been partially consumed / written to\r\n3. What happens if the socket has been errored\r\n4. What happens if the socket is configured to potentially use tls upgrade (internally we use a packet protocol to communicate with the proxy layer when tls upgrade is expected...)\r\n5. What happens if the remote end drops the connection prematurely\r\n",
        "Hmm... yeah, this is a bit weird. Generally it should be possible to use this fetch multiple times but each call to fetch should create it's own one-time-use `kj::HttpClient` under the covers, just reusing the same stream. Then, of course, it should forbid allowing multiple fetches to be in flight at the same time.\r\n\r\nEssentially a pattern like:\r\n\r\n* Calling `internalNewHttpClient` returns a fetcher that takes ownership of the underlying connection stream.. but...\r\n* Instead of creating an http client immediately, each call to fetch:\r\n  * Checks to see if a client currently exists, if so, error\r\n  * If not, create a new client over the stream, perform the fetch, release the stream when done to free it up for a new fetch (unless it errored, at which point the socket is errored)",
        "Hmm, in this case I would expect the `httpClient` to have complete ownership over the socket such that `socket.close()` should be a non-op. If every other way the socket is unusable so this is a bit strange.",
        "Oh, wait... it helps if I read the entire test case, lol. The httpclient does take over ownership and the `socket.close()` here is actually a non-op? I'd almost prefer that it errors."
      ]
    },
    "profile": {
      "location": "Clovis, California",
      "company": "@cloudflare",
      "blog": "http://jasnell.me",
      "site_admin": false,
      "followers": 1815,
      "following": 38
    }
  },
  "JesseStutler": {
    "repos": [
      "volcano-sh/volcano"
    ],
    "entries": [
      {
        "slug": "volcano-add-explicit-nil-checks",
        "title": "Add explicit nil checks"
      },
      {
        "slug": "volcano-algorithm-explanation-clarity",
        "title": "Algorithm explanation clarity"
      },
      {
        "slug": "volcano-always-check-errors",
        "title": "Always check errors"
      },
      {
        "slug": "volcano-api-validation-consistency",
        "title": "API validation consistency"
      },
      {
        "slug": "volcano-avoid-external-configuration-dependencies",
        "title": "Avoid external configuration dependencies"
      },
      {
        "slug": "volcano-avoid-redundant-operations",
        "title": "Avoid redundant operations"
      },
      {
        "slug": "volcano-comprehensive-test-structure",
        "title": "Comprehensive test structure"
      },
      {
        "slug": "volcano-document-security-implications",
        "title": "Document security implications"
      },
      {
        "slug": "volcano-extract-configuration-constants",
        "title": "Extract configuration constants"
      },
      {
        "slug": "volcano-extract-reusable-functions",
        "title": "Extract reusable functions"
      },
      {
        "slug": "volcano-implement-simulation-testing",
        "title": "Implement simulation testing"
      },
      {
        "slug": "volcano-justify-default-enablement",
        "title": "Justify default enablement"
      },
      {
        "slug": "volcano-network-topology-plugin-configuration",
        "title": "network topology plugin configuration"
      },
      {
        "slug": "volcano-optimize-algorithmic-efficiency",
        "title": "Optimize algorithmic efficiency"
      },
      {
        "slug": "volcano-resource-specific-optimization-factors",
        "title": "Resource-specific optimization factors"
      },
      {
        "slug": "volcano-review-security-permissions",
        "title": "Review security permissions"
      },
      {
        "slug": "volcano-secure-configuration-practices",
        "title": "Secure configuration practices"
      },
      {
        "slug": "volcano-standardize-configuration-formats",
        "title": "Standardize configuration formats"
      },
      {
        "slug": "volcano-use-controlled-concurrency-patterns",
        "title": "Use controlled concurrency patterns"
      },
      {
        "slug": "volcano-use-descriptive-consistent-naming",
        "title": "Use descriptive consistent naming"
      },
      {
        "slug": "volcano-use-descriptive-naming",
        "title": "Use descriptive naming"
      },
      {
        "slug": "volcano-use-structured-logging",
        "title": "Use structured logging"
      }
    ],
    "comments": {
      "volcano-use-descriptive-consistent-naming": [
        "`volcano.sh/is-reserve:1` is not such formal, maybe use `volcano.sh/reserveable: true` is better.",
        "same as `volcano.sh/runsec-max: 500`, such name is not so formal, use `volcano.sh/maximum-runtime: 500s` is better",
        "Why don't we just call `ReserveNodesMap`...? `nodeForbidMap` seems to have to be explained from the perspective of non-reserve tasks.",
        "We should capitalize these fields? `type` conflicts with keywords now. Can the `type` field here be enumerated with constants? There are only two types: `Node` and `HyperNode`, right?"
      ],
      "volcano-secure-configuration-practices": [
        "I think encryption tools need to be integrated here. Currently, it is assumed that users configure plain text passwords, which is not only unsafe to store locally, but also unsafe to transmit over the network. This is not conducive to Volcano's security audit and introduces new security risks.",
        "This is the feedback from the security audit team, the original words were \"The debug endpoint should be opt-in instead of opt-out,\" so it is turned off by default, and users can choose to turn it on"
      ],
      "volcano-resource-specific-optimization-factors": [
        "If you submit a pod requiring 4GPU, 20GB device memory, and an empty A100-PCIE-40GB node has the following MIG template below: \r\n```yaml\r\n  2g.10gb: 3\r\n  1g.5gb: 1\r\n```\r\nThen two `2g.10gb` profiles will be allocated to this pod, should be like this? Your example here doesn't say how much resources the Pod requests.",
        "typo `chosse`"
      ],
      "volcano-justify-default-enablement": [
        "We may also allow user to specify `deserved` and `guarantee.resource` when in installation",
        "OK, ignore me. I think set the capability when installation is enough",
        "What do you think? Do we need to enable it by default? I think that if users need to enable dra plugin, they also need to reconfigure the scheduler plugins configmap to enable it, it also looks like featuregate. If we don't enable this feture-gate, users need to do reconfiguring the scheduler plugins configmap and enabling the feature-gate two steps.",
        "OK we need not to enable this feature-gate by default, I will add some informers in schedulerCache, some users may need to use k8s below v1.31, if there are no APIs like `DeviceClass`, `ResourceClaim`, etc registered, it will cause some errors.",
        "DRA plugin has two feature gates `DynamicResourceAllocation` and `DRAAdminAccess`, if we need to enable the DRA plugin, the feature gate `DynamicResourceAllocation` must be enabled, therefore we need to add command flag for scheduler"
      ],
      "volcano-implement-simulation-testing": [
        "Besides features validation, we also need scenarios for node simulation. (e.g., we don’t have GPU and NPU nodes, if there is some bugs in GPU or NPU scheduling, we can use simulation scheduling to debug. We do meet this scenario in our production). And to test the performance of the scheduler in large-scale clusters. "
      ],
      "volcano-add-explicit-nil-checks": [
        "I'm wondering that why `Others` field in `NodeInfo` is a `map[string]interface{}` type rather than `map[string]Devices` ? Will `Others` have other types of interface in the future? We cannot control other contributors who may put other interface implementations into Others. If other interfaces do not have `HasDeviceRequest` methods, there will be problems here.",
        "If we don't modify NodeInfo, there should be an assertion here",
        "I think \r\n```\r\nif ji.PodGroup == nil || ji.PodGroup.Spec.NetworkTopology == nil {\r\n\t\treturn false\r\n\t}\r\n```\r\n judgment is sufficient, because the mode can only be hard/soft, which has been verified when creating hypernodes",
        "Didn’t you initialize the nil annotation in the informer’s `AddPodGroupV1beta1` and `UpdatePodGroupV1beta1` event handlers?"
      ],
      "volcano-standardize-configuration-formats": [
        "This configuration seems a bit fragmented. SRA defines weights and resource types. The original definition by @LY-today also has a set of weights and resource types. Can this be reused? ",
        "Volcano does not have a unified format, but since you have already deployed it in an internal cluster, we can unify it into struct @LY-today ",
        "The format of the arguments here doesn't seem very formal. Can they be defined as APIs, or the arguments can be defined more formally? Like:\r\n```yaml\r\narguments:\r\n  reserveLabel:\r\n  - nodeSelector: label1\r\n     startHour:\r\n     endHour:\r\n     resources:\r\n        cpu:32\r\n        memory: 64\r\n        xxx\r\n     ...\r\n  - nodeSelector: label2\r\n     ...\r\n```\r\nJust give an example, we can discuss more later."
      ],
      "volcano-comprehensive-test-structure": [
        "The plugin has been already renamed to `NodeStrategyFit`, but the func name here is still `NodeResourcesFit`, Please check your PR for any places that still have `NodeResourcesFit` and rename them",
        "BTW, we only need UT coverage of the main logic. For example, after adding the NodeStrategyFit plugin, we should check whether the scheduling process is correct.",
        "Should also add test case just like https://github.com/volcano-sh/volcano/blob/068b7bc3d8ecfc8274a82e4f458bbb84fac883bf/pkg/scheduler/actions/allocate/allocate_test.go#L68 `TestAllocate` to use TestCommonStruct to completely check the scheduling process",
        "You should write this test like\r\n```\r\n\tfor i, test := range tests {\r\n\t\tt.Run(test.Name, func(t *testing.T) {\r\n\t\t\ttest.Plugins = plugins\r\n\t\t\ttest.RegisterSession(tiers, nil)\r\n\t\t\tdefer test.Close()\r\n\t\t\ttest.Run([]framework.Action{New()})\r\n\t\t\tif err := test.CheckAll(i); err != nil {\r\n\t\t\t\tt.Fatal(err)\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n```\r\nhttps://github.com/volcano-sh/volcano/blob/068b7bc3d8ecfc8274a82e4f458bbb84fac883bf/pkg/scheduler/actions/allocate/allocate_test.go#L259-L269\r\nNot only NodeOrder"
      ],
      "volcano-document-security-implications": [
        "We can add a `> Warning` or `> Notice` here to remind users that Secrets are still unencrypted by default. If users need to encrypt Secrets, please refer to: https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/"
      ],
      "volcano-avoid-redundant-operations": [
        "After thinking about it, the judgement added here was to prevent scoring twice in hard mode, but after your modification, now hard mode will scoring twice( About hardnode tier and task nums). I also discuss with @Monokaix, we think we can remove scoring logic about hardnode tier and task nums, only keep hypernode resource usage judgement in the future: https://github.com/volcano-sh/volcano/issues/4368. What do you think?",
        "No, I don't mean you're conflicting with #4368. I know this is two different scenarios. What I mean is that after turning off the hard mode validation here in the node scoring, both hypernode and node have the same scoring logic in hard mode (for the number of tasks in tier and hypernode), so there's no need to score the same logic twice. You can remove the scoring logic for tier and tasknum from the hypernode scoring",
        "Yes, there may be some other fields in spec or status need to be updated. We can't only just update the condition. What I want to do is not to refresh running/failed/succeeded fields in every session, because if session interval is short, there may be a lot of podgroup update requests, which may burden kube-apiserver. ",
        "Only the `realNodesSet` on the ancestors chain will be reset. Can the HyperNode that is not on the ancestor chain don't need to recursively enter `BuildHyperNodeCache`? Just check whether the realNodesSet of this member is not empty.",
        "Otherwise there will be some repetitive builds, like: \r\n![image](https://github.com/user-attachments/assets/654f72f7-4e25-45f1-bae2-99cdb5b2d2ac)\r\n"
      ],
      "volcano-api-validation-consistency": [
        "The validation using kubebuilder is not consistent with here. MemberType is also verified here. I think we should keep consistent? @Monokaix ",
        "I mean, do we need to distinguish `MemberTypeHyperNode` from `MemberTypeNode`? Currently, our kubebuilder rules do not distinguish between them. We should keep consistent",
        "I looked at the cel validation in kubebuilder and it also supports referencing other fields. Perhaps if type validation is needed here, when memberType is HyperNode, it can only use ExactMatch. You should also update in apis repo to distinguish memberType is HyperNode and Node to keep consistent",
        "Currently, in api definition, the tier is a type of string, what if the user wants to specify tier name other than numbers? Is there any purpose in defining the type of tier as string before?"
      ],
      "volcano-optimize-algorithmic-efficiency": [
        "The meaning of gemini is like this:\r\n```\r\n    if !strings.HasSuffix(pattern, \"*\") {\r\n\t\treturn false\r\n\t}\r\n\r\n\treturn strings.Count(pattern, \"*\") == 1\r\n```\r\nThis already ensures that * needs to be at the end and can only have one",
        "Why should we add a new field rather than just directly modify `allocated`?",
        "I think we should learn from Kube-Scheduler's logic rather than from printer",
        "https://github.com/volcano-sh/volcano/blob/91c92e9d619538c4470a1fa2a46a734ad39c961a/pkg/scheduler/api/job_info.go#L770-L775\r\nThe pod is only refreshed with nominatedNodeName when marked as Pipeline in allocate action, but it is not marked as Pipeline state during backfill. Is it necessary to write these for Besteffort pods as well?",
        "OK, can we extract a unified func for these codes? Because it's duplicate with https://github.com/volcano-sh/volcano/pull/4079",
        "Maybe this new public func can accept an additional callback function as an argument? Where backfill passes nil, but allocate carries these:\r\n```\r\ntask.InitResreq.LessEqual(nominatedNodeInfo.Idle, api.Zero) \r\n```",
        "OK, I will take a look later, a little bit busy in recent days",
        "There is already a taskEligibleToPreempt function, which can be unified together",
        "I think use reflect.DeepEqual is better?\r\n```go\r\nif !reflect.DeepEqual(oldNode.GetLabels(), newNode.GetLabels()) {\r\n\t\tsc.hyperNodesQueue.Add(string(hyperNodeEventSourceNode) + \"/\" + newNode.Name)\r\n\t}\r\n```",
        "The logic here is not right. If Hostname is not empty, there will be an extra comma at the end. The extra comma at the end will be removed in the old logic, but the current writing will add an extra comma at the end and didn't remove it",
        "No, I mean if your replicas is greater than 1, and both hostname and subdomain exist, then it will be \"hostname.subdomain,\" with an extra comma. If the hostname is not empty, then it break here, and then return builder directly, so there will be an extra comma."
      ],
      "volcano-extract-reusable-functions": [
        "Why do we need to add a `strings.Contains` check in the outer layer? Shouldn't this be put in `isValidWildcardPattern`?",
        "L397-L410 is the same as in `normalPreempt` L277-L285(Swapping the order of PrePredicate and taskEligibleToPreempt should not affect？), I think we can extract a public func",
        "It is best to extract a function that validate memberType separately, otherwise the responsibility of the func validateHyperNodeMemberSelector does not match",
        "extract a public function, has the same code in FindLCAHyperNode"
      ],
      "volcano-use-controlled-concurrency-patterns": [
        "OK, I have refactored it to use sync.Once"
      ],
      "volcano-use-structured-logging": [
        "I think it's better to log as `node %s is not ready/unschedulable, skip considering this node`"
      ],
      "volcano-extract-configuration-constants": [
        "Extract `device-config.yaml` as a constant is better",
        "I remember that we discussed to set the interval that can be configured or using backoff. Direct set `60` here is \r\ntoo empirical."
      ],
      "volcano-always-check-errors": [
        "We still need to judge err here to avoid override\r\n```\r\nif err != nil{\r\n  return err\r\n}\r\n```",
        "OK I have added resyncTask inside the executePreBinds if PreBind of the task failed",
        "Why is this code placed before `allocateResourcesForTask`? It should at least be inside. You need to determine that the node allocation was successful before updating the `jobAllocatedNewHyperNode`",
        "Is there a need to return err here? I don't see any handling of `err != nil` in the outer func"
      ],
      "volcano-algorithm-explanation-clarity": [
        "The explanation is too abstract.  Need to explain in detail why the overkill phenomenon occurs.",
        "Can you rearrange the explanation here according to the order of the flow chart? If it is a reserve task, it can be scheduled to this node directly? Doesn't it check whether the resources are sufficient?"
      ],
      "volcano-use-descriptive-naming": [
        "Should not name variable as `ok1`. `ok1` --> `ok`",
        "Better to rename as `preemptorPodPriority` for better clarification",
        "Go does not use underscores in variable naming but uses camel case instead. However, to avoid ambiguity, I recommend changing it like this:\r\n```\r\nfilterVictimsFn := fun(evictingTask *api.TaskInfo, candidateVictims []*api.TaskInfo) ([]*api.TaskInfo, int) \r\n```",
        "The name `reScoreHyperNodes` is strange, better to rename as `candicates` or `candidateHyperNodes`",
        "This variable name is very strange. You can set 1 as a constant (as defaultMinMember), and then the variable here is initialized to defaultMinMember. `minMember` here is ok"
      ],
      "volcano-review-security-permissions": [
        "Is it necessary to set permissions for priorityclass?",
        "Added more privilege: DAC_OVERRIDE,SETUID,SETGID,SETFCAP"
      ],
      "volcano-network-topology-plugin-configuration": [
        "I think there is something wrong with this soft mode case. The reason for scheduling to kwok-node-5 and kwok-node-7 is only because of the influence of binpack plugin. There is already a `network-topology-aware` plugin: https://github.com/volcano-sh/volcano/blob/network-topology/pkg/scheduler/plugins/network-topology-aware/network_topology_aware.go. If the tier1 hypernode has enough resources to schedule a vcjob, then in soft mode, all pods in a vcjob are expected to be under the same hypernode.",
        "If you need to test soft mode, you should also open the `network-topology-aware` plugin in https://github.com/volcano-sh/volcano/blob/network-topology/installer/helm/chart/volcano/config/volcano-scheduler-ci.conf",
        "How do we ensure that this job spans two layers? kwok-node-5 only consumes 1U1G, and this job requests 4U4G. Shouldn't it be scheduled to only kwok-node-5? This does not reflect that this job spans two layers. We need to specify more replicas, right?"
      ],
      "volcano-avoid-external-configuration-dependencies": [
        "Where is the logic of this `AddPod`, in the mig-agent of nos? I'm wondering whether our dynamic GPU slice plugin is strongly dependent on the nos project. You can see that the annotation has the watermark of nos, and nos project is not updated frequently.",
        "/cc @Monokaix , I think we'd better rewrite them as part of volcano and evolve with us.",
        "And I feel that the design of nos is strange. All the MIG profiles whether they are free or used are annotated as annotation entries of the node. And the MIG profiles requested by Pod are also annotated as annotation entries. That's not how annotation is meant to be used this way. Can we define a CRD to manage these specs and status, and node can refer to this CRD? Or we can aggregate these specs and status into one JSON struct and annotated as only one annotation entry.",
        "OK, what's the plan of our codes? "
      ]
    },
    "profile": {
      "location": "Hangzhou,China",
      "company": "@Huawei",
      "blog": "https://jessestutler.github.io/",
      "site_admin": false,
      "followers": 12,
      "following": 9
    }
  },
  "howardjohn": {
    "repos": [
      "istio/istio"
    ],
    "entries": [
      {
        "slug": "istio-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "istio-api-compatibility-preservation",
        "title": "API compatibility preservation"
      },
      {
        "slug": "istio-api-version-compatibility-first",
        "title": "API version compatibility first"
      },
      {
        "slug": "istio-audit-license-dependencies",
        "title": "audit license dependencies"
      },
      {
        "slug": "istio-avoid-expensive-operations",
        "title": "Avoid expensive operations"
      },
      {
        "slug": "istio-avoid-repeated-expensive-operations",
        "title": "avoid repeated expensive operations"
      },
      {
        "slug": "istio-avoid-timing-dependencies",
        "title": "avoid timing dependencies"
      },
      {
        "slug": "istio-configuration-documentation-standards",
        "title": "Configuration documentation standards"
      },
      {
        "slug": "istio-conservative-networking-defaults",
        "title": "Conservative networking defaults"
      },
      {
        "slug": "istio-consistent-log-formatting",
        "title": "consistent log formatting"
      },
      {
        "slug": "istio-document-observability-clearly",
        "title": "Document observability clearly"
      },
      {
        "slug": "istio-document-observability-rationale",
        "title": "Document observability rationale"
      },
      {
        "slug": "istio-early-nil-checks",
        "title": "Early nil checks"
      },
      {
        "slug": "istio-feature-flag-lifecycle-management",
        "title": "Feature flag lifecycle management"
      },
      {
        "slug": "istio-flexible-configuration-defaults",
        "title": "Flexible configuration defaults"
      },
      {
        "slug": "istio-leverage-kernel-network-structures",
        "title": "leverage kernel network structures"
      },
      {
        "slug": "istio-maintain-consistent-naming-patterns",
        "title": "maintain consistent naming patterns"
      },
      {
        "slug": "istio-network-configuration-consistency",
        "title": "network configuration consistency"
      },
      {
        "slug": "istio-prevent-race-conditions",
        "title": "prevent race conditions"
      },
      {
        "slug": "istio-return-errors-explicitly",
        "title": "Return errors explicitly"
      },
      {
        "slug": "istio-simplify-code-structure",
        "title": "Simplify code structure"
      },
      {
        "slug": "istio-telemetry-consistency-standards",
        "title": "telemetry consistency standards"
      },
      {
        "slug": "istio-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "istio-validate-access-scope",
        "title": "validate access scope"
      },
      {
        "slug": "istio-verify-logging-configuration-functionality",
        "title": "verify logging configuration functionality"
      }
    ],
    "comments": {
      "istio-add-explanatory-comments": [
        "I think this makes sense but it was super confusing to me how we can have multiple Services. Can we add some comments here to explain why? Something like\r\n\r\n```go\r\n// WithServices marks multiple services as part of the selection criteria. This is used when we want to find **all** policies attached to a specific proxy instance, rather than scoped to a specific service. This is useful when using ECDS, for example, where we might have:\r\n// * Each unique service creates a listener, and applies a policy selected by `WithService` pointing to ECDS\r\n// * All policies are found, by `WithServices`, and returned in ECDS\r\n```",
        "can we add some comments? Like\r\n\r\n```go\r\n// For most proxy types, we include only the root namespace and same-namespace objects. However, waypoints allow\r\n// cross-namespace access based on attached Service objects. In this case, include all referenced services in the selection criteria\r\n```",
        "Please add a LOT of comments on what the semantics of this field is now that there are multiple.",
        "can you add comments what each one means",
        "This is really nit, but Go standard practice is to comment above the field, not beside. This supports godocs.",
        "Maybe we can comment where `10` came from?"
      ],
      "istio-early-nil-checks": [
        "This is always setting `initialmetadata` to non nil, can we add `if metadata == nil { return nil }`?"
      ],
      "istio-avoid-expensive-operations": [
        "Hmm, it seems like if we are getting the _wrong pod_ does that mean we could get some services and just the wrong one? Or will it always be empty?\r\n\r\nOne concern here is its valid to have 0 services and now we need to recompute every time which could be pretty expensive?",
        "+1, I am pretty strongly against using config_dump in anything other than debugging context.\r\n\r\n*if* we did, there is a way to filter it though: https://www.envoyproxy.io/docs/envoy/latest/operations/admin#get--config_dump?mask=",
        "Making unbounded direct List calls to the API server is not scalable. We already have an informer, please use `s.podLister` instead",
        "instead of a new channel, is it possible to just do `r.refreshTicker.Reset(r.refreshInterval)`? Not sure if that can be done in concurrent context though\r\n\r\nReason is I am just slightly worried that this will block in edge cases",
        "oops, misunderstood ticker. Replace what I said with `r.refreshTicker.C <- time.Now()`.\r\n\r\nThe issue would be if the refreshTicker select block is running and slow (it could be making 100s of http calls). We do the send on jwksuriChannel >5 times, it will block until the refreshTicker part is done.\r\n\r\nAlternatively we can just have the on-demand async calls decoupled entirely from the refreshTicker (and probably just refresh that 1 url, not loop over all of them",
        "Actually the single call may be better, otherwise for every new jwks we need to refreshall other jwks - may not scale well"
      ],
      "istio-verify-logging-configuration-functionality": [
        "This is a part of the removal of the \"access logging policy\". I discussed this briefly with you on chat.\r\n\r\nI don't see any real purpose for this field/filter. We just set this field, its not _actually_ sampled as far as I can tell -- the `log_sampled=false` requests still go to the real SD backend(!).\r\n\r\nIf we do want it still, we can add it as an environment var like we did with the AUDIT mode, but it doesn't seem useful. From my (imperfect) research, it has no usage, nor can I come up with even theoretical ideas on how it could be useful.",
        "this is from removal of \"access log policy\"",
        "There's no docs, and it's impossible to use on MCP. I don't think there's any usage based on ~15min of digging",
        "Seems like a half baked featured that was never completed from what I can tell ",
        "even if it is somehow configured, I am fairly sure it bills exactly the same. It's purely an opaque label .\r\n\r\nWhich doesn't make much sense, so either I am missing something or the feature was never really completed ",
        "I'll double check what the mode is but just to clarify, this PR only changes behavior if someone is configured an undocumented and unsupported config to set access_logging_policy (not possible in MCP, either)\r\n\r\nFor anyone else there is no change at all. \r\n\r\neven if they did set it, though, it has nothing to do with billing as far as I can tell. It does *not* sample despite the name it only sets a bool flag that has no impact",
        "Thanks I did not find that during my search (largely because it never actually says *how* to configure it, so I assume no one actually knows).\r\n\r\nEven with this, the feature does not work and isn't usable with MCP, so I cannot imagine anyone is using it. I would still encourage us to move forward with this PR as-is and I will take a follow-up to update the docs.\r\n\r\nIf there is a disagreement I can add it back, but we are maintaining a ton of code that does nothing ",
        "Thanks for pushing on this - I think I was wrong.\r\n\r\nI went through the docs, and they were incorrect initially, and now provide no information on how to configure it. Here is some options, in my order of preference:\r\n\r\n1. Remove everything, as this PR does. Users can manually apply the EnvoyFilter if they need this.\r\n2. Remove everything, as this PR does, but add a pilot env var to set this.\r\n3. Keep just access_log_policy EnvoyFilter\r\n\r\nIn all cases I will also go back and add the tests back for this feature.",
        "Done, I did option (1) and added the test back with this enabled.",
        "The user guide today already says \"go figure it out\". It doesn't say _how_ to do it at all. So I can improve the doc, but to be clear its already ~broken."
      ],
      "istio-configuration-documentation-standards": [
        "nit: here a below, if its not enabled no need to set values.\r\n\r\nWe should on put required+non-default values here to reduce the confusion about what is needed",
        "Is a user expected to configure this? It would be nice to auto-detect based on their Kubernetes version.\r\n\r\nSee manifests/charts/gateway/templates/deployment.yaml `ip_unprivileged_port_start` for an example doing this"
      ],
      "istio-flexible-configuration-defaults": [
        "Its not clear why we need an override, but regardless\r\n1. Please make this in a separate PR\r\n2. These files are synced from https://github.com/istio/common-files/tree/master/files, so they will need to be changed there to have effect (same with the run.sh change)"
      ],
      "istio-validate-access-scope": [
        "I am not sure this is secure?\r\n\r\nOne is saying: \"the proxy can access and pull secrets referenced by WasmPlugins that apply to it\"\r\n\r\nThe new code is saying: \"the proxy can access any pull secrets referenced by _any_ WasmPlugin they request, even if the WasmPlugin doesn't apply to them\". i.e. in a different namespace, etc?"
      ],
      "istio-document-observability-rationale": [
        "> I don't see any code using Telemetry API to program Istiod itself.\r\n\r\n`PILOT_TRACE_SAMPLING` is not about programming Istiod-internal traces, its about setting the trace sampling for Envoys.",
        "These EnvoyFilters were already unused in 99% of scenarios and instead set via https://github.com/istio/istio/blob/cd981f42ba33497af1e5ec7b596838295e7f5355/manifests/charts/istio-control/istio-discovery/templates/configmap.yaml#L24 since 1.18. (fwiw on our MCP product, this has been the case since launch)\r\n\r\nThere is no change to behavior for almost all users. For the very very few users using audit log or access log policy (as discussed in the other thread), there is a slightly different enablement mechanism; the docs will be updated for this."
      ],
      "istio-return-errors-explicitly": [
        "return an error; generally we never want to exit in code, only by returning error up to the top level",
        "I feel fairly uncomfortable with this.. a NACK is a *huge* risk, has the possibility to cause cluster wide outage by blocking all pod startup. Its worth a `return nil, true` to explicitly signal this rather than implicitly NACKing",
        "Return like this is bad practice in Go in general, IMO, but especially in this case",
        "Actually i guess in this case since we are proxying its a bit different but still a good policy. \r\n\r\nAlso its not clear - can `convert` return `nil, false`? What happens if so?\r\n\r\nAlso - On NACK, shouldn't we apply the valid configs and just NACK the invalid ones?",
        "I am not saying we shouldn't NACK, just that the default being NACK and then unsettting it is confusing/risky. Just a syntax concern; we should explicitly annotate on each return nack or not",
        "using the return value (jwksURI) when an `err` was returned is not a great practice IMO. If you are intentionally setting it to `\"\"` on err, then I would make it explicit."
      ],
      "istio-consistent-log-formatting": [
        "Thanks, I updated to `removes`",
        "For ECDS we know there are likely only a few values, but others have O(10k) so it would be very noisy to add them all. I can make this `len` if you want though",
        "```suggestion\r\n\t\t\t\tlog.Infof(\"envoy connection is not closed successfully with error: %v\", err)\r\n```\r\n\r\nReally nit but we shouldn't have spaces before `:` and `%s` doesn't really work with err I thought? ",
        "sorry to be nit-picky but since I already had another comment...\r\n\r\n```suggestion\r\n\t\tlog.Infof(\"connection is not closed: %v\", err)\r\n```\r\n\r\nIs the go recommendation for error formats"
      ],
      "istio-audit-license-dependencies": [
        "How feasible is it to make this not depend on any GPL code? Otherwise we are going to need to dimskme investigation on the implications here given GPL is current banned in Istio"
      ],
      "istio-maintain-consistent-naming-patterns": [
        "This is not really a component anymore. Do we want to use the legacy name to align with the `app` in this object, or change it to `istiod` to align with other objects?"
      ],
      "istio-simplify-code-structure": [
        "Do we still need this now that we have RevisionOrDefault",
        "nit: No need to make 2 variables, if its complex you can use a function (including inline function)"
      ],
      "istio-document-observability-clearly": [
        "```suggestion\r\n> This is because `namespaceSelector` is ignored for tenancy isolation.\r\n```"
      ],
      "istio-use-descriptive-names": [
        "nit: rename `AddService` to something like `WithService`. `Add` makes it sound like it mutates the existing object which it doesn't necessarily?\r\n\r\nI guess sometimes it does which maybe makes it even more confusing",
        "```suggestion\r\nfunc (ep *IstioEndpoint) Key() string {\r\n```\r\n\r\nGet and IstioEndpoint are redundant, we already know we are getting something from an IstioEndpoint",
        "nit: `\"a\"` can be clt.Config().Service()? Otherwise its hard to follow what `\"a\"` means",
        "This behavior was unexpected to me, we should either document it or change? Or maybe make the function named something closer to RetryUntilMaxDurationHit to make it clear",
        "Ignoring both return values on a method called \"Get\" seems very confusing. if its purpose is something else, maybe it should be renamed like \"update cache\" or something more appropriate?"
      ],
      "istio-leverage-kernel-network-structures": [
        "hopefully we have a reasonable way to fetch the NS and VETH of all pods on the node in the CNI...?",
        "I think having one entry here is fun, but an FYI we need to support ztunnel rolling restart. How we do it today is we pick the \"newest\" ztunnel and redirect to that. So I think this is compatible with this approach, as we will always only have 1 ztunnel \"active\""
      ],
      "istio-conservative-networking-defaults": [
        "I am not confident this works in most environments. The api-server is not always in kube-system nor a pod. Nor on these ports. Unless there is documentation suggesting a supported portable way to allow access here this seems problematic",
        "nit: is it better to just not specify this at all for Egress? Else user cannot lock it down\r\n\r\nOr is this so when a user has a deny-all rule istiod works?",
        "oops, good call. I agree, thought it was off",
        "What if users are using EnvoyFilter to configure ECDS today, won't they break if they haven't added this?"
      ],
      "istio-feature-flag-lifecycle-management": [
        "Can we add to compatibility profile then?",
        "+1 to **on** by default. \r\n\r\nOff by default is good for cool new features that users will want to use, but have some risk associated such that we cannot turn them on.\r\n\r\nOn by default is good for improvements we make that we are worried about unknown-risks, so we have an escape hatch if something goes wrong.\r\n\r\noff-by-default for something like this will not help us. No one will opt into an internal refactoring, so we will not get feedback.\r\n\r\neven better, this is merging that the start of the release cycle so we have 3 months before it ships to users (unless we planned to backport to 1.19)",
        "IMO no.\r\n\r\nEither:\r\n* Long term we want to have granular config, and should use something other than env var to expose that API\r\n* This is only a short-term opt out, and we will remove it soon. If this is the case, there is no need for granular opt-in to new functionality.\r\n\r\n"
      ],
      "istio-avoid-timing-dependencies": [
        "can we avoid sleeping? https://github.com/istio/istio/wiki/Test-Flakes#timing-dependencies"
      ],
      "istio-api-version-compatibility-first": [
        "These don't need to change; current prior art from networking is to use the oldest version everywhere (its strictly better - same schema but more compatible, and requires no noise from changing a bunch of files)",
        "v2beta2 just pushes us back 1 release to 1.26. So we may need to have support for v2beta2 and v2 unfortunately... Or just v2beta1 and v2 possibly. See https://kubernetes.io/docs/reference/using-api/deprecation-guide/#horizontalpodautoscaler-v126",
        "Which can just be a simple `if` in the api version I think - the schema is probably the same",
        "This one is a bit trickier since it ends up as plain YAML.. but this will break users. Maybe we should be more conservative for now and use v2beta2 for a bit here?",
        "I think we still need the v2beta2 option?\r\n\r\n* v2beta1: using legacy IOP config. Never used for helm (unless they touch the internal autoscalingv2API flag, but they shouldn't\r\n* v2beta2: anyone else on <1.23\r\n* v2: anyone else >=1.23"
      ],
      "istio-prevent-race-conditions": [
        "```\r\n=== RUN   TestNestedJoin2WithMergeSimpleCollection\r\n2025-07-31T14:08:34.611209Z\tinfo\tcluster \"fake\" kube client started\r\n2025-07-31T14:08:34.613144Z\tinfo\tsync complete\tname=test/Services attempt=2 time=2.090289ms\r\n2025-07-31T14:08:34.613213Z\tinfo\tkrt\ttest/Services synced\towner=test/Services\r\n2025-07-31T14:08:34.613943Z\tdebug\tkrt\tsync complete\tname=test/Services time=2.173568ms\r\n2025-07-31T14:08:34.614064Z\tinfo\tsync complete\tname=crd watcher attempt=2 time=2.739084ms\r\n2025-07-31T14:08:34.614084Z\tdebug\tkrt\tsync complete\tname=test/Services time=832ns\r\n2025-07-31T14:08:34.614110Z\tinfo\tcontrollers\tstarting\tcontroller=crd watcher\r\n2025-07-31T14:08:34.616486Z\tinfo\tsync complete\tname=test/Services handler attempt=2 time=2.366225ms\r\n2025-07-31T14:08:34.616596Z\tdebug\tkrt\thandled\towner=test/SimpleServices res=namespace/svc type=add\r\n2025-07-31T14:08:34.616633Z\tdebug\tkrt\tcalling handlers\towner=test/SimpleServices events=1\r\n2025-07-31T14:08:34.616673Z\tinfo\tkrt\ttest/SimpleServices synced (uid 3)\towner=test/SimpleServices\r\n2025-07-31T14:08:34.616711Z\tdebug\tkrt\tsync complete\tname=test/SimpleServices time=5.424606ms\r\n2025-07-31T14:08:34.618660Z\tinfo\tsync complete\tname=AllServices attempt=3 time=7.371443ms\r\n2025-07-31T14:08:34.618664Z\tinfo\tsync complete\tname=crd watcher attempt=3 time=7.204315ms\r\n2025-07-31T14:08:34.618730Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=add\r\n2025-07-31T14:08:34.618982Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n2025-07-31T14:08:34.619044Z\tinfo\tkrt\tAllServices synced (uid 5)\towner=AllServices\r\n2025-07-31T14:08:34.619087Z\tdebug\tkrt\tsync complete\tname=AllServices time=7.915789ms\r\n2025-07-31T14:08:34.619895Z\tdebug\tkrt\tsync complete\tname=AllServices time=842ns\r\n2025-07-31T14:08:34.649077Z\tinfo\tcluster \"fake\" kube client started\r\n2025-07-31T14:08:34.649544Z\tinfo\tsync complete\tname=crd watcher attempt=1 time=5.14µs\r\n2025-07-31T14:08:34.649604Z\tinfo\tcontrollers\tstarting\tcontroller=crd watcher\r\n2025-07-31T14:08:34.653241Z\tinfo\tsync complete\tname=crd watcher attempt=2 time=4.108561ms\r\n2025-07-31T14:08:34.653478Z\tinfo\tsync complete\tname=test/Services attempt=2 time=4.297801ms\r\n2025-07-31T14:08:34.653525Z\tinfo\tkrt\ttest/Services synced\towner=test/Services\r\n2025-07-31T14:08:34.653554Z\tdebug\tkrt\tsync complete\tname=test/Services time=4.380459ms\r\n2025-07-31T14:08:34.653704Z\tdebug\tkrt\tsync complete\tname=test/Services time=521ns\r\n2025-07-31T14:08:34.653730Z\tinfo\tsync complete\tname=test/Services handler attempt=1 time=2.215µs\r\n2025-07-31T14:08:34.653819Z\tdebug\tkrt\thandled\towner=test/SimpleServices2 res=namespace/svc type=add\r\n2025-07-31T14:08:34.653843Z\tdebug\tkrt\tcalling handlers\towner=test/SimpleServices2 events=1\r\n2025-07-31T14:08:34.653883Z\tinfo\tkrt\ttest/SimpleServices2 synced (uid 7)\towner=test/SimpleServices2\r\n2025-07-31T14:08:34.653927Z\tdebug\tkrt\tsync complete\tname=test/SimpleServices2 time=247.201µs\r\n2025-07-31T14:08:34.654106Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=update\r\n2025-07-31T14:08:34.654134Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n2025-07-31T14:08:34.658853Z\tdebug\tkrt\thandled delete\towner=AllServices res=namespace/svc\r\n2025-07-31T14:08:34.660104Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=add\r\n2025-07-31T14:08:34.660316Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n2025-07-31T14:08:34.660447Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=update\r\n2025-07-31T14:08:34.660475Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n    nestedjoinmerge_test.go:166: unexpected events: [update/namespace/svc]\r\n--- FAIL: TestNestedJoin2WithMergeSimpleCollection (0.61s)\r\nFAIL\r\n\r\nERROR: exit status 1\r\n```\r\n\r\nthis one happens often, about 1% of the time",
        "not clear why we reverse.\r\n\r\nAlso we are mutating in place AND returning - seems weird?",
        "I don't see how we can mutate without race conditions.",
        "^ all these concerns apply to Key as well",
        "I think the map we are setting is shared between sidecar injection as well. Is mutating this going to be safe? At the very least seems like we might get a race from read+write at the same time.\r\n\r\nIt may make sense to re-use the code in overwriteClusterInfo which does this after rendering instead of as input?\r\n\r\nWe could also just add a Cluster param to TemplateInput and use that.\r\n\r\n",
        "If using overwriteClusterInfo is hard due to Unstructured, I may have a change coming that makes it easier... let me get back to you soon (if its a blocker).",
        "https://github.com/istio/istio/pull/43541 was the change. Not sure it helps much",
        "One concern...\r\n\r\nconsider a namespace is added:\r\n* We have 2 informers for namespace, namespacecontroller and DiscoveryFilter\r\n* Events are triggered in random order to these\r\n* namespacecontroller gets \"Add ns foo\" event. It checks the discovery filter, and it is not present so it ignores it\r\n* DiscoveryFilter gets \"Add ns foo\" event. It should be included, so we add it\r\n\r\nNow we have failed to configure `foo`?",
        "Seems simpler to do @irisdingbj's suggestion, then we are not dependent on tricky race conditions ",
        "I think we want `<-time.After(next)` in a select with ctx.Done so it can pre-empt it?"
      ],
      "istio-network-configuration-consistency": [
        "If we are doing a patch won't k8s do the merge for us?\r\n\r\nAlso this means we can never remove things IIUC patches correctly (not certain I do)",
        "Why is dualstack IPv6 only? ",
        "Why not auto? ",
        "I think we use different logic in cluster_builder,does it make sense to differ? ",
        "The clusters being generate here may be for external addresses I think, by the way. so we cannot rely only on internal k8s config",
        "Its not clear to me why we use IPv6 only for Dual stack. What if I want to connect to github.com (no IPV6)?",
        "sorry, I still don't get it. This configures the bootstrap clusters. Why do we want that to be different from the dynamic clusters? https://github.com/istio/istio/blob/373fc89518c986c9f48ed3cd891930da6fdc8628/pilot/pkg/networking/core/v1alpha3/cluster_builder.go#L370\r\n\r\nThey are used for the same purpose, by the same proxies. Shouldn't we be consistent?",
        "> The DnsLookupFamily setting in cluster_builder.go is for cluster DNS, and this PR is for basically for every pod's sidercar DnsLookupFamily\r\n\r\nIt's the same field in envoy",
        "I see the change but cluster_builder logic still doesn't match this file. They are the exact same field in Envoy so I don't see why they should be different",
        "Curious if we want ALL or AUTO. Looks like curl and Golang both default to ALL equivilent so maybe it is a good default. Any opinions @kyessenov @ramaraochavali @jacob-delgado ?\r\n\r\n"
      ],
      "istio-telemetry-consistency-standards": [
        "what if its not a pod",
        "Actually workload name is more commonly Deployment anyways?not pod name",
        "We should probably use the same values we are setting in `baggage` for ambient",
        "@kyessenov @lei-tang @costinm ",
        "It seems like we should probably align with Otel and baggage. So if those aren't already aligned maybe we should align those as well",
        "Just to clarify - We should, IMO, have the same logic as https://github.com/istio/istio/blob/373fc89518c986c9f48ed3cd891930da6fdc8628/pilot/pkg/networking/core/v1alpha3/listener.go#L1653. If that logic is considered wrong, I have no objections to changing it, but they should be consistent (and consistent with OTEL)",
        "this doesn't seem to be an OTEL one either?",
        "`k8s.namespace.name`. We should be aligning with https://github.com/istio/istio/blob/373fc89518c986c9f48ed3cd891930da6fdc8628/pilot/pkg/networking/core/v1alpha3/listener.go#L1653",
        "Can we please use the same logic in baggage and here? ",
        "I meant the business logic of populating the fields more so than the `go.opentelemetry.io/otel/baggage` package.. although that is interesting, I didn't know they had a package. It seems like just a very inefficient version of fmt.Sprintf though....\r\n\r\n\r\nIts not clear to me we should be using `k8s.pod.name` in all cases. In other telemetry aspects, we are using WorkloadName, which is quite different - even on k8s, this is not always a Pod, but more often the Deployment name. There is currently a `TODO do not hardcode deployment. But I think we ignore it anyways?` in the Go code, but in the proxy code (`istio.telemetry.baggagehandler.v1.Config`). The proxy already actually returns the info we need to access this via OWNER.\r\n\r\nMore concretely, we shouldn't be parsing ID here, beyond the initial computation of model.Proxy it should be thought of as an opaque string. There are other, more reliable, ways to access the info we are after.\r\n"
      ],
      "istio-avoid-repeated-expensive-operations": [
        "Mutating the resource in an `Equal` function is unexpected and undesired. Plus you can do this more efficiently - see listEqualUnordered in ads.go.\r\n\r\nHowever, again, I strongly encourage us to treat this as an ordered list NOT an unordered one. Or always sort it during creation. We should not be sorting on each access",
        "Its _slightly_ more complex for strings - but there are 100s of places where we _should_ be using Sets throughout the code but do not because it isn't generic.\r\n\r\nIf the concern is `sets.Set` -> `sets.Set[string]` is verbose I can add `type String = Set[string]` and then you can use it like `sets.String`?"
      ],
      "istio-api-compatibility-preservation": [
        "I am worried about this. First, let me say I think our existing API is quite problematic which is not your fault of course -- but something that is widely used and that we need to keep compatibility with its behavior.\r\n\r\nToday, a user can put `credentialName: blah-cacert` and it will be treated as an opaque string. Now, the behavior is changed and we are implicitly deriving that they mean to use this as-is.",
        "overloading the string content like `cacert://` wouldn't work well since we already used (abused) that to have a meaning about the type.\r\n\r\nIt does seem like a new field `caCertCredentialName` may really be appropriate. WDYT @keithmattix ",
        "Yeah I am good with the new field as well. So that puts a bit more work for this! Thanks for working through this."
      ]
    },
    "profile": {
      "location": "Sunnyvale, CA",
      "company": "Solo.io",
      "blog": "",
      "site_admin": false,
      "followers": 646,
      "following": 0
    }
  },
  "tjgq": {
    "repos": [
      "bazelbuild/bazel"
    ],
    "entries": [
      {
        "slug": "bazel-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "bazel-avoid-eager-cache-invalidation",
        "title": "Avoid eager cache invalidation"
      },
      {
        "slug": "bazel-comprehensive-edge-case-testing",
        "title": "comprehensive edge case testing"
      },
      {
        "slug": "bazel-configuration-clarity-standards",
        "title": "Configuration clarity standards"
      },
      {
        "slug": "bazel-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "bazel-environment-aware-configuration-testing",
        "title": "Environment-aware configuration testing"
      },
      {
        "slug": "bazel-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "bazel-optimize-algorithm-choices",
        "title": "optimize algorithm choices"
      },
      {
        "slug": "bazel-organize-code-for-readability",
        "title": "Organize code for readability"
      },
      {
        "slug": "bazel-prefer-simple-api-designs",
        "title": "prefer simple API designs"
      },
      {
        "slug": "bazel-prefer-simple-readable-code",
        "title": "prefer simple readable code"
      },
      {
        "slug": "bazel-preserve-exception-causes",
        "title": "preserve exception causes"
      },
      {
        "slug": "bazel-simplify-complex-code",
        "title": "Simplify complex code"
      },
      {
        "slug": "bazel-use-descriptive-unambiguous-names",
        "title": "Use descriptive, unambiguous names"
      },
      {
        "slug": "bazel-use-semantically-clear-names",
        "title": "Use semantically clear names"
      },
      {
        "slug": "bazel-use-standard-api-abstractions",
        "title": "Use standard API abstractions"
      }
    ],
    "comments": {
      "bazel-use-standard-api-abstractions": [
        "Modify this to accept a `std::string_view`, then the `c_str()` in the caller becomes unnecessary."
      ],
      "bazel-configuration-clarity-standards": [
        "Should we make it so that `0` disables the GC? (This is the convention for the other GC flags)",
        "For the similarly named options, `0` actually means run immediately (while still respecting the other flag wrt enabled/disabled). Consider doing the same here to reduce confusion?",
        "Can we use a syntax that includes a time unit, e.g. `30s` or `5m` or `1h`, for added clarity? (See `DURATION_REGEX` in src/main/java/com/google/devtools/common/options/Converters.java for inspiration.)"
      ],
      "bazel-explicit-null-handling": [
        "Pass `input` to `checkNotNull` so that a stack trace will indicate which input failed the precondition.",
        "Let's mark this `@Nullable` and append a comment `// null if not yet known` to make the convention clear."
      ],
      "bazel-optimize-algorithm-choices": [
        "Yeah, I think it's worth fixing in a separate PR (alternatively, please file an issue if you don't plan to fix it for now).",
        "But why do we need to compile the regex with `Pattern.UNIX_LINES` to begin with? If we omitted it, wouldn't `^$` match only against the beginning/end of input? This is my understanding of the `Pattern` documentation, and it seems to be experimentally true (`Pattern.compile(\"abc$\").matcher().match(\"abc\\n\")` is false).",
        "Nevermind, I re-read the `Pattern` documentation and it does vindicate the `find` behavior:\r\n\r\n```\r\nIf MULTILINE mode is not activated, the regular expression ^ ignores line terminators and only matches at the beginning of the entire input sequence. The regular expression $ matches at the end of the entire input sequence, but also matches just before the last line terminator if this is not followed by any other input character.\r\n```\r\n\r\ni.e., the treatment of line terminators is not the same for `^` and `$`.\r\n\r\nAnother alternative would be to use `\\z` instead of `$` which *I believe* doesn't suffer from the final line terminator edge case even for `find` (but the current version with lookahead is fine too)."
      ],
      "bazel-simplify-complex-code": [
        "Let's maybe move this assertion (which also appears in other places) into a utility method in `StringUnsafe`?"
      ],
      "bazel-use-semantically-clear-names": [
        "In the spirit of avoiding double negatives (which I suspect might have contributed to the bug) can we call the method `IsKnown()` instead?",
        "Thinking some more about it: if the only consequence of `IsUnknown()` is that we don't set a `--command_wait_time` flag, why not set it to 0 in that case? The flags library can't distinguish between 0 and unset anyway."
      ],
      "bazel-use-descriptive-unambiguous-names": [
        "Is \"underlying\" the best descriptor we can use? For RemoteActionFileSystem I think this can get confusing, because it's not a overlay but more like a union of two filesystems (that could both be said to \"underlie\" it).\r\n\r\nIf we can't come up with a better term, let's at least include a paragraph along these lines in the Javadoc: \"for an action filesystem, this should return the on-disk component (or its underlying filesystem, if it is itself an overlay).",
        "Nit: this is a bit of a mouthful. I asked the friendly neighborhood LLM to suggest a single word to describe this, and it proposed \"hierarchical\". WDYT about just `HIERARCHICAL_COMPARATOR`?",
        "For your consideration: would it be better to write \"which requires sandboxing due to path mapping\" here, thus making the reason absolutely clear? (But then we should also rename `requiresSandboxing` to `requiresPathMapping`.)",
        "My understanding (per https://bazel.build/extending/platforms) is that we've retroactively redefined \"host\" to mean \"the platform Bazel runs on\" and introduced \"exec\" to mean \"the platform where an action executes\". The only reason it's ambiguous is that we haven't followed through in replacing \"host\" with \"exec\" everywhere it matters (notably, `--host_*` flags which should really be called `--exec_*`).\r\n\r\nA comment would be welcome, yes, but IMO we should stick to \"host\" or \"exec\" instead of introducing yet another term.",
        "Can you rebase and document the `parentExecPath` in the `OutputChecker` interface (added after you sent this PR)?\r\n\r\nYou might also want to call it `treeRootExecPath` - I know `Artifact.getParent()` is prior art, but the name is a bit misleading, as the tree artifact root isn't necessarily the immediate parent directory.",
        "Let's make the docstring and the variable name less vague: with the benefit of now-established terminology, instead of \"maybe UTF-8\" we can simply write \"internal\"."
      ],
      "bazel-prefer-simple-api-designs": [
        "This class should be private and interacted with exclusively through the `FileArtifactValue` API - `instanceof` checks aren't guaranteed to work, since a `ResolvedSymlinkArtifactValue` can now wrap any other `FileArtifactValue`.\r\n\r\nMy preference would be for `FileWriteOutputArtifactValue` to implement the same sub-API as `InlineFileArtifactValue`: return `true` from `isInline` and implement `getInputStream`. Is this a viable approach?\r\n\r\nOtherwise, the alternative is to add `canWriteTo` and `writeTo` methods to `FileArtifactValue`, but I'd like to avoid expanding the API if possible (unfortunately, we can't avoid it by refactoring in the opposite direction -`writeTo` isn't a good fit for the internal uses of `InlineFileArtifactValue#getInputStream`).",
        "This works, thanks."
      ],
      "bazel-preserve-exception-causes": [
        "It looks like the `getExpandedTemplateUnsafe` implementation doesn't actually throw `EvalException` (and there are no google3 overrides). Can we just amend the method signature?"
      ],
      "bazel-comprehensive-edge-case-testing": [
        "Can you add tests for:\r\n\r\n* `json.encode`ing a UTF-16 string with a non-ASCII BMP character\r\n* `json.encode`ing a UTF-16 string with a surrogate pair\r\n* `json.encode`ing a Latin1-hack string containing (the UTF-8 encoding of) a non-ASCII character\r\n* `json.encode`ing a Latin1-hack string containing invalid UTF-8 (I think it's worth capturing the current behavior in a test even if we intend to change it in the future)\r\n\r\n* `json.decode`ing `\\uXXXX`-encoded surrogates (paired high+low, paired low+high, unpaired followed by non-surrogate, unpaired in final position)",
        "Thanks - I think using a `CharsetEncoder` is indeed a more maintainable option. It's a little unfortunate that we have to go through all of `StringBuilder` -> `CharBuffer` -> `ByteBuffer` -> `byte[]` -> `String`, but I also couldn't find a shorter path given the available APIs."
      ],
      "bazel-avoid-eager-cache-invalidation": [
        "I'd rather not use `NotifyOnCacheHit`. Can we bump the GUID instead?"
      ],
      "bazel-prefer-simple-readable-code": [
        "Isn't this just an obfuscated way of writing:\r\n\r\n```\r\nif not _utf8_byte_strings:\r\n  assert_eq(json.encode(...))\r\n```\r\n\r\nor is there a reason we can't use that syntax in these test files?",
        "I wonder if we really need this class: we either did an extraction and know the time it took, or we didn't and we don't; we never inspect `archive_extracted`, except in tests. Wouldn't a `DurationMillis` suffice, with \"unknown\" signifying \"not extracted\"?",
        "Since we're already taking a dep on absl elsewhere, can you use `AsciiStrToUpper` here for better readability?"
      ],
      "bazel-ensure-comprehensive-test-coverage": [
        "Some more ideas for test cases:\r\n\r\n- A regex with an alternation matching two different test strings\r\n- A regex with at least one metacharacter following a `\\Q`, with that character matching one of the test strings literally\r\n- Test strings containing metacharacters not already covered above\r\n\r\nFuzzing is a great way to obtain additional confidence that the implementation is correct, but since it won't be triggered by CI, I'd still want some more test coverage.",
        "Let's also add unit tests for LazyFileWriteStrategy (it's fine to use a mock `ActionOutputMetadataStore` if constructing a real one is too laborious).",
        "Mind adding tests in `FilesystemValueChecker` (you can take the existing tests for remote metadata as inspiration)?",
        "Can you also parameterize these tests for `ctx.actions.write_file` and `ctx.actions.expand_template`? (It's fine to use a dummy template with no substitutions, I'm only concerned about covering the separate code path in `LocalTemplateExpansionStrategy`)",
        "If I understand it correctly, the implementation also recognizes `{\"timeout:42\": \"\"}` in `execution_requirements`. Can you parameterize the test to cover both cases?",
        "Why change this to a string literal?\r\n\r\nAlso, it's unclear to me how `tags` interacts with the `timeout` attribute for test targets (or the `--test_timeout` flag, which overrides it). Can you add integration tests to clarify the behavior?",
        "IMO there's a couple more things we should test:\r\n\r\n- helper returns empty object (\"no headers required for this URL\")\r\n- environment variables (i.e., pass an env var to the helper and embed it in the response to verify that it was passed in correctly)",
        "I'd prefer to see some tests here that explicitly check the byte contents of the string before and after conversion, because the two halves of the round-trip can both be wrong."
      ],
      "bazel-organize-code-for-readability": [
        "Prefer to exit early on error conditions to keep the indentation level as small as possible:\r\n\r\n```\r\nif argv[1] != \"get\":\r\n  eprint(\"Unknown command ...\")\r\n  return 1\r\n\r\n...\r\n```"
      ],
      "bazel-environment-aware-configuration-testing": [
        "It should be possible to delete this test (or move it to a more suitable place) once mtime-based hashing of source directories is no longer a thing, right? Please leave a comment to that effect.",
        "I'd expect it to fail when metadata is collected for the source directory, similarly to what happens when a source file is a dangling symlink.\r\n\r\nObviously that can't happen until we switch to hash-based metadata, but when we do, we should test it somewhere that applies to non-remote builds as well."
      ],
      "bazel-add-explanatory-comments": [
        "Please amend the docstring for the `setEnv` method and add one for the constructor describing the role of `clientEnv`."
      ]
    },
    "profile": {
      "location": "Munich, Germany",
      "company": "Google",
      "blog": "",
      "site_admin": false,
      "followers": 59,
      "following": 2
    }
  },
  "ololobus": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-connection-transitions",
        "title": "Document connection transitions"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-log-level-appropriately",
        "title": "Log level appropriately"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-use-descriptive-identifiers": [
        "Am I right that instead of these cryptic `prewarm_info[n]` you can use here `total, prewarmed, skipped` defined above?"
      ],
      "neon-stage-configuration-changes-gradually": [
        "I think it shouldn't be a part of feature flags, which are meant to be temporary. Instead, it should be a part of the main spec body because it's a permanent feature/mode\r\n\r\n(This `ActivityMonitorExperimental` is a bit special, I left it for longer, because statistics-based monitoring is tricky, but we should also remove it already)"
      ],
      "neon-proactive-cache-warming": [
        "Makes sense, @MMeent can you suggest what the raw numbers should be? Like `target_lfc_size_pages` and `processed_lfc_pages`? Will it work?",
        "Added, thanks",
        "I mean that we just start primary from scratch with empty caches, the only option to improve the situation is to do async auto-prewarm, while already accepting new connections. So from cplane perspective, it's just a normal start from with auto-prewarm enabled",
        "Well, that's debatable whether we need auto-prewarm or not at all. It exists in vanilla Postgres. The idea is that we can prewarm caches faster when we do it intentionally vs. when user tries to prewarm by just doing their normal workload\r\n\r\nImagine cplane, it eventually accesses all non-deleted projects/endpoint/branches. If we just restart it at Neon, it will take some time to visit all objects. Yet, if we actively prewarm caches in the brackground, the chance that next project read will hit the cache will be higher, as it will be already there, even though cplane hasn't read it explicitly\r\n\r\nIn practice, it may not suite all workloads, but we cannot answer for sure until we implement it and battle-test, but imo it exists in vanilla Postgres for a reason, so there are use-cases",
        "Well, seconds is just a unit, it can be set to 5, 15 minutes. For cplane-orchestrated there is a separate API, I imagined periodic dumping to be useful for\r\ni) auto-prewarm, i.e. compute periodically dumps LFC content, so later it can be used at restart. In theory, we can only dump at graceful shutdown, but then it won't help with accidental compute crash/restart, as there might be no LFC state to warm up from\r\nii) later for having a hot standby, i.e. it can periodically fetch fresh content from S3 and do prewarming\r\n\r\nI don't want to wire too complex logic via cplane, so TBH, I don't see other options to have a robust auto-prewarm without periodic dumping of the LFC state, pg_prewarm does the same via `pg_prewarm.autoprewarm_interval`\r\n\r\n@MMeent @knizhnik do you have any specific suggestions of how we can implement it without periodic dumping?",
        "Thanks for the comment about hot standby\r\n\r\n> We can make it part of endpoint shutdown procedures?\r\n\r\nYes, this is what I meant by 'In theory, we can only dump at graceful shutdown'. That'd work in most of the cases, but what I don't like is that it doesn't cover any abnormal termination like OOM, VM restarts, etc.\r\n\r\nWith your cost estimation, dumping every 5 minutes is completely reasonable",
        "> Note that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.\r\n\r\nRe-reading it after a long time and now it still looks like it should work. Like\r\n\r\n1. We did prewarm once\r\n2. After some time, we fetch LFC content again and iterate over blocks to check if they are present in the LFC\r\n2.1. If block is in LFC -- good, WAL replay should keep it up-to-date\r\n2.2. If it's not in LFC -- we will fetch it from pageserver\r\n\r\nThat way, we do not need any eviction explicitly, and it will help with keeping the LFC relatively warm. Not saying that we need to do it exactly like that, I like you suggestion with switching the replay mode and replaying all pages, it's just this could be a viable alternative\r\n\r\nOr do I miss something?",
        "I was actually thinking about using the default that pg_prewarm uses -- 300s. I think it's frequent enough for this purpose. This will lower it it to ~$40 per 1k computes per month, which is good enough, imo",
        "Mentioned this default explicitly, thanks for the estimation"
      ],
      "neon-clear-consistent-identifier-names": [
        "NIT, but it impacts readability a lot -- `state.state` -- what state of what state? I suggest making it more clear\r\n\r\n```suggestion\r\n        compute: &Arc<ComputeNode>,\r\n```"
      ],
      "neon-hierarchical-semantic-naming": [
        "As discussed, I'd use `compute_pg_` prefix to indicate that it comes from Postgres. Same for both metrics",
        "I think we should drop `min` from the metric. Yes, in the view it's min_mxid across all tables in this DB (same about `frozen_xid`, note PG naming consistency), but when you take age() and sort DESC you actually get oldest as it's properly mentioned in the description.\r\n\r\nSo I guess at the end metrics could be named like\r\n- compute_pg_oldest_frozen_xid_age\r\n- compute_pg_oldest_mxid_age\r\n\r\nor something"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "Replied here https://github.com/neondatabase/neon/pull/11294#discussion_r2006276800 and here https://github.com/neondatabase/neon/pull/11294#discussion_r2006283261 as well\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThis PR https://github.com/neondatabase/neon/pull/10442 introduced additional locking when accessing LFC, so it's now considered safe to write there concurrently, so that's the base for all this work.\r\n\r\nDuring prefetch, we always request the latest pages from the pageserver. If, after loading page gets modified, then it will be either updated in LFC (in case of primary) or evicted from LFC after receiving the corresponding WAL record (in case of replica). In other words, if pages is not present in the LFC, we will fetch it from the pageserver; if someone (backend, normal client workload) tries to write it concurrently, then the access will be synchronized, and we should still get a freshness guarantee. @knizhnik or @MMeent can correct me, as I'm not fluent in the underlying mechanism, I consider it as given here\r\n\r\nThus, it should be safe to prewarm LFC concurrently with user load. The only problem is performance, I wrote about it in other comments, but anyway. Yes, if it's highly intensive workload, then prewarm can compete for storage resources with user workload, so we can consider auto-prewarm to be user-togglable feature, I wrote about it in the section about auto-prewarm concerns\r\n\r\n@VladLazar @mtyazici let me know if it makes it clearer",
        "> Logical Replication\r\n\r\nI recall that Konstantin did a POC like that. We discarded that because it only helps with keeping a warm replica, and it's not possible to implement autoprewarm with that + it bloats the WAL on safekeepers and eats the network bandwidth. It's not a big problem, as Pageservers should discard such records during ingestion, so it wont bloat the data files, but it's still nice to avoid\r\n\r\n> The idea here is to expose some LFC primitives at the SQL level on the primary. The API should allow for fetching the current state of the LFC in a way that it can be reproduced\r\n\r\nI don't remember that we considered any diffing, but otherwise it's pretty much how it works -- we have SQL funcs to dump/load caches state",
        "If we consider this compute data as non-critical, could we avoid explicit deletion completely? I was thinking about setting a TTL for perfix/bucket https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html (never used it personally, though)\r\n\r\nThat should most likely work for prewarm/caches content. Assuming we set it to a high enough value (like 7d or 30d), if one doesn't start endpoint for that long, they likely don't care about prewarming much. For `pg_stat_statements` it's pretty much the same -- well, your perf data will expire after N days -- sounds fair. For stats it could be a bit more annoying, but again should be not critical at all\r\n\r\nAt the same time, with TTL we avoid implementing a huge piece of deletion orchestration.\r\n\r\nWhat do you think?",
        "Explicit deletion would work as well, I think, just more work on the control plane side"
      ],
      "neon-document-parameter-choices": [
        "Can you please add test comments clarifying what parameters mean and what are the different test modes? After quickly eyeballing the test, I cannot easily grasp the `with_compute_ctl` and why we pass `ids` as a test parameter"
      ],
      "neon-avoid-flaky-tests": [
        "Any sleep-based waiting in tests almost certainly causes flakiness. Please, rewrite it into waiting for the LFC content to appear in the remote storage. There is a generic helper in python tests for that -- `wait_until()`, see usage in other tests"
      ],
      "neon-keep-files-focused-small": [
        "NIT: this `compute.rs` is really huge already. Should we move all new method implementations and structs into `compute_prewarm.rs`? It will be pretty well-scoped and should improve navigation and readability. WDYT?\r\n\r\nPersonally, I was already struggling with the `compute.rs` size",
        "> It seems like we have a good pattern going where the ComputeNode methods become small wrappers around functions in other files.\r\n\r\nThis could be the way to go as well, but I was more thinking about just having a second `impl` block like\r\n```rust\r\nimpl ComputeNode {\r\n    pub async fn prewarm_status(&self) -> PrewarmStatus {...}\r\n\r\n    // The rest of prewarm methods\r\n}\r\n\r\n// The rest of prewarm structs\r\n```\r\nin a separate file `compute_prewarm.rs`. If that's possible in Rust (afaik, it's)"
      ],
      "neon-design-metrics-for-insights": [
        "It usually makes sense to track 2 parameters out of 3: total, failed, success, so that you can always reconstruct all 3. Because just number of requests doesn't tell us much, we care more about success/error rates. Could be a separate PR, up to you"
      ],
      "neon-document-connection-transitions": [
        "Primary in the diagram is the old primary, and it's shut down first, it's also mentioned in text. Or what do you mean?",
        "My intent was to keep it reasonably high-level. Do you see some important interactions missing here?",
        "This step is right after we terminate the primary, so yes, during normal termination, we can expect that at this moment primary will be already terminated and all connections to it will be closed.\r\n\r\nI added this item after talking to Stas, as he had a fair point that the old primary could be unresponsive during this promotion flow, so we will send termination and k8s resources deletion requests, but we cannot generally guarantee that it will be dead by this time. So this step is more to protect from the situation, when old connections will still be connected to the old primary\r\n\r\nSee also item 7 in failure modes. I'm not quite sure how big the problem is. Safekeepers will guarantee that there is only one running writer at a time, so it's more like a nice-to-have, than must-have feature, just to prevent unnecessary interference and side effects (I worried about some stale reads from the old primary and failing writes because safekeepers should reject them)"
      ],
      "neon-proper-metrics-design": [
        "I think it's not considered a best-practice, in the docs it's formulated like\r\n\r\n> As a rule of thumb, either the sum() or the avg() over all dimensions of a given metric should be meaningful (though not necessarily useful). If it is not meaningful, split the data up into multiple metrics.\r\n\r\nhttps://prometheus.io/docs/practices/naming/ (see other suggestions there)\r\n\r\nI suggest you split it into two separate metrics with a clear meaning"
      ],
      "neon-log-level-appropriately": [
        "NIT: I'd reverse this and instead log 'Skipping pgbouncer and local_proxy termination because in dev mode' when dev_mode is true. Otherwise, we log this in real envs, but it doesn't make any sense as we log separate line when we actually send signals"
      ],
      "neon-document-api-specs-completely": [
        "Let's provide a brief API spec for this EPUFS service, i.e. what are the methods and parameters we are going to have:\r\n- PUT: tenant, timeline, endpoint, relative path, data -> json response\r\n- GET: tenant, timeline, endpoint, relative path -> file content response\r\n- DELETE: tenant [ timeline [ endpoint ] ] -> json response"
      ],
      "neon-comprehensive-code-documentation": [
        "> endpoint_id is set to None while prewarming from other endpoint, see replica promotion\r\n\r\nThis doesn't sound right. When we promote we should prewarm from another endpoint, so endpoint_id should **not** be None, right?",
        "@myrrc, please, do not merge incorrect code (including comments) into `main` with the hope of fixing it in another PR. Another PR may never happen, it might be delayed for an arbitrary amount of time, you can forget, etc.",
        "My understanding is that the difference between /// and !// is that the former applies to the following block, while the latter applies to the upper, which is frequently used for the top-level comments for the crate/module. Here, the comment applies to the struct, so /// seems applicable, or do I miss something?"
      ],
      "neon-flexible-documented-configurations": [
        "I wonder, should we do the same for compute_ctl connections? Especially activity monitor, it runs a bunch of queries pretty often. It probably should be enough to put this option here https://github.com/neondatabase/neon/blob/24d7c37e6ee7b730f983487351721f40922a9745/compute_tools/src/compute.rs#L362",
        "There is a TODO two lines above the place I've linked\r\nhttps://github.com/neondatabase/neon/blob/24d7c37e6ee7b730f983487351721f40922a9745/compute_tools/src/compute.rs#L358-L360\r\n\r\nI was thinking about passing all essential parameters from the compute_ctl without relying on control plane.\r\n\r\nWe can probably still keep an option for control plane to override, not sure if reversing the order here\r\n\r\nSome(options) => format!(\"{} {}\", options, EXTRA_OPTIONS)\r\n\r\nto\r\n\r\nSome(options) => format!(\"{} {}\", EXTRA_OPTIONS, options)\r\n\r\nwill work\r\n",
        "> And then the pseudocode that you wrote actually already exists at\r\n\r\nYeah, but I meant that we should swap `options, EXTRA_OPTIONS` if we want cplane values to take precedence"
      ],
      "neon-cache-performance-preservation": [
        "Not a fan of wiring even more stuff via cplane. The flushLSN should be just the last 'consensus' LSN from sefekeepers, right? Cannot compute figure out it on its own before/during promotion (kinda incomplete sync-safekeepers, just without data copying, or we can even do a normal sync-safekeepers on compute before promotion)?\r\n\r\nThat'd be much more robust and less bug-prone because it doesn't put any implicit assumptions that someone passes the right LSN to us"
      ],
      "neon-configuration-context-alignment": [
        "It should default to false",
        "@knizhnik I still see that it defaults to true"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "I think we need to add endpoint_id to the token. It won't hurt to have this extra protection to ensure that endpoints cannot write to each other sub-paths. Any problems with adding it?",
        "@myrrc looks mostly good to me, thanks, I only have minor comments. I suggest we put it into other PR -- https://github.com/neondatabase/neon/pull/9661 as it belongs to the unlogged storage/S3 proxy RFC, not just to prewarm flow specifically\r\n\r\ncc @MMeent ",
        "If we make prefix like `/epufs/tenants/{tenant_id}/{endpoint_id|any_other_lower_level_key}/...`, we could decide whether to use tenant or tenant+endpoint pair. I think that from the security standpoint, the tenant should be enough as the tenant is our level of multi-tenancy, and we use it for storage auth already",
        "This info was added in another section, so resolving",
        "I think we could elaborate on that, i.e. that we will use JWTs with tenant+timeline IDs, which both provides good tenants isolation and adds an additional protection layer for different timelines to do not mess with each other data "
      ]
    },
    "profile": {
      "location": "Berlin, Germany",
      "company": "@neondatabase",
      "blog": "https://alexk.uk",
      "twitter_username": "ololobuss",
      "site_admin": false,
      "followers": 70,
      "following": 44
    }
  },
  "lmiller1990": {
    "repos": [
      "cypress-io/cypress"
    ],
    "entries": [
      {
        "slug": "cypress-avoid-brittle-css-patterns",
        "title": "Avoid brittle CSS patterns"
      },
      {
        "slug": "cypress-avoid-redundant-io-operations",
        "title": "avoid redundant I/O operations"
      },
      {
        "slug": "cypress-await-promise-returning-functions",
        "title": "await promise-returning functions"
      },
      {
        "slug": "cypress-choose-appropriate-log-levels",
        "title": "Choose appropriate log levels"
      },
      {
        "slug": "cypress-consistent-formatting-preferences",
        "title": "Consistent formatting preferences"
      },
      {
        "slug": "cypress-consistent-naming-patterns",
        "title": "Consistent naming patterns"
      },
      {
        "slug": "cypress-document-non-obvious-code",
        "title": "Document non-obvious code"
      },
      {
        "slug": "cypress-ensure-async-synchronization",
        "title": "ensure async synchronization"
      },
      {
        "slug": "cypress-explain-non-obvious-code",
        "title": "Explain non-obvious code"
      },
      {
        "slug": "cypress-function-decomposition-clarity",
        "title": "function decomposition clarity"
      },
      {
        "slug": "cypress-graphql-mutation-design",
        "title": "GraphQL mutation design"
      },
      {
        "slug": "cypress-network-data-encoding",
        "title": "Network data encoding"
      },
      {
        "slug": "cypress-optimize-dynamic-loading",
        "title": "optimize dynamic loading"
      },
      {
        "slug": "cypress-prefer-existence-over-truthiness",
        "title": "prefer existence over truthiness"
      },
      {
        "slug": "cypress-prefer-modern-composition-patterns",
        "title": "prefer modern composition patterns"
      },
      {
        "slug": "cypress-prefer-semantic-test-selectors",
        "title": "Prefer semantic test selectors"
      },
      {
        "slug": "cypress-prefer-standard-terminology",
        "title": "Prefer standard terminology"
      },
      {
        "slug": "cypress-prioritize-backward-compatibility",
        "title": "prioritize backward compatibility"
      },
      {
        "slug": "cypress-prioritize-jsx-readability",
        "title": "Prioritize JSX readability"
      },
      {
        "slug": "cypress-prioritize-naming-clarity",
        "title": "Prioritize naming clarity"
      },
      {
        "slug": "cypress-safe-null-access-patterns",
        "title": "Safe null access patterns"
      },
      {
        "slug": "cypress-simplify-complex-expressions",
        "title": "Simplify complex expressions"
      },
      {
        "slug": "cypress-standardize-api-patterns",
        "title": "standardize API patterns"
      },
      {
        "slug": "cypress-test-version-compatibility",
        "title": "Test version compatibility"
      },
      {
        "slug": "cypress-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "cypress-use-exact-dependency-versions",
        "title": "Use exact dependency versions"
      }
    ],
    "comments": {
      "cypress-test-version-compatibility": [
        "I do not have a good solution here. This test cannot execute - in fact, it doesn't even compile,. since `cypress/react` assumes `react-dom/client` exists, which is a React 18 only API. This causes Vite to throw a compilation error.",
        "Interesting idea... I haven't `proxyquire` - It might be possible, we'd need to make sure it works for both bundlers... at this point, I feel like a clean migration UI might even be more simple and cleaner, without adding too much technical complexity. As we also discussed earlier, we won't rush this for Cy 12, so maybe we can spend a bit more time thinking about it."
      ],
      "cypress-standardize-api-patterns": [
        "I think it would be good to have a standard way to communicate between server <-> desktop GUI. The standard payload might be:\r\n\r\n```js\r\n{\r\n  type: 'success' | 'error',\r\n  data: any\r\n  event: string\r\n}\r\n```\r\n\r\nThis makes it easy to have a type safe event emitter - see below."
      ],
      "cypress-await-promise-returning-functions": [
        "`getConfig` no longer returns a promise..., so no more `then`, and no more `tap`, so I just made it into a regular old function, replacing `then` with `await`. ",
        "Right, good catch - I forgot that `fs.writeFile` is actually wrapped with promisify."
      ],
      "cypress-document-non-obvious-code": [
        "Maybe a link to the GH issue? This is a hack, but I don't know if we will ever isolate the issue (seems incredibly hard to narrow down). It might be good to at least reference why we had to do this in the first place."
      ],
      "cypress-graphql-mutation-design": [
        "I am still new to GraphQL and Nexus - why do we return `true` for all these instead of the value such as `ctx.app`?\r\n\r\nEdit: is this because we now use the `t.livePlugin`, which means that the client will automatically refetch instead of relying on the return type of the mutation?",
        "Good idea, will do - so then we just decode the ID and get the absolute path (which is what we want).",
        "I updated all the mutations and queries to return `Project`, it feels much more natural now."
      ],
      "cypress-choose-appropriate-log-levels": [
        "This PR was purely a \"move the things around\" PR, so I guess whatever is there was the original intention. \r\n\r\nThat said, I'd say `debug` is probably a better fit for this. I will make this change.",
        "I did it: https://github.com/cypress-io/cypress/pull/18054/commits/599e113c0bd93f4c592e73a3ffd709040f0be14d"
      ],
      "cypress-avoid-brittle-css-patterns": [
        "Should we give this a class instead of just styling the raw HTML elements? I feel like this is a bit more brittle and a little hard to understand in general."
      ],
      "cypress-use-descriptive-names": [
        "Having an async getter seems kind of strange. Should `storybookInfo` be a method called `loadStorybookInfo` or something to that meaning?",
        "nit: I think `hasInitialBuildSucceeded` is a little more natural."
      ],
      "cypress-optimize-dynamic-loading": [
        "I am not super familiar w/ code splitting - what exactly is the benefit of naming this chunk here?"
      ],
      "cypress-prefer-modern-composition-patterns": [
        "Instead of creating a ref for `promptToShow` then watching and mutating it based on `props`, shouldn't we just make `promptToShow` a computed value? \r\n\r\n```ts\r\nconst promptToShow = computed(() => {\r\n  // ... various logic around shouldShowPrompt, etc ...\r\n  return prompts[props.gql?.app?.activeProject?.savedState]\r\n})\r\n```\r\n\r\nThen for `clear-force-open`, instead of setting `promptToShow = ''`, we would just have a boolean such as `shouldShowPrompt` and set it to `false`. Unless I'm missing something, it seems like this would reduce the amount of computations and watchers needed.\r\n\r\nBasically when you combine a `watch` with a reactive value that then assigns another reactive value, you are basically just re-inventing `computed`. In this case, the prompt to show is just derived data from `props`, so we should be able to do it with a single `computed`. I think `watch` is generally useful for a side effect such as making an API call, etc, it feels like a code smell to use it to just reassign to another reactive variable. \r\n\r\nIf it helps, `computed` also takes an object of options with `get` and `set`: https://v3.vuejs.org/guide/reactivity-computed-watchers.html#computed-values",
        "Can consider `watchEffect` which is like `watch` and `immediate: true` by default. It will automatically collect all reactive dependencies and re-run when they change. Could also be a ternary:\r\n\r\n```ts\r\nwatchEffect(() => {\r\n  docsMenuVariant.value = props.forceOpenDocs ? 'ci1' : 'main'\r\n})\r\n```",
        "We should not do `$emit`, since we do not get type safety. We should always use the `emit` function returned from `defineEmits`.",
        "Any reason not to use the more concise and closer to the TS generic syntax?\r\n\r\n```ts\r\ndefineProps<{\r\n  type: IconType\r\n}>\r\n```\r\n\r\netc? Then we don't need the `as` or `PropType` casts - less work for Volar, so the completion should be much faster.\r\n```",
        "Typing `props.` gives us immediate TS inference - no awkward pause while the Volar language server figures out what to do."
      ],
      "cypress-prioritize-naming-clarity": [
        "Is there really a compelling reason to have all these aliases? `@cy/components` is quite confusing - unless I'm missing something, why don't we just write `@packages/frontend-shared/src/gql-components`? Basically we just are avoiding writing `@packages/frontend-shared` and instead writing `@cy`, not sure if this is really that valuable.\r\n\r\nHappy to hear other thoughts, but I think it raises the complexity of the code base (hard to know where an alias points) for little to no actual value."
      ],
      "cypress-safe-null-access-patterns": [
        "```suggestion\r\n    const showMarkdown = computed(() => props.command.renderProps?.message ?? false)\r\n```",
        "unless we don't have support for `.?` ?",
        "will `message` or `markdown` *always* exists? Can they be undefined?"
      ],
      "cypress-ensure-async-synchronization": [
        "I'd say so. If some activities need to occur before the UI is rendered, I suppose something like\r\n\r\n```js\r\nasync function start (openElectron) {\r\n  // do stuff\r\n  await initializeThings()\r\n\r\n  openElectron()\r\n}\r\n```\r\n\r\nI'd say once we untangle the mess that is `open_project`, this problem will natural disappear."
      ],
      "cypress-simplify-complex-expressions": [
        "I think we can make this cleaner and more idiomatic from a TS and JS point of view with a type guard and using `includes` instead of `indexOf`. I created a PR against this branch, what do you think? \r\n\r\nI created a PR with the change to show you how it looks: https://github.com/cypress-io/cypress/pull/20381/files\r\n\r\n",
        "Can be more concise:\r\n\r\n```ts\r\nconst docsMenuVariant = ref<DocsMenuVariant>('main')\r\n```"
      ],
      "cypress-prioritize-backward-compatibility": [
        "Actually this *is* backwards compatible - I updated e2e to always work with an array of specs. At this point it will always be an array with 1 spec in it, like `[{ relative: '...', absolute: '...' }]`. For all tests, it will be `[{ relative: '_all' }]` (or whatever it is).\r\n\r\nThe idea here was to avoid having two modes, and just have one - multi-spec. Then, eventually, e2e can be updated to support multiple specs too (eg, either using a checkbox in the GUI or passing multiple specs to the `--spec` arg). Win win!\r\n\r\nDoes this make sense? We can jump on a call to discuss this more, I went over this design idea w/ @chrisbreiding and @JessicaSachs. See here for a more detailed breakdown: https://cypress-io.atlassian.net/browse/CT-164?focusedCommentId=12692\r\n\r\nWhat do you think?"
      ],
      "cypress-network-data-encoding": [
        "that's right! It was encoded everywhere else correctly except here."
      ],
      "cypress-use-exact-dependency-versions": [
        "Will do."
      ],
      "cypress-avoid-redundant-io-operations": [
        "should cache these instead of hitting filesystem each time but outside scope of this PR"
      ],
      "cypress-consistent-naming-patterns": [
        "Not really related to this PR but is there a specific reason we don't like to refer to props as `props.`?\r\n\r\nI prefer to use `props.` but either way we should lint for one of the other.\r\n\r\nLet's add this to our list of \"to lint\", including\r\n\r\n- enforce using `props.` (or not using `props.`)\r\n- kebab vs camel case\r\n\r\n",
        "Just for consistency - don't we want **all** fragments to be named `ComponentName_UniqueName` (even if there is only one fragment per component)? In this case it'd be `OpenBrowserList_Browsers`, if I'm understanding this correctly.\r\n\r\nEg, because of how we generate the TS from the fragments, without a `ComponentName_` prefix it would be possible to have conflicting fragment names (even if unlikely). If we do `ComponentName_FragmentName` all the time, we will avoid this edge case.",
        "Ok, done",
        "Should this be `RouterLink`? We should either use PascalCase or kebab-case, but not a mix.\r\n\r\nI wonder if we can lint for this.",
        "Sure, so in this case `RouterLink` should be title case?"
      ],
      "cypress-prefer-existence-over-truthiness": [
        "Any reason to prefer `=== undefined` over `(!keyEntry)`?"
      ],
      "cypress-prioritize-jsx-readability": [
        "I'm interested in the team's opinion on this style, especially @dmtrKovalenko and @agg23 who have worked on very big React code-bases. In my past experience in React apps, we tried to keep the `return` function simple and logic free, instead opting for a variable. Eg:\r\n\r\n```tsx\r\nconst specListContent = state.specs.length < 1 \r\n  ? <NoSpec />\r\n  : <SpecList />\r\n\r\n// ...\r\n\r\nreturn (\r\n  <div>\r\n    {specListContent}\r\n  </div>\r\n)\r\n```\r\n\r\nI don't have a super strong opinion, while I do personally prefer this style it's not really a big deal either way. What do you guys think?\r\n\r\nUnrelated but since Vue does not give you this option, since it uses `<template>`, the big Vue code-bases I worked on just inline everything, same as what is done here."
      ],
      "cypress-explain-non-obvious-code": [
        "Can we put a link to where these came from? So if someone looks at in the future and wants to update they can copy paste again - actually reading this type is basically impossible unless you know Vue 3 inside-out",
        "What does this do? Can you put a comment explaning (very hard to understand regexp without knowing expected input)"
      ],
      "cypress-prefer-semantic-test-selectors": [
        "I agree with this, the main reason I used data selectors here is there are no accessibility selectors (yet), I just hacked together a UI to test the wiring out and give us some basic protection against regressions as we iterate quickly over the next few weeks. \r\n\r\nWe should add those and update the tests with better a11y conventions when someone gets around to actually styling these based on the mocks (probably you, since you seem to be emerging at the resident UI and a11y guy). This should be in the next sprint."
      ],
      "cypress-prefer-standard-terminology": [
        "Let's make all new files `tsx` - it \"just works\", no reason not."
      ],
      "cypress-consistent-formatting-preferences": [
        "not a blocker, but what do you think about putting the positive statement first in the if statement? eg:\r\n\r\n```\r\nif (state.initialBuildSucceed) {\r\n  rerun() // most likely path\r\n} else {\r\n  // ... not often happening\r\n}\r\n```\r\n\r\njust a style thing, no need to make this change to get the PR merged :)"
      ],
      "cypress-function-decomposition-clarity": [
        "Not sure we need to await `outputFileSync` - does this return a `Promise`?"
      ]
    },
    "profile": {
      "location": "Brisbane, Australia",
      "blog": "https://lachlan-miller.me",
      "twitter_username": "Lachlan19900",
      "site_admin": false,
      "followers": 1330,
      "following": 10
    }
  },
  "ZeeshanTamboli": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-avoid-render-cycle-allocations",
        "title": "Avoid render cycle allocations"
      },
      {
        "slug": "material-ui-distinguish-nextjs-routers",
        "title": "Distinguish Next.js routers"
      },
      {
        "slug": "material-ui-document-design-decisions",
        "title": "Document design decisions"
      },
      {
        "slug": "material-ui-document-implementation-decisions",
        "title": "Document implementation decisions"
      },
      {
        "slug": "material-ui-effect-hook-best-practices",
        "title": "Effect hook best practices"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-graceful-component-errors",
        "title": "Graceful component errors"
      },
      {
        "slug": "material-ui-isolate-dom-security-boundaries",
        "title": "Isolate DOM security boundaries"
      },
      {
        "slug": "material-ui-maintain-configuration-accuracy",
        "title": "Maintain configuration accuracy"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-parameter-interaction-design",
        "title": "Parameter interaction design"
      },
      {
        "slug": "material-ui-strict-mode-proof-hooks",
        "title": "Strict mode-proof hooks"
      },
      {
        "slug": "material-ui-test-behavior-not-implementation",
        "title": "Test behavior not implementation"
      },
      {
        "slug": "material-ui-use-direct-path-imports",
        "title": "Use direct path imports"
      },
      {
        "slug": "material-ui-use-screen-queries",
        "title": "Use screen queries"
      },
      {
        "slug": "material-ui-use-slots-for-composition",
        "title": "Use slots for composition"
      },
      {
        "slug": "material-ui-use-theme-utilities-consistently",
        "title": "Use theme utilities consistently"
      }
    ],
    "comments": {
      "material-ui-distinguish-nextjs-routers": [
        "This is incorrect. The difference isn’t between the Next.js pages router and app router. The `id=\"__next\"` is used by **Next.js**, while `id=\"root\"` is used by **Vite** and other SPA frameworks.\r\n\r\nIt's that we need to mention that since Next.js 13+, you need to _manually_ add `id=\"__next\"` to the root element i.e `body`. It isn't automatically added like before. For Vite, the root element typically looks like this:\r\n\r\n```html\r\n<div id=\"root\"></div>\r\n```\r\nSo, the `important` option should use `#__next` for Next.js and `#root` for Vite.",
        "> Inclusion of Troubleshooting block is okay , or needs to be removed ?\r\n\r\nIt's fine. Just need to tweak point 2 by removing the mentioning of `root` as Next.js ID."
      ],
      "material-ui-document-implementation-decisions": [
        "Added."
      ],
      "material-ui-use-theme-utilities-consistently": [
        "According to the [docs](https://mui.com/material-ui/migration/upgrade-to-v7/#theme-behavior-changes:~:text=It%27s%20recommended%20to%20use%20the%20theme.vars.*%20as%20values%20in%20your%20styles%20to%20refer%20to%20the%20CSS%20variables%20directly%3A), it's recommended to use theme.vars.* directly in your styles.",
        "I suggest using the `styled` API, like in the Customization section, and naming the component `StyledToggleButtonGroup`. Since the `sx` prop uses `styled` under the hood, `styled` is more performant and, in my opinion, easier to read—especially with longer style definitions.",
        "I don't think all this logic is needed.  We can simply reset inherited `line-height` style using `line-height: normal`:\r\n```diff\r\n--- a/packages/mui-material/src/Chip/Chip.js\r\n+++ b/packages/mui-material/src/Chip/Chip.js\r\n@@ -89,6 +89,7 @@ const ChipRoot = styled('div', {\r\n       alignItems: 'center',\r\n       justifyContent: 'center',\r\n       height: 32,\r\n+      lineHeight: 'normal',\r\n       color: (theme.vars || theme).palette.text.primary,\r\n       backgroundColor: (theme.vars || theme).palette.action.selected,\r\n       borderRadius: 32 / 2,\r\n```\r\nDocs: https://developer.mozilla.org/en-US/docs/Web/CSS/line-height#normal"
      ],
      "material-ui-avoid-render-cycle-allocations": [
        "Makes sense. While useMemo technically works here, it is meant for **derived values** based on dependencies. Updated in https://github.com/mui/material-ui/pull/46333/commits/fd86ebd2caee0137c24e72eb2d5c83706759a317.",
        "Done in https://github.com/mui/material-ui/pull/46333/commits/acc42c28a2a484209f2196d11bb5e5e51a9e5948"
      ],
      "material-ui-effect-hook-best-practices": [
        "Are you sure layout effect is absolutely necessary here?",
        "I used our util `useEnhancedEffect` instead which handles SSR as well."
      ],
      "material-ui-document-design-decisions": [
        "This is internal component so maybe we don't need to document in the types that `getTabbable` prop now accepts three parameters: https://github.com/mui/material-ui/blob/master/packages/mui-material/src/Unstable_TrapFocus/FocusTrap.types.ts#L13, but it would be better to add it."
      ],
      "material-ui-test-behavior-not-implementation": [
        "Let's not test the code implementation details.  Instead, we can verify that it does not throw an error when nested options are provided to `Autocomplete`. We should trigger the user interactions step by step as provided in the issue and check that it does not crash in the end.\r\n\r\n1. The user opens the list and selects an option.\r\n2. The user clears the selected option from input.\r\n3. The user reopens the autocomplete.\r\n4. It should not crash.\r\n\r\nAlso, please add this test in `Autocomplete` test file (`Autocomplete.test.js`). Let me know if you need any help.   ",
        "Please delete the test from the `useAutocomplete.test.js`. Any logic we have in future related to highlighting will be in the `useAutocomplete` hook only which will always be used in the Material-UI `Autocomplete` component.",
        "We could. Done."
      ],
      "material-ui-follow-library-recommendations": [
        "If you test it manually in the documentation preview you'll see that a double-click is necessary to grab the handle for resizing. However, it's essential not to release the mouse button after the second click. Simply clicking once with the mouse doesn't work. I'm using the touchpad on my laptop.\r\n\r\n_Update:_ You're correct. Left-clicking using the mouse button on touchpad and resizing while keeping the button pressed works fine. I've adjusted it to use only `mouse.down()`."
      ],
      "material-ui-parameter-interaction-design": [
        "No need to define it again here since `useAutocomplete` types already has it and Joy UI and Material UI extends the hook's types.\r\n```suggestion\r\n```"
      ],
      "material-ui-graceful-component-errors": [
        "A standalone `Tab` isn't functional: https://stackblitz.com/edit/j4aahksg — I don't see how it could be useful on its own."
      ],
      "material-ui-maintain-configuration-accuracy": [
        "`eslint` related packages and `globals` packages isn't used anywhere. They can be removed."
      ],
      "material-ui-meaningful-and-consistent-names": [
        "Done.",
        "Why is this type needed above in docs/src/pages/premium-themes/onepirate/modules/components/Button.tsx?",
        "This still doesn't reply my question. It's not about `a` tag here, it is the `type` attribute that is redefined and why you need to have a new TS type `ConstrainedButtonProps` which is used in `docs/src/pages/premium-themes/onepirate/modules/components/Button.tsx` file. Why `ButtonProps` can't be used there like earlier? Why is there an error with `ButtonProps` in that file?",
        "> However, this fix created another potential issue - the updated `ButtonProps` would allow any string value for the `type` attribute, not just the valid HTML button types ('button', 'submit', 'reset').\r\n\r\nWhy the updated `ButtonProps` is allowing any string value for the `type` atrribute instead of the valid HTML button types?\r\n",
        "> The error occurs because the updated ButtonProps type allows the type property to be a string | undefined (inherited from React.ButtonHTMLAttributes), which is too general.\r\n\r\nWhat you are saying is wrong. `React.ButtonHTMLAttributes` already does support `type` `\"submit\" | \"reset\" | \"button\" | undefined;`.",
        "```suggestion\r\n    const hiddenElements = getHiddenElements(container);\r\n```",
        "I'm not sure what you mean by `textareaHandle`. But you're right, it shouldn't be `event`. I've updated it to `textareaElement`. According to the documentation here: https://playwright.dev/docs/next/api/class-locator#locator-evaluate, `pageFunction` takes a element as an argument.",
        "I had the same thought. The issue is that `getTagProps` returns properties specific to a Material UI Chip (or tag?), like `disabled`, and `onDelete`. This callback is meant to be spread only when using a _custom_ Material UI Chip. Maybe we should rename it to `getChipProps`.",
        "@michaldudak @DiegoAndai I've made the changes and updated the docs. I only replaced \"tag\" in the public API. Internally, some methods and variables still use \"tag\" since the deprecated `renderTags` depends on them. Renaming them would mean duplicating methods with the same logic for `renderValue`."
      ],
      "material-ui-use-direct-path-imports": [
        "I'm not sure this works. Can you share a reproduction where all these steps succeed and bundle size is reduced? I think Vite uses the esm bundle by default, but I'm not certain. cc @Janpot",
        "I wouldn’t replace the entire “Option two: use a Babel plugin” section. I’d keep the original content and avoid adding all the new material."
      ],
      "material-ui-use-screen-queries": [
        "You can use `screen.getByRole` below instead of passing `getByRole` parameter,"
      ],
      "material-ui-isolate-dom-security-boundaries": [
        "Can you explain this part of the logic with an example? It's a little hard to follow."
      ],
      "material-ui-strict-mode-proof-hooks": [
        "You're right that the initializer inside `useState` only runs once per mount. However, in React’s Strict Mode (development only), it's **intentionally called twice** to detect impure logic. Since `registerTab` mutates internal state, calling it twice would incorrectly register the tab multiple times, shifting tab indices and breaking the selection or indicator logic.\r\n\r\nTo avoid this, we guard it with `hasRegisteredRef`, ensuring `registerTab` runs only once — even in development. In production, the guard has no effect because the initializer runs only once as expected.\r\n\r\nIdeally, the initializer should be pure (per [React docs](https://react.dev/reference/react/useState#my-initializer-or-updater-function-runs-twice)), but we intentionally break that rule here to **support SSR** — specifically, to precompute tab metadata so that the correct tab is marked selected on the first render (see [test case](https://github.com/mui/material-ui/blob/6c0f14b50dc7c86134b8bb549da47dc33bf8b06a/packages/mui-material/src/Tabs/Tabs.test.js#L901-L912)). Without it, we get hydration mismatches..\r\n\r\nI considered making `registerTab` idempotent, but that’s not feasible when we need to assign an implicit `value` based on the tab's render order. That requires incrementing a shared index counter (`childIndexRef`) — and we can’t require users to always provide explicit values to `Tab` without introducing a breaking change.\r\n\r\nThis approach strikes a balance: it ensures SSR correctness, avoids hydration issues, and works with wrapper components like `<Tooltip><Tab /></Tooltip>`, while remaining safe under React’s development behavior.\r\n\r\nOpen to suggestions if you think there's a cleaner way to achieve this.",
        "> If I understand it correctly this won't work well if you remove a tab dynamically (as there's no unregister function)\r\n\r\nIt won't. But it isn't supported even in latest version.\r\n\r\nThis PR: https://stackblitz.com/edit/ry4fan5c-t3b4771r\r\nMaster: https://stackblitz.com/edit/ry4fan5c-oqugmytq",
        "Yes, registering during the effect phase would prevent it from running on the server, which breaks SSR.\r\n\r\n> Perhaps we could register both during rendering and in an effect and make the register operation idempotent (or register conditionally if it hasn't been registered yet). This will allow the use of the unregister function in the effect cleanup.\r\n\r\nThat could work well only if tabs always have explicit `value` props. But in our case, we also support implicit values based on render order, like this:\r\n\r\n```tsx\r\nconst [tab, setTab] = React.useState(1);\r\n\r\nconst handleChange = (event, newValue) => {\r\n  setTab(newValue);\r\n};\r\n\r\nreturn (\r\n  <Tabs value={tab} onChange={handleChange}>\r\n    <Tab label=\"one\" />\r\n    <Tooltip title=\"two helper\">\r\n      <Tab label=\"two\" />\r\n    </Tooltip>\r\n  </Tabs>\r\n);\r\n```\r\n\r\nHere, tabs derive their `value` from their render position (i.e., first tab = 0, second = 1), using an internal `childIndexRef`.\r\n\r\nIf we register both during render and in an effect:\r\n\r\n* In React Strict Mode (dev only), render and effect each run twice i.e total of 4 registrations.\r\n* Even without Strict Mode, a single tab would be registered twice. (one in first render and second in effect).\r\n\r\nSo, there would be 4 child indexes.\r\n\r\nWhile `registerTab` is already idempotent when used with explicit values (via `valueToIndex.has(finalValue)`), we can't enforce `value` on Tab without introducing a breaking change.\r\n",
        "> Deriving the value from the position sounds good to me 👍🏼\r\n\r\nThis logic was already present when `cloneElement` was used. Just picked that up here.\r\n\r\n> But if we create a `value` for the position on our side, then we would have a `finalValue`, no?\r\n\r\nYes, we do generate a `finalValue` based on position internally when value isn't provided. The issue is that this implicit value depends on render order and a shared index (`childIndexRef`), which is incremented during registration.\r\n\r\nIf we allow `registerTab` to run multiple times — as suggested above, in both render and effect — the index keeps increasing, and the same tab ends up with different `finalValue`s across renders. That breaks selection and causes hydration mismatches.\r\n\r\nWith explicit `value`, we don’t rely on index state, so idempotency is safe. But for implicit values, the act of generating `finalValue` is tied to mutable state — so calling `registerTab` multiple times isn’t safe unless we move to require explicit values, which would be a breaking change.",
        "I don't think this will work.\r\n\r\n> In any subsequent register call, we use `finalValue` to identify the Tab\r\n\r\nWhy do we want to pass the same `finalValue` to the next registration call? If we want to pass this stored `finalValue`, we shouldn't call `registerTab` again in the first place (supposedly in the effect).\r\n\r\nFeel free to edit the code if you have any ideas.\r\n\r\n----\r\n\r\nEven in Base UI, they are making the `value` prop required on Tab: https://github.com/mui/base-ui/pull/2124. \r\n\r\nSupporting implicit `value` in Tab cause them issues like https://github.com/mui/base-ui/issues/1880.",
        "> Because we want to return the unregistering callback from the effect, so it's run on unmount.\r\n\r\nBut wouldn't the useEffect's setup function do nothing? It would simply return the output given as an input **always** when doing `registerTab(finalValue)`.\r\n\r\n> This is not an option for us unless we want to wait for a new major.\r\n\r\nYes, not an option now.\r\n\r\n> We're already supporting implicit value, aren't we? With or without my suggestion.\r\n\r\nYes, we are already. Just wanted to point out some issues. I thought it would help us to understand.",
        "I tried doing this in https://github.com/mui/material-ui/pull/46333/commits/7221ea83aeb156e555c031bd9b0062124a1971b6 but the tests fail. Any idea why? However, it works locally on browser.",
        "Tests pass now. The tests run with Strict Mode, which helped me catch the issue that I wasn’t decrementing the child index on tab unregistration."
      ],
      "material-ui-use-slots-for-composition": [
        "```suggestion\r\n- You should provide a tooltip title using `slotProps.tooltip.title` for each speed dial action.\r\n```"
      ]
    },
    "profile": {
      "location": "Pune, Maharashtra",
      "company": "MUI",
      "blog": "https://in.linkedin.com/in/zeeshantamboli",
      "twitter_username": "ZeeshanTamboli",
      "site_admin": false,
      "followers": 57,
      "following": 8
    }
  },
  "myrrc": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-cargo-dependencies",
        "title": "Optimize cargo dependencies"
      },
      {
        "slug": "neon-optimize-what-matters",
        "title": "Optimize what matters"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      }
    ],
    "comments": {
      "neon-environment-specific-config-defaults": [
        "We should vendor these changes once PR gets ready for review, downloading data from Internet in tests is a bad idea"
      ],
      "neon-stage-configuration-changes-gradually": [
        "We need to parse GUC in case ComputeSpec's fields are missing for backward compatibility",
        "Fixed",
        "Fixed"
      ],
      "neon-optimize-what-matters": [
        "Yeah, that's why I want to gate it",
        "A very minor thing, but as `threshold = UINT64_MAX` only for last bucket, we can loop from 0 to `NUM_QT_BUCKETS - 1`, write last iteration manually and avoid branching in assigning `bucket_le`"
      ],
      "neon-clear-consistent-identifier-names": [
        "Fixed"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "We already have a service for prewarm (endpoint_storage) in neon/"
      ],
      "neon-structure-endpoints-for-rest": [
        "Fixed",
        "Fixed, made POST",
        "success doesn't allow you to omit the body"
      ],
      "neon-proper-option-type-usage": [
        "Lsn isn't discriminated against Lsn::INVALID so Option<> would increase its size, not sure it's the best option",
        "On the other hand, making it a discriminant (or Lsn:::MAX) isn't an option as well, so I'll use your approach."
      ],
      "neon-document-parameter-choices": [
        "Fixed"
      ],
      "neon-avoid-flaky-tests": [
        "```suggestion\r\n@pytest.mark.skipif(not USE_LFC)\r\ndef test_lfc_prewarm(neon_simple_env: NeonEnv):\r\n```"
      ],
      "neon-keep-files-focused-small": [
        "Fixed"
      ],
      "neon-extract-and-reuse": [
        "This check and loop are further used in other functions, can we extract them to a separate helper function?"
      ],
      "neon-proper-metrics-design": [
        "I thing `counter` is better since \"number of autovacuum runs\" is non-decreasing"
      ],
      "neon-minimize-unnecessary-allocations": [
        "nit: May we use `itertools::join` here to avoid constructing a separate Vec?",
        "Also nit: if some of privileges may already be granted, maybe we should \r\n1. Take an iter of privileges\r\n2. Filter out those contained in `already_granted`?\r\n3. If iterator is non-empty, join using itertools and grant?",
        "Fixed"
      ],
      "neon-optimize-cargo-dependencies": [
        "I see there are a lot of Cargo.lock changes which may not be needed.\r\nTry this to possibly reduce the diff\r\n```\r\ngit checkout origin/main -- Cargo.lock\r\ncargo hakari manage-deps\r\ncargo hakari generate\r\ncargo b --locked --frozen\r\n```"
      ],
      "neon-comprehensive-code-documentation": [
        "Yeah, a comment typo. It's None when we prewarm from ourselves' data, will fix in promotion PR.",
        "`//!`?"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "```suggestion\r\n## Authentication and authorization\r\n1. Control plane should generate a JWT token which would be used by compute for authorizing requests\r\nto S3 proxy. This token should be passed in `/compute/api/v2/computes/{id}/spec` route,\r\nparsed by `compute_ctl` as an optional field (to preserve backward compatibility) and passed as-is\r\nto proxy requests during prewarm or prewarm offload. If no token is passed, requests to proxy\r\nshould return 505 Not Supported (other services have `--dev` flags which disable token check or\r\nconfig fields which bypass authentication when absent but that's error-prone) and log the error.\r\n\r\n2. S3 proxy should have `auth_public_key.pem` similar to pageserver\r\nfor spawning an https server for compute requests (this would also further help in getting\r\nPCI-DSS certification) and verifying compute requests. The pemfile should be supplied by\r\ninfra/. As with pageserver, S3 proxy should have `/reload_auth_validation_keys` endpoint\r\nto reload the pemfile from config should it change.\r\n\r\n## Metrics\r\n\r\nIn addition to changing `compute_ctl`'s /status, proxy should provide request duration\r\nmetrics along with result server codes as labels\r\n```\r\n",
        "I believe Matthias's RFC focuses more on the high-level overview, but ok, I'll copy the comments. Btw this will probably be split into tasks for https://github.com/neondatabase/cloud/issues/24225 (see sub-issues)"
      ]
    },
    "profile": {
      "company": "Neon",
      "blog": "https://myrrc.dev",
      "site_admin": false,
      "followers": 20,
      "following": 3
    }
  },
  "lhecker": {
    "repos": [
      "microsoft/terminal"
    ],
    "entries": [
      {
        "slug": "terminal-api-clarity-over-convenience",
        "title": "API clarity over convenience"
      },
      {
        "slug": "terminal-api-parameter-explicitness",
        "title": "API parameter explicitness"
      },
      {
        "slug": "terminal-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "terminal-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "terminal-implement-defensive-validation",
        "title": "Implement defensive validation"
      },
      {
        "slug": "terminal-manage-object-lifetimes-carefully",
        "title": "manage object lifetimes carefully"
      },
      {
        "slug": "terminal-minimize-reference-counting-overhead",
        "title": "Minimize reference counting overhead"
      },
      {
        "slug": "terminal-optimize-algorithmic-choices",
        "title": "optimize algorithmic choices"
      },
      {
        "slug": "terminal-prefer-const-declarations",
        "title": "prefer const declarations"
      },
      {
        "slug": "terminal-protocol-response-formatting",
        "title": "Protocol response formatting"
      },
      {
        "slug": "terminal-reuse-defined-resources-consistently",
        "title": "Reuse defined resources consistently"
      },
      {
        "slug": "terminal-safe-null-handling-patterns",
        "title": "Safe null handling patterns"
      },
      {
        "slug": "terminal-safe-optional-handling",
        "title": "Safe optional handling"
      },
      {
        "slug": "terminal-unicode-homoglyph-validation",
        "title": "Unicode homoglyph validation"
      },
      {
        "slug": "terminal-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "terminal-use-established-configuration-patterns",
        "title": "Use established configuration patterns"
      },
      {
        "slug": "terminal-validate-before-configuration-generation",
        "title": "Validate before configuration generation"
      }
    ],
    "comments": {
      "terminal-reuse-defined-resources-consistently": [
        "As far as I understand this will introduce a duplicate constant into every compilation unit. I think it'd be better if you moved it into `rapidhash_withSeed`. I mean, unless this constant is meant to be part of the API of course."
      ],
      "terminal-implement-defensive-validation": [
        "I don't know that, but I don't believe that we should rely on the assumption that it doesn't. I doubt that this was tested in some way too (after all, can this call even fail?).",
        "After reviewing the kernel code, I found that the call can never fail.",
        "Should we still call `_setPaneContent` if `_takePaneContent` returned null?\r\n\r\n(Thanks for fixing this btw!)",
        "If the hyperlink starts at the origin of the search, couldn't this accidentally leave the boundaries of it?"
      ],
      "terminal-api-parameter-explicitness": [
        "BTW I wonder if we should avoid this Clear/Append code. Is `Inlines` observable? If so, then every call raises an event after all. Wasn't there some kind of ReplaceAll in WinRT?",
        "No, I mean would it make sense to create a `std::vector` locally from the runs first and then do a single call to `ReplaceAll`?",
        "At least in WinUI 1.4 there seems to be some internal lazy logic, so this may be unnecessary: https://github.com/microsoft/microsoft-ui-xaml/blob/winui3/release/1.4-stable/dxaml/xcp/core/text/TextBlock/InlineCollection.cpp#L38",
        "BTW shouldn't we pass `sender` instead of `*this` if anything? The documentation makes it sound like we can also just pass `nullptr` btw: https://learn.microsoft.com/en-us/windows/windows-app-sdk/api/winrt/microsoft.ui.xaml.input.pointerroutedeventargs.getcurrentpoint",
        "Always write `--hyperlinkStartIter` if you don't want the previous result."
      ],
      "terminal-api-clarity-over-convenience": [
        "Indexing our x macros numerically seems unusual to me. Why not use stringly indexing (i.e. as if ActionArgs was a hashmap and the keys strings)?",
        "You already have the `_argDescriptors` list, right? Why not return that one as-is? Each `ArgDescriptor` includes the `Name` as a string already.",
        "Yes, but doesn't that feel more natural for a reflection API? If you were to debug it and see \"5\" it's not immediately clear what you want, whereas with \"windowTitle\" you kind of know already.",
        "FWIW I don't consider this to be an important issue. The only thing I do think we should address is the non-static std::vector."
      ],
      "terminal-protocol-response-formatting": [
        "* Avoid emitting another de-/iconify VT sequence when we encounter a (de)iconify VT sequence during parsing."
      ],
      "terminal-validate-before-configuration-generation": [
        "Do we need to generate the installer profile at all then?"
      ],
      "terminal-manage-object-lifetimes-carefully": [
        "FWIW I personally prefer reference counting, if possible, because of our history of race conditions. However, in this case I think `this` was technically sufficient. I'm fine with either approach.",
        "`get_weak()` to ensure pending calls during revocation can complete without `this` being deallocated.",
        "I wasn't aware we had such an issue. It's quite likely that this fixes it.",
        "`get_weak()` to ensure pending calls during revocation can complete without `this` being deallocated. (Same below.)",
        "Fuck. I just realized this can't work because it allows `this` to be destructed on a background thread which breaks WinUI. Got to use a lock or something instead."
      ],
      "terminal-use-descriptive-identifiers": [
        "I believe it might be better to rename these to contain \"Tab\". Without that, it feels like the functions refer to the entire page including the terminal contents.",
        "I'm always a little confused if XAML wants DIPs or pixels, but I think it wants DIPs in this case, right? Assuming that is correct, I've just changed the variable name and left the rest as-is."
      ],
      "terminal-unicode-homoglyph-validation": [
        "> This is worth thinking about instead of just taking unconditionally.\r\n\r\nI'm unfortunately not entirely sure what you meant. Are you referring to how we should not adopt this pattern unconditionally, or that it may have false-positives in general?"
      ],
      "terminal-safe-optional-handling": [
        "I'd prefer if this was `.value_or(0) != 0` because that would more accurately represent \"truthy\"."
      ],
      "terminal-optimize-algorithmic-choices": [
        "FWIW this should've used a `til::static_map` or some equivalent of it.",
        "The rules around & for return values are just unnecessarily complex in C++. It makes no sense that you can't just write `let x = foo()` and x is inferred to be a reference if `foo()` returns one. The `let` equivalent is `decltype(auto)` which is mouthful no one types.\r\n\r\nInitially I had the stance (for myself) that we should try to take WinRT objects by reference to avoid copies, but looking at the complexity this results in I've come to conclude that simply copying parameters is simpler. (This doesn't apply to STL objects though, of course, since they're not ref-counted.)",
        "Could use a std::move here.",
        "Yeah exactly.",
        "There are multiple uses of `lstrcmpiW` throughout this file (and in the current matching logic before this PR) and I don't think that's a good way to implement this. Each call to `lstrcmpiW` goes through multiple layers just to arrive at `CompareStringEx` which is rather expensive for just comparing two characters with folding.\r\n\r\nIn this particular instance, `currentChar` is already lowercase so the only problem is `currentPatternChar` which could simply get lowercases with ICU and then compared against the `currentChar`. Technically we then also need to take care of the difference between lowercasing and casefolding (e.g. the lowercase of Greek \"sigma\" has two variants), but I think that's a secondary issue.\r\n\r\nFWIW, we also _technically_ need to iterate by codepoint here instead of by character. Right now, this code is limited to UCS2 (no surrogate pair support), but I think that's similarly a secondary concern.",
        "Insert returns a tuple/pair whose second member is a bool. That bool is true if the item got inserted. This would simplify the code here because you don't need to call contains() first.",
        "C++ is a whack language so be careful when binding return values to references. Doing so incorrectly breaks return-value-optimization (as it did here)!",
        "When you have a function that returns something by-value and you bind it to a reference here's what actually happens:\r\n```cpp\r\nconst auto __hidden_variable__ = ActionAndArgs::Deserialize(content);\r\nconst auto& args = __hidden_variable__;\r\n\r\n// ...\r\n\r\nreturn args;\r\n```\r\n\r\nAs you can see in the example, you return a reference at the end, not the original object. This forces C++ to make a copy of the object. If you don't bind it to a reference, then you'll also return the original object at the end and \"return value optimization\" will kick in.\r\n\r\nThe problem is when you have a function that returns a reference, but you bind it to a value. Because then you'll also make a copy:\r\n```cpp\r\nconst auto args = return_a_reference_to_arguments();\r\n//     ^        ^\r\n//     |   Because `args` is a value type, but the function returns a reference,\r\n//     |   this invokes the `vector(const vector& other)` copy constructor.\r\n//     |\r\n// \"auto\" always infers a value type, never refences, etc.\r\n// So, in this case it may be `std::vector`, not `std::vector&`.\r\n```\r\n\r\nC++ has a fix for this con job of supposed type inference and it's this:\r\n```cpp\r\ndecltype(auto) args = return_a_reference_to_arguments();\r\n//   ^\r\n// This always infers the exact type, including references, constness, pointers, etc.\r\n```\r\n\r\nIt couldn't have been just `let` or `var` or something. 😐\r\nIn any case, if you use Resharper (currently free to use via the so-called EAP), you get little inlay hints that tell you when an invisible copy constructor is called. This allows me to quickly spot these issues.",
        "Since RGB is a cube, I think it makes more sense to calculate the squared distance `r * r + g * g + b * b`. That also removes the need for `abs`."
      ],
      "terminal-use-established-configuration-patterns": [
        "Thanks! I'll look into how that's done.",
        "I guess we can just keep it as \"At least Windows Terminal 1.21\" given that the next time we can raise the inboxed version will be far in the future anyway."
      ],
      "terminal-cache-expensive-computations": [
        "This will execute the fzf matching logic twice. I think we should replace the two functions with a single one called `_update` (or similar) which runs `ParsePattern` only once and updates both members.",
        "You can avoid calling the same WinRT function repeatedly here:\r\n```cpp\r\nif (const auto windowName = _WindowProperties.WindowName())\r\n```",
        "AGH! STD REGEX MY MORTAL ENEMY! 😄\r\n\r\nThat said, regex construction can be fairly costly in general, and we should not treat this as some kind of cheap validation. It involves constructing DFAs, etc., after all. I'm not entirely sure anymore where `MatchProfilesEntry` fits into the callstack, but seeing that it's also part of `TerminalSettingsModel`, I'm sure that we can make it cache every regex instance after load and store any warnings into a list. We can then splice its list into the overall `SettingsLoadWarnings` list (or store it directly there, or something like that). Basically, a way to only construct these once.\r\n\r\nFWIW if you want to give `icu.h` a try:\r\n```cpp\r\n#include \"path/to/buffer/out/UTextAdapter.h\"\r\n\r\n// Cache this\r\nUErrorCode status = U_ZERO_ERROR;\r\nconst auto re = Microsoft::Console::ICU::CreateRegex(pattern, 0, &status);\r\nif (status > U_ZERO_ERROR) {\r\n    // bad regex\r\n}\r\n\r\n// Run this\r\nUErrorCode status = U_ZERO_ERROR;\r\nuregex_setText(re.get(), hstring.data(), hstring.size(), &status);\r\nconst auto match = uregex_matches(re.get(), 0, &status);\r\nreturn status == U_ZERO_ERROR && match;\r\n```"
      ],
      "terminal-minimize-reference-counting-overhead": [
        "You should avoid using the WinRT collection types. `IVector` is basically like using a linked list, performance wise. Instead, as far as I can tell, you can just wrap it into a `winrt::multi_threaded_vector` whenever its actually needed. You can avoid making a copy of `_warnings` by using `std::move`, since `_warnings` is only read exactly once anyways, after a settings reload."
      ],
      "terminal-prefer-const-declarations": [
        "I believe we're alone in our usage of the `auto foo{ ... }` syntax. Not even the C++ Core Guidelines use it: https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Res-auto\r\nAs you know, the brace-initializer syntax is historically overladen and rather brittle, so I see people recommend using it only for constructing explicitly named types, and I concur. In fact, since ~C++20 it's actually been going back from there with the consortium's trend towards \"we can be like Rust if we have static analyzers\", hence members recommending even things like `int foo = bar()`, despite the risk for unintentional narrowing."
      ],
      "terminal-safe-null-handling-patterns": [
        "Above you do `_ContentArgs.try_as<NewTerminalArgs>` but here just `try`. If it's not safe to cast it, shouldn't this avoid doing so as well?",
        "This member was left uninitialized."
      ],
      "terminal-choose-efficient-data-structures": [
        "Does `_argDescriptors` need to be a mutable copy for each individual ActionArgs instance?",
        "In that case we should consider making these lazy initialized constant, immutable vectors right? It may tie into my other suggestion to return it as-is.",
        "This seems wasteful. Why not just use the `default:` case in the switch/case you generate below?",
        "It seems as if the `_colorMap` items are supposed to be `IndexType`s. Should we swap `size_t` with `IndexType` here?\r\n\r\nSide note: As an alternative you could store the `_colorMap` items as `int` and initialize them to a negative value to indicate that they're currently not used.",
        "As far as I know, the only thing that C++23 adds are more heterogenous overloads, but it remains opt-in so as to not break existing code. It would also not help us here, because just like C++, cppwinrt is also frozen in time and it's what would require the `is_transparent` attribute on its hash/equal_to structs ([here](https://github.com/microsoft/cppwinrt/blob/f9ec1986083a70d2f99d726b06b00a38cb2d1054/strings/base_std_hash.h#L37-L43))."
      ]
    },
    "profile": {
      "location": "Germany",
      "company": "@microsoft",
      "blog": "",
      "site_admin": false,
      "followers": 591,
      "following": 9
    }
  },
  "lgrammel": {
    "repos": [
      "vercel/ai"
    ],
    "entries": [
      {
        "slug": "ai-async-error-callbacks",
        "title": "Async error callbacks"
      },
      {
        "slug": "ai-consistent-camelcase-naming",
        "title": "Consistent camelCase naming"
      },
      {
        "slug": "ai-consistent-provider-options",
        "title": "Consistent provider options"
      },
      {
        "slug": "ai-consistent-semantic-naming",
        "title": "Consistent semantic naming"
      },
      {
        "slug": "ai-document-api-schemas",
        "title": "Document API schemas"
      },
      {
        "slug": "ai-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "ai-explicit-code-organization-patterns",
        "title": "Explicit code organization patterns"
      },
      {
        "slug": "ai-flexible-api-inputs",
        "title": "Flexible API inputs"
      },
      {
        "slug": "ai-format-for-rendering-compatibility",
        "title": "Format for rendering compatibility"
      },
      {
        "slug": "ai-keep-tests-simple",
        "title": "Keep tests simple"
      },
      {
        "slug": "ai-maintain-api-naming-consistency",
        "title": "Maintain API naming consistency"
      },
      {
        "slug": "ai-optimize-ci-type-checking",
        "title": "Optimize CI type checking"
      },
      {
        "slug": "ai-place-configurations-appropriately",
        "title": "Place configurations appropriately"
      },
      {
        "slug": "ai-provide-actionable-examples",
        "title": "Provide actionable examples"
      },
      {
        "slug": "ai-test-before-documenting",
        "title": "Test before documenting"
      },
      {
        "slug": "ai-type-safe-null-handling",
        "title": "Type-safe null handling"
      },
      {
        "slug": "ai-validate-pattern-matching",
        "title": "Validate pattern matching"
      },
      {
        "slug": "ai-versioning-for-migrations",
        "title": "Versioning for migrations"
      }
    ],
    "comments": {
      "ai-async-error-callbacks": [
        "Wondering if that's the desired behavior - might be good to think through use cases",
        "wonder if it's worth pushing the onError object into `consumeStream` to keep the behavior between `consumeStream` here and the general helper aligned?",
        "the helper is afaik only used internally so we can change the main `stream` object to become a parameter object \r\n\r\n```\r\n{\r\n  stream,\r\n  onError?\r\n}\r\n```"
      ],
      "ai-validate-pattern-matching": [
        "this can lead to issues where the wrong enum value is selected as a partial result, leading to changing enum values. i'd prefer a solution where that cannot happen (either by only returning fully matched enum values, or (more complicated) by returning enum values for which only one value is possible per prefix)",
        "What is this for? Can we do this more elegantly?",
        "interesting - looks like this is not going to work for base64? we need tests around this and make it work for base 64 as well. Also it might be good to have some way of opting (or flagging on the signatures) since this removal won't be needed for images presumably.",
        "btw, is this a separate bugfix that should be extracted into a standalone PR?"
      ],
      "ai-optimize-ci-type-checking": [
        "which one will run in CI? important that we keep full checks there",
        "where will the js files go?"
      ],
      "ai-document-configuration-decisions": [
        "this is a downgrade - why?",
        "we can make zod a normal dependency since this is not a lib we publish",
        "is there a risk that this breaks common js usage of the ai sdk?",
        "needed for the rsc move?"
      ],
      "ai-flexible-api-inputs": [
        "replace tool schemas with json schemas\n\nthis affects structured outputs as well",
        "out of curiosity, what happens if the url is entered as a string? (should ideally be supported)",
        "(no need for PR updates re this comment)",
        "we have special conversion mechanisms that we should be using, no need to this build one-off, and we can also delay and do it on v5 once all providers are there to reduce # of ports.",
        "(the mechanism has been introduced / updated on v5 recently)"
      ],
      "ai-provide-actionable-examples": [
        "need to set expectations:\r\n\r\n- only for green-field prototypes (no migrations yet)\r\n- not for production use\r\n- please provide feedback\r\n- expect large breaking changes while in alpha"
      ],
      "ai-place-configurations-appropriately": [
        "can you make it a constructor parameter (in the options) for the model that is then defined in the providers? (which would have that knowledge)",
        "include the default object generation mode in `config`",
        "This seems like something we could turn into a top-level option eventually?",
        "voice and response_format are setting we have standardized. we usually do not duplicate standardized settings in the providerOptions"
      ],
      "ai-consistent-semantic-naming": [
        "rn to `create...` (start function w/ verb)",
        "Prefer full words for generic in upper case, e.g. `CONTEXT`",
        "convention: start non-classes/types (ie regular vars like schemas) with lowercase letter",
        "wonder how far we should to here. usually we do this but it is also overhead",
        "rename to `maxParallelCalls` or `maxParallelRequests` or `maxConcurrentRequests`. If we rename to `Concurrent` here it would be good to do the same in the model spec",
        "We should return `GenerateObjectResult<TYPE>` (not the default, that's internal)",
        "also can you change `TYPE` to `RESULT` (which is more specific, a generic will always be a type)",
        "rename to `mediaType` and refer to IANA media types (see other examples esp. on v5 branch)",
        "let's fix that in `v5` and move to a pattern similar to groq where we parse all options first",
        "what is the unit? can we include it in the var name to avoid confusion, e.g. `durationInSeconds`"
      ],
      "ai-format-for-rendering-compatibility": [
        "this is somewhat specific. we also have a `contributing/` folder where we can have more detailed guides, architecture docs, etc. What do you think about having a `contributing/how-to-create-a-codemode.md` and then referring to the contributing directory here?"
      ],
      "ai-consistent-provider-options": [
        "make provider-specific. different providers have different structures. have default that matches openai",
        "this is going to be brittle as they change their APIs. Can we expose the raw body from the final result and strip this to a minimum?",
        "Please keep specs fully separate for now.",
        "I've extracted `SharedV2ProviderOptions` and `SharedV2ProviderMetadata` that we can use in all model specs: https://github.com/vercel/ai/pull/5733",
        "Please use the validation approach for provider options that we use elsewhere (so we get a type for type checking and can throw errors before the request). See https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L122 and https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L724 for an example"
      ],
      "ai-type-safe-null-handling": [
        "`undefined` or `never`? (i have used `never` in other places for something similar)",
        "`never` is actually better ts wise imo",
        "and you might want it to be optional ",
        "most stream parts are defined in stream parts afaik. do we need to expose this? otherwise would prefer if it's consistent with the other stream parts. also, `any` is risky. is `unknown` possible?",
        "shouldnt this be automatically typed?",
        "can you add a zod schema and validate instead?",
        "avoid any",
        "also return type should be narrowed via is",
        "you can remove indent via \r\n\r\nfor (const citation of response.message.citations ?? [])",
        "can leave undefined (no `|| ''`) and then use `?.` before `startsWith`"
      ],
      "ai-test-before-documenting": [
        "While this is generally true, it is really a misleading explanation.\r\n\r\nThis happens when a provider API returns with an error. Most of the time, this means that there API cannot be used in this way, e.g. invalid message structure. The user needs to carefully read the actual error message from the provider and figure out what to do.",
        "@Und3rf10w have you tested this with the gemini api? couldn't find it in their docs",
        "I guess we can add the fs.readFile from the executable example"
      ],
      "ai-versioning-for-migrations": [
        "on v5 it does not matter, the release will be `major` regardless. ",
        "Lets have them always use a `patch` changeset - only we should be doing minor/major"
      ],
      "ai-keep-tests-simple": [
        "instead of flags, just set up custom prepare methods or if it's a single test define the test input in the test\r\n\r\nprefer less magic / thinking in tests, often it is very helpful to be able to look at the raw input without any indirections / logic",
        "just inline the parts. unit tests should be as straightforward as possible. if you want to check that includeRawChunks is passed correctly, add a separate test for just that.",
        "an inline snapshot on content is prob easier, also checks order. in general such logic in tests is usually a code smell",
        "is this needed? might make sense to just call `new ChatStore` in the tests to be explicit.",
        "(then you can also inline some variables in the tests to see how the store would look like)",
        "this test can be split up into several tests for the individual callbacks. should also test that they are invoked with the expected objects. testing `store.getmessages` should be a separate test as well.\r\n\r\n(one `it` test should ideally only test 1 \"thing\" so they can fail individually)",
        "please stub the dates instead. see e.g. \r\n\r\nhttps://github.com/vercel/ai/blob/main/packages/ai/core/generate-text/generate-text.test.ts#L1280\r\n\r\n```ts\r\n  _internal: {\r\n    generateId = originalGenerateId,\r\n    currentDate = () => new Date(),\r\n  } = {},\r\n  ```\r\n  \r\nTests should never have any variable aspects such as dates. With stubbed values we can exactly test the passthrough."
      ],
      "ai-document-api-schemas": [
        "add model id and jsdoc?"
      ],
      "ai-consistent-camelcase-naming": [
        "why was this changed to snake case? if that is the case in the code we should fix the code instead",
        "we usually use `camelCase` in the provider options to match conventions (even if the provider uses `snake_case` in their apis)"
      ],
      "ai-maintain-api-naming-consistency": [
        "We need to consider renaming this on the `embed` method returns as well, and potentially for `embedMany`"
      ],
      "ai-explicit-code-organization-patterns": [
        "The value is set in the constructor. Would prefer `private strictMode: boolean` to prevent the impression that the `false` setting here matters.",
        "avoid export `*` - we need to control what we re-export",
        "this is strange - can we directly import (from provider utils or the file)?",
        "can't you replace it with:\r\n\r\n```\r\nimport { convertArrayToReadableStream } from '@ai-sdk/provider-utils/test';\r\n```",
        "ideally for imports we avoid barrel files as much as possible and go for the original source"
      ]
    },
    "profile": {
      "company": "Vercel",
      "blog": "",
      "twitter_username": "lgrammel",
      "site_admin": false,
      "followers": 596,
      "following": 30
    }
  },
  "cipolleschi": {
    "repos": [
      "facebook/react-native"
    ],
    "entries": [
      {
        "slug": "react-native-avoid-synchronous-main-dispatch",
        "title": "Avoid synchronous main dispatch"
      },
      {
        "slug": "react-native-avoid-unnecessary-allocations",
        "title": "avoid unnecessary allocations"
      },
      {
        "slug": "react-native-component-initialization-state",
        "title": "Component initialization state"
      },
      {
        "slug": "react-native-configuration-validation-and-defaults",
        "title": "Configuration validation and defaults"
      },
      {
        "slug": "react-native-descriptive-specific-naming",
        "title": "descriptive specific naming"
      },
      {
        "slug": "react-native-document-configuration-logic",
        "title": "Document configuration logic"
      },
      {
        "slug": "react-native-eliminate-unnecessary-computations",
        "title": "Eliminate unnecessary computations"
      },
      {
        "slug": "react-native-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "react-native-extract-complex-logic",
        "title": "extract complex logic"
      },
      {
        "slug": "react-native-follow-established-naming-conventions",
        "title": "Follow established naming conventions"
      },
      {
        "slug": "react-native-minimize-public-api-surface",
        "title": "minimize public API surface"
      },
      {
        "slug": "react-native-optimize-ci-platform-builds",
        "title": "Optimize CI platform builds"
      },
      {
        "slug": "react-native-platform-aware-configuration-messages",
        "title": "platform-aware configuration messages"
      },
      {
        "slug": "react-native-preserve-component-patterns",
        "title": "preserve component patterns"
      },
      {
        "slug": "react-native-simplify-redundant-logic",
        "title": "simplify redundant logic"
      },
      {
        "slug": "react-native-use-appropriate-log-levels",
        "title": "Use appropriate log levels"
      },
      {
        "slug": "react-native-validate-configuration-formats",
        "title": "Validate configuration formats"
      }
    ],
    "comments": {
      "react-native-component-initialization-state": [
        "Hi @zhongwuzw I made some investigation and I don't think that this is the right fix.\r\n\r\nThe problem with this is that, after recycle, the default value for the UISwitch is `true`. While the oldProp is holding a value of `false`. The new prop is `false`, so the value is not set.\r\n\r\nI think that the right fix should be applied in the `updateProps:` function, like this:\r\n\r\n```diff\r\n-  if (oldSwitchProps.value != newSwitchProps.value) {\r\n+  if (!_isInitialValueSet || oldSwitchProps.value != newSwitchProps.value) {\r\n    BOOL shouldAnimate = _isInitialValueSet == YES;\r\n    [_switchView setOn:newSwitchProps.value animated:shouldAnimate];\r\n  }\r\n```\r\nThis will force ReactNative to set the initial value on the switch in the first rendering, which is the correct behavior, because that will be the first rendering for the recycled component.\r\n",
        "Can you explain why removing this was relevant? Is it necessary to remove it?"
      ],
      "react-native-validate-configuration-formats": [
        "@shubhamguptadream11 assuming the `env.oncall1` and `env.oncall2` variables are correct, is this the right format to pass a list of ids to the `repo-monitor` action?",
        "Added this to avoid reading cocoapods. That's in general better, we can find a way to read those from react-native rather than have a duplication.\r\nHowever, these are fixed for the Release, so they should never change for 0.75"
      ],
      "react-native-avoid-synchronous-main-dispatch": [
        "this might deadlock... Do we have a list of devices that supports 120 vs 60?\r\nI kind of think that we don't have to support 60 FPS devices anymore...",
        "We had deadlocks in internal app at startup because multiple things can request access to the main queue.\r\n\r\nSo imagine that a process starts on the main queue, than dispatch sync to a BG queue, then the other queue calls `RCTSingleFrameInterval` which dispatch sync on the main queue. In this scenario the main queue is locked, waiting for the BG queue and we deadlock.\r\n\r\nI saw this happening in other parts of React Native that looked safe, unfortunately.\r\n\r\nPerhaps we can have `NativeAnimatedTurboModule` require the main queue to be initialized (if it does not need it already) and have the turbomodule compute this in the init. That would be safe.",
        "This could potentially deadlock. We should not run the unsafe variant of this method. Can you change it with `RCTExecuteOnMainQueue`?",
        "One thing that confuses me is that, in your stacktrace, the crash is happening in Thread 26... but this assert should force the app to be on the main thread, which is not the Thread 26... how's this possible?",
        "That's a good explanation, but then we should see crashes in development happening because of the assertion. And IIUC, the app does not crash in development, right?",
        "By looking at the crash log, the JS thread is triggering the invalidation. I think that this is the root of the problem: after the JS thread detect the invalidation, we should jump on the UI thread to invalidate everything...",
        "Can you use [RCTExecuteOnMainQueue](https://github.com/facebook/react-native/blob/main/packages/react-native/React/Base/RCTUtils.h#L41) instead? It avoids the jump if we are already on the main queue."
      ],
      "react-native-use-appropriate-log-levels": [
        "This should go in a different pr. Also:\r\n```suggestion\r\nfunction prebuildLog(\r\n  message /*: string */,\r\n  level /*: 'info' | 'warning' | 'error' */ = 'warning',\r\n) {\r\n  // Simple log coloring for terminal output\r\n  const prefix = '[Prebuild] ';\r\n  let colorFn = (x /*:string*/) => x;\r\n  if (process.stdout.isTTY) {\r\n    if (level === 'info') colorFn = x => `\\x1b[32m${x}\\x1b[0m`;\r\n    else if (level === 'error') colorFn = x => `\\x1b[31m${x}\\x1b[0m`;\r\n    else colorFn = x => `\\x1b[33m${x}\\x1b[0m`;\r\n  }\r\n\r\n  console.log(colorFn(prefix + message));\r\n}\r\n```"
      ],
      "react-native-descriptive-specific-naming": [
        "```suggestion\r\nfunction _downloadPrebuildReleaseTarball(\r\n```\r\nThe leading `_` is ok, that's the convention for \"private\" methods"
      ],
      "react-native-simplify-redundant-logic": [
        "@NickGerleman \r\n> nit: these checks are redundant, since one inverts both the subtraction order, and then inverts again for the difference.\r\n\r\n```suggestion\r\n  CGFloat difference = textHeight - lineHeight;\r\n  CGFloat verticalOffset = difference / 2.0;\r\n```",
        "instead of this prop... can't we just use the `_props` property and check whether it is initialized or not?",
        "Same as before. I'd extract the lambda to avoid duplicating code."
      ],
      "react-native-environment-variable-validation": [
        "```suggestion\r\n        if ENV[\"RCT_USE_PREBUILT_RNCORE\"] == \"1\"\r\n```\r\nIf `RCT_USE_PREBUILT_RNCORE` is not set should not enter in the `if`."
      ],
      "react-native-document-configuration-logic": [
        "@NickGerleman \r\n> It would be best to keep the changes to the Fabric component instead of this one.",
        "@ArekChr I believe that the suggestion here was to not touch the files in the old architecture. The PR already updates the New Architecture, so we are good.\r\n---\r\n@SimpleCreations Thanks for the feedback, I believe that in this case we can keep the fix here. As a general thought, though.. Yes, in the future the Old Architecture will not be maintained. \r\n\r\nRight now, we already released 0.76 and 0.77 where the New Architecture is the default. We are already working on 0.78 and once that's out, 0.75 will go out of support. That is the last version where the Old Architecture is used by default.\r\n\r\nConsider that new features are only developed for the New Architecture and many libraries will be only compatible with the New Architecture, moving forward (reanimated and react-native-vision-camera to mention two of them). So we strongly advise to start migrating to the New Architecture.\r\n\r\nI'd be curious to know what is holding you back in more details. If you can replicate Performance and bundle issues in separate reproducers, we are more than happy to look into them as soon as possible.",
        "shouldn't this be in the external pod? Or are we keeping it here while we support the old JSC and the plan is to move it away or delete it when we only have hermes or the external jsc pod from community?"
      ],
      "react-native-minimize-public-api-surface": [
        "do we really need this extra function in the header? This will be a new public API that we need to mmaintain. Given that it is used only in RCTUtils.mm, can we keep it private in the .mm file instead?"
      ],
      "react-native-configuration-validation-and-defaults": [
        "Thanks for raising this other case. I believe that if there is just the path, it links for all the platforms, right? ",
        "what's the default? The question holds for all the items below\r\n\r\nWhat I don't like is that to do everything, we need to call\r\n```\r\nnode <script> -s -w -b -c\r\n```\r\n\r\nI would rather have the default case that does everything."
      ],
      "react-native-extract-complex-logic": [
        "I'd rather extract all the validation in a separate function. "
      ],
      "react-native-preserve-component-patterns": [
        "shouldn't these two be inverted? or, putting it in another way, can `SafeAreView` work as `View` (and we can remove one indentation level)?"
      ],
      "react-native-eliminate-unnecessary-computations": [
        "@javache No value in calling hash_combine again with seed 0.\r\n```suggestion\r\nreturn color.getUIColorHash();\r\n```"
      ],
      "react-native-platform-aware-configuration-messages": [
        "No, android can't disable the new arch at runtime. You can disable some pieces, but it is not recommended."
      ],
      "react-native-optimize-ci-platform-builds": [
        "we only need to add maccatalyst. We don't care about the others because they are OOT platform.\r\n",
        "yep. But for building in CI, we still have to pass them as a separate param, so we can parallelize the build",
        "we only need to build for ios, ios-simulator, mac catalyst. All the others are OOT platforms that have their own fork, so we can't really build for them."
      ],
      "react-native-avoid-unnecessary-allocations": [
        "I'm not sure we need to create a new pointer with `make_shared` here.\r\nThe type of the scrollEvent is already the right one. If previously we were passing the scrollEvent, we should be able to pass the scrollEvent even now, types should match.",
        "`ScrollEndDragEvent` event is a subclass of `ScrollEvent`, we probably don't need to change this method at all, as you are allowed to pass a subclass to a method that accepts a superclass. The language should take care of it. ([LSP - Liskov substitution principle](https://en.wikipedia.org/wiki/Liskov_substitution_principle))\r\n\r\nCan you remove these changes and test if this works properly without these?\r\n"
      ],
      "react-native-follow-established-naming-conventions": [
        "Method says `download_prebuild_release_tarball`... should we call it `stable`?\r\n```suggestion\r\n        url = release_tarball_url(@@react_native_version, :release)\r\n```",
        "The convention we are following is:\r\n- REACT_NATIVE is shortened to RCT\r\n- The default case, running just `bundle exec pod install` should not change. We want the dependecies to be an opt-in for now, not an opt out.\r\n\r\nI think we should use: `RCT_USE_DEP_PREBUILD` instead of `REACT_NATIVE_DEPS_BUILD_FROM_SOURCE`."
      ]
    },
    "profile": {
      "location": "London, UK",
      "company": "Meta",
      "blog": "https://medium.com/@riccardocipolleschi",
      "site_admin": false,
      "followers": 479,
      "following": 0
    }
  },
  "MikeMcQuaid": {
    "repos": [
      "Homebrew/brew"
    ],
    "entries": [
      {
        "slug": "brew-avoid-variable-name-abbreviations",
        "title": "Avoid variable name abbreviations"
      },
      {
        "slug": "brew-clear-code-examples",
        "title": "Clear code examples"
      },
      {
        "slug": "brew-clear-error-recovery-paths",
        "title": "Clear error recovery paths"
      },
      {
        "slug": "brew-decouple-ci-from-code",
        "title": "Decouple CI from code"
      },
      {
        "slug": "brew-document-ci-pipeline-comprehensively",
        "title": "Document CI pipeline comprehensively"
      },
      {
        "slug": "brew-document-non-obvious-decisions",
        "title": "Document non-obvious decisions"
      },
      {
        "slug": "brew-environment-variable-safety",
        "title": "Environment variable safety"
      },
      {
        "slug": "brew-evaluate-security-control-effectiveness",
        "title": "Evaluate security control effectiveness"
      },
      {
        "slug": "brew-fail-with-messages",
        "title": "Fail with messages"
      },
      {
        "slug": "brew-follow-established-naming-patterns",
        "title": "Follow established naming patterns"
      },
      {
        "slug": "brew-follow-support-tiers",
        "title": "Follow support tiers"
      },
      {
        "slug": "brew-minimize-unnecessary-operations",
        "title": "Minimize unnecessary operations"
      },
      {
        "slug": "brew-optimize-collection-operations",
        "title": "Optimize collection operations"
      },
      {
        "slug": "brew-prefer-explicit-nil-handling",
        "title": "Prefer explicit nil handling"
      },
      {
        "slug": "brew-prefer-flags-over-conditionals",
        "title": "Prefer flags over conditionals"
      },
      {
        "slug": "brew-secure-api-url-parsing",
        "title": "Secure API URL parsing"
      },
      {
        "slug": "brew-simplify-complex-code-blocks",
        "title": "Simplify complex code blocks"
      },
      {
        "slug": "brew-standardize-api-integration-patterns",
        "title": "Standardize API integration patterns"
      },
      {
        "slug": "brew-structure-test-fixtures-clearly",
        "title": "Structure test fixtures clearly"
      },
      {
        "slug": "brew-structured-environment-configuration",
        "title": "Structured environment configuration"
      },
      {
        "slug": "brew-use-ascii-only-urls",
        "title": "Use ASCII-only URLs"
      }
    ],
    "comments": {
      "brew-document-ci-pipeline-comprehensively": [
        "It does a few more things, too?",
        "@Rylan12 `brew readall`, `brew test-bot --only-formulae --test-default-formula`, `brew doctor` seem like the important bits."
      ],
      "brew-prefer-explicit-nil-handling": [
        "```suggestion\r\n      @name = name.presence\r\n      @version = Version.new(version) if version.present?\r\n```",
        "@abitrolly Why does it make sense to accept a blank name from the user here?",
        "Ok, I understand now, thanks.",
        "```suggestion\r\n        if (pypi_extras = extras.presence)\r\n          out += \"[#{pypi_extras.join(\",\")}]\" \r\n        end\r\n```",
        "This feels like it would be nicer to:\r\n- move this logic to an `else` in `livecheck_url_to_string`\r\n- make `livecheck_url_to_string` always return a `String` rather than a `T.nilable(String)`\r\n\r\nAs a general rule/concept: whenever you can remove `T.nilable` usage and `raise` instead: it's nicer to do so when using Sorbet.",
        "```suggestion\r\n          if (bottle = formula.bottle)\r\n```",
        "```suggestion\r\n    resource = github_packages_manifest_resource\r\n    return unless resource&.downloaded?\r\n```",
        "```suggestion\r\n    resource = github_packages_manifest_resource\r\n    return unless resource&.downloaded?\r\n```"
      ],
      "brew-prefer-flags-over-conditionals": [
        "Can you explain why we'd want to claim HTTP2 support is present when `curl` says its not?",
        "Gotcha, thanks. I'd be tempted to just use Homebrew's `curl` in that case rather than have a shim. Can we use `uses_from_macos \"curl\", since:` instead to handle macOS 13 and below?",
        "> @MikeMcQuaid that hack (`uses_from_macos \"curl\", since: :sonoma`) would actually enable the linkage against system curl, see [this build log](https://github.com/Homebrew/homebrew-core/actions/runs/11872759852/job/33086872512).\r\n\r\n@chenrui333 on which macOS versions would it link against/not link against system `curl`? Thanks!",
        "> I know this would be only for ventura builds.\r\n\r\nFor one, non-latest macOS version this seems overkill, personally, but I'm open to thoughts from other maintainers.",
        "@chenrui333 Thanks, not worth doing for a single OS version IMO, sorry."
      ],
      "brew-standardize-api-integration-patterns": [
        "Does `brew services list --json` contain the information we need? If so, would be nice to use that instead. If not, maybe it'd be nice to add that information in there.",
        "I think this needs to be more specific as to where it's used. It's not using this instead for e.g. all GitHub searches."
      ],
      "brew-document-non-obvious-decisions": [
        "Please add a comment, and ideally turn this into a constant/variable, explaining why 100 is used otherwise it seems arbitrary.",
        "```suggestion\r\n              # maximum length of PR body is 65,536 characters so let's truncate release notes to half of that.\r\n              body = github_release_data[\"body\"].truncate(32_768)\r\n```\r\n\r\nwould be nicer to truncate based on an actual length here",
        "@bevanjkay worth handling in a follow-up I think. My suggestion would be that `--bump-synced` does not include any release notes or it handles the truncation its own way/as well.",
        "As mentioned before: I'd love to see this be a temporary stop-gap while we add a DSL. I think this needs some pretty hefty amounts of comments until then explaining why these particular formulae need to be doing what they are doing here."
      ],
      "brew-follow-established-naming-patterns": [
        "- This is a behaviour change as it'll no longer look for `archive|releases` like it did before\r\n- `m` is a poor variable name here, please use a better, longer one\r\n- I think it's worth continuing to set `user` and `repo` without `fetch`",
        "```suggestion\r\n                basename = if File.directory?(path)\r\n                  File.basename(path)\r\n                 else\r\n                   File.basename(path, \".*\")\r\n                 end\r\n                excluded_names.include?(basename)\r\n```",
        "```suggestion\r\n  def self.binary_linked_to_library?(binary, library, prefix = HOMEBREW_PREFIX)\r\n```\r\ngiven it returns a `T::Boolean`",
        "How about:\r\n```suggestion\r\n      return unless cask.livecheck?\r\n```\r\nWould that naming work?",
        "> It makes sense to me, as `#livecheck` returns a [`Livecheck` object](https://github.com/Homebrew/brew/blob/84823d80f71a01eba4febbbdf4259a687219885b/Library/Homebrew/livecheck.rb) (the DSL values), so `#livecheck?` would align with that.\r\n\r\nAnother option would be `bottle_defined?` which is the equivalent for the `bottle` block.",
        "Yup, sounds good thanks!"
      ],
      "brew-clear-error-recovery-paths": [
        "Will this not now be output even if `Homebrew::EnvConfig.github_api_token` is unset? If so, that's undesirable.",
        "I think an extra case checking if `if Homebrew::EnvConfig.github_api_token.present?` and saying `HOMEBREW_GITHUB_API_TOKEN is unset` or similar would improve this, thanks!",
        "Should better handle here where there's no group name. because `Failed setting group \"\"` doesn't seem great.",
        "```suggestion\r\n      sleep_time = 2 ** @attestation_retry_count[bottle]\r\n```\r\nwould be fine with me if you want this to take smaller jumps/be quicker to fail",
        "```suggestion\r\n    ATTESTATION_MAX_RETRIES = 2\r\n```\r\nor \r\n```suggestion\r\n    ATTESTATION_MAX_RETRIES = 3\r\n```\r\nwould be fine with me if you want to fail earlier"
      ],
      "brew-simplify-complex-code-blocks": [
        "```suggestion\r\n    print_stderr = if verbose && show_info\r\n      true\r\n    else\r\n      false\r\n    end\r\n```\r\nis a bit easier to read",
        "Can these be dedicated methods rather than a lambda? The method this is part of is getting very long and we don't typically use `lambda` like this just for variable scoping."
      ],
      "brew-environment-variable-safety": [
        "```suggestion\r\n  if [[ -r \"/var/tmp\" && -w \"/var/tmp\" ]]\r\n  then\r\n    HOMEBREW_DEFAULT_TEMP=\"/var/tmp\"\r\n  else\r\n    HOMEBREW_DEFAULT_TEMP=\"/tmp\"\r\n  fi\r\n```",
        "@carlocab great point, will adjust and add an explicit `PATH` suggestion"
      ],
      "brew-follow-support-tiers": [
        "```suggestion\r\ncommand -v brew || export PATH=\"/opt/homebrew/bin:/home/linuxbrew/.linuxbrew/bin:/usr/local/bin\"\r\ncommand -v brew && eval \"$(brew shellenv)\"\r\n```",
        "@colindean This seems sufficient to handle all platforms without error output and is short and easier to understand. \r\n\r\nFor future: can you allow maintainers to commit to your fork? Thanks."
      ],
      "brew-evaluate-security-control-effectiveness": [
        "```suggestion\r\nNote that unlike formulae, casks do not consider the `sha256` stanza to be a meaningful security measure\r\nas maintainers cannot realistically check them for authenticity. Casks download from upstream; if a malicious\r\nactor compromised a URL, they could potentially compromise a version and make it look like an update.\r\n```"
      ],
      "brew-fail-with-messages": [
        "This shouldn't fail silently.",
        "How/when will this exit with a non-zero result? Not currently seeing any e.g. `return 1` in there?",
        "@ZhongRuoyu whoops, still not seeing it!",
        "> made sure at the call site that the function is only called when there are no `brew tap` arguments\r\n\r\nGotcha, I see that now. Might be a little clearer if this is above the wildcard commands and doesn't do `&& exit 0` but instead has `exit 0` be unconditional as there's no `return` status to check here."
      ],
      "brew-minimize-unnecessary-operations": [
        "If it's slow: be worth putting this in the loop lazily evaluated as late as possible and memoized so that we can avoid more cases where it might not need to be called at all e.g. for the last possible `next`",
        "Would be nicer to put this in `if adopt` above so it avoids reading the `source_plist` etc. unnecessarily."
      ],
      "brew-structure-test-fixtures-clearly": [
        "Why was this a loop?\r\n```suggestion\r\n        result = { name: fc.name, version: fc.version }\r\n        expect(result).to eq(test.fetch(:expected))\r\n```",
        "What do you propose? `expected_name` and `expected_version`? Criticising without offering an alternative is not helpful.",
        "> The original idea is fully declarative table test fixture.\r\n\r\nThis is not something we do in Homebrew.\r\n\r\n> Dynamic enrichment of static test data with \"default values\" makes it less readable for me.\r\n\r\nAs someone who is neither an expert in Ruby nor Homebrew, readability for you is not the desired outcome here.\r\n\r\n> If there is an error that sets `head` to nil, and test expects `false`, the test won't catch it.\r\n\r\nThe Sorbet type system will catch it.\r\n\r\n> The same with version here - if we need a test that checks version is null after parsing, it should be explicitly set in fixture.\r\n\r\nI disagree.\r\n\r\n> No need to complicate the logic to repeat it for all URLs where the version is irrelevant.\r\n\r\nThe version is never irrelevant.\r\n\r\n> What could be relevant is to add a fixture entry that tests that version from params overrides the one parsed from URL.\r\n\r\nWill review a follow-up PR to do that."
      ],
      "brew-secure-api-url-parsing": [
        "```suggestion\r\n        if [[ \"${UPSTREAM_REPOSITORY_URL}\" = \"https://github.com/\"* ]] && \r\n           [[ -n \"${HOMEBREW_GITHUB_API_TOKEN}\" ]]\r\n```\r\n\r\nor similar as otherwise `HOMEBREW_GITHUB_API_TOKEN` being set at all may limit this.\r\n\r\nIt may be desirable to have a single `[[ \"${UPSTREAM_REPOSITORY_URL}\" == \"https://github.com/\"* ]]` check which sets a variable e.g. `UPSTREAM_REPOSITORY_GITHUB_GLOB` or something which you can then check in these multiple places later.",
        "```suggestion\r\n        if [[ -n \"${UPSTREAM_REPOSITORY_TOKEN}\" ]]\r\n        then\r\n          CURL_GITHUB_API_ARGS=(\"--header\" \"Authorization: token ${UPSTREAM_REPOSITORY_TOKEN}\")\r\n        elif [[ -n \"${HOMEBREW_GITHUB_API_TOKEN}\" ]]\r\n        then\r\n          CURL_GITHUB_API_ARGS=(\"--header\" \"Authorization: token ${HOMEBREW_GITHUB_API_TOKEN}\")\r\n```"
      ],
      "brew-use-ascii-only-urls": [
        "I'd rather we be excessively strict for now, provided homebrew/core and homebrew/cask pass, and loosen it later."
      ],
      "brew-avoid-variable-name-abbreviations": [
        "No idea what `uphpp` means. Please use longer variable names.",
        "```suggestion\r\n  repository=\"$(tr '[:upper:]' '[:lower:]' <<<\"${dir#*/}\")\"\r\n  repository=\"${repository#@(home|linux)brew-}\"\r\n  echo \"${user}/${repository}\"\r\n```",
        "```suggestion\r\n  user=\"$(tr '[:upper:]' '[:lower:]' <<<\"${directory%%/*}\")\"\r\n  repo=\"$(tr '[:upper:]' '[:lower:]' <<<\"${directory#*/}\")\"\r\n```"
      ],
      "brew-optimize-collection-operations": [
        "```suggestion\r\n        json[\"old_tokens\"] = [old_token, *json[\"old_tokens\"]].compact.uniq\r\n```",
        "```suggestion\r\n          formulae_and_casks &= excluded_autobump\r\n```\r\nor \r\n```suggestion\r\n          formulae_and_casks |= excluded_autobump\r\n```\r\nI can't remember which 😅 ",
        "That works for me also! anything that avoids a loop.",
        "```suggestion\r\n              installed_formula.deps.required.map(&:to_formula).any? { |dep| sized_formulae.include?(dep) }\r\n```\r\nany may be able to simplify this further still with `intersect?`",
        "```suggestion\r\n        installed_formula_tap_names = Formula.installed.filter_map(&:tap).uniq.reject(&:official?)\r\n```",
        "```suggestion\r\n        installed_cask_tap_names = Cask::Caskroom.casks.filter_map(&:tap).uniq.reject(&:official?)\r\n```",
        "```suggestion\r\n        @non_core_taps ||= Tap.installed.reject(&:core_tap?).reject(&:core_cask_tap?)\r\n```"
      ],
      "brew-clear-code-examples": [
        "Love this version, thanks @jvns! @colindean could you use this verbatim?\r\n\r\nThanks both!",
        "```suggestion\r\n```\r\n\r\nthis should instead rely on the rubydoc.brew.sh documentation rather than duplicating it",
        "```suggestion\r\n| `require_root`          | `false`      |  yes  |  yes  | whether the service requires root access. If true, Homebrew hints at using `sudo` on various occasions, but does not enforce it |\r\n```"
      ],
      "brew-structured-environment-configuration": [
        "@Bo98 Is `HOMEBREW_TEMP` not the per-user temp? If not: ideally we'd do that and: yes, agreed.",
        "@Bo98 done!",
        "This will override e.g. `brew search` behaviour which seems undesirable.",
        "I'm game for searching internal taps but this would break searching non-internal taps."
      ],
      "brew-decouple-ci-from-code": [
        "We should stop Monterey CI as soon as we start any Sequoia CI.",
        "@Bo98 Bit confused here, sorry! Can you make a code suggestion on whatever you'd want to see changed in this PR specifically before it is merged e.g. today? Thanks ❤️ ",
        "Can you elaborate a bit more on what \"separate these controlling CI \" and \"separating CI control\" means here? It's not totally clear. Thanks!",
        "Thanks for explaining. Sounds like the main blocker therefore is updating the CI runners to not use the same logic as these numbers here.",
        "> We'll probably not merge this until there's good bottle coverage.\r\n\r\nWe should merge this as soon as we're running Sequoia in CI.\r\n\r\nIt doesn't matter for end-users if we have zero Sequoia bottle coverage as we can still support it before that."
      ]
    },
    "profile": {
      "location": "Edinburgh, Scotland",
      "company": "@mikemcquaid",
      "blog": "https://mikemcquaid.com",
      "twitter_username": "MikeMcQuaid",
      "site_admin": false,
      "followers": 4042,
      "following": 0
    }
  },
  "jacoblee93": {
    "repos": [
      "langchain-ai/langchainjs"
    ],
    "entries": [
      {
        "slug": "langchainjs-ai-dependency-management",
        "title": "AI dependency management"
      },
      {
        "slug": "langchainjs-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "langchainjs-chunked-data-processing",
        "title": "Chunked data processing"
      },
      {
        "slug": "langchainjs-comprehensive-ai-documentation",
        "title": "Comprehensive AI documentation"
      },
      {
        "slug": "langchainjs-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "langchainjs-constructor-over-setter",
        "title": "Constructor over setter"
      },
      {
        "slug": "langchainjs-dependency-classification-standards",
        "title": "Dependency classification standards"
      },
      {
        "slug": "langchainjs-follow-documentation-standards",
        "title": "Follow documentation standards"
      },
      {
        "slug": "langchainjs-platform-appropriate-environment-variables",
        "title": "Platform-appropriate environment variables"
      },
      {
        "slug": "langchainjs-prefer-nullish-coalescing",
        "title": "Prefer nullish coalescing"
      },
      {
        "slug": "langchainjs-preserve-api-backward-compatibility",
        "title": "Preserve API backward compatibility"
      },
      {
        "slug": "langchainjs-simplify-code-organization",
        "title": "Simplify code organization"
      },
      {
        "slug": "langchainjs-throw-meaningful-errors",
        "title": "Throw meaningful errors"
      },
      {
        "slug": "langchainjs-typescript-naming-standards",
        "title": "TypeScript naming standards"
      },
      {
        "slug": "langchainjs-use-comprehensive-jsdoc",
        "title": "Use comprehensive JSDoc"
      },
      {
        "slug": "langchainjs-use-database-native-types",
        "title": "Use database-native types"
      },
      {
        "slug": "langchainjs-validate-untrusted-input",
        "title": "Validate untrusted input"
      }
    ],
    "comments": {
      "langchainjs-dependency-classification-standards": [
        "I don't think this is necessary? There is a web built-in",
        "Should be a peer + dev dep, not a direct dependency:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md#third-party-dependencies",
        "This also should be an optional peer dep:\r\n\r\nhttps://js.langchain.com/docs/contributing/code/#adding-an-entrypoint"
      ],
      "langchainjs-simplify-code-organization": [
        "Can we put this in an existing entrypoint? `types` maybe?",
        "Want to avoid fragmentation"
      ],
      "langchainjs-chunked-data-processing": [
        "Why not just interact with instances of `this.backingStore` directly?\r\n\r\nWould rather just init three different instances of it or just have some kind of `.bind()` semantic if absolutely necessary - there's a lot of inheritance already\r\n\r\nIf just used for tests, put in a `utils/testing` file"
      ],
      "langchainjs-use-comprehensive-jsdoc": [
        "Let's just log this once on initial call to avoid flooding the console and add some example code",
        "Ah right... this is used in multiple places\r\n\r\n@hntrl maybe let's make a small docs page with agnostic info here?",
        "nit: Add docstring with usage example",
        "Might be nice to have chat history/vectorstore classes just accept params to instantiate an engine on init in addition to taking a full engine class for connection reuse so that the user has one less import/abstraction to worry about",
        "Ok, sounds good. Yeah definitely better to do it this way for prod, just more pieces and classes to set up for getting started"
      ],
      "langchainjs-ai-dependency-management": [
        "Did we need to add this?",
        "We don't accept hard dependencies, see:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md "
      ],
      "langchainjs-consistent-naming-conventions": [
        "Let's emphasize somewhere that this wraps Unstructured\r\n\r\nShould we call this `DropboxUnstructuredLoader` instead?",
        "Separate words with underscore:\r\n`GOOGLE_DRIVE_CREDENTIALSPATH` -> `GOOGLE_DRIVE_CREDENTIALS_PATH`"
      ],
      "langchainjs-validate-untrusted-input": [
        "May be worth a small note in docs emphasizing that `tableName` and `schemaName` in these files are not escaped and to not pass in end-user input here",
        "nit: can we escape column names too/disallow nonalphanumeric?",
        "Yeah good call"
      ],
      "langchainjs-preserve-api-backward-compatibility": [
        "Isn't removing all of these a breaking change?",
        "Not sure what current usage is, but would love a shim for principle's sake. I can try to have a look later if you don't have time.",
        "Thank you! This is a very recent integration, but we really try to not have any breaking changes in patches unless they are completely unavoidable.",
        "This conflicts with the `ToolCall` declared in `base.ts` - you should import it like this:\r\n\r\n```ts\r\nimport { type ToolCall } from \"@langchain/core/messages/tool\";\r\n```\r\n\r\nWe will remove the old type on next breaking change.",
        "Would prefer this as a static method like this:\r\n\r\nhttps://v02.api.js.langchain.com/classes/langchain_community_vectorstores_pgvector.PGVectorStore.html#initialize\r\n\r\nSince I'm not sure this will show up well in API refs, but won't block on it",
        "Hey sorry, no I mean a single param like this so as not to break the interface:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/vectorstores/pinecone.ts#L174"
      ],
      "langchainjs-prefer-nullish-coalescing": [
        "Only if someone's passing `disableStreaming: false` right? I think fine",
        "You can just do `fields?.disableStreaming ?? true`",
        "Don't think we need the `.length` check\r\n\r\nAlso should use triple equals if at all",
        "May be slightly safer to add these fields only if they are present - not sure how Azure or other model proxies will react to adding `audio: undefined`\r\n\r\n"
      ],
      "langchainjs-constructor-over-setter": [
        "Prefer `.invoke()`",
        "Also make sure this is tested",
        "`PyPDFLoader` isn't in JS",
        "So you need to click a link every hour to use this integration?",
        "I don't fully understand from the docs -  is there a way to just get tht refresh token during setup and just use that?"
      ],
      "langchainjs-avoid-hardcoded-configurations": [
        "Let's avoid defaults at this level when possible",
        "We should ideally let the backend set this in case they change best practices, would prefer to have things unset if it won't cause issues (which it wasn't before)",
        "Thank you, old defaults have started causing issues for OpenAI so it's top of mind right now",
        "Should we be hardcoding this here?\r\n\r\nIf it's only available in one region now, would prefer to have this configurable or even not have a default at all and just have it documented"
      ],
      "langchainjs-use-database-native-types": [
        "Would suggest storing this as an ISO string -  not all (most?) vector stores will not deserialize dates",
        "OOC why not just support `metadataJsonColumn` vs spreading metadata into other columns?",
        "And the JSON column is just to make onboarding easier?"
      ],
      "langchainjs-platform-appropriate-environment-variables": [
        "JavaScript uses `process.env`",
        "Yeah let's cut these if we can't find an automatic way to import them. If we really need something Node specific we can just use `.mdx`\r\n\r\nI can also reach out to the Deno team to see if they have suggestions, we have a channel with them."
      ],
      "langchainjs-throw-meaningful-errors": [
        "Good to leave `response` directly on the error object - `AsyncCaller` uses fields on it to decide retries."
      ],
      "langchainjs-comprehensive-ai-documentation": [
        "I would show how to initialize `.fromDocuments` as well as from an existing store",
        "Can we have this use the standard retriever template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb",
        "Can you use the embeddings docs template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md#documentation-and-integration-tests\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb"
      ],
      "langchainjs-typescript-naming-standards": [
        "Let's not take a default here\r\n\r\nAlso, we are standardizing on `model` over `modelName`",
        "By convention we capitalize types/interfaces",
        "We are standardizing as `model` instead of `modelName`"
      ],
      "langchainjs-follow-documentation-standards": [
        "Please make the docs pages follow this format:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb",
        "Can we make the docs follow this template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb\r\n\r\nI need to update the contributing instructions...",
        "We should definitely add docs for `JsonOutputFunctionsParser`",
        "Should we have the `## Overview` boilerplate?",
        "Should at least link to vector store conceptual docs and details table"
      ]
    },
    "profile": {
      "location": "San Francisco",
      "company": "@langchain-ai",
      "blog": "https://jacobscript.dev",
      "twitter_username": "hacubu",
      "site_admin": false,
      "followers": 1274,
      "following": 0
    }
  },
  "Mytherin": {
    "repos": [
      "duckdb/duckdb"
    ],
    "entries": [
      {
        "slug": "duckdb-avoid-repeated-expensive-operations",
        "title": "avoid repeated expensive operations"
      },
      {
        "slug": "duckdb-comprehensive-database-testing",
        "title": "comprehensive database testing"
      },
      {
        "slug": "duckdb-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "duckdb-consistent-null-validation",
        "title": "consistent null validation"
      },
      {
        "slug": "duckdb-constructor-configuration-injection",
        "title": "Constructor configuration injection"
      },
      {
        "slug": "duckdb-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "duckdb-explicit-ci-configurations",
        "title": "Explicit CI configurations"
      },
      {
        "slug": "duckdb-explicit-null-state-management",
        "title": "explicit null state management"
      },
      {
        "slug": "duckdb-generate-test-data-dynamically",
        "title": "generate test data dynamically"
      },
      {
        "slug": "duckdb-guard-expensive-logging-operations",
        "title": "Guard expensive logging operations"
      },
      {
        "slug": "duckdb-maintain-api-backward-compatibility",
        "title": "maintain API backward compatibility"
      },
      {
        "slug": "duckdb-maintain-api-backwards-compatibility",
        "title": "maintain API backwards compatibility"
      },
      {
        "slug": "duckdb-maintain-codebase-consistency",
        "title": "maintain codebase consistency"
      },
      {
        "slug": "duckdb-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "duckdb-optimize-algorithm-complexity",
        "title": "Optimize algorithm complexity"
      },
      {
        "slug": "duckdb-optimize-hot-path-performance",
        "title": "optimize hot path performance"
      },
      {
        "slug": "duckdb-prefer-environment-variables",
        "title": "prefer environment variables"
      },
      {
        "slug": "duckdb-prefer-settings-over-pragmas",
        "title": "prefer settings over pragmas"
      },
      {
        "slug": "duckdb-preserve-api-backward-compatibility",
        "title": "Preserve API backward compatibility"
      },
      {
        "slug": "duckdb-preserve-serialization-compatibility",
        "title": "preserve serialization compatibility"
      },
      {
        "slug": "duckdb-protect-shared-state",
        "title": "Protect shared state"
      },
      {
        "slug": "duckdb-secure-sensitive-data-handling",
        "title": "secure sensitive data handling"
      },
      {
        "slug": "duckdb-thoughtful-configuration-design",
        "title": "thoughtful configuration design"
      },
      {
        "slug": "duckdb-use-default-serialization-methods",
        "title": "use default serialization methods"
      },
      {
        "slug": "duckdb-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "duckdb-use-descriptive-naming",
        "title": "Use descriptive naming"
      },
      {
        "slug": "duckdb-use-specialized-sensitive-types",
        "title": "Use specialized sensitive types"
      },
      {
        "slug": "duckdb-validate-inputs-comprehensively",
        "title": "validate inputs comprehensively"
      },
      {
        "slug": "duckdb-validate-inputs-early",
        "title": "validate inputs early"
      }
    ],
    "comments": {
      "duckdb-generate-test-data-dynamically": [
        "Instead of adding all of these data files - can we use existing data files or generate data in the tests themselves?"
      ],
      "duckdb-optimize-hot-path-performance": [
        "Performance seems to degrade a bit - maybe we can spend some time optimizing this, e.g. by using a look-up table to find the special characters instead of doing `std::find_first_of`\r\n\r\nBelow is a benchmark that we can use to profile this\r\n\r\n```sql\r\n# name: benchmark/micro/cast/cast_string_list_varchar.benchmark\r\n# description: Cast list values to string\r\n# group: [cast]\r\n\r\nname Cast LIST -> VARCHAR\r\ngroup cast\r\n\r\nload\r\nCREATE TABLE lists AS SELECT ['thisisastring_'||i, 'str_'||(i+1)::VARCHAR, NULL, 'specialcharacter('||(i+2)::VARCHAR] l FROM range(0, 10000000) tbl(i);\r\n\r\nrun\r\nSELECT MIN(CAST(l AS VARCHAR)) FROM lists;\r\n\r\nresult I\r\n[thisisastring_0, str_1, NULL, 'specialcharacter(2']\r\n\r\n```",
        "We seem to be doing this check twice - can we do this only once and cache this in a `unsafe_unique_array<bool>` (not vector bool because vector bool is evil)",
        "This shouldn't happen for every chunk but should happen for after `TryInitializeBatch` (and maybe then only if the file idx changed)"
      ],
      "duckdb-maintain-api-backward-compatibility": [
        "Instead of modifying `duckdb_slice_vector`, maybe we can add a separate method `duckdb_vector_slice_dictionary` that does this?",
        "The `out_names` here is problematic for various reasons:\r\n\r\n* \"Must be appropriately deleted\" is unclear \r\n* It is not actually possible to appropriately delete - given that the out_names are allocated with `new` which has no equivalent in the C API. Anything that requires the user to delete it should be allocated with `duckdb_malloc` so it can be freed with `duckdb_free`\r\n* I wonder if the `out_names` and `out_column_count` are required to begin with? We are already emitting an opaque object (`duckdb_arrow_converted_schema`). Can we not store the names/column count in that opaque object and expose them using accessors?"
      ],
      "duckdb-use-descriptive-naming": [
        "Maybe we can make this an enum, and this should perhaps be `all_inputs_valid`?",
        "Python does not allow the short-hands - I would follow them here and require this to be spelled out (i.e. `replace`, `strict`, etc)",
        "The naming here is confusing to me.\r\n\r\nThis is the behavior as I understand it:\r\n\r\n* extension_loading = none -> we load extensions on `require` with `ExtensionHelper::LoadExtension`. This usually only loads statically linked extensions, but there are several paths to get this to load using `LOAD` (e.g. with defines `DUCKDB_TEST_REMOTE_INSTALL` and `DUCKDB_EXTENSIONS_TEST_WITH_LOADABLE`)\r\n* extension_loading = autoload -> we skip loading altogether during `require`, and instead enable autoloading so that we can autoload extensions\r\n* extension_loading = all -> we load extensions using `LOAD {extension_name}` in addition to `ExtensionHelper::LoadExtension`\r\n\r\nI think this naming is quite confusing. When `extension_loading = none` we load extensions, when `extension_loading = autoload` we don't. \r\n\r\nThere's also a bunch of other testing code (`TEST_REMOTE_INSTALL` / `TEST_WITH_LOADABLE_EXTENSION`) that heavily overlaps with these newly added options.\r\n\r\nI think it would be good to revisit this because as it stands this is piling more things onto something that is already very messy. Maybe we can give these things clear names, and remove the existing options for testing / replace them with the new options?",
        "Maybe this method should be renamed to `SerializeToDisk`",
        "Can we just rename this method to `LogFailure`? The fact that it writes to two destinations is more of a detail of the method."
      ],
      "duckdb-prefer-environment-variables": [
        "Instead of making this a command line option maybe we can make it an environment variable? That way this also works with the CI runs that run e.g. `make allunit`."
      ],
      "duckdb-consistent-null-validation": [
        "In other functions we check for `NULL` - could we do that here as well?"
      ],
      "duckdb-preserve-serialization-compatibility": [
        "We should not need to re-generate the serialized plans here. If this is necessary something is going wrong with serialization which can impact forward and/or backward compatibility. Can you revert this change and instead fix what caused this to be necessary?"
      ],
      "duckdb-eliminate-code-duplication": [
        "Could we maybe try to extract the CSV writer methods into a separate `CSVWriter` class that is used both by the copy csv and the logger?",
        "This seems like a copy-paste from below - what's the difference here between `could_autoload` and not? Can we unify this code?",
        "It looks like there is a lot of duplicated code between the two `EncryptTemporaryBuffer` methods - can we unify them?",
        "This pattern seems to be present all over the code - perhaps it would be nicer to have a dedicated class for this? e.g.:\r\n\r\n\r\n```cpp\r\nstruct EncryptionTag {\r\n    EncryptionTag() {\r\n        memset(tag, 0, MainHeader::AES_TAG_LEN);\r\n    }\r\n    data_ptr_t data();\r\n    idx_t size();\r\n\r\n    uint8_t data[MainHeader::AES_TAG_LEN];\r\n};\r\n```\r\n\r\nSame with the nonce.",
        "As far as I can tell all that changes based on `lhs_first` is the index into the final chunk, is that correct? If so, rather than duplicating code perhaps you can do:\r\n```sql\r\nidx_t left_offset = lhs_first ? 0 : right_projection_map.size();\r\nidx_t right_offset = lhs_first ? left_projection_map.size() : 0;\r\n...\r\nchunk.data[left_offset + i].Reference(...)\r\n...\r\nchunk.data[right_offset + i].Reference(...)\r\n```",
        "It feels like we are doing a lot of duplicate work here which complicates the code (`result.Print()` followed by `result.ToString()`).\r\n\r\nGenerally the way we would achieve either printing or collecting a string is to write to a stream object, e.g. an `std::ostream &ss`. Then we can pass in a `std::stringstream` to obtain a string, or pass in another output stream (e.g. `stdout`) to render to a different location.\r\n\r\nCan we rework this to use that construct to avoid having to change/duplicate all of this code?"
      ],
      "duckdb-maintain-codebase-consistency": [
        "Can we stick to the old code pattern here instead of making this public?",
        "This constructor should not be necessary - `optional_ptr<ClientContext>` is already implicitly constructable from `ClientContext &`"
      ],
      "duckdb-maintain-consistent-naming-patterns": [
        "Could we call this `duckdb_value_to_string` instead?"
      ],
      "duckdb-comprehensive-database-testing": [
        "In general we prefer to avoid using the hash comparison feature - we can verify two results are the same using the labeled results, we can use that to verify results instead, e.g.:\r\n\r\n```sql\r\nquery III nosort read_csv_result\r\nSELECT * FROM read_csv('test/sql/ordinality/a1.csv') WITH ORDINALITY ORDER BY Numbers,Chars,ordinality;\r\n\r\nquery III nosort read_csv_result\r\nSELECT *, row_number() OVER () AS ordinality FROM read_csv('test/sql/ordinality/a1.csv') ORDER BY Numbers,Chars,ordinality;\r\n```",
        "We should execute the query here - not just check the plan. \r\n\r\nWe should add more tests here:\r\n\r\nWhat if the view already has references to subsets of columns?\r\n\r\n```sql\r\ncreate or replace view test as select s.c1, s.c2, s from t;\r\nselect s1.c2, s2.c1 from test;\r\n```\r\n\r\nWhat if the view contains the same reference multiple times?\r\n\r\n```sql\r\ncreate or replace view test as select s s1, s s2 from t;\r\nselect s1.c2, s2.c1 from test;\r\n```\r\n\r\n"
      ],
      "duckdb-optimize-algorithm-complexity": [
        "We should be able to directly go from the time_point to our timestamp using `time_since_epoch`, e.g.:\r\n\r\n```sql\r\nstd::chrono::duration_cast<std::chrono::microseconds>(time_point.time_since_epoch()).count();\r\n```",
        "We can avoid `find_first_of` here by predicating, not sure if this is worth it, maybe you can benchmark it:\r\n\r\n```cpp\r\nchar c = string_data[current_pos];\r\nresult[result_offset] = '\\\\';\r\nresult_offset += c == '\\\\' || c == '\\'';\r\nresult[result_offset] = c;\r\nresult_offset++;\r\n```",
        "For the size measurement we can do something similar:\r\n\r\n```cpp\r\nchar c = string_data[current_pos];\r\nstr_len += c == '\\\\' || c == '\\'';\r\nstr_len++;\r\n```"
      ],
      "duckdb-constructor-configuration-injection": [
        "We should be able to pass a `ClientContext` into the constructor of the `ThriftFileTransport` and use that here",
        "For now this is fine - but we can't use globals in the final design. Globals can't be extended by extensions. Extensions can be loaded per DuckDB instance - and different DuckDB instances can have a different set of extensions loaded. The current design will therefore prohibit extensions from registering additional keywords.\r\n\r\nI think this needs to live in the `DatabaseInstance` or `DBConfig` somehow in the future, i.e. similar to where the parser extensions currently live.",
        "Should this be a `string`?\r\n\r\nI also wonder - do we need to pass this into these individual methods? Can't we pass it in using the `StorageManagerOptions` in the constructor?"
      ],
      "duckdb-validate-inputs-early": [
        "Can we maybe verify there are no temporary files currently active when setting this - and throw an error if there are any - given that this will lead to not being able to read/interact with the existing temporary files?",
        "I think this is the wrong place to fix the error - NOT operators should not be generated with multiple expressions. I think we should be throwing this error when expanding the `*COLUMNS(*)` expression instead "
      ],
      "duckdb-protect-shared-state": [
        "Given that the other scan state is all located in the local state - I suspect the global state is the wrong place for the `current_ordinality_idx`. Do we have a test with a larger `UNNEST` or similar table function that triggers multi-threading?",
        "This needs to use `std::call_once`, otherwise this is prone to race conditions"
      ],
      "duckdb-avoid-repeated-expensive-operations": [
        "Maybe we can make `ExceptionFormatValue` a const reference here to try to avoid having to do the `ToStdString`",
        "For a performance optimization - if the expression has `IsFoldable()`, we can cache the `result_value`. This should greatly improve performance in the general case (as this can essentially just become `AppendValue`).",
        "That's fine. We don't offer any guarantees around Appenders aside from when they are flushed anyway.\r\n\r\nI think the bigger issue is that there is no guarantee that `AppendDefault` is called for the same columns for every row - although you could detect that and bail-out of this optimization of course.",
        "Instead of copying the `string_t`, adjusting the pointer, and then copying it over into the row layout - we can instead directly copy over:\r\n\r\n* Prefix + length (first 8 bytes of `source`) to `row_location + offset_in_row`\r\n* Pointer (`heap_location`) to `row_location + offset_in_row + 8`",
        "Using a string here is quite inefficient since this creates a copy of the string for every call - it would be better to use `StringUtil::CIEquals` directly on the data in the `string_t`, e.g.:\r\n\r\n```cpp\r\nDecodeErrorBehavior GetDecodeErrorBehavior(const string_t &specifier) {\r\n    auto size = specifier.GetSize();\r\n    auto data = specifier.GetData();\r\n    if (StringUtil::CIEquals(data, size, \"strict\", 6)) {\r\n        return DecodeErrorBehavior::STRICT;\r\n    } ....\r\n}\r\n```",
        "Can we quickly figure out the size of the resulting varint, then allocate that size with `StringVector::EmptyString`, then do the actual addition? Allocating in the arena first and then copying it over seems rather wasteful - esp since figuring out the size of the resulting varint doesn't seem that complex in the standard case of adding together two positive numbers."
      ],
      "duckdb-thoughtful-configuration-design": [
        "Can we perhaps have this option be `EXTENSION_DIRECTORIES` - which defaults to `~/.duckdb/extensions` - and can then be extended using e.g. a colon separated list similar to how `PATH` works (`~/.duckdb/extensions:/my/directory`)"
      ],
      "duckdb-prefer-settings-over-pragmas": [
        "Can we maybe turn this into a table function? `CALL enable_logging(....)`",
        "Maybe add a comment as well to the pragmas so that this is clear for devs looking at adding a new one and to encourage them to instead add a setting or table function",
        "Ideally the `LOCAL_EXTENSION_REPO` environment variable should also be removed and everything should be moved to the new config",
        "We try to avoid pragmas nowadays - can you replace this with a setting (e.g. `SET wal_encryption=...`)",
        "It feels to me like the distinction between \"provided using `user_key` vs provided as `ATTACH` option\" should be made at a higher level, and we should just get the key as part of the provided StorageOptions here. Perhaps that could be done in order to simplify the code here?"
      ],
      "duckdb-explicit-null-state-management": [
        "We only need to call `FlatVector::SetNull` when the value is null, so this could be:\r\n```cpp\r\nif (value.is_null) {\r\n    FlatVector::SetNull(result, i, true);\r\n}\r\n```",
        "Maybe instead of having `ConstructType` return false if any child is `NULL`, we can have a method `ContainsNull()` on the type, so we could do this instead:\r\n\r\n```cpp\r\nA_TYPE input = A_TYPE::ConstructType(state, i);\r\nif (input.ContainsNull()) {\r\n     FlatVector::SetNull(result, i, true);\r\n     continue;\r\n}\r\n```\r\n\r\nThen we can also remove the validity check from a few lines up.",
        "These fields are not necessary - `a_val` and `b_val` have their own `is_null` field (i.e. we should just use `a_val.is_null`)",
        "Can we have a `ExecutorBaseType` class that has this member that all types inherits from? Every `*Type` class needs this field",
        "`ContainsNull` should return whether or not this field, or **any** child field is null, i.e. this should return `is_null || a_val.is_null` - same for all of the other elements",
        "Note that nulls can be at different levels - i.e. the struct itself can be NULL, as well as the child elements. This should also check if the struct itself is `NULL` (so `is_null || a_val.is_null || b_val.is_null`)"
      ],
      "duckdb-use-default-serialization-methods": [
        "This needs to be `WritePropertyWithDefault` and `ReadPropertyWithDefault` otherwise backwards compatibility is broken",
        "I think the `ShouldSerialize` is incorrect here - this means we lose the information about the initial value if we are serializing to an older storage version. Throwing an error on deserialization is fine when the initial value is defined since the alternative is that we have incorrect results.\r\n"
      ],
      "duckdb-maintain-api-backwards-compatibility": [
        "Can we avoid changing the name in the serialization? Otherwise the JSON serialization is not backwards compatible, can we do e.g. `\"property\": \"projection_map\"` instead of renaming the serialized member?"
      ],
      "duckdb-guard-expensive-logging-operations": [
        "StdoutLogStorage should not buffer by default and should flush after every row (this can be configurable of course)"
      ],
      "duckdb-use-descriptive-names": [
        "Can we call this `duckdb_data_chunk_to_string`",
        "Can we call this `additional_authenticated_data` for clarity?"
      ],
      "duckdb-use-specialized-sensitive-types": [
        "Should this be a string, or an `EncryptionKey`?",
        "Similar - should this be a string?"
      ],
      "duckdb-explicit-ci-configurations": [
        "Instead of doing a glob over all configs - which will be hard to understand for people trying to work with CI failures - it would be much nicer to have individual config files that have logically named components in the CI (e.g. \"Encrypted Database Test\" -> run encrypted_db.config)."
      ],
      "duckdb-secure-sensitive-data-handling": [
        "Given we are going through lengths here not to page out the memory of the encryption key - I think we should not use a `std::string` to store the encryption key. It is very easy to accidentally copy over an `std::string` which would then negate this protection.\r\n\r\nCan we perhaps rewrite the keys to use a separate class that is not copy constructible (and that calls `mlock`/`munlock` itself perhaps on memory that it is allocating to hold the key?).",
        "We should only set this if it is not specified by the user itself - otherwise the encryption key option overrides the user-provided setting. \r\n\r\ni.e. this should throw an error:\r\n\r\n```sql\r\nATTACH '...' (STORAGE_VERSION 'v1.1', ENCRYPTION_KEY '...');\r\n```\r\n\r\nAnd this should use storage version v1.4, not v1.3:\r\n\r\n```sql\r\nATTACH '...' (STORAGE_VERSION 'v1.14, ENCRYPTION_KEY '...');\r\n```"
      ],
      "duckdb-comprehensive-test-coverage": [
        "Can we add some more tests?\r\n\r\n* `EXCEPT` and `INTERSECT` tests\r\n* Tests using `default_collation` instead of `COLLATE` expressions\r\n* Tests using collations stored in columns\r\n* Tests using mismatching collations\r\n* Tests using collations in nested set operations (e.g. `(SELECT 'A' UNION SELECT 'b') UNION (SELECT 'c' UNION SELECT 'A' COLLATE NOCASE);`)\r\n* Tests with ICU collations\r\n",
        "Could we add some tests with:\r\n\r\n* `'0x'::INT` (empty hex: should fail)\r\n* `'0'::INT` (should succeed and return 0)\r\n* Upper-case `x` (`'0XFF'::INT`)\r\n* Mix of different cases (`'0XfEfE'::INT`)\r\n* `'0xFFFFFFFFFFFFFFFFF'::INT` (i.e. very large overflow)\r\n* `'0x000000000000000'::INT` (many zeros - should succeed)\r\n* Exact bound tests for all types - should all succeed (`'0x7F'::TINYINT`, `'0x7FFF'::SMALLINT`, `'0x7FFFFFFF'::INT`, `'0x7FFFFFFFFFFFFFFF'::BIGINT`)\r\n* Exact bound tests exceeded for all types - should all fail (`'0x80'::TINYINT`, `'0x8000'::SMALLINT`, `'0x80000000'::INT`, `'0x8000000000000000'::BIGINT`)\r\n* `TRY_CAST` with an out-of-bounds integer (e.g. `SELECT TRY_CAST('0x80' AS TINYINT)` - should return `NULL`)\r\n* The same exact bound tests with casts to unsigned types (`'0xFF'::UTINYINT`, etc)",
        "`statement error` is indeed the correct approach\r\n\r\nNote that you should only test one out-of-bounds per query, otherwise the query will fail on the first error on not correctly test the remaining, so e.g.:\r\n\r\n```sql\r\nstatement error\r\nSELECT '0x80'::TINYINT;\r\n\r\nstatement error\r\nSELECT '0x8000'::SMALLINT;\r\n\r\n...\r\n```",
        "Can we add a test for a negative hugeint, as well as for the min/max values of hugeint? (these can be obtained from `select hugeint, uhugeint from test_all_types();`)",
        "Could we add some more tests with failure conditions?\r\n\r\n* What if the key is not present in the table\r\n* What if there are duplicate keys\r\n* Given you are using an `expr_list_opt_comma`, what if the keys are arbitrary expressions (or perhaps this should be changed to `name_list_opt_comma` in the parser?)\r\n* Can we add a test using these in subqueries?\r\n* Can we add a test nesting these (similar to `test/sql/cte/test_nested_recursive_cte.test`)?\r\n* Can we add a test nesting these alongside regular recursive CTEs?\r\n* Can we add a test that uses `UNION ALL`?\r\n* Can we add tests with more complex key types (e.g. string or list)?"
      ],
      "duckdb-preserve-api-backward-compatibility": [
        "Instead of using `rest` for directories, we can use `nargs='*'`, see [here](https://docs.python.org/3/library/argparse.html#usage). This should also allow the previous usage with revision to remain unchanged. "
      ],
      "duckdb-validate-inputs-comprehensively": [
        "* Can we add a test for `DEFAULT nextval('seq')`?\r\n* Can we add a test for `DEFAULT random()`?\r\n* Can we add a test for `DEFAULT now()`?\r\n* Can we add a default value that triggers an error, e.g.:\r\n\r\n```sql\r\ncreate table integers(i integer default 'hello'::int);\r\ninsert into integers default values;\r\n-- Conversion Error: Could not convert string 'hello' to INT32\r\n```\r\n\r\n* Can we add a test for the \"default default\", i.e. returning `NULL` when no value is specified",
        "That’s supported in SQL normally, I think it makes sense for this to be equivalent to the SQL DEFAULT expression so I would vote for allowing this"
      ]
    },
    "profile": {
      "location": "Amsterdam, Netherlands",
      "company": "DuckDB Labs",
      "blog": "www.markraasveldt.com",
      "site_admin": false,
      "followers": 1117,
      "following": 9
    }
  },
  "Gargron": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-avoid-deprecated-sass-syntax",
        "title": "avoid deprecated SASS syntax"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-complete-translatable-sentences",
        "title": "Complete translatable sentences"
      },
      {
        "slug": "mastodon-comprehensive-authorization-checks",
        "title": "comprehensive authorization checks"
      },
      {
        "slug": "mastodon-extract-view-complexity",
        "title": "Extract view complexity"
      },
      {
        "slug": "mastodon-framework-aware-text-composition",
        "title": "Framework-aware text composition"
      },
      {
        "slug": "mastodon-hook-responsibility-separation",
        "title": "Hook responsibility separation"
      },
      {
        "slug": "mastodon-leverage-existing-configuration-sources",
        "title": "leverage existing configuration sources"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-minimize-html-attack-surface",
        "title": "Minimize HTML attack surface"
      },
      {
        "slug": "mastodon-network-resource-limits",
        "title": "Network resource limits"
      },
      {
        "slug": "mastodon-optimize-database-queries",
        "title": "Optimize database queries"
      },
      {
        "slug": "mastodon-optimize-react-hooks",
        "title": "Optimize React hooks"
      },
      {
        "slug": "mastodon-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "mastodon-prefer-established-configuration-patterns",
        "title": "prefer established configuration patterns"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-contextually-descriptive-names",
        "title": "Use contextually descriptive names"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-null-handling",
        "title": "Use semantic null handling"
      }
    ],
    "comments": {
      "mastodon-prefer-early-returns": [
        "Refactored."
      ],
      "mastodon-complete-translatable-sentences": [
        "I really doubt that a number you can check once per year will encourage any kind of day-to-day behaviour change. This feature is meant to be fun rather than simply a collection of metrics, so presentation matters a lot. But \"fun\" is difficult to localize equally to all languages and cultures. ",
        "Okay, it's a bit clunky but it works."
      ],
      "mastodon-use-contextually-descriptive-names": [
        "I am not so sure about long variable names but I added a comment to clarify it in the code at least.",
        "It was somewhat intentional, since this is a global controller, I thought the name params would be fitting. But I can think of something else."
      ],
      "mastodon-leverage-existing-configuration-sources": [
        "I think it might be simpler to fetch a full page of tags from the API and limit to displaying the first 4 in the sidebar, if it means avoiding keeping track of \"where will this be displayed\" in actions/reducers. That's what we do with lists."
      ],
      "mastodon-optimize-react-hooks": [
        "You can define `handleDocumentClick` locally within the `useEffect` and avoid having to use `useCallback` for it.",
        "Refs do not need to be passed as dependencies. This `useEffect` can have a dependency of `[]`."
      ],
      "mastodon-network-resource-limits": [
        "The alternative is to have one channel for every public or unlisted post by every account, do I understand that correctly?"
      ],
      "mastodon-choose-appropriate-exception-types": [
        "Thank you!"
      ],
      "mastodon-use-semantic-null-handling": [
        "Thanks, I think I made it better!"
      ],
      "mastodon-extract-view-complexity": [
        "This would make sense to me if multiple views reused the form, but this is the only occurence, so I do not really see the benefit."
      ],
      "mastodon-use-descriptive-specific-names": [
        "I would suggest `silo` as well..."
      ],
      "mastodon-framework-aware-text-composition": [
        "I think this may be wrong. When I sign in and get this message, the terms will long have been in effect already. I think it should be in past tense.",
        "This is a common footgun, Rails will combine the message string here with the localized attribute name (`:fields`) so the printed out message will be \"Fields Names of extra profile fields with values cannot be empty\""
      ],
      "mastodon-comprehensive-authorization-checks": [
        "It seems like currently there is no safeguard for streaming from private groups here?"
      ],
      "mastodon-hook-responsibility-separation": [
        "Maybe this could've been part of `useLinks`?",
        "I see that you have to \"fake\" typing into the input here because most of the logic happens in the `handleChange` handler. Perhaps it is a sign that some of that logic should be moved out to a `useEffect`.",
        "I still get a bit confused about `useCallback` depending on another `useCallback` so wanted to avoid that by extracting the helper function outside the component. The most important bit here (and I think I should've left a comment maybe) is that it's important the `event` literal is instantiated *outside* the `setHotkeyEvents` callback because it runs at a later time and so the `videoRef.current.paused` value will already be different. By passing the `event` literal to this function instead, I keep it all on one line where it's called, and the `event` value is static."
      ],
      "mastodon-prefer-established-configuration-patterns": [
        "We have `reduceMotion` exported from `mastodon/initial_state` which is manually set in preferences. Probably we should use that since that's used everywhere else?"
      ],
      "mastodon-migration-data-dependencies": [
        "I'm not sure how much it makes sense to modify people's blocks after the initial import. We can't really ever remove anything, right? There's something similar for user roles."
      ],
      "mastodon-minimize-html-attack-surface": [
        "I think allowing `style` might actually introduce potential security issues. Not sure this is necessary."
      ],
      "mastodon-optimize-database-queries": [
        "Another alternative is fetching all partial words from the table and building a regex from them, this is what we do for IP blocks. Let me know what you'd prefer. Neither option seems super optimized for very large numbers of blocks.",
        "I believe this index is not needed, as the index above will be used since it begins with `account_id`",
        "This could be rewritten with `exists?` without having to perform a `COUNT()`"
      ],
      "mastodon-avoid-deprecated-sass-syntax": [
        "I noticed it just now, I should've just done the math in my head right away."
      ],
      "mastodon-use-accessible-terminology": [
        "https://en.wikipedia.org/wiki/Scunthorpe_problem"
      ]
    },
    "profile": {
      "location": "Germany",
      "company": "@mastodon",
      "blog": "https://zeonfederated.com",
      "site_admin": false,
      "followers": 2275,
      "following": 17
    }
  },
  "DHowett": {
    "repos": [
      "microsoft/terminal"
    ],
    "entries": [
      {
        "slug": "terminal-api-clarity-over-convenience",
        "title": "API clarity over convenience"
      },
      {
        "slug": "terminal-api-parameter-explicitness",
        "title": "API parameter explicitness"
      },
      {
        "slug": "terminal-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "terminal-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "terminal-clear-accurate-error-messages",
        "title": "Clear accurate error messages"
      },
      {
        "slug": "terminal-document-implementation-rationale",
        "title": "Document implementation rationale"
      },
      {
        "slug": "terminal-implement-defensive-validation",
        "title": "Implement defensive validation"
      },
      {
        "slug": "terminal-manage-object-lifetimes-carefully",
        "title": "manage object lifetimes carefully"
      },
      {
        "slug": "terminal-minimize-reference-counting-overhead",
        "title": "Minimize reference counting overhead"
      },
      {
        "slug": "terminal-optimize-algorithmic-choices",
        "title": "optimize algorithmic choices"
      },
      {
        "slug": "terminal-prefer-const-declarations",
        "title": "prefer const declarations"
      },
      {
        "slug": "terminal-protocol-response-formatting",
        "title": "Protocol response formatting"
      },
      {
        "slug": "terminal-safe-optional-handling",
        "title": "Safe optional handling"
      },
      {
        "slug": "terminal-specify-security-requirements",
        "title": "Specify security requirements"
      },
      {
        "slug": "terminal-unicode-homoglyph-validation",
        "title": "Unicode homoglyph validation"
      },
      {
        "slug": "terminal-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "terminal-use-established-configuration-patterns",
        "title": "Use established configuration patterns"
      },
      {
        "slug": "terminal-use-semantic-naming",
        "title": "Use semantic naming"
      },
      {
        "slug": "terminal-validate-before-configuration-generation",
        "title": "Validate before configuration generation"
      }
    ],
    "comments": {
      "terminal-implement-defensive-validation": [
        "does `SystemParametersInfoW` destroy the value on failure? bah!"
      ],
      "terminal-api-parameter-explicitness": [
        "oh, actually, this is to handle the case where the text is itself emptied",
        "and, yes, `InlineCollection` supports a `ReplaceAll`. I can use it if you'd like."
      ],
      "terminal-api-clarity-over-convenience": [
        "we don't need [default_interface] on any of these, FWIW.",
        "(or at least, we should not.)",
        "So, you only need `[default_interface]` when the runtimeclass has (1) no other interface and (2) no identifying characteristics at all; it forces the creation of an interface even if one wouldn't be necessary."
      ],
      "terminal-protocol-response-formatting": [
        "Thought: Xterm specifies that OSC responses are returned with the same terminator (`ST` or `BEL`) as the request that generated them. I suppose having this function affords us a much easier way to find/fix all occurrences of us being out of spec :)\r\n\r\n>         XTerm accepts either BEL  or ST  for terminating OSC\r\n>         sequences, and when returning information, uses the same\r\n>         terminator used in a query.  While the latter is preferred,\r\n>         the former is supported for legacy applications:\r\n>         o   Although documented in the changes for X.V10R4 (December\r\n>             1986), BEL  as a string terminator dates from X11R4\r\n>             (December 1989).\r\n>         o   Since XFree86-3.1.2Ee (August 1996), xterm has accepted ST\r\n>             (the documented string terminator in ECMA-48).\r\n\r\n"
      ],
      "terminal-specify-security-requirements": [
        "```suggestion\n    <value>If your shell does not support \"bracketed paste\" mode, we recommend setting this to \"Always\" for security reasons.</value>\n```"
      ],
      "terminal-validate-before-configuration-generation": [
        "Alright, I hate it but... it [looks like](https://learn.microsoft.com/en-us/visualstudio/install/visual-studio-on-arm-devices?view=vs-2022) 17.4 is the version that added arm64 native hosting.\n\nWould you mind adding a check like we have on line R85 in the powershell dev shell generator?\n\nhttps://github.com/microsoft/terminal/blob/1142b6c0928a9a9127c679ffb081ae3f9b007da5/src/cascadia/TerminalSettingsModel/VsDevShellGenerator.cpp#L85"
      ],
      "terminal-manage-object-lifetimes-carefully": [
        "this is only necessary if there's a chance the hosting page goes away before the dropdown button goes away. generally for UI-speed things (like the user clicking on something) all this weak reference stuff is just noise honestly.",
        "in general, this should follow the same pattern as above. Use `{get_weak(), &Foo::Function}` instead of authoring your own weak reference resolver. :)\n\nIt will require adding a useless argument to `_ShowLoadWarningsDialog`, but perhaps that's fine",
        "This is going to explode the moment you close a tab and then the contrast settings change. You're leaving a dangling event handler _with a raw pointer_ to every termcontrol, registered in the AccessibilitySettings.",
        "does this by chance fix the issue where closing a connection while the debug tap is on it crashes terminal",
        "I'm a bit way of doing this in the constructor, but I guess it's fine..?",
        "you will probably need a way to capture a strong reference to `this` so that it's kept alive across the coroutine body",
        "is it possible for you to do this all in one coroutine invocation (check all bell sound files) rather than spawning one coroutine per bell sound?"
      ],
      "terminal-use-descriptive-identifiers": [
        "Wait up - CharacterDimensions calls FontSizeInDips **and should not be in pixels**"
      ],
      "terminal-unicode-homoglyph-validation": [
        "I'm alright with this - we can always refine"
      ],
      "terminal-document-implementation-rationale": [
        "nit: in general in this file, I should document _WHY WE HAVE TEMPORARY HSTRINGS_. i had to rediscover it for myself.",
        "I believe these should be rephrased to `False otherwise.`; otherwise, this is somewhat of an unusual outcome",
        "Many of these are great examples for Leonard's \"It's capacitity\". We seriously overdocument some otherwise obvious stuff and keep doing it out of sheer inertia.\r\n\r\n\"GetIsEnabled\" \"Gets whether the thing is enabled\" \"Arguments: thing - the thing for which to report the enablement status\" \"Returns: true of the thing is enabled; false, otherwise,.\"\r\n\r\nNow, I alone am the person to blame for using `iff`."
      ],
      "terminal-use-semantic-naming": [
        "💭 what's a JsonSource vs a Source and a Json?",
        "That sounds like a `Filename` 😁 "
      ],
      "terminal-safe-optional-handling": [
        "You should be careful with this `move` here. I think you're moving from `_warnings` when the settings are loaded (to construct an event for every window) and then again when a new window is created (to construct an event for its creation).\r\n\r\nThe second time will be a no-op, but also it will produce an empty vector. If that is the desired behavior, it's better to make it explicit.",
        "you'll need a `_richBlock{ nullptr }` here, otherwise we will construct one on line 460 and then overwrite it on line 465.",
        "it's an `optional` - `nullopt` is `!=` 1 :)"
      ],
      "terminal-optimize-algorithmic-choices": [
        "```suggestion\n            const auto currentPkgVM = _ViewModel.CurrentExtensionPackage();\n            const auto currentPkg = currentPkgVM.Package();\n```",
        "The & only matters if you're getting a reference from the function you're calling. It is meaningless and misleading otherwise, as it ALWAYS causes a copy (in the case of a projection pointer, a copy is an AddRef) when the function you've called does not return a reference.",
        "(which thing will we move? i guess we could move all of it)",
        "presumably `path{ move(image) }`; everything else is an rvalue",
        "/cc @carlos-zamora this is an excellent example of where `const auto&` on a _local_ goes wrong!"
      ],
      "terminal-use-established-configuration-patterns": [
        "FWIW, I would much rather we not expose this broadly across the project -- especially with a generic name like this. **Nobody** should be calling this at runtime to make decisions based on branding, because that violates the rule that all branding-specific code should be compiled out when that branding isn't set.",
        "Does this config file automatically bail out if _Enterprise_ is installed? If not, how do we make it safe and idempotent?",
        "we will need to figure out what to do when this version of terminal is pulled into Windows, too..."
      ],
      "terminal-clear-accurate-error-messages": [
        "```suggestion\n    <value>An invalid regular expression was found.</value>\n```"
      ],
      "terminal-cache-expensive-computations": [
        "TODO: we could cache both of these, and only regenerate the runs if nothing changed. that would make it cheaper in the general case. we could also cache the _runs_ and only regenerate the ones that differed, but that's starting to become a lot of work for little gain.",
        "you can save a call to `AddRef` and `Release` every time by returning an `AccessibilitySettings&`."
      ],
      "terminal-minimize-reference-counting-overhead": [
        "we could make this `TerminalSettingsCache*`, honestly, and dispense with the shared_ptr completely. _We never construct a new cache._ Pane content does not outlive the parent Page.",
        "nit: should we `std::move` from `pattern` to reduce the reference count churn? then we would use `_pattern` everywhere else (and maybe `wil::hide_name pattern;`? would that pass auditing?)",
        "nit: in general for the winrt typed properties you can save an `AddRef`/`Release` on each one by taking them in as `T` instead of `const T&`, and then using `std::move` when you assign them to members.\r\n\r\nto wit:\r\n\r\n```c++\r\nConstructor(IVector foo): _foo{std::move(foo)} {}\r\n```"
      ],
      "terminal-prefer-const-declarations": [
        "nit: this is a copy/addref/release. either move from it or make it a const ref.",
        "replace all these with\n\n\n```suggestion\n        if (WI_IsFlagSet(bellStyle, BellStyle::Audible))\n```\n\nand get rid of the temporary booleans. :)",
        "why the syntax change? was the previous one wrong?"
      ],
      "terminal-choose-efficient-data-structures": [
        "can we get transparency from `unordered_set` with C++23 without our own hash/equal types?",
        "🥲 "
      ]
    },
    "profile": {
      "location": "St. Louis, MO",
      "company": "@Microsoft ",
      "blog": "http://howett.net",
      "twitter_username": "DHowett",
      "site_admin": true,
      "followers": 2025,
      "following": 4
    }
  },
  "vaxerski": {
    "repos": [
      "hyprwm/Hyprland"
    ],
    "entries": [
      {
        "slug": "hyprland-api-inputoutput-validation",
        "title": "API input/output validation"
      },
      {
        "slug": "hyprland-avoid-expensive-hot-paths",
        "title": "avoid expensive hot paths"
      },
      {
        "slug": "hyprland-avoid-unintentional-defaults",
        "title": "Avoid unintentional defaults"
      },
      {
        "slug": "hyprland-configuration-consistency-validation",
        "title": "configuration consistency validation"
      },
      {
        "slug": "hyprland-consistent-naming-conventions",
        "title": "consistent naming conventions"
      },
      {
        "slug": "hyprland-dynamic-configuration-handling",
        "title": "Dynamic configuration handling"
      },
      {
        "slug": "hyprland-explain-documentation-rationale",
        "title": "Explain documentation rationale"
      },
      {
        "slug": "hyprland-maintain-clean-code-structure",
        "title": "Maintain clean code structure"
      },
      {
        "slug": "hyprland-no-braces-short-ifs",
        "title": "no braces short ifs"
      },
      {
        "slug": "hyprland-optimize-computational-efficiency",
        "title": "optimize computational efficiency"
      },
      {
        "slug": "hyprland-optimize-with-bit-manipulation",
        "title": "optimize with bit manipulation"
      },
      {
        "slug": "hyprland-optimize-workflow-triggers",
        "title": "optimize workflow triggers"
      },
      {
        "slug": "hyprland-precise-configuration-patterns",
        "title": "Precise configuration patterns"
      },
      {
        "slug": "hyprland-precise-documentation-language",
        "title": "Precise documentation language"
      },
      {
        "slug": "hyprland-prefer-managed-pointers",
        "title": "prefer managed pointers"
      },
      {
        "slug": "hyprland-prefer-stderror-code-parameters",
        "title": "prefer std::error_code parameters"
      },
      {
        "slug": "hyprland-prevent-null-dereferences",
        "title": "prevent null dereferences"
      },
      {
        "slug": "hyprland-semantic-variable-naming",
        "title": "Semantic variable naming"
      },
      {
        "slug": "hyprland-validate-environment-variables",
        "title": "validate environment variables"
      },
      {
        "slug": "hyprland-weak-pointer-callback-safety",
        "title": "weak pointer callback safety"
      }
    ],
    "comments": {
      "hyprland-dynamic-configuration-handling": [
        "does libinput verify the passed val? I'd clamp it.",
        "yep",
        "why are we handling this? This is handled by hyprlang, no?!",
        "I think it's just because this MR has a weird approach. Why not use the approach like for device configs? After a dyncall, or after parsing, recheck monitorv2 stuff. If anything changed, update and reload monitor mode",
        "don't do this. Instead, do `handleMonitorV2` on reload in the reload func. Here, just do:\r\n```cpp\r\ng_pEventLoopManager->doLater([this] {\r\ng_pConfigManager->m_wantsMonitorReload = true;\r\n});\r\n```",
        "this will make it unchangeable dynamically. Why not do the same as with `debug:disable_logs`?",
        "dynamically means with hyprctl.\r\n\r\n`disable_logs` is a static pointer, not a value copy.",
        "you can use `configStringToInt`, it handles all of these cases and more.\r\n\r\n`PANIM->second.internalEnabled = configStringToInt(ARGS[1]);`",
        "why not use the same `->set` flag here? instead of the entire change to how `deviceConfigExists` works?",
        "1. getDeviceConfig()->set\r\n2. I don't think so. If a device config exists, all vars belonging to it exist",
        "Again, you can get the struct. We cannot receive ptrs to device configs as they are not static IIRC.\r\n\r\nWhat is your problem? It returns the struct. The struct has a `set` property.",
        "final print of the MR looks good to me.\r\n\r\nI squash everything anyways\r\n\r\nIf there isnt anything else, I am alright with merging this as-is"
      ],
      "hyprland-avoid-expensive-hot-paths": [
        "this should probably be static. It won't change, and I don't know how expensive drmGetCap is. Furthermore, you're duplicating this here and in protocolmanager. Maybe add a bool to CCompositor that is set in `::initServer`?",
        "can we skip damageBox if box.empty",
        "this check can be moved above PMONITOR finding a monitor for better optimization",
        "we should try the cached ones first instead of bothering the X server all the time",
        "Extreme performance kill. Move the variable setting to `setTabletConfigs` and store a copy inside STablet itself.\r\n\r\nUsing `getDevice*` outside `set*Config` is banned",
        "actually, no need to update if tickdelta is less than 1,1, so can you leave that in?",
        "if we are moving, rn, even if delta.x < 1 and delta.y < 1 we still process it, which is redundant",
        "tldr:\r\n```cpp\r\nif (((abs(TICKDELTA.x) < 1.f && abs(TICKDELTA.y) < 1.f) || (TIMERDELTA < MSMONITOR && canSkipUpdate)) && g_pInputManager->dragMode != MBIND_MOVE)\r\n```\r\ninto \r\n```cpp\r\nif ((abs(TICKDELTA.x) < 1.f && abs(TICKDELTA.y) < 1.f) || (TIMERDELTA < MSMONITOR && canSkipUpdate && g_pInputManager->dragMode != MBIND_MOVE))\r\n```"
      ],
      "hyprland-precise-documentation-language": [
        "See I don't want to add that because imagine a scenario one day where a corporation starts using any part of my projects, and I want a cut from them. I have no leverage, as this disallows me from adding a clause prohibiting commercial use / use by commercial entities.",
        "OSS definition? Where do I define as that? You mean line 9?\r\n\r\nAlso, I believe it's more like dropping the F rather than the OSS xD\r\n\r\nHow about `license does not hinder the ability of any non-commercial entity to contribute to, redistribute or view the source code of the project.`",
        "In all honesty - based on a lot of the open source initiative points I've seen - it's a damn pipe dream and completely unrealistic for the modern world. You will _not_ get rid of the tech giants fucking people over in any way other than by law.",
        "committed, thanks!",
        "then you have an open end to the traits, and someone can argue very nasty illegal stuff are their \"trait\". Protected traits are a law term used throughout countries to make it _not_ open ended, so that this doesn't occur.",
        "  well that's how it's worded, no?\r\n  \r\n  Do not harass, attack, or in any other way discriminate against anyone, **including** for their protected traits,",
        "but why? There is a strong \"no harassment\" point. You can make this argument forever, adding traits until the end of time. We put some traits as examples, not as an exhaustive list (note the _including_ and _etc._)\r\n\r\nPeople who want to harass others will always find another reason to harass someone. (as you said, \"I've witnessed discrimination based on so many traits\")",
        "It does, but at some point it clutters the CoC, and makes people less inclined to read it because of the length.\r\n\r\nI believe we've covered enough common sources of specific targeted hate, and the rest fall under \"etc\".\r\n\r\n\"no harassment\" is still \"no harassment\", IMO.",
        "as we are the only members of the org :)"
      ],
      "hyprland-optimize-computational-efficiency": [
        "prefer distanceSq",
        "I meant `CCompositor::ensurePersistentWorkspacesPresent`",
        "well then it should, no?",
        "that is fine ye",
        "why this change? why not just multiply it beforehand?",
        "makes sens",
        "wouldn't `std::erase_if` work better here?"
      ],
      "hyprland-maintain-clean-code-structure": [
        "this was unnecessary, please keep function bodies outside of headers",
        "all .frag files: I'd prefer them in `src/render/shaders/glsl`, then compiled to `src/render/shaders`",
        "no need to add additional includes here - my original mistake of doing that pays now.\r\n\r\nMove them to keybindmanager.cpp to shorten compile time.",
        "oh yeah I missed this... we can't have imports using src paths\r\nalways relative",
        "fixed",
        "better?\r\n",
        "why not put them inside CHyprCtl? It's a singleton. If you prefer, you can even make these static.",
        "private comes last",
        "I'd avoid global funcs tbh. Why not `namespace FontUtils`?",
        "we don't use `this` where unnecessary"
      ],
      "hyprland-api-inputoutput-validation": [
        "hm, this might bug out if the rest of the request has the thing. Maybe substring only the part after the first / and before the first space?",
        "you didn't but when already changing it why not improve?",
        "it's not input validation - it's making sure you choose the right value in case of e.g.\r\n\r\n```\r\n/notify blah blah /decorations\r\n```",
        "no, the /decorations is a part of the parameter. Notify for example takes a string, so the user can put whatever.",
        "missing escapeJSONStrings"
      ],
      "hyprland-precise-configuration-patterns": [
        "shouldn't this be `src/*.h*`? Otherwise it will consume all the subprojects and shit",
        "`.xml`s and `.c`s should also be excluded. Ideally it should be pattern `.h` only"
      ],
      "hyprland-avoid-unintentional-defaults": [
        "you still left the (always include)",
        "actually, it's fine how it is."
      ],
      "hyprland-prefer-stderror-code-parameters": [
        "while we are redoing this, this can throw.\r\n\r\n```\r\nstd::error_code ec;\r\nif (std::filesystem::exists(..., ec) && !ec)\r\n```",
        "canonical can throw. Needs an `std::error_code` and handling",
        "all of this might throw so probably should be in a try catch",
        "every std::filesystem call can throw unless supplied with a std::error_code as the second arg, so the entire thing",
        "instead of try catch which kinda sucks can't you just `std::error_code e1, e2;` and pass them to the `is_*` fns?\r\n\r\nthen just `if (e1 || e2) continue`"
      ],
      "hyprland-optimize-workflow-triggers": [
        "I'd drop the cron. If something is skipped, we can do it manually. Especially when the number of issues grows, this can get ratelimited and slow, etc. I've already seen it with other actions.",
        "will this not just fail on regular commits? Should we just dump this to a separate workflow?",
        "like one that runs only on MRs"
      ],
      "hyprland-explain-documentation-rationale": [
        "too long. \"This is important to avoid clutter, spam, and make the issues more readable\".",
        "If you explicitly state that failing to do so will get your issue removed they will do it I'd say"
      ],
      "hyprland-weak-pointer-callback-safety": [
        "```cpp\r\nPBUFFER->onBackendRelease([wb = WP<IHLBuffer>{PBUFFER}] {\r\n    if (wb)\r\n        wb->unlock();\r\n});\r\n```",
        "double .lock() is redundant"
      ],
      "hyprland-no-braces-short-ifs": [
        "style: no {} around short ifs",
        "no {} around short ifs",
        "needs to ignore windows that:\r\n - aren't mapped\r\n - are fading out\r\n - are XWayland OR",
        "prefer guards.\r\n\r\n```cpp\r\nif (bad)\r\n   continue;\r\n```",
        "no {}\r\n\r\nno `!= NULL`\r\n\r\nconst are CAPS\r\n\r\nyou can use auto\r\n\r\n```\r\nif (const auto CFG_ENV = getenv(\"HYPRLAND_CONFIG\"); CFG_ENV)\r\n```",
        "no {} around short ifs",
        "this is not a short if, please {}",
        "style nit: no {} around short ifs",
        "style: here and hereafter, no {} around short ifs",
        "style: no {} around short ifs",
        "no {} around short ifs",
        "no {} around short ifs",
        "style nit: no `{}` around short ifs",
        "a) no {} around short ifs\r\nb) `forceFloat != 0 && forceFloat - 1 == !PCURRENT->m_bIsFloating`",
        "also why is this even in the if, can be extracted outside of it to avoid duplicating code",
        "> It's there twice because it depends on either PWINDOW or PCURRENT, which is defined inside the outer if.\r\n\r\nright. The other can be extracted, though.",
        "> You sure you want that shortened version? Seems way less clear what the logic is.\r\n\r\nyea",
        "yeah tho tbh I don't see why the PCURRENT check is necessary, all windows in a group should have their floating state synced. You can remove the second check mostl ikely",
        "yea lgtm",
        "prefer guards\r\n```cpp\r\nif (!sls->mapped)\r\n    continue;\r\n```",
        "style: no {} around short ifs",
        "1: style, no {} around short ifs\r\n2: leak: not destroyed on .destroy",
        "RemoveBracesLLVM IIRC removes too much. We only remove around short ifs.",
        "style nit: no {} around short ifs"
      ],
      "hyprland-prevent-null-dereferences": [
        ".lock() != nullptr redundant, Hyprutils::Memory::CWeakPointer has an operator bool",
        "needs a check for nulls here. This could crash if you call `hyprctl layers` when a layer is fading out, I believe.",
        "whats with all those parentheses. Also, `== nullptr` is the same as `!var`",
        "this will crash idiot",
        "wait, lastmonitor can be null",
        "redundant != nullptr",
        "I meant only the `!= nullptr` part\r\n\r\n`if (g_pCompositor->m_pLastFocus)`",
        "m_pLastWindow still can be null. This would be a nullptr dereference aka segfault.\r\n\r\n```cpp\r\ng_pCompositor->m_pLastWindow ? g_pCompositor->m_pLastWindow->m_bIsFullscreen : false;\r\n```",
        "what?"
      ],
      "hyprland-configuration-consistency-validation": [
        "is there any reason for this to be configurable? Shouldn't it be on by default? What are the caveats?",
        "unlikely. I'd just keep it hardcoded on",
        "you've adjusted it here, but forgot src/config/defaultConfig.hpp\r\n\r\nAlso IIRC these are in the wiki as well, so might wanna change them there too",
        "I'd rather not have this one, people usually set it in /etc/environment etc"
      ],
      "hyprland-consistent-naming-conventions": [
        "`m_enabled`. We are moving to drop hungarian see #9061\r\n\r\nalso please initialize it",
        "no snake. `wantsEnabled`",
        "naming convention: classes have a `C` prefix, so `CXCBConnection`",
        "enums in CAPS and with their short name in the beginning, e.g.:\r\n`DIR_AUTO_UP`",
        "enums prefix `e`: `eAutoDirs`",
        "if we are rewriting this, some style things to consider.\r\n\r\nFirstly, this could be wrapped in a namespace e.g. `namespace Systemd`\r\n\r\nSecondly, no_snake, we use camelCase"
      ],
      "hyprland-validate-environment-variables": [
        "instead of this, just pass a `add_compile_definitions` in the cmake",
        "I'd **highly** recommend to get nix to set  _at least_ GIT_COMMIT_HASH, otherwise hyprpm will not work."
      ],
      "hyprland-semantic-variable-naming": [
        "this is a bannable offense in software development. \"duration\" without a unit specifier, that is. `durationUs` is ok if you really want to avoid µ",
        "a name more like `allow_pin_fullscreen` makes more sense intuitively\r\n\r\n+ missing documentation in configDescriptions.hpp",
        "style:\r\n these arent pointers, no `p` prefix\r\n consts are `CAPS`\r\n \r\n`CURRENTWINDOWFSMODE` for example",
        "no snake_case, we use camelCase\r\n\r\nadditionally, const values we SHOUT\r\n\r\nso `TARGETPORTION` and `POSITION`"
      ],
      "hyprland-optimize-with-bit-manipulation": [
        "first time I see this syntax what the fuck? xD",
        "can someone verify this works tho? xD like with a small cpp tester",
        "huh. I know what a bitfield is, just never seen this syntax. Great, guess we all learn every day. Thanks!",
        "here and under, use the bitshift notation `(1 << 0)` `(1 << 1)` etc",
        "with bitfields please make all fields except 0 a `(1 << x)`",
        "it's a stylistic choice, for consistency"
      ],
      "hyprland-prefer-managed-pointers": [
        "raw ptrs are banned in new hl code unless necessary, pass a SP",
        "generally, in new protocol impls, prefer a UP<> for m_resource. You can still take a WP<> to it.",
        "raw ptrs are banned in new code",
        "hm. I don't think they should be shared for no reason. I'll write a unique_ptr in hyprutils later today that allows weak pointers, if I don't by tomorrow remind me.\r\nI've wanted one anyways.",
        "class is boring, here you go https://github.com/hyprwm/hyprutils/commit/423c69d697f56af4f8f2de7e2279eead17901228",
        "https://github.com/hyprwm/Hyprland/pull/9143",
        "sls container should now store a UP if you rebase on main and you can make this func return a WP",
        "the reason popup and subsurface are C pointers is that they haven't yet been rewritten. Please use managed pointers (a WP here)"
      ]
    },
    "profile": {
      "location": "Acheron, Hades",
      "blog": "https://vaxry.net",
      "site_admin": false,
      "followers": 2353,
      "following": 10
    }
  },
  "anthonyshew": {
    "repos": [
      "vercel/turborepo"
    ],
    "entries": [
      {
        "slug": "turborepo-boundary-case-handling",
        "title": "Boundary case handling"
      },
      {
        "slug": "turborepo-configuration-precision-matters",
        "title": "Configuration precision matters"
      },
      {
        "slug": "turborepo-define-api-boundaries",
        "title": "Define API boundaries"
      },
      {
        "slug": "turborepo-document-cache-strategies",
        "title": "Document cache strategies"
      },
      {
        "slug": "turborepo-document-configuration-alternatives",
        "title": "Document configuration alternatives"
      },
      {
        "slug": "turborepo-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "turborepo-framework-specific-entrypoints-organization",
        "title": "Framework-specific entrypoints organization"
      },
      {
        "slug": "turborepo-hybrid-monorepo-testing",
        "title": "Hybrid monorepo testing"
      },
      {
        "slug": "turborepo-keep-build-tooling-updated",
        "title": "Keep build tooling updated"
      },
      {
        "slug": "turborepo-know-your-implicit-configurations",
        "title": "Know your implicit configurations"
      },
      {
        "slug": "turborepo-link-terms-provide-examples",
        "title": "Link terms, provide examples"
      },
      {
        "slug": "turborepo-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "turborepo-standardize-package-manager-commands",
        "title": "Standardize package manager commands"
      },
      {
        "slug": "turborepo-validate-configuration-structures",
        "title": "Validate configuration structures"
      },
      {
        "slug": "turborepo-validate-configurations-comprehensively",
        "title": "Validate configurations comprehensively"
      },
      {
        "slug": "turborepo-validate-performance-impact-first",
        "title": "Validate performance impact first"
      },
      {
        "slug": "turborepo-verify-test-commands",
        "title": "Verify test commands"
      }
    ],
    "comments": {
      "turborepo-hybrid-monorepo-testing": [
        "```suggestion\r\nYou can combine the benefits of both approaches by implementing a hybrid solution.This approach unifies local development using Vitest's Workspace approach while preserving Turborepo's caching in CI.  This comes at the tradeoff of slightly more configuration and a mixed task running model in the repository.\r\n```",
        "```suggestion\r\n      \"outputs\": [\"coverage/**\"]\r\n```"
      ],
      "turborepo-link-terms-provide-examples": [
        "In general, we like to provide the link the first time we mention a term. The game I like to play is: Imagine your coworker just linked you this page and you're only glancingly aware that Turborepo is in your company's codebase. You're not terribly familiar with Turborepo but you're here to learn more about it.\r\n\r\nPutting yourself in those shoes, you're likely to read this \"Environment Modes\" term and think \"Uh, what's that?\", and want to be linked to more information in that moment. You could, of course, keep reading and make it to the end of this section, but you'd probably appreciated the link sooner.\r\n\r\nSo, that said, I'd be lifting the link that's a few lines below to here, and removing the sentence on 153. Both a context and conciseness win!",
        "Ah, heard! Ignore then. 😄 ",
        "Nice, good call."
      ],
      "turborepo-configuration-precision-matters": [
        "I'm upgrading the `engines` in package.json. I don't know if this is contentious or not. I'm seeing multiple versions of CI around the repository so I'm not sure if we consider this value important or not.",
        "As it turns out, we should definitely be doing this. This is now our source of truth."
      ],
      "turborepo-validate-configuration-structures": [
        "```suggestion\r\n          // TODO: Our code was allowing both config files to exist. This is a bug, needs to be fixed.\r\n```",
        "No worries. Thanks for the tag. Will fix.",
        "Here's the fix: https://github.com/vercel/turborepo/pull/10105\r\n\r\nSorry about that!"
      ],
      "turborepo-validate-configurations-comprehensively": [
        "```suggestion\r\n    /// Validates field placement to ensure root-only and package-only fields\r\n    /// are used in the correct configuration types.\r\n    ///\r\n    /// This uses an allowlist approach - ALL fields must be explicitly\r\n    /// categorized.\r\n```",
        "```suggestion\r\n#[error(\"$$ROOT$$ syntax is not allowed in globalDependencies, since globalDependencies is already relative to the root of the Workspace.\")]\r\n```",
        "```suggestion\r\n    #[error(\"\\\"$TURBO_ROOT$\\\" must be used at the start of glob.\")]\r\n```"
      ],
      "turborepo-keep-build-tooling-updated": [
        "Run `npx @turbo/codemod upgrade` to upgrade to `turbo@2`. 🥳 ",
        "Run `npx @turbo/codemod upgrade` to upgrade to `turbo@2`. 🥳 "
      ],
      "turborepo-validate-performance-impact-first": [
        "Making sure to call extra attention here: Users mentioned they felt 1024 was too small, so I doubled it here. Not sure about if there's a reason I shouldn't do this or if a different value should be used. Just did this for the sake of discussion.",
        "Interesting, muchas gracias."
      ],
      "turborepo-document-cache-strategies": [
        "I wasn't sure if `no Remote Cache activity` is correct. Can someone confirm?",
        "We talked about this to make sure and it is correct!",
        "We need to keep this information. It's important that folks know that no artifacts will be cached."
      ],
      "turborepo-know-your-implicit-configurations": [
        "@chris-olszewski, I actually don't know this one. Can I ignore a lockfile? I'm figuring I can't since we go parse it separately from collecting file inputs.\r\n\r\n I shouldn't be able to, but can I? I'm trying to see if I can trick it into doing so and it doesn't look like it.",
        "```suggestion\n- `turbo.json`\n- Package manager lockfiles\n```",
        "```suggestion\r\n  may want to specify a range (for example, `\">=15\"`) according to your needs.\r\n  \r\n  Additionally, for older package managers, you may need to instruct your package manager to install peer dependencies with configuration, or add the dependency to `devDependencies` as a workaround.\r\n```",
        "```suggestion\r\nTo help with incremental migration or in situations where you can't use the `packageManager` field, you may use `--dangerously-disable-package-manager-check` to opt out of this check and assume the risks of unstable lockfiles producing unpredictable behavior. When disabled, Turborepo will attempt a best-effort discovery of the intended package manager meant for the repository.\r\n```",
        "```suggestion\r\nTo help with incremental migration or in situations where you cannot use the `packageManager` field, you may use `--dangerously-disable-package-manager-check` to opt out of this check and assume the risks of unstable lockfiles producing unpredictable behavior. When disabled, Turborepo will attempt a best-effort discovery of the intended package manager meant for the repository.\r\n```"
      ],
      "turborepo-verify-test-commands": [
        "```suggestion\r\ncargo coverage -- --open\r\n```"
      ],
      "turborepo-standardize-package-manager-commands": [
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```"
      ],
      "turborepo-document-configuration-alternatives": [
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```"
      ],
      "turborepo-framework-specific-entrypoints-organization": [
        "```suggestion\r\n- `./next-js/link`: A customized version of [the Next.js `Link` component](https://nextjs.org/docs/app/building-your-application/routing/linking-and-navigating#link-component) with props that are preset to your organization's preferences\r\n- `./svelte/link`: A customized version of an [`a` tag for Svelte](https://svelte.dev/docs/kit/link-options) with presets.\r\n```"
      ],
      "turborepo-propagate-errors-with-context": [
        "Would it make sense to do:\r\n\r\n```suggestion\r\n    #[error(\"Unable to persist preferences. Please file a bug report.\")]\r\n```"
      ],
      "turborepo-boundary-case-handling": [
        "Classic me not knowing methods I have available."
      ],
      "turborepo-eliminate-code-duplication": [
        "Some of my best \"past midnight with a screaming baby\" code, truly."
      ],
      "turborepo-define-api-boundaries": [
        "```suggestion\r\n/* Add x-artifact-tag header to artifact download endpoint response */\r\n```",
        "My gut reaction is to say no, but I could be talked into saying yes. This has come up _once_ in my memory, or has it been more? I'm hesitant to get into specifics in these sections beyond the feature lists in the pre-stable sections. Imagine stepping onto this page with no prior context...Seeing a specific mention for unstructured terminal output would feel...odd.\r\n\r\nMaybe a broader question: Do other CLI tools consider their stdout/stderr as semver-protected if its meant for human readability? I've never looked into or thought about this...",
        "Aligned, let's omit.",
        "```suggestion\r\n        needed for self-hosted Remote Caches that implement an endpoint that dynamically creates tokens.\r\n```",
        "```suggestion\r\nNote that this example uses the [Just-in-Time Package](/repo/docs/core-concepts/internal-packages#just-in-time-packages) pattern for simplicity. It exports TypeScript directly, but you might choose to use the [Compiled Package](/repo/docs/core-concepts/internal-packages#compiled-packages) pattern instead.\r\n```"
      ]
    },
    "profile": {
      "company": "Vercel",
      "blog": "https://shew.dev",
      "twitter_username": "anthonysheww",
      "site_admin": false,
      "followers": 310,
      "following": 4
    }
  },
  "sydney-runkle": {
    "repos": [
      "pydantic/pydantic"
    ],
    "entries": [
      {
        "slug": "pydantic-avoid-shared-structure-mutation",
        "title": "Avoid shared structure mutation"
      },
      {
        "slug": "pydantic-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "pydantic-balance-documentation-thoroughness",
        "title": "Balance documentation thoroughness"
      },
      {
        "slug": "pydantic-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "pydantic-categorize-error-types",
        "title": "Categorize error types"
      },
      {
        "slug": "pydantic-consistent-configuration-patterns",
        "title": "Consistent configuration patterns"
      },
      {
        "slug": "pydantic-document-code-rationale",
        "title": "Document code rationale"
      },
      {
        "slug": "pydantic-document-configuration-relationships",
        "title": "Document configuration relationships"
      },
      {
        "slug": "pydantic-eliminate-redundant-computation",
        "title": "Eliminate redundant computation"
      },
      {
        "slug": "pydantic-enforce-style-with-linters",
        "title": "Enforce style with linters"
      },
      {
        "slug": "pydantic-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "pydantic-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "pydantic-preserve-language-conventions",
        "title": "Preserve language conventions"
      },
      {
        "slug": "pydantic-robust-error-messaging",
        "title": "Robust error messaging"
      },
      {
        "slug": "pydantic-safe-attribute-access-pattern",
        "title": "Safe attribute access pattern"
      },
      {
        "slug": "pydantic-semantic-over-syntactic",
        "title": "Semantic over syntactic"
      },
      {
        "slug": "pydantic-simple-defaults-flexible-overrides",
        "title": "Simple defaults, flexible overrides"
      },
      {
        "slug": "pydantic-specific-types-for-performance",
        "title": "Specific types for performance"
      },
      {
        "slug": "pydantic-standardize-dependency-management",
        "title": "Standardize dependency management"
      },
      {
        "slug": "pydantic-structured-configuration-management",
        "title": "Structured configuration management"
      },
      {
        "slug": "pydantic-write-targeted-specific-tests",
        "title": "Write targeted, specific tests"
      }
    ],
    "comments": {
      "pydantic-cache-expensive-computations": [
        "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
        "I'm ok with leaving this dupe for now",
        "Why the removal of the `defer_build` check?",
        "```suggestion\r\n                # Deleting the validator/serializer is necessary as otherwise they can get reused in\r\n                # pydantic-core. Same applies for the core schema that can be reused in schema generation.\r\n```",
        "I wouldn't expect to have a `slow_memo_handler`, given that the point of memoization is to make things fast. Maybe we could use `setattr_handler` instead?"
      ],
      "pydantic-simple-defaults-flexible-overrides": [
        "See changes - I've sort of gone the other way here - wrap val is the same, but `__init__` is greatly simplified 👍 "
      ],
      "pydantic-document-code-rationale": [
        "As discussed in person, let's add a comment that this warning is only omitted when calling super. I think we need to make it more clear what the consequences of this deprecation are.",
        "Maybe add a note about what successfully collected means - specifically, that annotations were successfully evaluated?",
        "Let's be even more explicit here.\r\n\r\nCan we:\r\n* Explain subsituting definition refs\r\n* Explain why / how we add tagged info to discriminators?\r\n\r\nThis isn't your doing at all, but our internal `GenerateSchema` logic is poorly documented, so I think it's good to have thorough improvements as a standard for PRs here going forward.",
        "Yeah, that probably makes more sense, given that's where we do the namespace fetching anyways. Happy to move there.",
        "Might put this under a closed by default block as to not overwhelm users not using this complex functionality.",
        "Maybe the class itself, honestly. Bc it applies to both `__init__` and `rebuild`",
        "Can we add a docstring to this function and the one above? If you're switching contexts into the world of typing and types management, a little description here could really help."
      ],
      "pydantic-maintain-code-consistency": [
        "```suggestion\r\n            model_frozen = cls.model_config.get('frozen')\r\n            field_frozen = getattr(cls.__pydantic_fields__.get(name), 'frozen', False)\r\n            if model_frozen or field_frozen:\r\n```",
        "We should make this consistent with the above pattern.\r\n\r\nIt's hard - we're not at the 3 repetition rule here that necessitates abstraction. I'm ok with keeping this duplicated code as long as it's as consistent as possible."
      ],
      "pydantic-structured-configuration-management": [
        "Will do, this is going to take a while.",
        "Just curious, why did we move this up?",
        "I think we can only remove this bound if we change the lower bound for sqlalchemy?",
        "Or just add a conditional one for 3.13"
      ],
      "pydantic-safe-attribute-access-pattern": [
        "Do we not even need to try this?",
        "I like the version @Viicos suggested.",
        "Can you avoid the ignore with a `cast` like `metadata = cast(schema.setdefault('metadata', {})`?",
        "Can we leave this as is? What `AttributeError` might occur here?",
        "Makes sense, thanks for the improvement!"
      ],
      "pydantic-balance-documentation-thoroughness": [
        "Agreed, good call.",
        "I've opened an issue and linked this comment. See https://github.com/pydantic/pydantic/issues/11491",
        "Isn't this something worth including in the conceptual docs?",
        "Agreed.",
        "I'm fine with this as long as we link to model docs. Maybe we could add a few other notes as well like json schema, etc.",
        "I feel like the explicit specifications here were helpful - people often ask about `init_typed`..."
      ],
      "pydantic-robust-error-messaging": [
        "Is this true? Doesn't it only need to be not `None` when `_fields.takes_validated_data_argument(self.default_factory)` is True?",
        "Fine by me, feel free to merge then :)."
      ],
      "pydantic-document-configuration-relationships": [
        "> Also, what should happen if you set validate_by_alias=False, but explicitly set by_alias=True or by_name=True during validation?\r\n\r\nValidation time settings always take priority, when set. This is the same with `strict`.",
        "I'm sympathetic to the literal pattern argument. If we were starting fully from scratch, I think it might make more sense. Specifically, the boolean traps can be a bit confusing. In particular, the fact that you have to set `validate_by_name=True` if `validate_by_alias=False` explicitly is a bit confusing, especially for new users.\r\n\r\nOne thing we could do to mitigate this challenge is automatically set `validate_by_name=True` if a user sets `validate_By_alias=False`.\r\n\r\nMy thoughts re why we should stick with the 2 boolean flags:\r\n\r\n* It represents less change to this setting compared to a switch to literals - there's already a lot of change going on here, and I'm hesitant to introduce a setting `type` change as well.\r\n* 2 boolean flags provide greater configurability for interaction between config and runtime settings, as you can override one behavior and not the other. It's also helpful to have unset markers for each thing. For example:\r\n\r\n```\r\nM1: validate_by_alias = True, validate_by_name = False\r\nM2: validate_by_alias = False, validate_by_name = True\r\n\r\nruntime setting: by_name = True\r\n\r\n==>\r\n\r\nM1: alias and name validation\r\nM2: name only validation\r\n```\r\n\r\nThis can't be achieved with the literal approach. Either you'd use:\r\n* `validate_by='name'`, and M1 would lose alias validation\r\n* `validate_by='name and alias'` and M2 would no longer avoid validating with alias\r\n\r\n* Autocomplete is easier with boolean flags, and the behavior is relatively intuitive\r\n\r\nAliases are one of the most common (if not the most commonly used) field tool, so I do think this decision is quite important. I also understand that if we go with bools here, we're stuck with that until at least V4. ",
        "Agreed, definitely important. I've pushed a [commit](https://github.com/pydantic/pydantic/pull/11468/commits/2341a58997683df514a7dcd27a615a5d05678f55) to make this backwards compatible.\r\n\r\nThe new behavior:\r\n\r\n```py\r\nfrom pydantic import BaseModel, ConfigDict, Field\r\n\r\nclass Model1(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=False, validate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm1 = Model1(my_field='foo')\r\n\r\nclass Model2(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=False)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm2 = Model2(my_alias='foo')\r\n\r\nclass Model3(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\n# for this case, we prefer the field name over the alias, as we did in the past\r\n# if populate_by_name was True\r\nm31 = Model3(my_field='foo')\r\nm32 = Model3(my_alias='foo')\r\n#> test.py:23: error: Missing named argument \"my_field\" for \"Model3\"  [call-arg]\r\n\r\nclass Model4(BaseModel):\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm4 = Model4(my_alias='foo')\r\n\r\nclass Model5(BaseModel):\r\n    model_config = ConfigDict(populate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\n# for this case, we prefer the field name over the alias\r\nm51 = Model5(my_field='foo')\r\nm52 = Model5(my_alias='foo')\r\n#> test.py:39: error: Missing named argument \"my_field\" for \"Model5\"  [call-arg]\r\n```",
        "Fixed this problem by consolidating the settings as you had previously :)"
      ],
      "pydantic-avoid-shared-structure-mutation": [
        "Seems like we lost this logic - is this needed anywhere?",
        "Would it make sense to break this change out into a different PR?",
        "Just curious, why do we need to add this here now?"
      ],
      "pydantic-categorize-error-types": [
        "Maybe we could offer more clarity on usage errors, like providing an example or two"
      ],
      "pydantic-preserve-language-conventions": [
        "`validate_by_name` logic still applies to `__init__` :)",
        "@stevapple, feel free to open an issue with a summary of this discussion!",
        "```suggestion\r\nimport json\r\nfrom pydantic import BaseModel, computed_field\r\n\r\n\r\nclass Box(BaseModel):\r\n    width: float\r\n    height: float\r\n    depth: float\r\n\r\n    @computed_field\r\n    @property  # (1)!\r\n    def volume(self) -> float:\r\n        return self.width * self.height * self.depth\r\n\r\n\r\nprint(json.dumps(Box.model_json_schema(mode='serialization'), indent=2))\r\n\"\"\"\r\n{\r\n  \"properties\": {\r\n    \"width\": {\r\n      \"title\": \"Width\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"height\": {\r\n      \"title\": \"Height\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"depth\": {\r\n      \"title\": \"Depth\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"volume\": {\r\n      \"readOnly\": true,\r\n      \"title\": \"Volume\",\r\n      \"type\": \"number\"\r\n    }\r\n  },\r\n  \"required\": [\r\n    \"width\",\r\n    \"height\",\r\n    \"depth\",\r\n    \"volume\"\r\n  ],\r\n  \"title\": \"Box\",\r\n  \"type\": \"object\"\r\n}\r\n\"\"\"\r\n```\r\n\r\nLooking great, just a formatting fix here."
      ],
      "pydantic-eliminate-redundant-computation": [
        "Why is `is_generic_alias` not included in `typing_inspection`?",
        "Good question, looking into this.",
        "So, this happens when we have a model that deferred (or failed) schema building.\r\n\r\nI think this logic needs to be more complex - should we reassign `__pydantic_core_schema__`? What about `__pydantic_validator__` and `__pydantic_serializer__`?",
        "I've updated this - we no longer do the recursive call :).",
        "Good call, done.",
        "Hmm, I don't think there's an easy way to do this - you can't easily create a `TypeAdapter` with just a core schema. Even if we could hack that together, should we?",
        "Found a relatively simple way with `TypeAdapter`. That's easiest for now, as we already have the `isinstance` conditional in the wrap validator in the custom core schema.\r\n\r\nDown the line, maybe it does make sense to just use `SchemaValidator` inside these types to perform internal validation. I'm going to write up an issue with next steps based on the feedback here, and I'll include this 👍 "
      ],
      "pydantic-specific-types-for-performance": [
        "I like this example overall.\r\n\r\nI think we should:\r\n* Link to the related [performance section](https://docs.pydantic.dev/latest/concepts/performance/#sequence-vs-list-or-tuple-mapping-vs-dict)\r\n* Give a bit more context re efficiency here - the more specific you can be with a type, the better (sort of in a philosophical sense)",
        "Sure - basically, in one line or two, you can explain that generally, the more specific a type, the faster validation will be, as we don't have to perform as many checks / coercion attempts."
      ],
      "pydantic-avoid-unnecessary-operations": [
        "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
        "I'm ok with leaving this dupe for now"
      ],
      "pydantic-explicit-over-implicit": [
        "This feels cleaner to me than the way we had it before, I wasn't a fan of having `tzdata` in `dev`. Thanks!",
        "It does seem redundant..."
      ],
      "pydantic-semantic-over-syntactic": [
        "Would `is_type_alias_type` make more sense?",
        "I'd prefer `is_type_alias_type` just bc it's easier to read, but it's up to you, doesn't really matter!",
        "Maybe add `Schema` to the name of this? Or `CoreSchema`?",
        "Can we make this typevar more clear, maybe like `PropertyReturnType` or `ReturnType` or something like that?",
        "Good call"
      ],
      "pydantic-consistent-configuration-patterns": [
        "Should we use `ConfigDict(str_max_length=10)`?",
        "Should we recommend one over the other?",
        "The default does not override the config, this only overrides config if set. Good question!",
        "I believe in the migration guide we talk about priority here (sometimes it's not intuitive). Can we make a note on that here, and add a dupe key to the merged configs and show which one is preserved in the result?"
      ],
      "pydantic-enforce-style-with-linters": [
        "Did you want to add a lint step to precommit that helps enforce standardization here?"
      ],
      "pydantic-standardize-dependency-management": [
        "```suggestion\r\n          enable-cache: true\r\n          python-version: '3.12'\r\n```"
      ],
      "pydantic-write-targeted-specific-tests": [
        "Perhaps this could be a bit more verbose:\r\n\r\n1. Can we include the `SchemaError` information in this result?\r\n2. Could we add a note about the fact that this is purely a testing feature, not a runtime `pydantic` check (at this point)?\r\n\r\nPerhaps this is a bit excessive, but can we test this test? As in, run a test within a test, and check that this is raised for invalid schema? If not, no worries, but could you document an example of a failing test on this PR just to have as a reference for the blame later?",
        "Yeah, I think having their own tests would be good so that we can avoid the conditional warning checks.",
        "Why remove this?"
      ]
    },
    "profile": {
      "location": "Somerville, MA",
      "company": "@langchain-ai",
      "blog": "",
      "site_admin": false,
      "followers": 356,
      "following": 51
    }
  },
  "dsherret": {
    "repos": [
      "denoland/deno"
    ],
    "entries": [
      {
        "slug": "deno-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "deno-avoid-ambiguous-naming",
        "title": "Avoid ambiguous naming"
      },
      {
        "slug": "deno-avoid-implementation-detail-leakage",
        "title": "avoid implementation detail leakage"
      },
      {
        "slug": "deno-avoid-panics-gracefully",
        "title": "avoid panics gracefully"
      },
      {
        "slug": "deno-avoid-tooling-workarounds",
        "title": "avoid tooling workarounds"
      },
      {
        "slug": "deno-choose-appropriate-algorithms",
        "title": "Choose appropriate algorithms"
      },
      {
        "slug": "deno-comprehensive-test-verification",
        "title": "comprehensive test verification"
      },
      {
        "slug": "deno-control-cache-lifecycle",
        "title": "Control cache lifecycle"
      },
      {
        "slug": "deno-defensive-null-handling",
        "title": "defensive null handling"
      },
      {
        "slug": "deno-enhance-error-message-clarity",
        "title": "Enhance error message clarity"
      },
      {
        "slug": "deno-environment-loading-order",
        "title": "environment loading order"
      },
      {
        "slug": "deno-explain-non-obvious-decisions",
        "title": "Explain non-obvious decisions"
      },
      {
        "slug": "deno-explicit-dependency-configuration",
        "title": "explicit dependency configuration"
      },
      {
        "slug": "deno-minimize-memory-allocations",
        "title": "minimize memory allocations"
      },
      {
        "slug": "deno-organize-code-structure",
        "title": "organize code structure"
      },
      {
        "slug": "deno-prefer-safe-optional-returns",
        "title": "prefer safe optional returns"
      },
      {
        "slug": "deno-use-appropriate-synchronization-mechanisms",
        "title": "Use appropriate synchronization mechanisms"
      },
      {
        "slug": "deno-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "deno-validate-operation-permissions",
        "title": "Validate operation permissions"
      }
    ],
    "comments": {
      "deno-choose-appropriate-algorithms": [
        "Could use strategy pattern here in the future (reporting strategy and remove strategy).",
        "Why do we need to parse an AST now? It seems we really only need the comments which the lexer provides?",
        "Should we be using `deno_semver::Version` here? https://docs.rs/deno_semver/0.7.1/deno_semver/struct.Version.html"
      ],
      "deno-defensive-null-handling": [
        "```suggestion\r\n  const formattedMessage = message.length === 0 ? \"\" : `${message} `;\r\n  return op_read_line_prompt(formattedMessage, `${defaultValue}`);\r\n```"
      ],
      "deno-organize-code-structure": [
        "Instead of this being here and on `Permissions`. I think it should be a constructor parameter to `RuntimePermissionDescriptorParser`? That way the value isn't being passed around horizontally through the code.",
        "We'll need to figure out how to extract out the progress bar stuff from here before extracting this to a new crate.",
        "```suggestion\r\n  let user_data_dir = if let Some(env_var) = std::env::var_os(TEST_ENV_VAR_NAME) {\r\n    PathBuf::from(env_var)\r\n```\r\n\r\nDeja-vu. Should this code be extracted out to a reusable function?",
        "Nitpick: Maybe move this about PathTree so that the PathTrie struct w/ impl is beside each other?",
        "Nitpick: maybe use named properties at this point? (especially because it's public)",
        "Can you make this private again? See the comment above (basically when we previously had this exposed bugs would happen where people rely on this instead of also doing stuff like looking at the configuration file)"
      ],
      "deno-add-comprehensive-test-coverage": [
        "In this file there are unit tests for flag parsing. Maybe we should add some for the clean command now?"
      ],
      "deno-control-cache-lifecycle": [
        "It's better to do this at a higher level so the caller can decide when to free these from the cache.",
        "Separated the caching to separate the concerns (and it allows for a long lived service and short lived cached)",
        "We should improve this to be smarter. Right now I just have it clearing before and after each op_resolve.",
        "Oh, actually I think we can just clear when the dependents and config are recreated. I will move this there.",
        "Ah, seems tricky. Yeah I'm going to switch to clear after the request is completed."
      ],
      "deno-comprehensive-test-verification": [
        "It might be better to test the full output so that the test is more resilient to change (ex. the fixes being moved somewhere else)"
      ],
      "deno-explicit-dependency-configuration": [
        "Can you revert these changes to the `deno_` crates? I don't think deno_doc is used anywhere else. I'd like to keep this here so that if we ever switch other crates to depend on this crate it's more obvious and we can have more of a discussion. The non-deno crates are fine though.\r\n\r\nAlso, the CI is failing because `cargo check` needs to be run I think."
      ],
      "deno-use-descriptive-identifiers": [
        "Not sure about this name. What do other people do?"
      ],
      "deno-enhance-error-message-clarity": [
        "Maybe we should show which code did this by showing part of the stack?",
        "Yup!"
      ],
      "deno-avoid-panics-gracefully": [
        "Why are these warning and returning the error?",
        "I think this is doing what `error.maybe_range()` does internally? https://github.com/denoland/deno_graph/blob/961df5f16a0bb28a530612172b2f58e4482f287e/src/graph.rs#L426C14-L426C24"
      ],
      "deno-validate-operation-permissions": [
        "Is it ok this is accessible without permissions?",
        "I think we'll need to add write permission checks here?",
        "This might be a security issue. I think there's a reason why this isn't allowed without `-A`, but I can't remember at the moment.",
        "I looked into this a bit and I think it's more complicated than it seems. Here's some points from an old internal document:\r\n\r\n> - If users are using `\\\\.\\` paths to access things like named pipes, raw devices, those should trigger `--allow-sys` .\r\n> - Network resources should perform DNS lookup in permission checks, and the fully-resolved permission check object should include the the IP address. When we make the outgoing network requests, we must use the resolved IP.\r\n\r\nThat said, I'm not sure about what the right solution is here. At the moment, this just requires `--allow-all` permissions."
      ],
      "deno-avoid-implementation-detail-leakage": [
        "Nitpick: Maybe just reexport `quinn::ConnectionError` to not leak this detail to the rest of the code?",
        "It's probably too much to expose all the way down here and also it creates a dependency on deno_graph for Deploy as well as `deno compile`. Probably we should have a trait that gets passed down here and then the CLI and deno compile can implement this separately (maybe just put it as a method on `NodeRequireLoader`)",
        "Before we merge this, we should open a PR in deno_graph to expose these and then add a todo here to remove this code.",
        "I noticed we don't need to expose this and only need to expose the options. Saves having to import one extra type in the caller."
      ],
      "deno-use-appropriate-synchronization-mechanisms": [
        "Yeah. Let's remove these.",
        "Is there a helper function we could introduce to ensure we always do this safely?",
        "I mean, something like have it panic if not done from the same thread or we just put a lock over all the usages on non-windows platforms?",
        "I kind of feel like we should be using a conditional variable here in order to kill the thread more quickly in case this is hanging shutdown, but this is probably fine.",
        "@nayeemrmn do you think we could move the init_flag and shutdown_flag into our tower-lsp fork? Might help simplify things up here."
      ],
      "deno-environment-loading-order": [
        "I think you want to move this above creating the factory and use `flags.env_file` instead. That way the changed environment variables propagate into everything created by the factory.",
        "We'll need to remove the usage of `.env` because the env file hasn't been loaded at this point. Do a search through the other code to see how it's done, but basically we need to load the env variables after loading the env file.",
        "I have a pending custom lint rule I need to implement for this: https://github.com/denoland/deno/issues/29886"
      ],
      "deno-avoid-tooling-workarounds": [
        "It looks like an swc syntax error.",
        "It's an swc diagnostic that's not surfaced in the playground. I'll open an swc issue.",
        "Looked into it and here's an example of the swc playground erroring: https://play.swc.rs/?version=1.11.20&code=H4sIAAAAAAAAA0vOzysuUUgrys9VsFWorrXmSq0oyC8qUUhJTUsszYHIWAMAg8l5qiUAAAA%3D&config=H4sIAAAAAAAAA1WPsQ6DMAxEd74Cee7aDp2rbv0IKzUoKCGRbSQQ4t8JkNCyxXfPd%2FFc1TV0YuBZz%2BmZhogsxOecFJl6xTEpQMajGLZR4VZcGmNgfVGDg9M3B59A5YFOoJNtt0EntEvL4YAit6R7rNxzHrgQhAqeNW9720z%2FPzLBRyaRK7ih2LeOrnVVrgQfvsNu5kt1inTUP%2BAHlbIzGKx8yuZ2WLWst%2B%2Fb9zUBAAA%3D\r\n\r\nIt's because of the `export default from` proposal.",
        "https://github.com/swc-project/swc/issues/10372 -- Using identifiers that are also contextual keywords is kind of asking to hit edge cases.",
        "We should try to move off `@ts-ignore` and instead just remove the `/** @internal */`s in TypeScript's codebase. I updated https://github.com/denoland/TypeScript/pull/15 as part of this change."
      ],
      "deno-minimize-memory-allocations": [
        "Can we change this to do the allocations in the constructor by precomputing the two possible values? (and preferably avoid any allocations there when `self.options.conditions` is empty, though that's not as big of a deal) I think this could return a `&[Cow<'static, str>>]`\r\n\r\nSorry, this might seem nitpicky, but it's not really. I did a lot of work removing allocations from node resolution in order to make it faster.",
        "Seems like a lot of work and allocations. Maybe we should just do something that stops after X chars on the first line? (ex. read the first 400 chars and if there's no newline then it's probably minified or maybe read all the chars and if it's over X length and only a max of 2 lines then it's minified)",
        "I think there's cloning that shouldn't be done because we have a static string here."
      ],
      "deno-explain-non-obvious-decisions": [
        "Done, and used the enum instead.",
        "It's up to the caller. I'll add a doc comment on the option."
      ],
      "deno-prefer-safe-optional-returns": [
        "Maybe just return `None`? dprint-plugin-typescript could change in the future and that would break this code.",
        "Prefer `deno_path_utils::url_to_file_path` and maybe handle these errors?"
      ],
      "deno-avoid-ambiguous-naming": [
        "I see from a resolved comment that we're going to change this to an enum? Consider that or have a `parse_for_list_with_wildcards` method.",
        "Don't we usually name the unstable options in an options bag rather than parameters? Or we should be and I've been updating some of them like in https://github.com/denoland/deno/pull/29473\r\n\r\nGiven it's permissions code I think we should make it very clear what's going on here and this `true` here isn't clear.",
        "Nitpick: It might be good to have the call sites for this be more descriptive and not use boolean parameters, but it's probably fine because it's internal.",
        "Maybe: `check_specifiers(specifiers: &[ModuleSpecifier], options: CheckSpecifierOptions)`"
      ]
    },
    "profile": {
      "location": "Toronto, Canada",
      "company": "@denoland",
      "blog": "https://david.deno.dev",
      "twitter_username": "DavidSherret",
      "site_admin": false,
      "followers": 1991,
      "following": 25
    }
  },
  "aimurphy": {
    "repos": [
      "langflow-ai/langflow"
    ],
    "entries": [
      {
        "slug": "langflow-ai-model-chunk-sizing",
        "title": "AI model chunk sizing"
      },
      {
        "slug": "langflow-ai-response-variability",
        "title": "AI response variability"
      },
      {
        "slug": "langflow-api-authentication-requirements",
        "title": "API authentication requirements"
      },
      {
        "slug": "langflow-api-documentation-completeness",
        "title": "API documentation completeness"
      },
      {
        "slug": "langflow-api-endpoint-documentation",
        "title": "API endpoint documentation"
      },
      {
        "slug": "langflow-configuration-documentation-clarity",
        "title": "Configuration documentation clarity"
      },
      {
        "slug": "langflow-consistent-formatting-standards",
        "title": "Consistent formatting standards"
      },
      {
        "slug": "langflow-consolidate-related-information",
        "title": "Consolidate related information"
      },
      {
        "slug": "langflow-database-configuration-clarity",
        "title": "Database configuration clarity"
      },
      {
        "slug": "langflow-document-connection-parameters-clearly",
        "title": "Document connection parameters clearly"
      },
      {
        "slug": "langflow-document-log-level-options",
        "title": "Document log level options"
      },
      {
        "slug": "langflow-document-security-implications-clearly",
        "title": "Document security implications clearly"
      },
      {
        "slug": "langflow-environment-variable-documentation",
        "title": "Environment variable documentation"
      },
      {
        "slug": "langflow-observability-documentation-structure",
        "title": "observability documentation structure"
      },
      {
        "slug": "langflow-organize-documentation-content",
        "title": "Organize documentation content"
      },
      {
        "slug": "langflow-precise-workflow-conditions",
        "title": "precise workflow conditions"
      },
      {
        "slug": "langflow-secure-credential-management",
        "title": "Secure credential management"
      },
      {
        "slug": "langflow-standardize-naming-patterns-consistently",
        "title": "Standardize naming patterns consistently"
      },
      {
        "slug": "langflow-use-fake-sample-data",
        "title": "Use fake sample data"
      }
    ],
    "comments": {
      "langflow-ai-response-variability": [
        "To account for differences in responses or if they change the model:\r\n\r\n```suggestion\r\n    This query demonstrates how an LLM, by itself, might not have access to information or functions designed to address specialized queries. In this example, the default OpenAI model provides a vague response, although the agent does know the current date by using its internal `get_current_date` function.\r\n```",
        "```suggestion\r\nAt this point, you can open the **Playground** and ask about the weather in your current location to test the IP geolocation tool.\r\nHowever, the IP geolocation MCP server is most useful in an application where you or your users want to ask about the weather from different places around the world.\r\n\r\nIn the last part of this tutorial, you'll learn how to use the Langflow API to run a flow in a script.\r\n```",
        "```suggestion\r\n\r\n    Given the example prompt, the LLM would respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.\r\n    \r\n    The **Playground** prints the agent's chain of thought as it selects tools to use and interacts with functionality provided by those tools.\r\n    For example, the agent can use the **Directory** component's `as_dataframe` tool to retrieve a [DataFrame](/concepts-objects#dataframe-object), and the **Web search** component's `perform_search` tool to find links to related items.\r\n```",
        "```suggestion\r\n   The default model is OpenAI's `text-embedding-3-small`, which is a balanced model, based on [OpenAI's recommendations](https://platform.openai.com/docs/guides/embeddings#embedding-models).\r\n```\r\n\r\nFuture proofing by avoiding references to cost and performance. Generally describing it as \"balanced\" is sufficient."
      ],
      "langflow-standardize-naming-patterns-consistently": [
        "I have the same question - Are these placeholders or variables on this page?",
        "```suggestion\r\n## Component menus\r\n```\r\n\r\nSince we have components menu + no-name component menu, we should either make up a separate name for the secondary menu (component header?) or rename this section to \"Component menus\" (to cover both menus)."
      ],
      "langflow-use-fake-sample-data": [
        "This goes to a real person's profile... we should probably remove the name, phone, email, linkedin from the sample data."
      ],
      "langflow-consolidate-related-information": [
        "This is confusing because make run_cli isn't in this section. Make run_cli is introduced on line 43.\r\n\r\nIts not clear whether \"Run langflow from source\" and \"set up langflow development environment\" are mutually exclusive or sequential.",
        "TBH, I would do what I did for the [Agent and MCP components](https://d5rxiv0do0q3v.cloudfront.net/langflow-drafts/docs-knowledge-base-feature/components-agents): Put everything on the /knowledge page, and then just point to that page from here. Then the user doesn't have to pop between two pages to read how to use it in a flow and learn about the parameters.\r\n\r\n```suggestion\r\nFor more information, see [Manage knowledge bases](/knowledge).\r\n```\r\n\r\nMove line 198 to 219 to the /knowledge page.\r\n",
        "Same comment as the previous component: I would move most of the info to the /knowledge page, and then point to that page from here.",
        "This page is hard to follow. You have a demo in the first two sections, and then sections for all the configuration options and output.\r\n\r\nInstead, I would recommend this structure:\r\n\r\n```\r\n## Use knowledge bases in flows\r\n\r\n<!-- One contiguous tutorial using both components -->\r\n\r\n## Knowledge Ingestion component\r\n\r\n### Knowledge Ingestion parameters\r\n\r\n## Knowledge Retrieval component\r\n\r\n### Knowledge Retrieval parameters\r\n\r\n## Knowledge base storage location\r\n```",
        "I started to make suggestions, but it might be faster for me to edit directly.\r\n\r\nWhat are the component names?",
        "```suggestion\r\n## Use knowledge bases in flows\r\n\r\nTo use knowledge bases in flows, you use the **Knowledge Ingestion** component to create and write to knowledge bases, and then you use the **Knowledge Retrieval** component to read from your knowledge bases.\r\n```\r\n\r\nSince components-data.mdx just points back to here, there's no value linking to it.",
        "This page is very long. I would consider breaking it apart:\r\n\r\n* Deployment architecture (Intro, Langflow deployment, Environment isolation)\r\n* Resource requirements (Scaling resources)\r\n* Best practices for Langflow K8s deployments (Rest of the page - However, I wonder if these are generic enough that they could just be \"Langflow deployment best practices\" at the top level of the Deploy section? The \"Monitoring recommendations\" sounds like it could be in the observability section too.)",
        "Instead of repeating, I think it would be better to just have the CLI API key instructions on one page.\r\n\r\nChoose either this page or the API key page as the source of truth for these instructions. Then, on the other page, replace the instructions with a link to the source of truth.",
        "This page has 2 links to the same page. I would probably just use the same link for both:\r\n\r\n```\r\n[API keys and authentication](/api-keys-and-authentication)\r\n```\r\n\r\nAnd on line 37, I would specify which API keys it stores (langflow or any \"credential\" global variable??)."
      ],
      "langflow-database-configuration-clarity": [
        "~~~suggestion\r\n5. To verify the configuration, create any flow using the Langflow UI or API, and then query your database to confirm new tables and activity. The content of the flow doesn't matter; you only need to confirm that the flow is stored in your PostgreSQL database.\r\n    \r\n    * Query the database container:\r\n    \r\n        ```\r\n        docker exec -it <postgres-container> psql -U langflow -d langflow\r\n        ```\r\n    \r\n    * Use SQL:\r\n    \r\n        ```\r\n        SELECT * FROM pg_stat_activity WHERE datname = 'langflow';\r\n        ```\r\n~~~",
        "```suggestion\r\n- Create a [PostgreSQL](https://www.pgadmin.org/download/) database\r\n```",
        "```suggestion\r\nThis example populates the values in `docker-compose.yml` with values from your Langflow `.env` file.\r\n```",
        "does this mean you dont need message history component with agent? or you should only use message history with agent if you want to connect external storage or use more fine grained memory settings?"
      ],
      "langflow-api-authentication-requirements": [
        "```suggestion\r\nAs of Langflow version 1.5, all API requests require authentication with a Langflow API key, even if `AUTO_LOGIN` is set to `True`.\r\n```",
        "Since this is the quickstart, I would just expose these steps directly.\r\n\r\n~~~suggestion\r\n\r\n## Create a Langflow API key\r\n\r\nA [Langflow API key](/configuration-api-keys) is a user-specific token you can use with Langflow.\r\n\r\nTo create a Langflow API key, do the following:\r\n\r\n1. In Langflow, click your user icon, and then select **Settings**.\r\n2. Click **Langflow API Keys**, and then click **Add New**.\r\n3. Name your key, and then click **Create API Key**.\r\n4. Copy the API key and store it in a secure location.\r\n5. Include your `LANGFLOW_API_KEY` in requests like this:\r\n    ```text\r\n    curl --request POST \\\r\n     --url 'http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID' \\\r\n     --header 'Content-Type: application/json' \\\r\n     --header 'x-api-key: LANGFLOW_API_KEY' \\\r\n     --data '{\r\n       \"output_type\": \"chat\",\r\n       \"input_type\": \"chat\",\r\n       \"input_value\": \"Hello\"\r\n     }'\r\n    ```\r\n6. Alternatively, the API pane's code snippets include a script to detect your local `LANGFLOW_API_KEY`.\r\nSet this variable in your terminal so you can copy and paste the commands.\r\n    ```bash\r\n    export LANGFLOW_API_KEY=\"sk...\"\r\n    ```\r\n\r\n~~~",
        "```suggestion\r\n  -H \"Content-Type: application/json\" \\\r\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\r\n```",
        "```suggestion\r\n   curl -X POST \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\r\n   -H \"Content-Type: application/json\" \\\r\n   -H \"x-api-key: $LANGFLOW_API_KEY\" \\\r\n```\r\n\r\nAre these placeholders or variables?",
        "```suggestion\r\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\r\n```",
        "```suggestion\r\n    curl -X POST \"http://localhost:7860/api/v1/webhook/$FLOW_ID\" \\\r\n        -H \"Content-Type: application/json\" \\\r\n        -H \"x-api-key: $LANGFLOW_API_KEY\" \\\r\n        -d '{\"id\": \"12345\", \"name\": \"alex\", \"email\": \"alex@email.com\"}'\r\n```",
        "```suggestion\r\nIn Langflow versions 1.5 and later, most API requests require a Langflow API key, even when `AUTO_LOGIN=true`.\r\n\r\nThe only exceptions are the MCP endpoints: `/v1/mcp`, `/v1/mcp-projects`, and `/v2/mcp`.\r\nThese endpoints don't require authentication, regardless of the `AUTO_LOGIN` setting.\r\n\r\n<details>\r\n```",
        "Please add the auth header to all 3 endpoints here: https://d5rxiv0do0q3v.cloudfront.net/langflow-drafts/docs-add-api-auth-requirement/api-reference-api-examples#try-some-langflow-api-requests (Get version, Get config, Get all)",
        "```suggestion\r\nThe API key has the same permissions and access as you do when you launch Langflow. This means your API key can only access your own flows, components, and data. You cannot access other users' resources with your own Langflow API keys.\r\n```",
        "```suggestion\r\n```",
        "```suggestion\r\n\r\nCode snippets generated in the **API access** pane automatically use your local Langflow API key, if one exists.\r\n```\r\n\r\nHow does it know which one to use if you have multiple Langflow API keys in your Settings?",
        "```suggestion\r\n    The code snippets automatically include the `LANGFLOW_SERVER_ADDRESS` and `FLOW_ID` values for the flow, and a script to include your `LANGFLOW_API_KEY` if you've set it as a variable.\r\n```\r\n\r\nI might have misunderstood this the first time - Do the code snippets not just include x-api-key as a `header`? \r\n\r\nDo you mean that it has an extra script portion to fetch a Langflow API Key from the Langflow server?",
        "I just realized there are some instances of `x-api-key` outside of the curl examples that need to edited or deleted in this PR:\r\n\r\n* webhook.md ~line 29\r\n* api-flows-run.md in \"Run endpoint headers\" section: This section also says \"accept\" header is required, but this header isn't included in the auto-generated code snippets. (asked about this in slack too)\r\n* api-build.md ~line 95 & 96: Same concerns as with api-flows-run.md regarding x-api-key and accepts headers.\r\n\r\nAlso, mcp-server.md includes `x-api-key` in various places, but this PR says those endpoints don't require auth?\r\n\r\n",
        "accept is optional (confirmed in Slack)."
      ],
      "langflow-precise-workflow-conditions": [
        "How does it know if it's docs only?",
        "@jordanrfrazier idk if CodeRabbit is hallucinating, but we do want to be sure this bypass only happens if the changes are exclusively in `langflow/docs`. "
      ],
      "langflow-api-documentation-completeness": [
        "➕  I think it depends on your composio api key.",
        "Also, the description of the Composio Tools component needs to specify that it _only_ outputs `Tool`, and it can _only_ be used with a Tools input (which is only on the main Agent component and a few other less-common agent components).",
        "```suggestion\r\nComposio components in Langflow primarily serve as [tools](https://app.composio.dev/) for your **Agent** components.\r\n\r\nIndividual services like Gmail, GitHub, and Slack are also available as individual components.\r\nYou can use these components for non-agentic actions in your flows, or you can enable **Tool Mode** to use them as specialized tools for **Agent** components.\r\n```",
        "```suggestion\r\n\r\n#### Advancing parsing limitations\r\n\r\nAdvanced parsing mode has the following restrictions:\r\n\r\n* **Only one file**: Advanced parsing mode processes only one file.\r\nIf you select multiple files, the **File** component processes the first file only, ignoring any additional files.\r\n\r\nTo process multiple files with advanced parsing, pass each file to a separate **File** components, or use the dedicated [**Docling** components](/integrations-docling).\r\n\r\n* **No tabular file types**:  Advanced parsing mode isn't available for tabular file types like `.csv`, `.xlsx`, and `.parquet`.\r\nAdvanced parsing is designed for document processing, such as extracting text from PDFs.\r\nFor structured data analysis, use the [**Parser** component](/components-processing#parser).\r\n```",
        "Consider explicitly mentioning that it's a Langflow API endpoint hosted on a FastAPI server.\r\n\r\nalso consider linking to the reference page: https://docs.langflow.org/api-flows-run\r\n(And, you may want to link from /api-flows-run to this page as well. Possibly as a \"See also\".)",
        "```suggestion\r\n4. Close the **Input Schema** pane to return to the **API access** pane.\r\nThe payload in each code snippet now includes `tweaks`, your **File** component's ID, and the `path` key that you enabled in **Input Schema**: \r\n\r\n    ```json\r\n    \"tweaks\": {\r\n\t    \"File-qYD5w\": {\r\n\t\t    \"path\": []\r\n\t    }\r\n\t}\r\n    ```\r\n\r\n5. When you run this flow programmatically, your script must upload a file to Langflow file management, and then pass the returned `file_path` to the `path` tweak in the `/run` request:\r\n\r\n    ```json\r\n    \"tweaks\": {\r\n        \"FILE_COMPONENT_ID\": {\r\n            \"path\": [ \"file_path\" ]\r\n        }\r\n    }\r\n    ```\r\n\r\n    For a complete example see [Create a chatbot that can ingest files](/chat-with-files) and [Files endpoints](/api-files).\r\n\r\n    If you want to upload multiple files, you can pass multiple `file_path` values in the `path` array, such as `[ \"path1\", \"path2\" ]`.\r\n```",
        "As a nudge, I would include a couple links here:\r\n```suggestion\r\n\r\nFor more information about forming Langflow API requests, see [Get started with the Langflow API](/api-reference-api-examples) and [Trigger flows with the Langflow API](/concepts-publish).\r\n\r\n```\r\n\r\n"
      ],
      "langflow-configuration-documentation-clarity": [
        "```suggestion\r\nLangflow Desktop for macOS cannot automatically use variables set in your terminal, such as those in`.zshrc` or `.bash_profile`, when launched from the macOS GUI.\r\n```",
        "```suggestion\r\nTo make environment variables available to GUI apps on macOS, you need to use `launchctl` with a `plist` file:\r\n```",
        "```suggestion\r\nThis example sets the `LANGFLOW_CONFIG_DIR` environment variable for all GUI apps launched from the macOS GUI.\r\n```",
        "```suggestion\r\n4. Load the file with `launchctl`:\r\n```",
        "```suggestion\r\nLangflow Desktop for Windows cannot automatically use variables set in your terminal, such as those defined with `set` in `cmd` or `$env:VAR=...` in PowerShell, when launched from the Windows GUI.\r\n```",
        "```suggestion\r\nTo make environment variables available to the Langflow Desktop app, you must set them at the user or system level using the **System Properties** interface or the Terminal.\r\n```\r\n\r\nWindows Terminal app can do both cmd.exe and PowerShell.",
        "This should be a step?\r\n\r\n```\r\n7. Launch or restart Langflow Desktop to apply the environment variables.\r\n```",
        "```suggestion\r\n```",
        "This is the only one you have marked required but most of the others aren't marked as either required or optional.",
        "Langflow API key should probably be a prerequisite tbh.",
        "```suggestion\r\n    * `FILE_COMPONENT_ID`: The UUID of the File component in your flow, such as `File-KZP68`. To find the component ID, open your flow in Langflow, click the File component, and then click **Controls**.\r\n```",
        "```suggestion\r\nTo add dependencies to Langflow Desktop, add an entry for the package to the application's `requirements.txt` file:\r\n```",
        "```suggestion\r\nAdd the dependency and version to `requirements.txt` on separate lines in the format `PACKAGE==VERSION`, such as `docling==2.40.0`.\r\n```",
        "```suggestion\r\n\r\nIf you need to change or uninstall custom dependencies. edit the `requirements.txt` file, and then restart Langflow Desktop.\r\n\r\n```",
        "You might want to compare this section with `## Install optional dependency groups` on line 8. The two sections seem to be related or redundant. At minimum, they both use Langflow OSS.",
        "```suggestion\r\nOn macOS, uninstalling Langflow Desktop deletes the `.app` file but doesn't delete files in `~/.langflow`, which includes files generated during usage like cache and settings.\r\n\r\nIf you reinstall Langflow Desktop, it starts with the existing data from the previous installation.\r\n```",
        "```suggestion\r\nTo fully remove a Langflow Desktop macOS installation, you must also delete `~/.langflow`:\r\n```"
      ],
      "langflow-document-log-level-options": [
        "```suggestion\r\n| `LANGFLOW_LOG_FILE` | String | `logs/langflow.log` | Path to the log file. If this option isn't set, logs are written to stdout. |\r\n```\r\n\r\nPer GDDSG, use contractions for negatives because it is less likely that the `not` will be overlooked.",
        "add the list of log level options"
      ],
      "langflow-ai-model-chunk-sizing": [
        "```suggestion\r\nThe **Split Text** component doesn't always enforce the exact chunk size you set, and individual chunks may exceed your specified limit.\r\n```",
        "```suggestion\r\nIf you encounter tokenization errors, modify your text splitting strategy by reducing the chunk size, changing the overlap length, or using a more common separator.\r\nThen, test your configuration by running the flow and inspecting the component's output.\r\n```\r\n\r\nBased on the definition of the `chunk_size` parameter, chunk size is a minimum target.\r\nThey may need to change the other settings to achieve consistent chunk sizes. For example, if the separator is rare, the component wont subdivide the chunks, even if they well exceed the chunk size.",
        "```suggestion\r\nBe aware of your embedding model's chunk size limit.\r\nTokenization errors can occur if your text chunks are too large. \r\nFor more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).\r\n```\r\n\r\nTo minimize maintenance, only put the bare minimum info here.",
        "```suggestion\r\n| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. Make sure the chunk size is compatible with your embedding model. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |\r\n```"
      ],
      "langflow-document-connection-parameters-clearly": [
        "might want to add that this creates a key pair and you are only copying the public key.",
        "Add the list of placeholders after the command.\r\n\r\n~~~\r\n5. To connect to your server with SSH, run the following command:\r\n\r\n    ```bash\r\n    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS\r\n    ```\r\n\r\n    Replace the following:\r\n\r\n    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server\r\n    * `SERVER_IP_ADDRESS`: Your server's IP address\r\n\r\n~~~",
        "suggestion for lines 114-132:\r\n\r\n~~~\r\nThis address uses HTTP because HTTPS isn't enabled yet.\r\n\r\n8. Recommended: Enable HTTPS:\r\n\r\n    1. Modify your domain's A record to point to your server's IP address. For example:\r\n\r\n        ```\r\n        Type: A\r\n        Name: langflow\r\n        Value: 5.161.250.132  # Set to your server's IP address\r\n        ```\r\n\r\n    2. Stop your server.\r\n    3. Modify your Caddyfile to include port `443` so Caddy can forward both HTTP (port 80) and HTTPS (port 443) requests to the Langflow service:\r\n\r\n        ```\r\n        :80, :443 {\r\n            reverse_proxy langflow:7860\r\n        }\r\n        ```\r\n\r\n    4. Start your server.\r\n\r\n        When users visit your domain, Caddy recognizes the incoming traffic and automatically routes it to your server with a secure, encrypted connection.\r\n~~~"
      ],
      "langflow-consistent-formatting-standards": [
        "```suggestion\r\n3. Optional: Install pre-commit hooks to help keep your changes clean and well-formatted. `make init` installs pre-commit hooks automatically.\r\n```",
        "```suggestion\r\n  -H \"Content-Type: application/json\" \\\r\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\r\n```\r\n\r\nWe need to standardize on \" or ' for the headers for all examples",
        "```suggestion\r\n4. Optional: Install pre-commit hooks to help keep your changes clean and well-formatted. `make init` runs this automatically.\r\n```\r\n\r\nhttps://developers.google.com/style/procedures#optional-steps"
      ],
      "langflow-environment-variable-documentation": [
        "```suggestion\r\n  Increased the default maximum size for file uploads from 100 MB to 1024 MB.\r\n  You can configure this with <!-- add link to environment variable page and name of variable -->\r\n```",
        "I believe COMPOSIO_API_KEY is also a default environment variable that Langflow can auto-detect. (see configuration-global-varaibles.mdx).\r\n\r\nYou could add that to the \"Authentication for Composio components\" section.",
        "```suggestion\r\n3. System environment variables are used only if not set elsewhere.\r\n\r\n    When running a Langflow Docker image, the `-e` flag sets system environment variables.\r\n```"
      ],
      "langflow-document-security-implications-clearly": [
        "```suggestion\r\n    The initial output contains the JSON response object from the file upload endpoint, with the internal path where Langflow stores the file.\r\n\r\n    The LLM then retrieves the file and uses it as context to produce a response. In this tutorial, the text input instructed the LLM to evaluate the suitability of a sample resume.\r\n```",
        "You might want to add something more about file storage, since there could potentially be privacy concerns. For example, loading medical records or something.\r\n\r\nAre there protections in place for sensitive files? Who has access to the files? And how can I delete them (can my script include a step to discard files at the end of the conversation)?",
        "```suggestion\r\nAn API key represents the user who created it. If you create a key as a superuser, then that key will have superuser privileges.\r\nAnyone who has that key can authorize superuser actions through the Langflow API, including user management and flow management.\r\n```",
        "```suggestion\r\n:::warning\r\n```\r\n\r\nI would increase this to warning because it potentially exposes the literal key."
      ],
      "langflow-observability-documentation-structure": [
        "```suggestion\r\n### Database failure\r\n\r\n* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.\r\n* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.\r\n\r\n### File system issues\r\n\r\n* **Impact**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\r\n* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.\r\n\r\n### Instance failures\r\n\r\n* **Impact**: A single instance failure can disrupt service if not replicated.\r\n* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.\r\n\r\n### Network and dependency failures\r\n```",
        "```suggestion\r\n### Database health\r\n\r\n* Monitor availability, query performance, and resource usage (CPU, memory, disk).\r\n* Use tools like pgAdmin or cloud-native monitoring for PostgreSQL.\r\n\r\n### Application logs\r\n\r\n* Collect and analyze logs for errors, warnings, and flow execution issues.\r\n* Centralize logs using tools like ELK Stack or Fluentd.\r\n\r\n### Resource usage\r\n\r\n* Track CPU, memory, and disk usage of Langflow instances.\r\n* Use Prometheus and Grafana for real-time monitoring in Kubernetes.\r\n\r\n### API performance\r\n\r\n* Monitor response times, error rates, and request throughput.\r\n* Set alerts for high latency or error spikes.\r\n\r\n### Observability tools\r\n\r\n* Integrate with LangSmith or LangFuse for detailed flow tracing and metrics.\r\n* Use these tools to debug flow performance and optimize execution.\r\n\r\n### Example monitoring setup\r\n\r\n* Deploy Prometheus for metrics collection.\r\n```",
        "```suggestion\r\nLangflow telemetry collects data on flow runs, your environment, and component usage.\r\n\r\n### Run\r\n```"
      ],
      "langflow-api-endpoint-documentation": [
        "Several of these missing the $ and I'm not sure if that was intentional",
        "```suggestion\r\n\r\n    This example assumes that you use the default Langflow listening address at `http://localhost:7860`. If you have a different listening address, you must modify this command accordingly.\r\n\r\n```",
        "```suggestion\r\n\r\n    The `Forwarding` line prints the forwarding address for your Langflow server:\r\n\r\n    ```\r\n    Forwarding https://94b1-76-64-171-14.ngrok-free.app -> http://localhost:7860\r\n    ```\r\n\r\n    The forwarding address acts as a reverse proxy for your Langflow server, and ngrok forwards your local traffic to this domain.\r\n\r\n4. To verify that your Langflow server is publicly available, navigate to the forwarding address URL, such as `https://94b1-76-64-171-14.ngrok-free.app`.\r\n```"
      ],
      "langflow-secure-credential-management": [
        "```suggestion\r\n### Container security\r\n\r\n* Enable `readOnlyRootFilesystem: true` in runtime containers to prevent unauthorized modifications.\r\n* Only disable if necessary and with compensating controls.\r\n\r\n### Secrets management\r\n\r\n* Store sensitive data like API keys in Kubernetes secrets or external secret managers.\r\n* Avoid embedding secrets in flow JSON files.\r\n\r\n### Authentication and authorization\r\n\r\n* Implement strong authentication for the IDE UI and runtime API like OAuth or API tokens.\r\n* Enforce role-based access control to limit user permissions.\r\n\r\n### Data privacy\r\n\r\n* Ensure compliance with regulations like GDPR if handling personal data.\r\n* Encrypt sensitive data at rest and in transit.\r\n\r\n### Encryption\r\n\r\n* Use HTTPS for all communications to secure data in transit.\r\n* Configure TLS for PostgreSQL connections.\r\n\r\n### Additional security measures\r\n\r\n```",
        "```suggestion\r\n* Monitor for suspicious activity using intrusion detection systems.\r\n\r\n## See also\r\n\r\n* [Database guide for enterprise DBAs](/deployment-enterprise-database)\r\n```",
        "```suggestion\r\n        LANGFLOW_SECRET_KEY=dBuu...2kM2_fb\r\n```\r\n\r\nIt is a good idea to truncate or omit actual credential examples. You can use a templated format like we do for Astra (`AstraCS:...`) or just cut the middle part out."
      ],
      "langflow-organize-documentation-content": [
        "I might be confused, but step 3 seems like it's missing some instructions in the current format.\r\n\r\nI think you are installing the server itself in your environment, and then connecting your Langflow deployment to the MCP server? If so:\r\n\r\n* Make \"install the server locally\" the first step in this part of the tutorial. Focus only on installing the server itself.\r\n* Follow with the steps to add the MCP Tools component and connect to the server.\r\n\r\n\r\nSeparately, this sentence is confusing: `There are multiple ways to install MCP servers, which are covered in the [MCP Tools](/mcp-client) component page.` I am confused by the difference between connecting to an MCP server that is running somewhere vs installing/hosting an MCP server on your machine (or a remote machine) before connecting to it. \r\n\r\nDo you always have to install the server somewhere first? For example, in the [Astra MCP Server instructions](https://docs.datastax.com/en/astra-db-serverless/integrations/model-context-protocol.html#add-an-astra-db-mcp-server-to-your-mcp-client), the client basically runs the server as a dependency (I think). (Edit: Oh this is explained on line 149 about npx.)",
        "```suggestion\r\n\r\nThis page summarizes significant changes to Langflow in each release.\r\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases/latest).\r\n\r\nDue to strict SemVer requirements, Langflow Desktop can have different patch versions than the core Langflow OSS Python package, but the major and minor versions are aligned.\r\n```\r\n\r\nI know coderabbit suggested this, but this doesn't need to be a note. It's just helpful information about the patch version.",
        "```suggestion\r\n* [Make](https://www.gnu.org/software/make/#documentation)\r\n\r\n:::tip Windows\r\nFor Windows installations, you don't need need Make, and you can find [Windows scripts](https://github.com/langflow-ai/langflow/tree/main/scripts/windows) in the Langflow repository.\r\n:::\r\n```\r\n\r\nImportant is a little strong here.\r\nAnother alternative would be to use tabs (One tab for Linux and macOS, one tab for Windows) with the prereqs/info for each OS on each tab.",
        "I think it would be clearer to separate the version-specific info on tabs, or put the \"pre-1.5\" info in a `<details>`.\r\n\r\nFor example, how I show the Java driver dependency examples for different versions:\r\nhttps://docs.datastax.com/en/datastax-drivers/compatibility/java-drivers.html#java-driver-ownership ",
        "Pls wrap this result in details. Only one key is relevant, so this is kinda eating real estate.",
        "```suggestion\r\nFor more information, see the [CrewAI Agents documentation](https://docs.crewai.com/core-concepts/Agents/).\r\n```",
        "Pls differentiate the CrewAI links throughout the page. Screen readers can announce all links on the page by link text alone, so the user cannot distinguish any difference in the CrewAI links if they are all announced as \"CrewAI documentation\". \r\n\r\nBoth screen reader and non-screen reader users will assume the links go to the same place because they all have the same text.",
        "Optional: Group all CrewAI components together:\r\n\r\n```\r\n### CrewAI bundles\r\n\r\nThe following bundles support integrations between Langflow and CrewAI.\r\n\r\n#### CrewAI Agent\r\n\r\n#### Hierarchical crew\r\n\r\n#### Sequential crew\r\n\r\n...\r\n```",
        "I would suggest leaving some connection to the Bundles page. Maybe as related link or just a small section. That way the user knows they are still agent components, just specialized ones (I think I am understanding that right?).\r\n\r\n```\r\n## Agent bundles\r\n\r\nSpecialized variations of the **Agent** component are available as provider-specific **Bundles**. For more information, see (Bundles)[/components-bundle-components].\r\n```",
        "```suggestion\r\nBundled components are based on standard Langflow functionality, so you add them to your flows and configure them in much the same way as the standard components.\r\nHowever, bundled components inherently use provider-specific functionality, commands, and configurations that aren't described in this documentation.\r\nFor details about the provider-specific aspects of a bundled component, see the component provider's documentation.\r\n```",
        "Alternative suggestion:\r\n\r\nThe CSV, XML, SQL, and the OpenAI agent bundles are missing doc links. If there are no applicable docs links for these (or any other bundles), here is a different suggestion for this intro segment:\r\n\r\n```suggestion\r\nBundled components are based on standard Langflow functionality, so you add them to your flows and configure them in much the same way as the standard components.\r\nThis documentation summarizes each bundled component and its parameters.\r\nFor details about provider-specific aspects of bundled components, this documentation provides links to relevant component provider documentation.\r\n```",
        "The other two categories are \"____ bundles\" so just aligning them:\r\n\r\n```suggestion\r\n## Agent bundles\r\n```",
        "Some of my earlier comments are unresolved, so not sure whether you were still considering them or not."
      ]
    },
    "profile": {
      "location": "United States",
      "company": "DataStax",
      "blog": "",
      "site_admin": false,
      "followers": 3,
      "following": 2
    }
  },
  "markdalgleish": {
    "repos": [
      "remix-run/react-router"
    ],
    "entries": [
      {
        "slug": "react-router-avoid-redundant-computations",
        "title": "avoid redundant computations"
      },
      {
        "slug": "react-router-avoid-timing-dependent-tests",
        "title": "avoid timing-dependent tests"
      },
      {
        "slug": "react-router-configuration-consistency-standards",
        "title": "configuration consistency standards"
      },
      {
        "slug": "react-router-configure-build-tools-properly",
        "title": "configure build tools properly"
      },
      {
        "slug": "react-router-dependency-version-ranges",
        "title": "dependency version ranges"
      },
      {
        "slug": "react-router-document-configuration-rationale",
        "title": "Document configuration rationale"
      },
      {
        "slug": "react-router-documentation-clarity-standards",
        "title": "documentation clarity standards"
      },
      {
        "slug": "react-router-graceful-error-handling",
        "title": "graceful error handling"
      },
      {
        "slug": "react-router-http-protocol-compliance",
        "title": "HTTP protocol compliance"
      },
      {
        "slug": "react-router-implement-recursive-safeguards",
        "title": "Implement recursive safeguards"
      },
      {
        "slug": "react-router-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "react-router-organize-related-code-together",
        "title": "organize related code together"
      },
      {
        "slug": "react-router-prefer-explicit-readable-constructs",
        "title": "prefer explicit readable constructs"
      },
      {
        "slug": "react-router-provide-explicit-error-handling",
        "title": "Provide explicit error handling"
      },
      {
        "slug": "react-router-remove-obsolete-configuration-options",
        "title": "Remove obsolete configuration options"
      },
      {
        "slug": "react-router-simplify-configuration-setup",
        "title": "Simplify configuration setup"
      },
      {
        "slug": "react-router-typescript-configuration-setup",
        "title": "TypeScript configuration setup"
      },
      {
        "slug": "react-router-use-descriptive-semantic-names",
        "title": "Use descriptive semantic names"
      }
    ],
    "comments": {
      "react-router-typescript-configuration-setup": [
        "```suggestion\r\nAfter we decided not to pursue \"zero-effort typesafety\" (as described above), our TypeScript plugin was already a simple passthrough that kicked off typegen as a side-effect.\r\n```"
      ],
      "react-router-avoid-redundant-computations": [
        "While looking at imports and exports with fresh eyes, I also realised there's room for an early bail out here."
      ],
      "react-router-configure-build-tools-properly": [
        "Now that we have the exports field, we can point to the `dist` folder and remove the hard-coded `install.js` and `install.d.ts` files."
      ],
      "react-router-document-configuration-rationale": [
        "This is suppressing dependency warnings that only affect dev dependencies due to test code. Everything still works.",
        "This is optional, but I've opted to keep automatic `pre` and `post` script behaviour so that the repo doesn't change too much in the migration. We can revisit this later.",
        "This is needed so that `pnpm format` doesn't format the lock file. We ran into an issue in the Remix repo where CI got stuck in a loop deduping and formatting `pnpm-lock.yaml` since pnpm and Prettier didn't agree, so this ensures we don't hit that problem again."
      ],
      "react-router-remove-obsolete-configuration-options": [
        "We have a few loose ends like this lying around, so I think we can merge this as-is for now and do a broader cleanup as a separate pass."
      ],
      "react-router-documentation-clarity-standards": [
        "With Vite's RSC support currently being a plugin rather than being built-in, my original use of the word \"native\" doesn't quite apply.\r\n\r\n```suggestion\r\nReact Router provides a set of APIs for integrating with RSC-compatible bundlers, allowing you to leverage [Server Components][react-server-components-doc] and [Server Functions][react-server-functions-doc] in your React Router applications.\r\n```",
        "Nit:\r\n\r\n```suggestion\r\nThe [Route Module API][route-module] up until now has been a [Framework Mode][framework-mode] only feature. However, the `lazy` field of the RSC route config expects the same exports as the Route Module exports, unifying the APIs even further.\r\n```",
        "```suggestion\r\nServer Components can also be returned from your loaders and actions. In general, if you are using RSC to build your application, loaders are primarily useful for things like setting `status` codes or returning a `redirect`.\r\n```",
        "```suggestion\r\nDuring our experiments we realized that we could offload type-safe context to an external package. This would result in a simpler implementation within React Router and avoid the need to try to patch on type-safety to our existing `context` API which was designed as a quick escape hatch to cross the bridge from your server (i.e., `express` `req`/`res`) to the Remix handlers.\r\n```"
      ],
      "react-router-avoid-timing-dependent-tests": [
        "This test was flaky in Firefox due to the order of requests not being deterministic."
      ],
      "react-router-implement-recursive-safeguards": [
        "This code was already targeting export statements specifically, it just wasn't explicitly scoped in the code. Now that we have much more general tracing of dependencies above, we can scope this extra check much more aggressively."
      ],
      "react-router-provide-explicit-error-handling": [
        "Review feedback: We should have our own error for when the file is missing rather than letting it fail within Vite when trying to load it.",
        "We should error when the default export is missing entirely."
      ],
      "react-router-use-descriptive-semantic-names": [
        "Updated 👍 "
      ],
      "react-router-http-protocol-compliance": [
        "Why did this need to be changed?"
      ],
      "react-router-graceful-error-handling": [
        "Pairing with @pcattori, we've opted to go with this to keep things moving:\r\n\r\n```ts\r\nerror.stack = process.env.NODE_ENV === \"development\" ? val.stack : \"\";\r\n```\r\n\r\nDeduping feels like a larger task given the current structure so I've left that for now."
      ],
      "react-router-configuration-consistency-standards": [
        "Now that this error can also be avoided by introducing a custom entry.server.tsx/jsx, I've expanded the error message to make this clear, similar to the error message following this one.",
        "@pcattori Just calling out that since the `--config` flag impacts the project root directory (and hence where routes live etc.), I think it makes sense to support this in all CLI commands for consistency, even those that don't use the Vite config.",
        "Side note, but I think this raises something I might have missed when we introduced `react-router.config.ts`, which is that the `--config` flag probably should have been updated to point to _our_ config, and any custom Vite config path should have been configured there, or via a separate `--vite-config` flag.\r\n\r\nThis is part of my rationale for this change. If our `--config` flag pointed to `react-router.config.ts`, I would want to support it here too.",
        "Note that if a root directory was explicitly provided (e.g. `react-router build my/app/dir`) this will still take precedence. We only fall back to inferring the root when this isn't provided, so anyone relying on this should maintain existing behaviour."
      ],
      "react-router-dependency-version-ranges": [
        "I think this should be `^4.0.0` so `4.0.x` and `4.1.x` are also valid.",
        "This should be `^3.28.2 || ^4.0.0` so it's backwards compatible.",
        "This was bumped because v3 is the first major version of vite-node to officially support both Vite v5 and v6: https://github.com/vitest-dev/vitest/blob/main/packages/vite-node/package.json#L89. I think it's okay we're running a beta version because we're using vite-node in a very limited capacity to run config files and we don't expose any Vite internals to consumers. However, just to be sure, we run the routes.ts tests against both v5 and v6 of Vite."
      ],
      "react-router-prefer-explicit-readable-constructs": [
        "Thanks for catching this, I've pushed a refactor that cleans this up."
      ],
      "react-router-maintain-naming-consistency": [
        "FWIW I've just been leaning into calling it `routes.ts` and `app/routes.ts` everywhere to avoid this sort of framing which can be confusing. Instead we could make sure to always link to a page with more detail about how it's more complicated than that?"
      ],
      "react-router-simplify-configuration-setup": [
        "The `modulePaths` option as configured didn't work with pnpm's folder structure. A simpler way to solve the same problem is to simply tell Jest to transform everything.",
        "Good call. I've simplified the script now so that the version specifier is required.\r\n\r\nIn the current package structure, the `isSnapshotVersion` is still required because it's used to detect experimental/nightly releases and ensure that `@remix-run/router` also gets a version update."
      ],
      "react-router-organize-related-code-together": [
        "This would have been force of habit for me, that typically when testing utils like this I prefer having the test right next to the implementation. Do you feel strongly about it always being in a `__tests__` directory?"
      ]
    },
    "profile": {
      "location": "Melbourne, Australia",
      "company": "@remix-run",
      "blog": "markdalgleish.com",
      "twitter_username": "markdalgleish",
      "site_admin": false,
      "followers": 3231,
      "following": 80
    }
  },
  "treo": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-centralize-dependency-management",
        "title": "Centralize dependency management"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-clear-descriptive-identifiers",
        "title": "Clear descriptive identifiers"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-ai-implementation-references",
        "title": "Document AI implementation references"
      },
      {
        "slug": "deeplearning4j-document-api-completely",
        "title": "Document API completely"
      },
      {
        "slug": "deeplearning4j-eliminate-redundant-code",
        "title": "Eliminate redundant code"
      },
      {
        "slug": "deeplearning4j-keep-configurations-current",
        "title": "Keep configurations current"
      },
      {
        "slug": "deeplearning4j-minimize-object-allocations",
        "title": "Minimize object allocations"
      },
      {
        "slug": "deeplearning4j-modular-adaptive-configurations",
        "title": "Modular adaptive configurations"
      },
      {
        "slug": "deeplearning4j-numerical-stability-practices",
        "title": "Numerical stability practices"
      },
      {
        "slug": "deeplearning4j-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "deeplearning4j-prevent-memory-leaks",
        "title": "Prevent memory leaks"
      },
      {
        "slug": "deeplearning4j-remove-debugging-artifacts",
        "title": "Remove debugging artifacts"
      },
      {
        "slug": "deeplearning4j-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-clear-descriptive-identifiers": [
        "I guess a better name here would be just `releaseGilAutomatically`."
      ],
      "deeplearning4j-prevent-memory-leaks": [
        "Missing a delete for this one?"
      ],
      "deeplearning4j-document-ai-implementation-references": [
        "The 1e9 is used by the tensor2tensor package as well. Bert on the other hand uses just 1e4, while GPT-2 uses 1e10."
      ],
      "deeplearning4j-use-appropriate-logging-levels": [
        "printf outside of an error condition.",
        "shouldn't this be an sd_debug? Same applies to other usages of sd_printf and printIndexedBuffer in this file",
        "is this supposed to be all `sd_printf`? Or did you want `sd_debug` instead?"
      ],
      "deeplearning4j-modular-adaptive-configurations": [
        "I'd probably split this some more into a base command and extension parameters. \r\n\r\nSomething like\r\n```bash\r\nif [ \"${HELPER}\" != '' ] && [ \"${EXTENSION}\" != '' ]; then\r\n    mvn_ext=\"-Djavacpp.platform.extension=-${{ matrix.helper }}-${{ matrix.extension }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nelif [ \"${HELPER}\" != '' ]; then\r\n    mvn_ext=\"-Djavacpp.platform.extension=-${{ matrix.helper }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nelse\r\n    mvn_ext=\"-Djavacpp.platform.extension=${{ matrix.extension }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nfi\r\n\r\ncommand=\"mvn -Possrh ${mvn_ext} -Dlibnd4j.buildThreads=${{ github.event.inputs.buildThreads }}  -Djavacpp.platform=linux-x86_64 -Dlibnd4j.chip=cuda --also-make -Pcuda clean --batch-mode package deploy -DskipTests\"\r\n```\r\n\r\nAnd having refactored it like that, I wonder, why do you even specify those values in the cases where they aren't defined?",
        "If you want to make the hack hackier, you can replace the version this with\r\n```\r\nsudo cp /usr/lib/gcc/x86_64-linux-gnu/`gcc --version | head -n 1 | grep -o '[^ ]*$'` /usr/lib\r\n```"
      ],
      "deeplearning4j-descriptive-error-context": [
        "While \"Failed execution\" is a better message than \"Boom\", I wonder if we can't have something more descriptive here?\r\n\r\nAlso, Do we need to both print and throw an exception? "
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Does it make sense to keep this at info log level? or should this maybe move to the debug log level, as you did with the logging in the other branch?"
      ],
      "deeplearning4j-document-api-completely": [
        "It may make sense to put this as Javadoc on the individual Enums? ",
        "Again, a comment why this is empty would be useful."
      ],
      "deeplearning4j-keep-configurations-current": [
        "Have you cleared this with eclipse EMO? I remember that they needed to clear any additional dependency or vendoring.",
        "We should probably update this to match the suggested maven version for the rest of DL4J - Or remove the maven wrapper entirely.",
        "Got an answer to that question?"
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "Do we need the `null` case anywhere? Or wouldn't it make more sense to handle `null` in the `outputVariables()` function, so that everything downstream doesn't need to have special case handling?"
      ],
      "deeplearning4j-eliminate-redundant-code": [
        "Looks like a PR breakout bug? The constant is defined twice. ",
        "printf outside of error condition. ",
        "Any reason why this files has soo many licence headers?"
      ],
      "deeplearning4j-preserve-api-compatibility": [
        "When adding new arguments, I think it makes sense to make them optional in order to keep backwards compatibility with existing code.",
        "This will probably break some backwards compatibility - at least for people that are using Op objects directly.\r\n\r\nI think we can probably accept that breakage, as most people will be using the factory methods and for them it should be transparent."
      ],
      "deeplearning4j-minimize-object-allocations": [
        "good idea, I'll change it accordingly in the createBias method as well then."
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "Is 0.01 really a good default value for this? Typical epsilon values are 1e-6, 1e-7 or even 1e-12 for double. "
      ],
      "deeplearning4j-centralize-dependency-management": [
        "If possible, let's not use rc versions.",
        "It would probably make sense to use the property defined above in all of those jmh dependencies "
      ],
      "deeplearning4j-numerical-stability-practices": [
        "The paper authors have found that a smaller default epsilon works better:\r\n\r\n> Epsilon in AdaBelief is different from Adam (typically eps_adabelief = eps_adam*eps_adam)\r\n> ( eps of Adam in Tensorflow is 1e-7, in PyTorch is 1e-8, need to consider this when use AdaBelief in Tensorflow)\r\n\r\nSo I guess we might want to use either 1e-14 or 1e-16 here. ",
        "maybe also check that 0 <= maskTokenProb + randomTokenProb <= 1?",
        "Should this even happen? Calculating with a zero gradient, should basically be \"don't do any change at all\" and therefore we shouldn't be performing backprop at all in that case?",
        "got it. Maybe add the explanation to the comment on top?"
      ],
      "deeplearning4j-configurable-resource-locations": [
        "We've had issues with people creating uberjars and not packaging resources previously. So having something that is dependent on loading a resource may introduce new problems. \r\n\r\nIn particular, Spring Boot has a special way of class loading and resource loading. I can imagine that it may break there."
      ],
      "deeplearning4j-clean-up-your-code": [
        "left over debug prints?",
        "I guess this print is a left over from debugging?",
        "Please don't leave any commented out code around.",
        "Please don't leave commented out code around.",
        "Please don't leave any commented out code to hang around",
        "please don't leave code commented out like that.",
        "let's not have commented out code. If this is just temporary, put at least a TODO there, so it is obvious that this is still work in progress. Otherwise it is easy to miss this when you do the cleanup. ",
        "Please don't leave any commented out code behind.",
        "Can we turn this magic number into a constant? "
      ],
      "deeplearning4j-remove-debugging-artifacts": [
        "Still contains a print.",
        "left over debug printing?",
        "printf outside of error condition.",
        "printf outside of error condition.",
        "printf outside of error condition.",
        "Let's not have commented out code like that. And in particular not have a for loop doing nothing. ",
        "Unconditional printf. I guess a leftover from debugging?",
        "Please don't leave any commented out code around. Either remove it or keep it, but just commenting things out without any indication for why that is, shouldn't make it into a merged PR.",
        "Please don't leave any commented out code around"
      ]
    },
    "profile": {
      "location": "Mainz, Germany",
      "company": "@XpressAI",
      "blog": "https://www.dubs.tech",
      "site_admin": false,
      "followers": 99,
      "following": 4
    }
  },
  "flotwig": {
    "repos": [
      "cypress-io/cypress"
    ],
    "entries": [
      {
        "slug": "cypress-clear-security-error-messages",
        "title": "Clear security error messages"
      },
      {
        "slug": "cypress-consistent-descriptive-naming",
        "title": "consistent descriptive naming"
      },
      {
        "slug": "cypress-consistent-formatting-preferences",
        "title": "Consistent formatting preferences"
      },
      {
        "slug": "cypress-document-non-obvious-code",
        "title": "Document non-obvious code"
      },
      {
        "slug": "cypress-document-security-policy-trade-offs",
        "title": "Document security policy trade-offs"
      },
      {
        "slug": "cypress-environment-variable-handling",
        "title": "Environment variable handling"
      },
      {
        "slug": "cypress-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "cypress-function-decomposition-clarity",
        "title": "function decomposition clarity"
      },
      {
        "slug": "cypress-informative-error-messages",
        "title": "Informative error messages"
      },
      {
        "slug": "cypress-meaningful-test-assertions",
        "title": "meaningful test assertions"
      },
      {
        "slug": "cypress-prefer-semantic-test-selectors",
        "title": "Prefer semantic test selectors"
      },
      {
        "slug": "cypress-prioritize-backward-compatibility",
        "title": "prioritize backward compatibility"
      },
      {
        "slug": "cypress-simplify-complex-expressions",
        "title": "Simplify complex expressions"
      },
      {
        "slug": "cypress-strengthen-test-assertions",
        "title": "Strengthen test assertions"
      },
      {
        "slug": "cypress-structured-debug-logging",
        "title": "structured debug logging"
      },
      {
        "slug": "cypress-thoughtful-error-handling",
        "title": "Thoughtful error handling"
      },
      {
        "slug": "cypress-use-precise-networking-terminology",
        "title": "Use precise networking terminology"
      }
    ],
    "comments": {
      "cypress-clear-security-error-messages": [
        "```suggestion\r\n      Cypress does not allow you to navigate to different origin within a single test.\r\n```\r\n\r\nAnd then, maybe below this, define what an \"origin\" is (scheme + host + port?)",
        "Yeah, I think that's fine, it's enough to give them a good hint at least."
      ],
      "cypress-structured-debug-logging": [
        "for readability: \r\n```suggestion\r\n      debug(\"plugins module does not exist %o\", { pluginsFile })\r\n```",
        "for readability:\r\n```suggestion\r\n        debug(\"checking if pluginsFile exists\", { pluginsFile, dirName: path.dirname(pluginsFile) })\r\n```",
        "can you try to consolidate these debug lines so we don't need a bunch of them scattered everywhere? we are logging out the same data across many modules already and i want to decrease duplication in debug logs as much as possible",
        "agreed, but you can accomplish the same thing with fewer debug lines by logging out branch conditions instead of logging out a different line for each branch, stuff like that... plus, pluginsFile is logged out prior to this point by other config code"
      ],
      "cypress-document-non-obvious-code": [
        "may be worth adding a comment? was non-obvious to me what this was doing\r\n```suggestion\r\n    // wait for the studio test file to be written to disk, then reload the test\r\n    return this.onTestFileChange(filePath).then(() => {\r\n```"
      ],
      "cypress-document-security-policy-trade-offs": [
        "The repro shared in #19697 uses `<meta http-equiv=\"Content-Security-Policy\" content=\"script-src 'self'\" />`, but that didn't actually recreate the issue when we tried it. What's with the change to `script-src 'unsafe-inline'`? Was this the actual repro for #19697 or something?"
      ],
      "cypress-environment-variable-validation": [
        "there needs to be a way to opt out of this, for example we use the `CYPRESS_CRASH_REPORTS` environment variable as an opt out here: https://github.com/cypress-io/cypress/blob/3726d7eb26f6489ef6ceb8c046de0f239f830e81/packages/server/lib/exception.js#L54-L64"
      ],
      "cypress-informative-error-messages": [
        "Thanks, I missed this when moving from `.filter(newFilter)` to `.filter = newFilter`."
      ],
      "cypress-simplify-complex-expressions": [
        "this nested if/else feels like an antipattern, maybe the logic for deciding `supportFile` should be in a function that is assigned to obj.supportFile here",
        "can this logic (and surrounding logic) be extracted into a separate function shared by `setSupportFileAndFolder` and `setPluginsFile`? it seems like it should be possible to clean this up"
      ],
      "cypress-prioritize-backward-compatibility": [
        "Why does `.wrap(null)` need to be added here?",
        "I pretty commonly use `cy.should()` just to retry functions that assert things, without a subject. Probably I'd do it less if I wasn't usually testing Cy internals, but still, I'm sure it happens in the wild too. \r\n\r\nAlso, I think `cy.should()` having a default subject of `null` or `undefined` makes logical sense.\r\n\r\nFor these reasons, I think I'd prefer # 2, since it avoids extra migration steps and it doesn't cause any logical inconsistency."
      ],
      "cypress-consistent-descriptive-naming": [
        "probably rename this since it's no longer a boolean\r\n\r\n```suggestion\r\n      writtenChunkCount++\r\n```"
      ],
      "cypress-prefer-semantic-test-selectors": [
        "Are this and `ChooseExternalEditorModal.cy.tsx`'s changes are to avoid flake unrelated to this PR?"
      ],
      "cypress-environment-variable-handling": [
        "In development, `@packages/electron` is used to bootstrap the Electron process, but in production, the entrypoint is actually `@packages/server/index.js`. So we have to use the CLI to set up any extra env we want to pass, since by the time the binary has launched, it's too late. And I couldn't find any way besides this env var to enable this behavior."
      ],
      "cypress-strengthen-test-assertions": [
        "This will pass even if both are `undefined`.\r\n\r\n```suggestion\r\n\texpect(Cypress.currentTest.title).to.be.a('string')\r\n    expect(Cypress.currentTest.title).eq(cy.state('runnable').ctx.currentTest.title)\r\n```\r\n\r\n... and move this to a function and do the same in the other hooks?",
        "`title` could be the same wrong string for both, so this test could use an assertion against a string literal to be more regression-proof\r\n\r\n```suggestion\r\n    .eq(cy.state('runnable').title)\r\n    .eq('returns current test runnable properties')\r\n\r\n```"
      ],
      "cypress-consistent-formatting-preferences": [
        "another example of something not matching how we prescribe to users to do this. we'd suggest something more compact like:\r\n\r\n```suggestion\r\n    describe('errors', { defaultCommandTimeout: 50 }, () => {\r\n```",
        "yeah. i wonder if there could be a prettier-cypress-plugin someday. IMO, in tests, it makes sense to have some specialized linting, since the `describe(...it(...` stuff is almost like a DSL"
      ],
      "cypress-use-precise-networking-terminology": [
        "```suggestion\r\n      A cross origin error happens when your application navigates to a new URL which does not match the origin policy above.\r\n```\r\n\r\nsince it's not necessarily the domain that's changed",
        "```suggestion\r\n      #{cmd('visit')} failed because you are attempting to visit a URL that is of a different origin.\r\n```",
        "```suggestion\r\n      The new URL is considered a different origin because the port is different.\r\n```",
        "I know that \"different origin\" is supposed to be read with the same inflection as \"same-origin\", but it doesn't really read like that to me, what do you think about \"a different origin\" instead?",
        "```suggestion\r\n      You may only #{cmd('visit')} same-origin URLs within a single test.\r\n```"
      ],
      "cypress-function-decomposition-clarity": [
        "nit: this function is >100 lines long, i get that it's a complicated procedure, but it would be easier to grok if it was broken up into smaller named functions",
        "what's the reason to use `cy.then` instead of regular Promises/async/await inside of this function? at first i thought you were doing this because you used other chainables inside of cy.session, but i don't see any chainables besides `cy.then`, so it seems unnecessary to me to use it"
      ],
      "cypress-thoughtful-error-handling": [
        "No need to log and then throw, I think, since the `ERROR_WRITING_FILE` error wraps `error`.\r\n\r\n```suggestion\r\n        error = errors.get('ERROR_WRITING_FILE', dest, error)\r\n```",
        "to check my understanding: previously, this error would crash the process, now it's dressed up in a warning?\r\n\r\nso e2e tests *can* still fail, but only in these 2 (more rare than now) conditions:\r\n\r\n1. `WAIT_FOR_MORE_FRAMES_TIMEOUT` is exceeded and <2 frames are received; or\r\n2. `ffmpeg` gives an error *not* caused by receiving 0 frames\r\n\r\nis that all correct?"
      ],
      "cypress-meaningful-test-assertions": [
        "there's no snapshot, and the spec is *, which means there's no assertion that the expected spec actually ran, just that there was some specfile, so i think you should add `snapshot: true` here"
      ]
    },
    "profile": {
      "location": "Atlanta, Georgia, USA",
      "company": "@Magnolia-Fi",
      "blog": "https://zach.bloomqu.ist",
      "site_admin": false,
      "followers": 353,
      "following": 72
    }
  },
  "jfagoagas": {
    "repos": [
      "prowler-cloud/prowler"
    ],
    "entries": [
      {
        "slug": "prowler-configure-observability-variables",
        "title": "Configure observability variables"
      },
      {
        "slug": "prowler-consistent-environment-variable-naming",
        "title": "Consistent environment variable naming"
      },
      {
        "slug": "prowler-document-configuration-variables",
        "title": "Document configuration variables"
      },
      {
        "slug": "prowler-endpoints-for-evolving-data",
        "title": "Endpoints for evolving data"
      },
      {
        "slug": "prowler-ensure-migration-compatibility",
        "title": "Ensure migration compatibility"
      },
      {
        "slug": "prowler-format-ai-code-identifiers",
        "title": "Format AI code identifiers"
      },
      {
        "slug": "prowler-least-privilege-principle",
        "title": "Least privilege principle"
      },
      {
        "slug": "prowler-log-exceptions-with-context",
        "title": "Log exceptions with context"
      },
      {
        "slug": "prowler-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "prowler-memory-usage-optimization",
        "title": "Memory usage optimization"
      },
      {
        "slug": "prowler-parameterize-configuration-values",
        "title": "Parameterize configuration values"
      },
      {
        "slug": "prowler-pin-github-actions-dependencies",
        "title": "Pin GitHub Actions dependencies"
      },
      {
        "slug": "prowler-precise-csp-configuration",
        "title": "Precise CSP configuration"
      },
      {
        "slug": "prowler-prioritize-code-readability",
        "title": "Prioritize code readability"
      },
      {
        "slug": "prowler-safe-attribute-access-patterns",
        "title": "Safe attribute access patterns"
      },
      {
        "slug": "prowler-secure-authentication-flows",
        "title": "Secure authentication flows"
      },
      {
        "slug": "prowler-service-layer-abstraction",
        "title": "Service layer abstraction"
      },
      {
        "slug": "prowler-specific-exception-handling",
        "title": "Specific exception handling"
      },
      {
        "slug": "prowler-task-signature-methods",
        "title": "Task signature methods"
      },
      {
        "slug": "prowler-tenant-aware-query-optimization",
        "title": "Tenant-aware query optimization"
      },
      {
        "slug": "prowler-use-configurable-default-values",
        "title": "Use configurable default values"
      },
      {
        "slug": "prowler-write-objectively",
        "title": "Write objectively"
      }
    ],
    "comments": {
      "prowler-configure-observability-variables": [
        "We should add these two to have more info in all the errors:\r\n- `SENTRY_ENVIRONMENT`\r\n- `SENTRY_RELEASE`"
      ],
      "prowler-task-signature-methods": [
        "`s` or `si` as we have in other tasks?",
        "`s` or `si` as we have in other tasks?"
      ],
      "prowler-service-layer-abstraction": [
        "Since you created this, could you add it as a replacement in lines 185-192?\r\n\r\nAlso please add tests to this function using `moto`.",
        "The `Content-Type` needs to be set because in AWS by default is `binary/octet-stream`. That's another reason why I recommended you to use the current SDK's S3 methods. We can adapt whatever is needed to reutilize code.",
        "S3 integration allows you to pass the same credentials as the provider and has a `test_connection` method, so you don't need to setup the AWS provider.\r\n```suggestion\r\n            S3.test_connection(**integration.credentials)\r\n```\r\n\r\nI see the `test_connection` does not support passing the raw credentials and it should. I'm going to add support to that.",
        "Why are you not using SDK's `send_to_bucket`? We should aim to have all this login in the SDK and to adapt whatever is missing for the API to use it. If not you will need to replicate all the logic in there to handle content-type and file paths.",
        "I get that. I think we should first analyse the SDK's status prior starting the work and make all the required changes. \r\n\r\nRegarding the `content-type`, we need to set it for each file because AWS sets only the file type."
      ],
      "prowler-secure-authentication-flows": [
        "We need to always redirect to the SAML endpoint, if not we will allow domain enumeration if responses are different based on existence.",
        "What's in the response body? We should find a way to prevent or mitigate it.",
        "Not sure about this step, I don't know if we should make these assumptions, like `easier and faster` or `always work with org`, for example the latter does not care about the type of authentication. Also assuming a role is not authentication but authorization, it always requires base credentials."
      ],
      "prowler-pin-github-actions-dependencies": [
        "SHA also here.\r\n```suggestion\r\n        uses: peter-evans/find-comment@3eae4d37986fb5a8592848f6a574fdf654e61f9e #v3.1.0\r\n```",
        "Why this?",
        "Out of curiosity, why do you need to install all these in the workflow system? ",
        "Interesting, can you create a TODO and a ticket to review this in the future once the issue is addressed?"
      ],
      "prowler-prioritize-code-readability": [
        "```suggestion\r\n                return {}\r\n```\r\n\r\nI think this is missing one level indentation, right?",
        "Again, to me this is really hard to follow.",
        "To me @vicferpoy has the final word on this PR.",
        "Totally agree, I'm getting older.",
        "`if` or `elif`?"
      ],
      "prowler-parameterize-configuration-values": [
        "Could you please store the Powershell version in a variable?\r\n```\r\nARG POWERSHELL_VERSION=v7.5.0\r\n```",
        "I don't know how are we going to manage Powershell dependencies but we'll need to find an automated way and try to have just one source of truth. Regarding the environment variable I think it is good to have it."
      ],
      "prowler-document-configuration-variables": [
        "Is there any real intention of having the SQLite connection chain in the environment variable? If not I think we can just set `sqlite` because it may lead to confusion, seems that the string needs to be completed.",
        "Thanks!",
        "```suggestion\n???+ note\n    The Mutelist configuration takes effect on the next scans.\n```"
      ],
      "prowler-specific-exception-handling": [
        "I forgot to point the SDK to this branch and I got this error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,487: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 output upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,503: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 compliance upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```\r\n\r\nI think we should not print this line because there was an error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```",
        "Thanks!",
        "Why this? Are we sure that this is not going to break anything?",
        "I knew that, only concerned about this changes because you know what happened in other cases, but better if this fails because something was not right.\r\n\r\nThanks!",
        "We need to handle `NoSuchKey` if the file got deleted from S3.",
        "Tested and fixed ✅  ",
        "We should also add `before_send=before_send` not to send several credential errors coming from the SDK.\r\n\r\n\r\n```python\r\nignored_exceptions = [\r\n    # Authentication Errors from AWS\r\n    \"InvalidToken\",\r\n    \"AccessDeniedException\",\r\n    \"AuthorizationErrorException\",\r\n    \"UnrecognizedClientException\",\r\n    \"UnauthorizedOperation\",\r\n    \"AuthFailure\",\r\n    \"InvalidClientTokenId\",\r\n    \"AccessDenied\",\r\n    # Shodan Check\r\n    \"No Shodan API Key\",\r\n    # For now we don't want to log the RequestLimitExceeded errors\r\n    \"RequestLimitExceeded\",\r\n    \"ThrottlingException\",\r\n    \"Rate exceeded\",\r\n    # The following comes from urllib3\r\n    # eu-west-1 -- HTTPClientError[126]: An HTTP Client raised an unhandled exception: AWSHTTPSConnectionPool(host='hostname.s3.eu-west-1.amazonaws.com', port=443): Pool is closed.\r\n    \"Pool is closed\",\r\n]\r\n\r\n\r\ndef before_send(event, hint):\r\n    \"\"\"\r\n    before_send handles the Sentry events in order to sent them or not\r\n    \"\"\"\r\n    # Ignore logs with the ignored_exceptions\r\n    # https://docs.python.org/3/library/logging.html#logrecord-objects\r\n    if \"log_record\" in hint:\r\n        log_msg = hint[\"log_record\"].msg\r\n        log_lvl = hint[\"log_record\"].levelno\r\n\r\n        # Handle Error events and discard the rest\r\n        if log_lvl == 40 and any(ignored in log_msg for ignored in ignored_exceptions):\r\n            return\r\n    return event\r\n```\r\n\r\nThis is not tested 😄 ",
        "I think this needs to be refined as we previously did\r\n```python\r\ntry:\r\n                s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\r\n            except ClientError as e:\r\n                error_code = e.response.get(\"Error\", {}).get(\"Code\")\r\n                if error_code == \"NoSuchKey\":\r\n                    return Response(\r\n                        {\"detail\": \"The scan has no reports.\"},\r\n                        status=status.HTTP_404_NOT_FOUND,\r\n                    )\r\n                return Response(\r\n                    {\"detail\": \"There is a problem with credentials.\"},\r\n                    status=status.HTTP_403_FORBIDDEN,\r\n                )\r\n```\r\n\r\nWe should not raise `HTTP 500 Internal Server Error`.",
        "I do not agree with you. A `ClientError` in `botocore`/`boto3` could not end up in an `HTTP 500 Internal Server Error` because is it a wrapper on top of different types of errors. If we are doing a `list_objects_v2` we should handle what's happening by:\r\n- Logging the error\r\n- Sending the exception to Sentry\r\n\r\nWe can maybe raise 500's for some cases but not for all the possible exceptions under `ClientError`.",
        "Thanks, I know that we need to improve the way we handle this things, but I'm not sure about it."
      ],
      "prowler-format-ai-code-identifiers": [
        "```suggestion\nFor example, the description of `getScanTool` is \"Fetches detailed information about a specific scan by its ID.\" If the description doesn't convey what the tool is capable of doing, LLM will not invoke the function. If the description of `getScanTool` was set to something random or not set at all, LLM will not answer queries like \"Give me the critical issues from the scan ID xxxxxxxxxxxxxxx\"\n```",
        "```suggestion\n- It uses a \"supervisor\" architecture that interacts with different agents for specialized tasks. For example, `findings_agent` can analyze detected security findings, while `overview_agent` provides a summary of connected cloud accounts.\n```"
      ],
      "prowler-memory-usage-optimization": [
        "Why not to add a parameter to this query?",
        "What is the point of this context manager? Is it intended for testing and benchmarking?",
        "Would not be easier to just call explicitly the GC from `main`?",
        "We are not sure about this, but we will review that later on.",
        "I thought we talked about writing findings to file in _streaming_ instead of all at once to reduce memory overhead."
      ],
      "prowler-safe-attribute-access-patterns": [
        "You can also do `project_info[\"source\"].get(\"location\",\"\") to have it in one line without the `if/else`"
      ],
      "prowler-ensure-migration-compatibility": [
        "Do we want to backfill all previous findings? This blocks the app startup while updating all the findings."
      ],
      "prowler-least-privilege-principle": [
        "Hello @maxi-bee, `iam:PassRole` action could lead into privilege escalation if the resource configured is `*` or list/single role with more privileges. In this case I recommend you to use the [Mutelist](https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/mutelist/) if the affected IAM Policy is flagged as `FAIL` by Prowler because in your environment/context it is not."
      ],
      "prowler-use-configurable-default-values": [
        "If the default value is `in-memory` we'd need to set that default.\r\n```suggestion\r\n    prowler_db_connection = os.environ.get('PROWLER_DB_CONNECTION', \"memory://\")\r\n```",
        "Can we add an environment variable for the batch size?",
        "What about `env.int(\"DJANGO_FINDINGS_BATCH_SIZE\", 1000)`",
        "Not having `DJANGO_OUTPUT_AWS_DEFAULT_REGION` makes the endpoint to raise a `HTTP 500 Internal Server Error`. I think we should handle that too.",
        "Fixed and tested ",
        "If credentials are not configured this is raising an exception, could you please check it when you get a chance?\r\n\r\nWhen this happens the call to _upload_to_s3 within generate_outputs raises an exception and the output location is not stored nor locally.",
        "Good point, thanks for pointing that out. The only issue I see with that is that if something fails there is no fallback to the local storage."
      ],
      "prowler-write-objectively": [
        "```suggestion\n2. Provide a valid Mutelist in `YAML` format. You can see full details about Mutelist [here](../tutorials/mutelist.md))\n```",
        "True, remove it 😄 \r\n```\r\n2. Provide a valid Mutelist in `YAML` format. See full details about Mutelist [here](../tutorials/mutelist.md))\r\n```"
      ],
      "prowler-log-exceptions-with-context": [
        "You can also log this in Sentry with\r\n```python\r\nimport sentry_sdk\r\n\r\nsentry_sdk.capture_exception(exception)\r\n```",
        "I added that log line within the `generate_output` `try/except` clause, if it is not enough I can add another one here."
      ],
      "prowler-consistent-environment-variable-naming": [
        "Why public cert and private key? I think it should be either private/public key or certificate and private key.",
        "In that case, why not to rename it to `SAML_CERTIFICATE`?",
        "I'd call it either `SAML_X509_CERT` or `SAML_CERT`, to me adding public there is confusing.",
        "It's not confusing but maybe I'm used to call it differently.",
        "All the `ARTIFACTS_*` environment variables should be prefixed with `DJANGO` because it is the convention we've been using for the ones used in Django."
      ],
      "prowler-meaningful-consistent-naming": [
        "Why this change? Just curious.",
        "For me `user_mail`/`mail` and `token` are more appropriate here.",
        "Thanks for the clarification!"
      ],
      "prowler-endpoints-for-evolving-data": [
        "@Chan9390 we've been reviewing the PR and get to the point that we need an API endpoint to fetch compliance frameworks by provider instead of having this file where all is hardcoded because we are constantly adding new frameworks and that'd add more steps when creating them.\r\n\r\nPlease talk with the API team (@vicferpoy and @AdriiiPRodri) because they can explain to you how to create a simple endpoint to get that from some memory objects the API service has.",
        "For now we'll use the Hub, included in [`15f98d7` (#7878)](https://github.com/prowler-cloud/prowler/pull/7878/commits/15f98d79e0fc2c5c7e9ac9bddb69ff4bf24a332c)",
        "@Chan9390 we've been reviewing the PR and get to the point that we need an API endpoint to fetch checks by provider instead of having this file where all is hardcoded because we are constantly adding new checks and eventually providers and that'd add more steps when creating them.\r\n\r\nPlease talk with the API team (@vicferpoy and @AdriiiPRodri) because they can explain to you how to create a simple endpoint to get that from some memory objects the API service has."
      ],
      "prowler-precise-csp-configuration": [
        "Do you need to add all of this to support GTM?",
        "Thanks for the clarification 👏 "
      ],
      "prowler-tenant-aware-query-optimization": [
        "That's concerning... why are you removing the `tenant_id` filter? It is a key part of our multi tenant system with RLS, we could not remove it unless there is a strong reason behind."
      ]
    },
    "profile": {
      "location": "Madrid, Spain",
      "company": "@prowler-cloud",
      "blog": "linkedin.com/in/jfagoagas",
      "twitter_username": "jfagoagas",
      "site_admin": false,
      "followers": 59,
      "following": 61
    }
  },
  "bartlomieju": {
    "repos": [
      "denoland/deno"
    ],
    "entries": [
      {
        "slug": "deno-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "deno-avoid-ambiguous-naming",
        "title": "Avoid ambiguous naming"
      },
      {
        "slug": "deno-avoid-implementation-detail-leakage",
        "title": "avoid implementation detail leakage"
      },
      {
        "slug": "deno-avoid-panics-gracefully",
        "title": "avoid panics gracefully"
      },
      {
        "slug": "deno-avoid-redundant-observability-data",
        "title": "avoid redundant observability data"
      },
      {
        "slug": "deno-benchmark-performance-assumptions",
        "title": "benchmark performance assumptions"
      },
      {
        "slug": "deno-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "deno-control-cache-lifecycle",
        "title": "Control cache lifecycle"
      },
      {
        "slug": "deno-explain-non-obvious-decisions",
        "title": "Explain non-obvious decisions"
      },
      {
        "slug": "deno-explicit-dependency-configuration",
        "title": "explicit dependency configuration"
      },
      {
        "slug": "deno-extract-complex-inline-logic",
        "title": "Extract complex inline logic"
      },
      {
        "slug": "deno-manage-async-operation-lifecycle",
        "title": "Manage async operation lifecycle"
      },
      {
        "slug": "deno-minimize-memory-allocations",
        "title": "minimize memory allocations"
      },
      {
        "slug": "deno-organize-code-structure",
        "title": "organize code structure"
      },
      {
        "slug": "deno-prefer-safe-optional-returns",
        "title": "prefer safe optional returns"
      },
      {
        "slug": "deno-prevent-prototype-pollution",
        "title": "prevent prototype pollution"
      },
      {
        "slug": "deno-use-appropriate-error-types",
        "title": "Use appropriate error types"
      },
      {
        "slug": "deno-use-appropriate-synchronization-mechanisms",
        "title": "Use appropriate synchronization mechanisms"
      },
      {
        "slug": "deno-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "deno-use-standard-api-interfaces",
        "title": "Use standard API interfaces"
      },
      {
        "slug": "deno-validate-configuration-schemas",
        "title": "validate configuration schemas"
      },
      {
        "slug": "deno-verify-algorithm-correctness",
        "title": "Verify algorithm correctness"
      }
    ],
    "comments": {
      "deno-organize-code-structure": [
        "Since this is an implementation detail, maybe you could move it to `bundle/esbuild.rs` module?",
        "Maybe move it to a helper function and add some comments on the purpose of this hack?\r\n\r\nAre there tests that cover it?",
        "I agree with David",
        "I'm wondering if we should bury `Arc` inside `FeatureChecker`. Seems like an implementation detail. Feel free to do it in a follow up",
        "We should just export this from `deno_core` - this is the third place we're redefining this macro"
      ],
      "deno-add-comprehensive-test-coverage": [
        "Agreed, especially that the parsing is a bit quirky",
        "Great, could you add a couple tests for other directives to ensure that they can accept the \"reason for ignoring\"?"
      ],
      "deno-control-cache-lifecycle": [
        "Could we clear it after a request is completed instead? I did a similar thing in https://github.com/denoland/deno/pull/27831"
      ],
      "deno-explicit-dependency-configuration": [
        "Let's remove this alias before landing",
        "Need to use an actual released version"
      ],
      "deno-use-descriptive-identifiers": [
        "Ditto, suggest to rename it to `readWithCancelHandle`"
      ],
      "deno-avoid-panics-gracefully": [
        "This unwrap is sus here 😬 maybe consider failing gracefully with an error message? Also you don't need to do the conversion to string first - `Url::from_file_path` will accept a `PathBuf`",
        "We shouldn't use `expect` here - we're gonna find all sorts of strange or broken files that are included by mistake - we should error out or print a warning in this case instead of panicking."
      ],
      "deno-avoid-implementation-detail-leakage": [
        "I'm not sure I understand what's the problem here - is this touching some internal implementation details?",
        "We could expose it as a public API down the road?"
      ],
      "deno-use-appropriate-synchronization-mechanisms": [
        "@manzt @rgbkrk if we go with `EventListener` approach for a comm then we can expect more than one consumer of the \"comm\". That means we need to use `tokio::sync::broadcast` channel which is buffered. We can go with a rather big number for the buffer like 1024 or 65536. If we expect only a single consumer then we could use an `mpsc` channel that is unbounded. Which one should we go with?"
      ],
      "deno-extract-complex-inline-logic": [
        "Consider creating another function that is called inside `try {} finally {}` instead of inlining all of this logic here."
      ],
      "deno-use-appropriate-error-types": [
        "Should this one be covered too? ",
        "Okay, but shouldn't it be a specific error instead of `notImplemented` error?",
        "😢 can we use `Deno.errors.<something>` here? This seems super brittle",
        "Add a limit here so the test won't hang if something goes wrong"
      ],
      "deno-minimize-memory-allocations": [
        "Pre-allocate based on `args.extra_stdio.len()`",
        "Maybe preallocate this vector to some sensible value? Like 512?"
      ],
      "deno-validate-configuration-schemas": [
        "Shouldn't this be `oneOf` with a string or an enum? Seems like adding a custom library will cause a diagnostic.",
        "Side note: we should add an automated test to make sure this is up to date, like in https://github.com/denoland/deno/blob/a9f404e479262af62179c38622785968888293d3/cli/tools/lint/mod.rs#L705",
        "```suggestion\r\n          \"description\": \"UNSTABLE: List of plugins to load. These can be paths, npm or jsr specifiers\",\r\n```"
      ],
      "deno-use-standard-api-interfaces": [
        "Shouldn't this be a direct import of `assert` module - since we use ESM instead of CJS I don't think there's a need for going through `process.getBuiltinModule`",
        "We don't support this syntax (yet!), so you'll need something like this:\r\n\r\n```ts\r\nexport default {\r\n  fetch(req: Request) {\r\n    return new Response(\"Hello from declarative server\");\r\n  },\r\n  onListen(info) {\r\n    console.log(info);\r\n  }\r\n} satisfies Deno.ServeDefaultExport;\r\n```"
      ],
      "deno-verify-algorithm-correctness": [
        "This appears to be wrong, I think you should use a logic similar to this:\r\n```\r\n((typeof listener.options === \"boolean\" &&\r\n          listener.options === options.capture) ||\r\n          (typeof listener.options === \"object\" &&\r\n            listener.options.capture === options.capture)) &&\r\n        listener.callback === callback\r\n```"
      ],
      "deno-explain-non-obvious-decisions": [
        "Maybe add a comment why are we forcing this to `true`?",
        "I think it might be useful to add a comment here when we want to skip this validation - it's not so obvious to me so far",
        "Thanks!"
      ],
      "deno-avoid-redundant-observability-data": [
        "I'm fine exposing `fd` in the Node compat API, but we shouldn't do it in the `Deno` APIs. IMO this should be handled the same way we're passing resource IDs between Deno APIs and Node APIs (via a private symbol)"
      ],
      "deno-prefer-safe-optional-returns": [
        "Yeah, I really think we should add specialized API to avoid all these `Default::default()` and rogue `None`s throughout the codebase",
        "Good point! It will always be a path, so probably error is unreachable, but I'll handle that"
      ],
      "deno-benchmark-performance-assumptions": [
        "Ugh, do we really want to create an array here? Maybe use 2 separate arguments here to make it so slightly faster?",
        "I think it's 3th, but yes",
        "Okay, let's not do that then."
      ],
      "deno-avoid-ambiguous-naming": [
        "Nit: since this is just `import.meta.main` transform maybe rename to `ImportMetaMainTransform` so we can chain more transforms in the future as needed?",
        "Please use an enum here as these booleans are easy to mistake:\r\n```\r\n#[derive(Default, Debug)]\r\nenum SubdomainWildcardSupport {\r\n  Enabled,\r\n  #[default]\r\n  Disabled\r\n}\r\n```",
        "+1 here - ideally we'd find a way to ban `bool` in permission code :D everything should be an enum here with descriptive name",
        "Nitpick: at first glance in JS I thought this means \"to cancel a read\" while in fact it's a \"read with maybe cancel\". Can you rename this op to `op_read_with_cancel_handle`?",
        "Nitpick: consider renaming to `UnconfiguredRuntime` or `PrewarmedRuntime` - I found `Unconfigured` not very intelligible when reviewing `cli/` part. Fine to do it in a follow up PR to not block this one"
      ],
      "deno-comprehensive-test-coverage": [
        "Won't it crash/error if you try to iterate again? Can you add a test for that?",
        "Could you wrap this whole thing in a `Deno.test()` so that we get the benefit of sanitizers?"
      ],
      "deno-prevent-prototype-pollution": [
        "I believe it should stay disabled in the TSC implementation. There won't be any user code executed here that might require it.",
        "Do you know of any package that only uses getter?",
        "These should use primordials like `ObjectDefineProperty`, `ObjectPrototype` and `ObjectGetPrototypeOf`. You will need to move this after line 53"
      ],
      "deno-manage-async-operation-lifecycle": [
        "Is this correct that we're unrefing and refing immediately after?"
      ]
    },
    "profile": {
      "location": "Warsaw, PL",
      "company": "@denoland",
      "blog": "",
      "twitter_username": "biwanczuk",
      "site_admin": false,
      "followers": 1035,
      "following": 8
    }
  },
  "amaanq": {
    "repos": [
      "tree-sitter/tree-sitter"
    ],
    "entries": [
      {
        "slug": "tree-sitter-algorithm-and-data-optimization",
        "title": "Algorithm and data optimization"
      },
      {
        "slug": "tree-sitter-api-pattern-consistency",
        "title": "API pattern consistency"
      },
      {
        "slug": "tree-sitter-api-type-definition-consistency",
        "title": "API type definition consistency"
      },
      {
        "slug": "tree-sitter-avoid-unnecessary-option-operations",
        "title": "avoid unnecessary Option operations"
      },
      {
        "slug": "tree-sitter-clarify-api-documentation",
        "title": "Clarify API documentation"
      },
      {
        "slug": "tree-sitter-conditional-compilation-guards",
        "title": "Conditional compilation guards"
      },
      {
        "slug": "tree-sitter-consistent-formatting-preferences",
        "title": "consistent formatting preferences"
      },
      {
        "slug": "tree-sitter-document-configuration-changes",
        "title": "Document configuration changes"
      },
      {
        "slug": "tree-sitter-ensure-documentation-clarity",
        "title": "Ensure documentation clarity"
      },
      {
        "slug": "tree-sitter-ensure-semantic-naming-clarity",
        "title": "Ensure semantic naming clarity"
      },
      {
        "slug": "tree-sitter-measure-performance-implications",
        "title": "measure performance implications"
      },
      {
        "slug": "tree-sitter-optimize-dependency-configurations",
        "title": "optimize dependency configurations"
      },
      {
        "slug": "tree-sitter-optimize-frequent-operations",
        "title": "Optimize frequent operations"
      },
      {
        "slug": "tree-sitter-optimize-memory-usage-patterns",
        "title": "optimize memory usage patterns"
      },
      {
        "slug": "tree-sitter-parameter-type-clarity",
        "title": "parameter type clarity"
      },
      {
        "slug": "tree-sitter-prefer-compile-time-configuration",
        "title": "prefer compile-time configuration"
      },
      {
        "slug": "tree-sitter-provide-clear-error-context",
        "title": "Provide clear error context"
      },
      {
        "slug": "tree-sitter-respect-environment-overrides",
        "title": "Respect environment overrides"
      },
      {
        "slug": "tree-sitter-simplify-complex-expressions",
        "title": "Simplify complex expressions"
      },
      {
        "slug": "tree-sitter-simplify-conditional-logic",
        "title": "Simplify conditional logic"
      },
      {
        "slug": "tree-sitter-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "tree-sitter-validate-algorithm-boundaries",
        "title": "validate algorithm boundaries"
      },
      {
        "slug": "tree-sitter-validate-inputs-early",
        "title": "validate inputs early"
      }
    ],
    "comments": {
      "tree-sitter-provide-clear-error-context": [
        "I think this could be better phrased as \"unexpected\" as we expected two types of inputs and got something else - the rule itself isn't invalid"
      ],
      "tree-sitter-api-pattern-consistency": [
        "Instead of exposing a function for just getting the # of supertypes (which I question the usefulness of), why not instead have the caller pass in a pointer to a `uint32_t`, called `length`, which is written to in the function body. This will let the user know how many supertypes there are, see `ts_tree_included_ranges` for an example of what I mean. The main reason being this will set the precedence for us to add `_count` functions for other node types, which we don't want to add."
      ],
      "tree-sitter-optimize-frequent-operations": [
        "A [`rename`](https://linux.die.net/man/2/rename) would be faster than a [`copy`](https://man7.org/linux/man-pages/man2/copy_file_range.2.html) + [`unlink`](https://linux.die.net/man/2/unlink)\r\n\r\n```suggestion\r\n                fs::rename(&legacy_apple_path, &xdg_path)?;\r\n```\r\n\r\nAlso, if `xdg_path`'s parent folder doesn't exist, you'll get an error either way because it can't write to that file. Use `create_dir_all` on the parent.",
        "for clarity and avoiding allocations on every call to format, store a `Vec` of extensions, and push to it on each iteration while there is an extension, then just join them with a `.` at the end",
        "you called `node.is_named()` in the caller of this function - just pass the value of the first call into here instead."
      ],
      "tree-sitter-parameter-type-clarity": [
        "Make this an Option, and remove the default",
        "Instead of 3 bool flags, this should be one argument, `bump`, that takes an enum, of which the enum has the major/minor/patch variants. Then, you can make patch the default variant."
      ],
      "tree-sitter-optimize-dependency-configurations": [
        "Hm, I'll just do ^1.0 then for compatibility"
      ],
      "tree-sitter-prefer-compile-time-configuration": [
        "oops, sorry"
      ],
      "tree-sitter-conditional-compilation-guards": [
        "Gate this with `LANGUAGE_VERSION_WITH_RESERVED_WORDS` instead, and remove the other define",
        "Did you try this out without `Py_GIL_DISABLED` defined? It doesn't fail to compile if the system Python version is 3.13, because of this part in setup.py:\r\n\r\n```py\r\nif limited_api := not get_config_var(\"Py_GIL_DISABLED\"):\r\n    macros.append((\"Py_LIMITED_API\", \"0x030A0000\"))\r\n```\r\n\r\nThe error I get is:\r\n\r\n```\r\n      bindings/python/tree_sitter_javascript/binding.c:13:6: error: ‘Py_mod_gil’ undeclared here (not in a function)\r\n         13 |     {Py_mod_gil, Py_MOD_GIL_NOT_USED},\r\n            |      ^~~~~~~~~~\r\n      bindings/python/tree_sitter_javascript/binding.c:13:18: error: ‘Py_MOD_GIL_NOT_USED’ undeclared here (not in a function)\r\n         13 |     {Py_mod_gil, Py_MOD_GIL_NOT_USED},\r\n            |                  ^~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nIf it's set to `0x030D0000` then it works, but Python bindings will *not* build with this change, so this should be updated.\r\n\r\nNote that my Python version is `3.13.1`",
        "Can you add this to wasm-stdlib.h then for consistency? Also, please add a newline character at eof"
      ],
      "tree-sitter-algorithm-and-data-optimization": [
        "~~yep you're right~~\r\n\r\nActually I think it is to do with inlined nodes? Zero width nodes *can* have children which are also zero width, the issue here is that inlined nodes don't have any children, right?",
        "Ahh gotcha, makes sense. Thanks for clarifying"
      ],
      "tree-sitter-respect-environment-overrides": [
        "right, was debugging why clang wasnt selected by the makefile, but LINK was unused"
      ],
      "tree-sitter-use-descriptive-identifiers": [
        "I think calling this field `supertype_symbols` would be clearer, and then the corresponding const array `ts_supertype_symbols`",
        "`ReservedWordSet` itself doesn't tell me much unless i look at the error message, let's call this `InvalidReservedWordSet` instead",
        "Supertype not SuperType, and this could be named `InvalidSupertype`"
      ],
      "tree-sitter-validate-algorithm-boundaries": [
        "Yeah that's a good point, thanks!",
        "This makes total sense, and is definitely a lot clearer too"
      ],
      "tree-sitter-simplify-conditional-logic": [
        "totally agree, thanks"
      ],
      "tree-sitter-validate-inputs-early": [
        "same thing\r\n\r\n```suggestion\r\n  if (start_byte > end_byte) {\r\n    return false;\r\n  }\r\n```"
      ],
      "tree-sitter-clarify-api-documentation": [
        "Clearer explanation\r\n\r\n\"This will return false if the start byte/point is greater than the end byte/point, otherwise it will return true.`\r\n\r\nAlso, do a favor and have the doc comments be associated to each function individually, (adjust byte/point semantics as needed). This will help w/ the rust doc comments in `bindings.rs`"
      ],
      "tree-sitter-simplify-complex-expressions": [
        "We don't need to duplicate data, since we're mutating the summary data inside `parse_file_at_path`, why not just have that return a Result of the unit type, set the fields `successful`, `duration`, and `bytes` inside that call, and then use *that* data later on when updating the cumulative stats? This way we avoid a couple things:\r\n\r\n- two sources of truth (error prone to updates)\r\n- two sites of mutation",
        "this is getting a bit unruly, can we refactor this to move these parameters into a `QueryFileOptions` struct, similar to the `ParseFileOptions` for the parse subcommand?",
        "can you hoist out the paths into its own variable above the tuple construction? This feels a bit unruly and messy...also, there's a repeated habit of calling `get_tmp_parse_file()?.to_str().unwrap().to_owned()`, and `get_tmp_parse_file()` isn't using its path besides to convert it as a string, so we should move that logic into `get_tmp_parse_file` and have it return a String, so in the end we have something like this:\r\n\r\n```suggestion\r\n            let paths =\r\n                if let Some(collected) = collect_paths(self.paths_file.as_deref(), self.paths)? {\r\n                    collected\r\n                } else {\r\n                    vec![get_tmp_parse_file()?]\r\n                };\r\n            (paths, None, true)\r\n```",
        "nit, since we check `has_arrow` later before we make an `Assertion`, I think we can just set `arrow_count` to 1 at the beginning\r\n\r\n```suggestion\r\n                        let mut arrow_count = 1;\r\n                        for (i, c) in text.char_indices() {\r\n                            arrow_end = i + 1;\r\n                            if c == '-' && has_left_caret {\r\n                                has_arrow = true;\r\n                                break;\r\n                            }\r\n                            if c == '^' {\r\n                                has_arrow = true;\r\n```",
        "~~why split it up?~~ nvm I see",
        "yeah this is fine, the diff w/ the match arms felt chaotic, and I'd rather not nest such a large block too deeply. thanks :slightly_smiling_face:",
        "two nits - `val_or_default` is a better name imo, and the `Into` chain is a bit crazy\r\n\r\n```suggestion\r\n        let val_or_default = |val: Option<Rgb>, default: Option<Color>| -> Option<Color> {\r\n            val.map_or(default, |v| Some(Color::Rgb(v.into())))\r\n        };\r\n        let default = Self::default();\r\n\r\n        Self {\r\n            node_kind: val_or_default(value.node_kind, default.node_kind),\r\n            node_text: val_or_default(value.node_text, default.node_text),\r\n            field: val_or_default(value.field, default.field),\r\n            token: val_or_default(value.token, default.token),\r\n            row_color: val_or_default(value.row_color, default.row_color),\r\n            row_color_named: val_or_default(value.row_color_named, default.row_color_named),\r\n            extra: val_or_default(value.extra, default.extra),\r\n            error: val_or_default(value.error, default.error),\r\n        }\r\n```"
      ],
      "tree-sitter-consistent-formatting-preferences": [
        "sure"
      ],
      "tree-sitter-avoid-unnecessary-option-operations": [
        "copied from https://github.com/tree-sitter/tree-sitter/blob/07aaf2322e0d5d7568d37e65cc7a951da252416b/cli/src/test_highlight.rs#L77\r\n\r\nI don't really expect anyone to use non-utf8 filenames for a test file, though it doesn't hurt to use `to_string_lossy` (I'll update the other callsite)"
      ],
      "tree-sitter-document-configuration-changes": [
        "Please explain in your PR description or in a comment as to why this change was made",
        "and explain this, is it needed? I don't mind, but I'd like to understand what the benefit is"
      ],
      "tree-sitter-measure-performance-implications": [
        "I don't think we should be writing C-style strings, as this function is giving us the subtype symbol names, of which we can't really do much with. It would make a lot more sense imo, to store the supertype info in the parser as a map of symbol ids to an array of symbol ids, which is a lot better for the parser binary size and memory usage (esp in wasm). With the symbol ids, they are a lot more useful to us and we can always fetch the symbol names with this array of ids."
      ],
      "tree-sitter-ensure-semantic-naming-clarity": [
        "Sure, but we have a variable called \"Parser\", which is of type \"Parser\" (but there is a conflict of names there, and we can call the default import whatever we want)",
        "The default export *is* the parser class, but when you import the default export, you can name it whatever you like. I did this because we have a declaration below, that looks like so:\r\n\r\n```ts\r\nimport TSParser from 'web-tree-sitter';\r\nconst Parser: typeof TSParser = await import('..').then(m => m.default);\r\n```\r\n\r\nBecause the variable, which is of type `Parser` from the `.d.ts` file, is *also* called `Parser`, we get a naming conflict. So I decided to just name the import `TSParser`. I'll swap them so it's a bit clearer.",
        "one's used for type hinting/info (TSParser), and one's used as the type (Parser, declared later)",
        "Yeah the default export is the Parser class currently, the only reason we import 'web-tree-sitter' (which pulls in type info from `tree-sitter-web.d.ts`, is so we can annotate the \"Parser\" used in the tests with the type, since otherwise it's just given `any` when we import `..` (because we're importing the final compiled JS file). I'll push up some changes that should make it a lot clearer imo.",
        "Also, I'd like to get rid of the type declarations file, but it's sorta hard. If we could restructure it a bit that'd work out, but the problem is, because every class/type is in its own file, tsc's generation of the types file throws each class in its own module like:\r\n\r\n```ts\r\ndeclare module \"language\" {\r\n    import { Internal } from \"constants\";\r\n    import { LookaheadIterator } from \"lookahead_iterator\";\r\n    import { Query } from \"query\";\r\n    export class Language {\r\n        private [0];\r\n        types: string[];\r\n        fields: (string | null)[];\r\n        constructor(internal: Internal, address: number);\r\n        get name(): string | null;\r\n        get version(): number;\r\n        get fieldCount(): number;\r\n        get stateCount(): number;\r\n        fieldIdForName(fieldName: string): number | null;\r\n        fieldNameForId(fieldId: number): string | null;\r\n        idForNodeType(type: string, named: boolean): number | null;\r\n        get nodeTypeCount(): number;\r\n        nodeTypeForId(typeId: number): string | null;\r\n        nodeTypeIsNamed(typeId: number): boolean;\r\n        nodeTypeIsVisible(typeId: number): boolean;\r\n        get supertypes(): number[];\r\n        subtypes(supertype: number): number[];\r\n        nextState(stateId: number, typeId: number): number;\r\n        lookaheadIterator(stateId: number): LookaheadIterator | null;\r\n        query(source: string): Query;\r\n        static load(input: string | Uint8Array): Promise<Language>;\r\n    }\r\n}\r\n```\r\n\r\nWhich is not what we want. I haven't been successful in wrangling this to throw everything under the Parser namespace, and remove \"internal\" stuff",
        "Oh yeah, I totally agree with that, and removing the default export + namespace mix would be a lot easier to manage w.r.t. generating the declaration file!",
        "Ok, this should be solved now, with some help from @savetheclocktower, [`dts-buddy`](https://github.com/Rich-Harris/dts-buddy) seems to be the best way to reliably generate a `.d.ts` file, and it also includes a sourcemap file, to enable rich goto def for users, going to the actual TypeScript source files and not just the `.d.ts` file."
      ],
      "tree-sitter-api-type-definition-consistency": [
        "Yes, it's auto-generated but I've tweaked it iirc because some definitions loosely used `any`, or were wrong. We also add some methods at runtime, in [imports.js](https://github.com/tree-sitter/tree-sitter/blob/853ca46899e41ce23ab8297bbebd26851482258a/lib/binding_web/lib/imports.js), which is why I manually added those methods here. I'd rather it not be touched, unless necessary, though your proposed changed would be good to add. I would be okay moving `@types/emscripten` to `peerDependencies`, and I agree with the change to make `options` `Partial<EmscriptenModule>`. I appreciate the investigation here :slightly_smiling_face:"
      ],
      "tree-sitter-optimize-memory-usage-patterns": [
        "It'd be cheaper to take an index parameter instead - shift the offset to start indexing the `TRANSFER_BUFFER` by `index * SIZE_OF_NODE` "
      ],
      "tree-sitter-ensure-documentation-clarity": [
        "Nit, `display` over `print`\r\n\r\n```suggestion\r\n    /// Display or increment the version of a grammar\r\n```",
        "Can we comment what the missing node is and where with a carat? It's not immediately obvious to anyone else *what* exactly is missing in this test. Also, try to minimize the test case, there's too much going on that is not needed to test that querying a missing node works."
      ]
    },
    "profile": {
      "location": "New York, NY",
      "blog": "",
      "site_admin": false,
      "followers": 268,
      "following": 41
    }
  },
  "louwers": {
    "repos": [
      "maplibre/maplibre-native"
    ],
    "entries": [
      {
        "slug": "maplibre-native-accurate-documentation-references",
        "title": "Accurate documentation references"
      },
      {
        "slug": "maplibre-native-configure-platform-specific-builds",
        "title": "Configure platform-specific builds"
      },
      {
        "slug": "maplibre-native-consistent-api-practices",
        "title": "Consistent API practices"
      },
      {
        "slug": "maplibre-native-cross-platform-ci-validation",
        "title": "Cross-platform CI validation"
      },
      {
        "slug": "maplibre-native-cross-platform-test-management",
        "title": "Cross-platform test management"
      },
      {
        "slug": "maplibre-native-descriptive-named-constants",
        "title": "Descriptive named constants"
      },
      {
        "slug": "maplibre-native-design-evolution-ready-apis",
        "title": "Design evolution-ready APIs"
      },
      {
        "slug": "maplibre-native-document-containerized-builds",
        "title": "Document containerized builds"
      },
      {
        "slug": "maplibre-native-document-public-api-completely",
        "title": "Document public API completely"
      },
      {
        "slug": "maplibre-native-dry-class-hierarchies",
        "title": "DRY class hierarchies"
      },
      {
        "slug": "maplibre-native-enforce-clear-data-ownership",
        "title": "Enforce clear data ownership"
      },
      {
        "slug": "maplibre-native-externalize-config-values",
        "title": "Externalize config values"
      },
      {
        "slug": "maplibre-native-externalize-configuration-values",
        "title": "Externalize configuration values"
      },
      {
        "slug": "maplibre-native-extract-workflow-scripts",
        "title": "Extract workflow scripts"
      },
      {
        "slug": "maplibre-native-follow-modern-c-guidelines",
        "title": "Follow modern C++ guidelines"
      },
      {
        "slug": "maplibre-native-group-related-properties",
        "title": "Group related properties"
      },
      {
        "slug": "maplibre-native-handle-errors-by-severity",
        "title": "Handle errors by severity"
      },
      {
        "slug": "maplibre-native-modern-c-style-practices",
        "title": "Modern C++ style practices"
      },
      {
        "slug": "maplibre-native-numerical-precision-considerations",
        "title": "Numerical precision considerations"
      },
      {
        "slug": "maplibre-native-optimize-compilation-flags",
        "title": "Optimize compilation flags"
      },
      {
        "slug": "maplibre-native-prefer-safe-null-handling",
        "title": "Prefer safe null handling"
      },
      {
        "slug": "maplibre-native-prefer-values-over-pointers",
        "title": "Prefer values over pointers"
      },
      {
        "slug": "maplibre-native-self-documenting-code-naming",
        "title": "Self-documenting code naming"
      },
      {
        "slug": "maplibre-native-standard-configuration-files",
        "title": "Standard configuration files"
      },
      {
        "slug": "maplibre-native-structure-documentation-effectively",
        "title": "Structure documentation effectively"
      },
      {
        "slug": "maplibre-native-style-compliant-example-code",
        "title": "Style-compliant example code"
      },
      {
        "slug": "maplibre-native-template-instantiation-trade-offs",
        "title": "Template instantiation trade-offs"
      },
      {
        "slug": "maplibre-native-use-proper-logging",
        "title": "Use proper logging"
      },
      {
        "slug": "maplibre-native-use-specific-test-assertions",
        "title": "Use specific test assertions"
      },
      {
        "slug": "maplibre-native-validate-noexcept-guarantees",
        "title": "Validate noexcept guarantees"
      },
      {
        "slug": "maplibre-native-variable-evaluation-context",
        "title": "Variable evaluation context"
      }
    ],
    "comments": {
      "maplibre-native-follow-modern-c-guidelines": [
        "Prefer enum class https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Renum-class",
        "Prefer `enum class`.",
        "Created an issue.",
        "We follow the C++ Core Guidelines, which state\r\n\r\n> [Specify the underlying type of an enumeration only when necessary](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Renum-underlying)\r\n\r\nEspecially in this case I think it makes sense to just use the default.",
        "There is something to be said for it, but since the space saving is minimal sticking to the default might be better.\r\n\r\nI will leave it up to your judgement!",
        "Can probably be const.",
        "Can be marked const.",
        "Maybe we can use something like https://github.com/zhihaoy/nontype_functional/blob/main/include/std23/move_only_function.h instead so we can more easily migrate to `std::move_only_function` when we can use C++23.\r\n\r\nIt's also better tested.",
        "If it's not too much hassle I would add it to `vendor/nontype_functional`. Could just be the two files we need.",
        "Can be const."
      ],
      "maplibre-native-enforce-clear-data-ownership": [
        "Use a lock guard? Can deadlock if anything throws I think.\r\n\r\nAlso applies to other methods.",
        "Somehow my brain can't parse this capture group."
      ],
      "maplibre-native-template-instantiation-trade-offs": [
        "Are there more lightweight `std::function` alternatives?",
        "This solution is pretty easy to understand. There may be a fancier solution but looks OK to me.",
        "I guess nameIDs are small so this doens't overflow right?\r\n\r\nShould this maybe be extracted in a function? "
      ],
      "maplibre-native-prefer-values-over-pointers": [
        "I think using unique pointers but then using `.get()` to get a raw pointer is dangerous because now someone might hold onto a dangling pointer.\r\n\r\nI would try to avoid using raw pointers as much as possible, in this case since `Sprite` is such a lightweight object copying is probably even OK.",
        "What is this?",
        "The return type is marked with `nonnull` and the interface with `null_resettable`, which means this getter should not return null if I understand it correctly. We should not modify this, because I think it would change the Swift API.\r\n\r\nMaybe you can return `[NSURL URLWithString:@\"local://style.json\"]` instead?"
      ],
      "maplibre-native-externalize-config-values": [
        "In any case getting rid of the magic number would be good."
      ],
      "maplibre-native-handle-errors-by-severity": [
        "What kind of exceptions can `std::exception_ptr` contain? Can all be safely ignored? ",
        "Is the callback passed to `renderStill` called multiple times? In that case the error would get overwritten in the case of multiple errors.",
        "You need to check the return value of `query.run()`. If it returns `false` no row is available and `query.get` will throw.",
        "Maybe it's better to let it crash here instead? If these functions are not initialized MapLibre will not be able to render anything.",
        "Maybe you could wrap the check in a function that returns a bool and logs a warning if there is no context anymore?\r\n\r\nYour call, but it may save someone a frustrating debugging session."
      ],
      "maplibre-native-group-related-properties": [
        "What kind of extra properties would be relevant for `MLNPluginLayerDrawingContext` but not `MLNStyleLayerDrawingContext`?",
        "Honestly I would make `ActionJournalOptions` objects immutable if possible. You could return a new `ActionJournalOptions` if you think that is a good API."
      ],
      "maplibre-native-optimize-compilation-flags": [
        "Yes it contains both:\r\n\r\n```\r\n$ ls MapLibre.xcframework\r\nInfo.plist*                  ios-arm64/                   ios-arm64_x86_64-simulator/\r\n```\r\n\r\nBut I am extracting the armv8 dynamic library from the XCFramework:\r\n\r\n```\r\ncp MapLibre.xcframework/ios-arm64/MapLibre.framework/MapLibre MapLibre_dynamic\r\n```\r\n\r\n```\r\n $ file MapLibre_dynamic\r\nMapLibre_dynamic: Mach-O 64-bit dynamically linked shared library arm64\r\n```\r\n\r\nSo should be fine."
      ],
      "maplibre-native-variable-evaluation-context": [
        "Yes it is manual, but people can make mistakes. Especially when making a pre-release, which does not require a PR review for a version release."
      ],
      "maplibre-native-modern-c-style-practices": [
        "Don't use C-style casts.",
        "Do you know structured bindings? I think this would work\r\n\r\n```\r\nfor (auto [texHandle, glyph, fontStack] : glyphsToUpload) {\r\n```",
        "Please have a look over all definitions in the PR and make sure that all immutable values are marked `const`.",
        "You should be able to default this `operator==` since we use C++20 now.",
        "Should be initialized to something.",
        "C++ Code Guidelines say yes. https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#es20-always-initialize-an-object"
      ],
      "maplibre-native-prefer-safe-null-handling": [
        "You are moving the data member `pendingReleases` here. It's no longer valid after that.",
        "<img width=\"659\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2b533993-5a56-4e51-82a7-340d5a5ae186\">\r\n<img width=\"673\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b509f10f-5280-4be1-9006-b3689361f869\">\r\n",
        "Yes I think that is OK.",
        "Maybe try passing `nullptr` here?\r\n\r\n> If zVfsName is NULL then the default VFS is returned.\r\n\r\nhttps://www.sqlite.org/c3ref/vfs_find.html",
        "The memory pointed to by the unique_ptr is not cleaned up after `.release()`.\r\n\r\n",
        "I think you can just `reset()` instead.",
        "Can you avoid using raw pointers?",
        "```suggestion\r\n                                             nullptr)) {}\r\n```\r\n\r\nMight as well update this as well.",
        "Yes I don't think there's a good reason to use `NULL` in C++ code, even when calling C libraries."
      ],
      "maplibre-native-document-containerized-builds": [
        "If we have to choose one, just include the command without the `___any_build_command___`, because it's nicer to have a valid command that people can copy and paste.\r\n\r\nOptionally mention that you can also run build commands directly.",
        "```suggestion\r\nYou can use a Docker container to build MapLibre Native. A `Dockerfile` that installes the required dependencies when the image is built is provided in this directory.\r\n```"
      ],
      "maplibre-native-externalize-configuration-values": [
        "Good suggestion, but that is outside the scope of this PR, because this is also what we do on `main`:\r\n\r\n```groovy\r\n                        // Enable ccache if the user has installed it.\r\n                        if (file(\"/usr/bin/ccache\").exists()) {\r\n                            arguments \"-DANDROID_CCACHE=/usr/bin/ccache\"\r\n                        } else if (file(\"/usr/local/bin/ccache\").exists()) {\r\n                            arguments \"-DANDROID_CCACHE=/usr/local/bin/ccache\"\r\n                        }\r\n```",
        "Could you create a new issue for this? "
      ],
      "maplibre-native-structure-documentation-effectively": [
        "I think we can assume that everyone knows JSON...\r\n\r\nWe can probably link to an explanation of GeoJSON elsewhere and focus on concepts that are specific to MapLibre Android.",
        "All code snippets (except really small ones) should be referenced so that we can be sure they compile in the future.",
        "Each section of the article should have a descriptive heading so people can easily skip to the part they are interested in.",
        "Sections can be short, that is not a problem. But we should have a heading for each part of the article that covers a different concept.",
        "Give the article a descriptive name that explains the contents of the article. \"Ways to Configure the Map\" or something similar."
      ],
      "maplibre-native-use-specific-test-assertions": [
        "Needs to use `EXPECT_THROW` otherwise the test will fail it doesn't throw."
      ],
      "maplibre-native-dry-class-hierarchies": [
        "Duplicated. I feel like this should be in a common base class.",
        "Most code in this class is identical to the GL version. They can probably use a common base class."
      ],
      "maplibre-native-accurate-documentation-references": [
        "```suggestion\r\n     * Learn more about above properties in the [Style specification](https://maplibre.org/maplibre-style-spec/).\r\n```"
      ],
      "maplibre-native-extract-workflow-scripts": [
        "Shellcheck detects an issue with this script:\r\n\r\n```\r\nArgument mixes string and array. Use * or separate argument\r\n```",
        "Since this is quite a large script, maybe it can be extracted in a file so it can be easily ran.",
        "Like this?",
        "Yes sounds better, thanks.",
        "I see it relies on caches. That will not speed things up because our cache is full after a single run of the workflows.\r\n\r\nGitHub promised they will allow a bigger cache than just 10GB in Q1 2025 though.",
        "I prefer to just keep it simple for now.",
        "Fixed."
      ],
      "maplibre-native-consistent-api-practices": [
        "```suggestion\r\nMultiple map instances are enabled using a unique context pointer.  A unique context pointer is passed back for every `initialize` invocation. The context pointer is released on `de_initialized` or when the library reference is destroyed.\r\n```",
        "```suggestion\r\nThe environment that consumes the FFI library is responsible for initializing its own graphics backend.  This limits a library artifact to a specific platform (Mac, Window, Linux) and runtime revision.  The required parts are passed into the FFI initialize call using an opaque data pointer (nativeWindow).  Backend/Platform build flags determine how this opaque data pointer is cast/used.\r\n```",
        "```suggestion\r\n* Introduce “Annotation” as a first class citizen of the MapLibre Native C++ Core.\r\n```\r\n\r\nSince this design proposal is for the MapLibre Native repo, it is understood that it does not apply to MapLibre GL JS.",
        "```suggestion\r\nThis proposal introduces the concept of annotations that are complex, animatable and interactive to MapLibre’s core. The first phase allows bitmap backed annotations to be rendered. The second phase will enable native platform views (iOS, Android) to be rendered directly by MapLibre Native.\r\n```",
        "```suggestion\r\nThese new annotations will provide a more flexible tool for drawing user content on the map that is more in line with what developers expect from the built-in mobile map toolkits. The focus on animations and interactivity allows developers to create differentiated map experiences that feel made for mobile.\r\n```"
      ],
      "maplibre-native-standard-configuration-files": [
        "Toolchain is already installed on the default image."
      ],
      "maplibre-native-configure-platform-specific-builds": [
        "@1ec5 mentioned on Slack that a source-only distribution is the prefered way of distribution for Apple platforms now. For MapLibre I don't know if would apply to just the SDK (probably written in Swift in the long term) or also the core written in C++.",
        "It's easier to just clone with `--recurse-submodules`"
      ],
      "maplibre-native-use-proper-logging": [
        "use mbgl::Log, remove macro",
        "There are commented out lines that print used for debugging throughout this file.\r\n\r\nPlease remove them or change them to (debug) logging.",
        "Should some logging be added in these cases?",
        "Thanks for clarifying.\r\n\r\nI didn't see any retry messages in the logs, so I didn't know if it was actually retrying."
      ],
      "maplibre-native-descriptive-named-constants": [
        "Avoid magic numbers",
        "Adding comments is good, but I would also assign these numbers to a const.",
        "I think all upper case names should be reserved for macros. https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#nl9-use-all_caps-for-macro-names-only",
        "Keep it `constexpr`, but lowercase them.\r\n\r\nAlso camelcase maybe `pmtilesHeaderOffset`?"
      ],
      "maplibre-native-validate-noexcept-guarantees": [
        "This can throw which would crash the program (because of `noexcept`). Is it possible to avoid allocation?",
        "If it is unavoidable we should remove `noexcept` here (and from the base class), otherwise the program will crash when an exception is thrown here.",
        "The move constructor of `type::Type` is not `noexcept`.\r\n\r\nbecause `mapbox::util::recursive_wrapper<Array>` is part of the variant\r\n\r\n```cpp\r\n    recursive_wrapper(recursive_wrapper&& operand)\r\n        : p_(new T(std::move(operand.get()))) {}\r\n```",
        "When adding `noexcept` to a templated function, the compiler won't check that the methods called on that type are really `noexcept`. It will just crash on runtime.\r\n\r\nIt is really hard to verify yourself as well (with all the deeply nested templates).\r\n\r\nWith long, complicated, templated functions, it may be worth leaving out `noexcept`. In this case we can maybe use something like:\r\n\r\n```cpp\r\n    using ReturnType = decltype(evaluated.template get<DataDrivenPaintProperty>());\r\n    static_assert(std::is_nothrow_invocable_v<decltype(&ReturnType::isConstant), ReturnType&>,\r\n                  \"isConstant() must be noexcept\");\r\n```",
        "`mbgl::util::clamp` (called somewhere down) is not `noexcept`. But it can be made `noexcept`.",
        "```suggestion\r\n    T operator()(const T& a, const T&, const double) const {\r\n```\r\n\r\nmay call throwing copy constructor",
        "```suggestion\r\nT interpolate(const T& a, const T& b, const float t) {\r\n```\r\n\r\nmay call throwing copy constructor",
        "```suggestion\r\nT interpolate(const T& a, const T& b, const double t) {\r\n```\r\n\r\nmay call throwing copy constructor",
        "`std::make_tuple` is not `noexcept`.",
        "Ah OK, but this is one compiler.\r\n\r\nI would err on the side of safety in general when it comes to `noexcept`. If it is not immediately obvious something is `noexcept`, I would not use it."
      ],
      "maplibre-native-document-public-api-completely": [
        "Public APIs could use some docstrings. Adding them to private APIs also does not hurt.",
        "Everything in `include` will become part of our public API. Is this intended? Can you add some triple slash comments to indicate what this template struct is for?",
        "It may make sense to make it internal, in that case you can move it to `/src`.\r\n\r\nSome comments (and a link to cppreference) would still be a good idea in that case.",
        "These could use some Doxygen comments.\r\n\r\n```\r\n    RequestedFromCache ///< like this\r\n```",
        "Suggestion: add a triple slash comment to each enum in the PR, describing what it does.\r\n\r\nIt is also possible to add descriptions to enum values.\r\n\r\n````\r\n/// This is an enum class\r\nenum class fooenum {\r\n    FOO, ///< this is foo\r\n    BAR, ///< this is bar\r\n};\r\n````",
        "A documentation comment should be added to this class.\r\n\r\nSome (public) methods including the constructors might also benefit from documentation comments, although they are relatively straightforward."
      ],
      "maplibre-native-self-documenting-code-naming": [
        "I understand `_setDirection` needs the current location so that it can do the animation in one go, but maybe consider passing the center instead of a boolean. I don't think the `_setDirection` method should have anything to do with the location manager.",
        "Looks good now, thanks!",
        "Perhaps this should (also) be called `setTileCacheEnabled(bool)` because it is a bit clearer that it actually disables the tile cache when passing `false`.",
        "`removeDrawablesIf` sounds like a much better name here."
      ],
      "maplibre-native-cross-platform-ci-validation": [
        "CI is complaining that this function is not used.",
        "It does find the provisioning profile for me now. Could we make a bug report over at https://github.com/MobileNativeFoundation/rules_xcodeproj ?\r\n\r\nBwX does not have first-class support from rules_xcodeproj and support for it might be removed altogether in the future. I will add your comment to the discussion [here](https://github.com/MobileNativeFoundation/rules_xcodeproj/discussions/2391).\r\n\r\n",
        "I made the `BUILD_MODE` configurable. You can set it to your liking in your `config.bzl`."
      ],
      "maplibre-native-cross-platform-test-management": [
        "Maybe we can only ignore it on the platforms where it is failing?\r\n\r\nFor me it is running fine locally on Android.\r\n\r\nIf you would prefer to merge this and look into this later, let's create an issue so we don't forget.",
        "Thanks!",
        "Well, looks like it failed on the Pixel 7 Pro on CI.\r\n\r\nMight be device-dependent. Maybe best to disable it after all so we can merge this."
      ],
      "maplibre-native-style-compliant-example-code": [
        "Maybe don't use this because it is an internal class to the test app. Instead use something that users could copy paste in their own apps.",
        "Yes just use a hardcoded style URL like demotiles.",
        "> Personally I'd argue for \"best practice, even if more complicated\" because people will copy and paste our examples into their code and location management is not an easy task.\r\n\r\nFully agreed. Even an example with an external dependency would be fine by me. Let's not forget the iOS MapLibre codebase is quite rusty, so it should not be surprising some things are not very modern anymore.\r\n\r\nI'm just transplanting some examples from https://github.com/mapbox/ios-sdk-examples, I'm not a veteran iOS developer like you, so I am happy for your critical eye!"
      ],
      "maplibre-native-numerical-precision-considerations": [
        "`std::numeric_limits<double>::epsilon()` was too small.\r\n\r\nMaybe we should define this globally somewhere."
      ],
      "maplibre-native-design-evolution-ready-apis": [
        "This is technically a breaking change right?",
        "Maybe we can deprecate this API and add a new one that has the full rendering stats?",
        "Would it be a breaking change to take a `Call.Factory` here?",
        "If it's source compatible I'd say go ahead and just change the signature, if you don't mind. 🙂 "
      ]
    },
    "profile": {
      "location": "Germany, Europe",
      "blog": "",
      "site_admin": false,
      "followers": 72,
      "following": 88
    }
  },
  "thaJeztah": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-add-logging-without-duplication",
        "title": "Add logging without duplication"
      },
      {
        "slug": "compose-avoid-ci-resource-conflicts",
        "title": "avoid CI resource conflicts"
      },
      {
        "slug": "compose-avoid-confusing-names",
        "title": "Avoid confusing names"
      },
      {
        "slug": "compose-avoid-variable-name-conflicts",
        "title": "Avoid variable name conflicts"
      },
      {
        "slug": "compose-ci-security-boundaries",
        "title": "CI security boundaries"
      },
      {
        "slug": "compose-consistent-formatting-choices",
        "title": "consistent formatting choices"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-explicit-configuration-management",
        "title": "explicit configuration management"
      },
      {
        "slug": "compose-isolate-test-dependencies",
        "title": "Isolate test dependencies"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-minimize-credential-access-scope",
        "title": "minimize credential access scope"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-optimize-docker-layer-caching",
        "title": "optimize Docker layer caching"
      },
      {
        "slug": "compose-precise-security-pattern-matching",
        "title": "precise security pattern matching"
      },
      {
        "slug": "compose-schema-changes-upstream-first",
        "title": "Schema changes upstream first"
      },
      {
        "slug": "compose-use-standard-api-fields",
        "title": "Use standard API fields"
      }
    ],
    "comments": {
      "compose-minimize-credential-access-scope": [
        "same comment as on the other open source repo; I'd prefer to have new credentials here, that are limited to just what's needed for this scan (so that we can easily rotate those if needed, and they don't provide access to things we don't want)"
      ],
      "compose-precise-security-pattern-matching": [
        "A minor optimisation could be to use `--format` (so that only the Security options are output), and/or to match `name=userns` instead of just `userns` (as that's what the daemon will return; https://github.com/moby/moby/blob/b6684a403c99aaf6be5b8ce0bef3c6650fcdcd12/daemon/info.go#L180-L182\r\n\r\n```suggestion\r\nif docker info --format '{{json .SecurityOptions}}' 2>/dev/null | grep -q 'name=userns'; then\r\n```"
      ],
      "compose-isolate-test-dependencies": [
        "Would it make sense to have a separate module for the e2e tests, so that these test dependencies don't become a dependency for the main module?\r\n\r\nWe took that approach in containerd, where we then replaced the main module  with a path, to make sure we use the code from the branch where it's needed ; https://github.com/containerd/containerd/blob/main/integration/client/go.mod#L79\r\n",
        "Thanks! So, yes, I think the \"ideal\" at some point would be to (e.g) have a docker image with the integration tests compiled in, which could be run with a compose binary and docker socket mounted; this would also allow for (e.g.) https://github.com/moby/moby to run the latest e2e/integration tests as part of CI."
      ],
      "compose-keep-code-structure-flat": [
        "Perhaps split the switch into a `switch direction`, and within each of those (from/to) do the further checks, which could be a nested `switch` if needed.\r\n\r\nCombining the check (especially if they're checking on the same variables, such as `index` is sometimes \"tricky\", and easy to overlook if there's things excluded or \"duplicated\".\r\n\r\nNote that instead of `||` you can also use `case <condition1>, <condition2>, <condition3> ...`, which (I think) is more common."
      ],
      "compose-optimize-docker-layer-caching": [
        "I _think_ this `ENV` is only used by `script/build/linux-entrypoint`, so better to move it lower to prevent unwanted cache-busts",
        "very minor nit; this file likely doesn't change much, so could be moved before the `COPY . . `, or even before the `COPY requirements.txt` (ordering from \"less frequently changing\" to \"most frequently changing\").\r\n\r\nIf this script (and `ENTRYPOINT` below is only used for `runtime`, might consider moving both to the start of the `runtime` stage)"
      ],
      "compose-network-api-precision": [
        "I think the trick also requires `--opt type=none` "
      ],
      "compose-ci-security-boundaries": [
        "This is a 3rd party action, so potentially less \"trusted\"; wondering if this is one that we should pin to a commit? (also make sure that we evaluate the changes in the release).\r\n\r\n\r\nDiff since last (v1.9.0) v1 release (but perhaps there's been other v1 updates since it was added); https://github.com/tibdex/github-app-token/compare/v1.9.0...v2.1.0",
        "Thanks!"
      ],
      "compose-consistent-formatting-choices": [
        "I'd keep the `pip install` separate; it likely won't overwrite files that were installed by `apk add`, so doing it in a separate step will add a new layer, but won't make the image bigger.\r\n\r\nAlso worth to keep the same convention as for `apk add`, and split the packages that will be installed to one-per-line, sorted alphabetically (it makes the Dockerfile longer, but can help making it more maintainable)\r\n\r\n```suggestion\r\nRUN pip install \\\r\n    tox==2.1.1 \\\r\n    virtualenv==16.2.0\r\n```\r\n\r\nIf these packages are expected to be updated individually, could even be two `RUN` lines",
        "`-f` specifies a path to the Docekrfile, so good practice to put quotes around it (even though that will likely not be hit here\r\n\r\n```suggestion\r\ndocker build -f \"${DOCKERFILE}\" -t \"${TAG}\" --target \"${DOCKER_BUILD_TARGET}\" .\r\n```"
      ],
      "compose-avoid-confusing-names": [
        "`Docker-Compose` (both capital) is definitely incorrect (should either be `docker-compose` (name of the binary), or `Docker Compose`). Perhaps avoid the name altogether and use something similar as the `docker` cli uses (`Print version information and quit`)\r\n\r\n```suggestion\r\n      version            Show version information and quit\r\n```\r\n\r\n(quit/exit, not sure what's clearer)? `curl` uses `quit`;\r\n\r\n```\r\n -V, --version       Show version number and quit\r\n```\r\n\r\n"
      ],
      "compose-explicit-configuration-management": [
        "Did it automatically update this one, or did it still allow go 1.21.0 here?\r\n\r\nI generally try to treat this one the same as other dependencies; list the minimum required version, and only update if it's _impossible_ to use with older versions; see https://github.com/containerd/containerd/pull/10596#discussion_r1721294997",
        "Because 26.1.1 is lower than 26.1.3. Thank Go for inventing pseudo versions and not understanding release branches",
        "before we have a tag, you can temporarily add replace rules; see https://github.com/docker/buildx/pull/2499",
        "Do we have PRs for this in the upstream repositories? Looks like compose is using master / v0.13.x as dependency, so if we could get the fix merged in upstream, that'd be good, I think?",
        "Could we set `GO111MODULE=auto` (or `on`), or `-mod=<what is it?>` in the makefile? That way we wouldn't have to think about setting that in `docker-ce-packaging`.",
        "But.. I have to admit that I kinda agree with https://github.com/docker/compose/pull/9776#discussion_r952832902, and wonder to what extend we need to have this complicated auto-detection.\r\n\r\nI think common scenarios would be either;\r\n\r\n- `docker` is installed (and `docker buildx` would be available as well for regular installs)\r\n- it's run in GitHub actions with only `buildx` installed; in that case we can set `BUILDX_CMD=buildx`\r\n- if neither is true, then I think a `/bin/sh: docker: not found` error may be \"just fine\" (after all, we're also not doing similar things to detect if `go` is installed, or `make`, or `git`)\r\n\r\nIf we agree with the above, just a;\r\n\r\n```make\r\nBUILDX_CMD ?= docker buildx\r\n```\r\n\r\nwould cover that scenario\r\n\r\n(feedback / thoughts welcome!)\r\n",
        "Thanks! Sorry for being nit-picky there (I can see some value for auto-detection in other scenarios), just looking \"can we simplify things (within reason)\"? In the end, the repository would have a \"how to build\" with some prerequisites, so if things fail, users should just \"read the manual\" 😂 ",
        "Looks like we're also updating various dependencies here; were these needed for Go 1.17? (otherwise it's good practice to do this separately)."
      ],
      "compose-avoid-variable-name-conflicts": [
        "Perhaps it would be good to use a different variable name for this; I know that some tools (`rpm`, `deb` packaging) also set `LDFLAGS` as environment variable, and in those cases it's important to reset them.\r\n\r\nWe could use something similar a containerd, which uses `GO_LDFLAGS` (as well as some other `GO_` prefixed variables to prevent conflicts); https://github.com/containerd/containerd/blob/be91a219c2ac5e65c00bbe85c5dff0827d41958b/Makefile#L92-L102\r\n",
        "It's often a good idea to use a different name for the `ARG` than for the `ENV`. Both act in the same \"space\" (both are set as (environment) variables, which can lead to run situations where the `ENV` is always overridden by the `ARG` (I can find some examples, don't have them at hand)\r\n\r\nSo, might want to consider, e.g.;\r\n\r\n```Dockerfile\r\nARG GIT_COMMIT=unknown\r\nENV DOCKER_COMPOSE_GITSHA=$GIT_COMMIT\r\n```\r\n\r\n(or vice-versa)"
      ],
      "compose-add-logging-without-duplication": [
        "Wondering; should we log something if the service _does_ have an `image:` specified, but either doesn't have `build:` or image has a _digest_ set (to give some clue why the image for a service wasn't pushed)?"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "Do we know what patches are in this fork, and if they were rejected upstream? I know `fsnotify` has had some time where maintenance was slow, but I think it improved in that respect (and I _think_ we have some maintainers on it that are also maintainers for moby/moby)",
        "Ah, thanks! Yes, saw the comment later on, and saw that (github indicated \"10\" commits in the fork);\r\n\r\nhttps://github.com/fsnotify/fsnotify/compare/main...tilt-dev:fsnotify:main\r\n\r\nPinning to a commit from upstream SGTM (short term). I'm mostly trying to avoid having 2 forks of the same dependency (as I know we have fsnotify as dependency in our tree already).\r\n\r\nIf someone has some cycles to spare to look what patches are not (yet) in upstream, we could contribute them there.\r\n\r\n@cpuguy83 were you a maintainer on that repo? Or do I misremember that? (Otherwise I _think_ there's some familiar people on it, that we may try to reach out to to ask for a (pre-)release)."
      ],
      "compose-schema-changes-upstream-first": [
        "Note that we can't update the 3.7 schema, as it's already been released, so to add this property to the schema, it probably has to be added to the upcoming 3.9 schema in https://github.com/docker/cli/blob/master/cli/compose/schema/data/config_schema_v3.9.json first"
      ],
      "compose-environment-variable-validation": [
        "Perhaps could also be worth (if compose reads the cli config) to consider either an option in `features`, or `plugins` (plugins allows plugin-specific options to be set), which would allow opt-in/opt-out of this without having to use an env-var;\r\nhttps://github.com/docker/cli/blob/9861ce90fd6b8ddca19db5f803dcbef9a583e9e1/cli/config/configfile/file.go#L42-L44\r\n\r\n```go\r\n\tPlugins              map[string]map[string]string `json:\"plugins,omitempty\"`\r\n\tAliases              map[string]string            `json:\"aliases,omitempty\"`\r\n\tFeatures             map[string]string            `json:\"features,omitempty\"`\r\n```\r\n\r\n(in addition to an env-var probably)",
        "Yeah, my thinking here was that the cli-config would more easily allow this to be set as a default, which could also allow (e.g.) it to be set through docker desktop \"settings\"."
      ],
      "compose-avoid-ci-resource-conflicts": [
        "Do we actually need `--privileged` for this container? I see it bind-mounts `docker.sock`, which means that all `docker` commands will actually run against the docker daemon running on the host (not a `dockerd` daemon running inside the container (which _would_ require privileged)",
        "Actually wondering if we need a container here at all, because it's running a container, just to run a script, that uses the `docker` CLI inside the container to start new containers on the host 🤔 "
      ],
      "compose-use-standard-api-fields": [
        "Wondering why you didn't use the `Status` field that's returned by the API (which is the field that's used by the `docker` CLI;\r\n\r\n```bash\r\ncurl --unix-socket /var/run/docker.sock \"http://localhost/containers/json\" | jq .\r\n```\r\n\r\n```json\r\n\r\n  {\r\n    \"Id\": \"82950c6535204a462c8a3c1f175408b456fbb91971d2d21a33ba04c8b91c74fd\",\r\n    \"Names\": [\r\n      \"/cranky_keldysh\"\r\n    ],\r\n    \"Image\": \"libnetworkbuild\",\r\n    \"ImageID\": \"sha256:bc6bbc6a0032300d8818182eff0101ecb7af2fc1fb21da6f290943c286946a1e\",\r\n    \"Command\": \"make unit-tests-local\",\r\n    \"Created\": 1573091888,\r\n    \"Ports\": [],\r\n    \"Labels\": {},\r\n    \"State\": \"running\",\r\n    \"Status\": \"Up 5 minutes\",\r\n    \"HostConfig\": {\r\n      \"NetworkMode\": \"default\"\r\n    },\r\n    \"NetworkSettings\": {\r\n      \"Networks\": {\r\n        \"bridge\": {\r\n          \"IPAMConfig\": null,\r\n          \"Links\": null,\r\n          \"Aliases\": null,\r\n          \"NetworkID\": \"5a59d43d598f910579535ffb2cbbb4d0987807d7b5593c264c83337c4220ec1a\",\r\n          \"EndpointID\": \"bf8b6b629b353988d047bcd0bbb897249d7a73a7811b2210b783338e6a975cd9\",\r\n          \"Gateway\": \"172.17.0.1\",\r\n          \"IPAddress\": \"172.17.0.5\",\r\n          \"IPPrefixLen\": 16,\r\n          \"IPv6Gateway\": \"\",\r\n          \"GlobalIPv6Address\": \"\",\r\n          \"GlobalIPv6PrefixLen\": 0,\r\n          \"MacAddress\": \"02:42:ac:11:00:05\",\r\n          \"DriverOpts\": null\r\n        }\r\n      }\r\n    },\r\n    \"Mounts\": [\r\n      {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"/Users/sebastiaan/projects/libnetwork\",\r\n        \"Destination\": \"/go/src/github.com/docker/libnetwork\",\r\n        \"Mode\": \"\",\r\n        \"RW\": true,\r\n        \"Propagation\": \"rprivate\"\r\n      }\r\n    ]\r\n  }\r\n]\r\n```",
        "should the new option deprecate the old (`--no-ansi`) one? (at least making them conflicting options, as (I think) `--no-ansi` is the equivalent of `--ansi=never`?)\r\n\r\nFor the `docker` cli, if there's an option that cannot be removed we usually _hide_ the option (to discourage use), and (depending on the case) print a deprecation warning if used (but keep it functional if needed).\r\n\r\nAlso looking if we should align the UX to the `--progress` option used on `docker build`;\r\n\r\n```\r\n      --progress string         Set type of progress output (auto, plain, tty). Use plain to show container output (default \"auto\")\r\n```"
      ]
    },
    "profile": {
      "location": "Netherlands",
      "company": "thaJeztah",
      "blog": "",
      "twitter_username": "thaJeztah",
      "site_admin": false,
      "followers": 1803,
      "following": 35
    }
  },
  "PeterSchafer": {
    "repos": [
      "snyk/cli"
    ],
    "entries": [
      {
        "slug": "cli-api-interface-design",
        "title": "API interface design"
      },
      {
        "slug": "cli-balance-concurrent-operations",
        "title": "Balance concurrent operations"
      },
      {
        "slug": "cli-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "cli-configuration-naming-consistency",
        "title": "Configuration naming consistency"
      },
      {
        "slug": "cli-defensive-shell-script-configuration",
        "title": "defensive shell script configuration"
      },
      {
        "slug": "cli-document-intent-and-reasoning",
        "title": "Document intent and reasoning"
      },
      {
        "slug": "cli-graceful-error-handling",
        "title": "Graceful error handling"
      },
      {
        "slug": "cli-handle-all-errors-explicitly",
        "title": "Handle all errors explicitly"
      },
      {
        "slug": "cli-maintain-build-environment-parity",
        "title": "maintain build environment parity"
      },
      {
        "slug": "cli-maintain-cicd-boundaries",
        "title": "maintain CI/CD boundaries"
      },
      {
        "slug": "cli-optimize-ci-resource-allocation",
        "title": "optimize CI resource allocation"
      },
      {
        "slug": "cli-optimize-variable-declarations",
        "title": "Optimize variable declarations"
      },
      {
        "slug": "cli-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "cli-prevent-silent-test-failures",
        "title": "prevent silent test failures"
      },
      {
        "slug": "cli-separate-build-from-runtime",
        "title": "separate build from runtime"
      },
      {
        "slug": "cli-synchronize-configuration-values",
        "title": "synchronize configuration values"
      },
      {
        "slug": "cli-use-centralized-configuration-access",
        "title": "Use centralized configuration access"
      },
      {
        "slug": "cli-use-centralized-loggers",
        "title": "Use centralized loggers"
      },
      {
        "slug": "cli-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "cli-use-descriptive-parameter-names",
        "title": "Use descriptive parameter names"
      },
      {
        "slug": "cli-use-file-locks",
        "title": "Use file locks"
      },
      {
        "slug": "cli-use-optional-chaining",
        "title": "Use optional chaining"
      },
      {
        "slug": "cli-use-secure-hash-functions",
        "title": "Use secure hash functions"
      },
      {
        "slug": "cli-validate-environment-variables-early",
        "title": "validate environment variables early"
      },
      {
        "slug": "cli-validate-security-configurations",
        "title": "Validate security configurations"
      },
      {
        "slug": "cli-write-actionable-documentation",
        "title": "Write actionable documentation"
      }
    ],
    "comments": {
      "cli-validate-environment-variables-early": [
        "Question: What is the developer experience when this Environment Variable is not specified? Is it worth to explicitly warn when it is empty?\r\n\r\nNitpick: Maybe move this to where `LS version:` is printed 😄 ",
        "This is the error I see, right now. \r\n<img width=\"613\" alt=\"image\" src=\"https://github.com/user-attachments/assets/05a0cb4b-3219-487a-8b4a-ba2a93529b31\" />\r\n\r\nCan we make the test a bit more descriptive and actionable? I'm worried about all the asks we will be getting because someone is using a dev CLI without the URL set.\r\n",
        "Maybe we fail the command execution instead?!",
        "yes, it fails, but it seems to do a lot of things until it fails with the message right now. So a proposal could be to do a check for the variable and fail early and actionable. Does this make sense?",
        "for example [here](https://github.com/snyk/cli-extension-iac/blob/0a63172561937695026101838e018d5574618785/internal/commands/iactest/iactest.go#L69)",
        "Question: DEBUG is a very generic environment variable name. There are good chances to have this conflicting somewhere for example in our CI/CD. Should we maybe solve enabling the debug build via an explicit target?",
        "Suggestion: How about using the `.mvnrc` file to determine the major version, instead of having it hardcoded here. Take a look at https://github.com/snyk/cli/blob/master/scripts/create-build-image.sh#L9C3-L9C32 for example"
      ],
      "cli-balance-concurrent-operations": [
        "Suggestion: Let's be a bit more pessimistic to not increase the load too much and set it to a lower value first. Let's start with 2."
      ],
      "cli-validate-security-configurations": [
        "Question: This looks like a change of behaviour to me. Previously if a token was specified sessionToken contained the plain token value, now it contains `token <tokenvalue>`. Is this considered in the plugins that consume the sessionToken value?",
        "Great! Thanks for the detailed explanation! "
      ],
      "cli-use-centralized-configuration-access": [
        "Issue: with the new gaf version, the temp directory is a configuration value. Instead of using this helper function directly, we should be using the configuration now, otherwise the behaviour wouldn't be consistent.\r\n\r\n```suggestion\r\n\treturn c.globalConfig.GetString(configuration.TEMP_DIR_PATH)\r\n```",
        "Issue: with the new gaf version, the temp directory is a configuration value. Instead of using this helper function directly, we should be using the configuration now, otherwise the behaviour wouldn't be consistent.\r\n\r\n```suggestion\r\n\treturn config.GetString(configuration.TEMP_DIR_PATH)\r\n```",
        "Issue: same as above, please use the configuration value for temp dir.",
        "Suggestion: Please use the configuration to access values, see [here](https://github.com/snyk/go-application-framework/blob/main/pkg/local_workflows/auth_workflow.go#L84). This way, there is no need for the implementation to know where the values come from and how they exactly need to be named.",
        "unfortunately not, please take a closer look at the linked code snippet. "
      ],
      "cli-pin-dependency-versions": [
        "Question: Did you miss to add the changes in the package.json?",
        "Thanks for updating. I do have a follow up question though, how come that the added test didn't fail? I would expect that the test doesn't succeed if the dependency is not updated.",
        "Suggestion: Personally I think that the usage of `^` is not a good practice as it means that we can't reproduce individual builds. Therefore I would like to propose that we use this chance to get rid of them step by step.",
        "withdrawn :) "
      ],
      "cli-synchronize-configuration-values": [
        "Because the documentation says so.\r\n> Update all NodeJS versions to match the NodeJS versions used in pkg."
      ],
      "cli-use-file-locks": [
        "This would probably be in addition to restoring the file I assume.",
        "@bastiandoetsch we can use flock but we would need to place the lock file outside of the cache directory, which we are going to create. We can do this, just that we need to consider this in the docs about filesystem access etc.",
        "And yes, I agree, the user shouldn't be bothered at all. Per default the CLI determines the cache directory based on OS defaults, these directories exists and we just place a subfolder inside of it, which we can create relatively safe with mkdir. so per default, the assumption is that this error would never happen.\r\n\r\nIt might happen, if a user specifies a custom path.",
        "Definitely Possible! "
      ],
      "cli-prevent-silent-test-failures": [
        "Suggestion: asserting against stdout is always a bit tricky as it is not a strong check or actually too strong as it breaks with minor rewrite of a message, which increases the maintenance. There is another way to check, which is using the instrumentation data, which would actually be a more complete check of the scenario.",
        "Issue: This  changes the expectation, `> 0` is not the same as `==2`.  You should be able to filter the requests and leave the expectations unchanged.",
        "Suggestion: This test should ensure that the content is a json data structure, for example by using a library like [this](https://www.npmjs.com/package/jsonparse). It should also ensure that the length of stdoutBuffer is greater 0.",
        "Suggestion: move this assert a bit up before trying to access the output file. Running the test with a non fixed CLI should fail in a planned way and not just because the file doesn't exists.",
        "Question: Shouldn't this be equal instead of contains?"
      ],
      "cli-defensive-shell-script-configuration": [
        "Good idea! Thank you! Please take another look!"
      ],
      "cli-use-descriptive-parameter-names": [
        "Question: win-something? What is it?",
        "Oh got it, it is just to trigger another case. This is a bit confusing. Can we please at least have a comment what we do here?",
        "Actually it might be better to rename the parameter from executor to something like OS, since executor is an existing construct that has a special meaning."
      ],
      "cli-api-interface-design": [
        "Suggestion: How about adding the CertificateLocation to ProxyInfo and thereby reducing the interface here.",
        "Without the interface it would be impossible to implement other authentication handlers in the future."
      ],
      "cli-optimize-variable-declarations": [
        "Suggestion: how about removing the port here, since the information is now available via ProxyInfo.",
        "issue: imported but not used"
      ],
      "cli-use-secure-hash-functions": [
        "We can optimize and just do it when `--debug` is used",
        "yes, salt would not be good 😄  the token-hash is being truncated to 16 bytes to avoid any reverse engineering of the actual token, see the lines below."
      ],
      "cli-graceful-error-handling": [
        "Suggestion: Let's revert this change here, since it is better to fail than send an empty string.",
        "I removed the duplicated logic.\r\nI realized that `err.jsonPayload` was already there, so I'm not changing this.",
        "Issue: From my understanding I think we need the non-pretty-print part of the original solution as well and only if this fails as well, we should fallback on the new solution. Removing the original code will cause issues for use cases of --json, which worked due to the non-pretty-print part.",
        "The associated unit test needs to be fixed and an additional one needs to be created.",
        "Question: Don't we need to move this out of the else case?"
      ],
      "cli-handle-all-errors-explicitly": [
        "Issue: Mimicking the gaf network stack behaviour, we need to use the error returned from the errorhandler and only if this error is not nil set the terminate header.",
        "Please use fmt.Errorf() the following way, see [the documentation](https://pkg.go.dev/fmt@go1.19.5#Errorf) for the reasoning\r\n`fmt.Errorf(\"Cache directory path is invalid: %w\", err)`",
        "done!",
        "done! used defer"
      ],
      "cli-use-descriptive-names": [
        "nitpick: Naming variables the same way as data structures can be confusing. Maybe just rename the variables here?"
      ],
      "cli-optimize-ci-resource-allocation": [
        "Caching only relates to the downloaded installers, they still need to be executed to install the applications. The different versions of the command install different subsets of the tools for efficiency reasons."
      ],
      "cli-use-optional-chaining": [
        "As I understand the case, not having vulnerabilities here is an error case. This means if they are missing, the comparison will fail. The intention is to not make this helper function crash. Appropriate error handling must and as far as I see, does happen outside."
      ],
      "cli-write-actionable-documentation": [
        "suggestion: replace directives are not necessarily a debugging step, maybe we separate them from the debugging docs and add them into a section about development in golang?"
      ],
      "cli-use-centralized-loggers": [
        "We want/need the information logged! \r\nBut rather than using the debug parameter the logger actually gets disabled centrally.",
        "Suggestion: try to use DebugLogger as zerolog logger, this will be possible every except of two places. For these places you could add an additional property here.",
        "Issue: you need to use the centrally supplied logger! It is not correct to create a new one here.",
        "Suggestion: I would use both logger types from the context instead of creating a ToZeroLogDebug below. The logic is at one place and if we want to remove one day the log.Logger usage, it will be easier to find. ",
        "Issue: this needs to use a log.Logger that uses a writer to zerolog `ToZeroLogDebug`! Otherwise, the log.Default logger doesn't respect the debug configuration and logger configuration that is centrally done.",
        "Issue: this needs to use a log.Logger that uses a writer to zerolog ToZeroLogDebug! Otherwise, the log.Default logger doesn't respect the debug configuration and logger configuration that is centrally done.\r\n\r\nSame as below!",
        "Suggestion: replace fmt.Println() by c.DebugLogger.Println() and add the currentPath"
      ],
      "cli-separate-build-from-runtime": [
        "Suggestion: We should build the dependencies when building the docker images and not in the script that shall just sign."
      ],
      "cli-maintain-cicd-boundaries": [
        "Issue: on main, there shouldn't be any release notes."
      ],
      "cli-document-intent-and-reasoning": [
        "Nitpick: a short comment what the new flag does would be helpful for future us."
      ],
      "cli-comprehensive-test-coverage": [
        "Suggestion: There is an opportunity to move this in a small function and write a test for it ;) ",
        "Suggestion: Use `t.Setenv()` instead of `os.Setenv()`",
        "Interesting point, to be honest I'm not completely sure about the root cause of why sleep is necessary. I just had the build failing on a circle runner while working locally and on other runners. \r\nMy assumption is that the final comparison of the modification time might not be the strongest check. Looking at time resolution etc, I was assuming that the sleep helps to ensure that modification times would be definitely different.\r\nLooking at the comment it seems misleading. I'm definitely changing the comment.",
        "done!",
        "Suggestion: extend the test to cover both cases more than 5 elements in the cache and less. maybe add second test case."
      ],
      "cli-configuration-naming-consistency": [
        "Question: is this on purpose that the log says `TEST_CONFIG_FILE` and the env var used is `SNYK_CONFIG_FILE`?",
        "Is `TEST_CONFIG_FILE` used somewhere?"
      ],
      "cli-maintain-build-environment-parity": [
        "Question: Do we really need a differentiation between build and build-local? I'm worried that local and CI build get out of sync at some point. I would like to see everyone using the same command to build.",
        "Just to expand the thought, I wonder if we can make the usage of the virtual environment invisible to the `make` user.",
        "it is an optimization to not always use clean-install. The pipeline itself does `npm ci` at the beginning, in this stage, we just want to install what is missing if anything."
      ]
    },
    "profile": {
      "location": "Cologne ",
      "blog": "",
      "site_admin": false,
      "followers": 10,
      "following": 0
    }
  },
  "Monokaix": {
    "repos": [
      "volcano-sh/volcano"
    ],
    "entries": [
      {
        "slug": "volcano-add-explicit-nil-checks",
        "title": "Add explicit nil checks"
      },
      {
        "slug": "volcano-algorithm-explanation-clarity",
        "title": "Algorithm explanation clarity"
      },
      {
        "slug": "volcano-always-check-errors",
        "title": "Always check errors"
      },
      {
        "slug": "volcano-avoid-redundant-operations",
        "title": "Avoid redundant operations"
      },
      {
        "slug": "volcano-comprehensive-test-structure",
        "title": "Comprehensive test structure"
      },
      {
        "slug": "volcano-document-security-implications",
        "title": "Document security implications"
      },
      {
        "slug": "volcano-document-technical-details-clearly",
        "title": "Document technical details clearly"
      },
      {
        "slug": "volcano-extract-configuration-constants",
        "title": "Extract configuration constants"
      },
      {
        "slug": "volcano-extract-reusable-functions",
        "title": "Extract reusable functions"
      },
      {
        "slug": "volcano-implement-simulation-testing",
        "title": "Implement simulation testing"
      },
      {
        "slug": "volcano-justify-default-enablement",
        "title": "Justify default enablement"
      },
      {
        "slug": "volcano-optimize-algorithmic-efficiency",
        "title": "Optimize algorithmic efficiency"
      },
      {
        "slug": "volcano-protocol-specific-network-discovery",
        "title": "Protocol-specific network discovery"
      },
      {
        "slug": "volcano-review-security-permissions",
        "title": "Review security permissions"
      },
      {
        "slug": "volcano-standardize-configuration-formats",
        "title": "Standardize configuration formats"
      },
      {
        "slug": "volcano-use-controlled-concurrency-patterns",
        "title": "Use controlled concurrency patterns"
      },
      {
        "slug": "volcano-use-descriptive-consistent-naming",
        "title": "Use descriptive consistent naming"
      },
      {
        "slug": "volcano-use-descriptive-naming",
        "title": "Use descriptive naming"
      },
      {
        "slug": "volcano-use-structured-logging",
        "title": "Use structured logging"
      }
    ],
    "comments": {
      "volcano-use-descriptive-consistent-naming": [
        "Here is just an example, and the API definition is below."
      ],
      "volcano-justify-default-enablement": [
        "Do we need this?"
      ],
      "volcano-implement-simulation-testing": [
        "functional correctness is also our consideration.",
        "Yeah, it is the visualization of the scheduling process and results. You can refer to https://github.com/kubernetes-sigs/kube-scheduler-simulator?tab=readme-ov-file#simulator."
      ],
      "volcano-add-explicit-nil-checks": [
        "Other place in this file like https://github.com/volcano-sh/volcano/blob/2efe281384f2ea3009ae4286480ade11ea514fc7/pkg/scheduler/plugins/predicates/predicates.go#L253 also has this assert, do we also need check wheher it's nil?",
        "Why these are removed?",
        "Here will set nil node, so we should keep nil node check.\r\nhttps://github.com/volcano-sh/volcano/blob/6b32f7bde9db76f40ddb19d253b06521ec7986ef/pkg/scheduler/cache/event_handlers.go#L214",
        "Will `job.PodGroup.GetAnnotations()` be a nil map?",
        "So how to avoid panic?"
      ],
      "volcano-standardize-configuration-formats": [
        "Please keep the format and fields consistent with default configMap."
      ],
      "volcano-comprehensive-test-structure": [
        "Should format the test table, please refer to other ut.",
        "like \r\n```go\r\n\t\t{\r\n\t\t\tname: \"test1\",\r\n\t\t\targs: args{framework.Arguments{\r\n\t\t\t\t\"ResourceStrategyFitPlusWeight\": 10,\r\n\t\t\t\t\"resources\": map[string]interface{}{\r\n\t\t\t\t\t\"cpu\": map[string]interface{}{\r\n\t\t\t\t\t\t\"type\":   \"MostAllocated\",\r\n\t\t\t\t\t\t\"weight\": 1,\r\n\t\t\t\t\t},\r\n\t\t\t\t\t\"memory\": map[string]interface{}{\r\n\t\t\t\t\t\t\"type\":   \"LeastAllocated\",\r\n\t\t\t\t\t\t\"weight\": 2,\r\n\t\t\t\t\t},\r\n\t\t\t\t},\r\n\t\t\t}},\r\n\t\t\twant: ResourceStrategyFit{\r\n\t\t\t\tResourceStrategyFitWeight: 10,\r\n\t\t\t\tResources: map[v1.ResourceName]ResourcesType{\r\n\t\t\t\t\t\"cpu\": {\r\n\t\t\t\t\t\tType:   config.MostAllocated,\r\n\t\t\t\t\t\tWeight: 1,\r\n\t\t\t\t\t},\r\n\t\t\t\t\t\"memory\": {\r\n\t\t\t\t\t\tType:   config.LeastAllocated,\r\n\t\t\t\t\t\tWeight: 2,\r\n\t\t\t\t\t},\r\n\t\t\t\t},\r\n\t\t\t},\r\n\t\t},\r\n```"
      ],
      "volcano-document-security-implications": [
        "Already add and moved to user guide."
      ],
      "volcano-avoid-redundant-operations": [
        "Why remove this?",
        "call `calculateWeight` once is enough.",
        "This logic should put in `createNormalPodPGIfNotExist`, because when the podgroup is already existed, there is no need to calculate the min member, it will request APIServer which is a heavy operation.\r\nAnd there is also a qustion mentioned here https://github.com/volcano-sh/volcano/issues/3970#issuecomment-2652862212",
        "Yeah, DaemonSet and Job should also be considered.",
        "Also this should also be considered https://github.com/volcano-sh/volcano/issues/3970#issuecomment-2638750095",
        "done"
      ],
      "volcano-optimize-algorithmic-efficiency": [
        "Does the modification in `allocateResourcesForTasks` func just want to get the hyperNode name of current task, if that's true, I think we should implement this in somewhere else because there is already the same logic in `allocateResourceForTasksWithTopology`, for instance we can abstract this in `allocateResources`.",
        "Why put device score in batchNodeOrder?",
        "Please make it score in parallel like other batchNodeOrder plugin: )"
      ],
      "volcano-extract-reusable-functions": [
        "There is already a same func in pkg/scheduler/plugins/predicates/predicates.go.",
        "Better abstract a function here like jobTerminated to increase code readability, and so does pod.",
        "These are duplicated logic with `validateQueueDeleting`, we can abstract a func."
      ],
      "volcano-use-controlled-concurrency-patterns": [
        "Should initiate once to avoid memory leak."
      ],
      "volcano-use-structured-logging": [
        "Yeah you're right, done.",
        "Please use klog.InfoS and ErrorS format, and then `=` is not needed. And please modify other places too.",
        "Please use klog.InfoS() nethod.",
        "This klog info is not so clear, we should mention that remain minmember =1, or users will don't  know the value finally set."
      ],
      "volcano-extract-configuration-constants": [
        "Abstract 10 as a default const is better."
      ],
      "volcano-protocol-specific-network-discovery": [
        "That's a good catch, currentlt roce type is not supported, and we have no roce environment to test this feature. Do you have any idea what fields should be filled in roce environment？",
        "Ok，I see. This interface is extensible enough to cover RoCE in the future, if we need a daemonset we can add it in RoCE scenario."
      ],
      "volcano-always-check-errors": [
        "When `sc.executePreBinds` failed, `sc.Bind` will not be executed, and also `resyncTask` will not be executed, is this reasonable?"
      ],
      "volcano-algorithm-explanation-clarity": [
        "Here can give a more concrete scenario, like pytorch job? master pod using CPU should disperse to avoid hot node while worker pods using GPU should aggregate to reduce resource fragment?",
        "```suggestion\r\n- Users expect different resource allocation strategies to be applied based on resource types. For example, in PyTorch jobs, the master pod (which uses CPU) should be distributed to avoid node hotspots, while worker pods (which use GPU) should be aggregated to minimize resource fragmentation.\r\n```",
        "```suggestion\r\n1. If a Job is being scheduled for the very first time, all HyperNodes that need to be scored will get a score of 0 and then return right away. The name of the HyperNode where the Job eventually gets scheduled successfully will be recorded in the Job's annotations under the key JobAllocatedHyperNode\r\n```",
        "Please also change to \"**plugin:** network-topology-aware\" in line 568.",
        "Besides the `publish` func of dra, we should also add prefilter, filter, scroe, reserved callback here and add postbind logic in after task bind successfully, please also add these details: )",
        "And draw a picture to show the whole process is better."
      ],
      "volcano-use-descriptive-naming": [
        "Here `!nodeIsNotReady` means node ready, please correct it.",
        "predict and score are both in one function, but funcname is predict, a little wired here. Can we seperate them?",
        "If so, we'd better rename this function name."
      ],
      "volcano-review-security-permissions": [
        "Agent need more privilege, this need to be verified."
      ],
      "volcano-document-technical-details-clearly": [
        "Should add comments to explain all the means of these arguments.",
        "Should be Klaus Ma, Kevin Wang "
      ]
    },
    "profile": {
      "company": "@Huawei",
      "blog": "monokaix.github.io",
      "site_admin": false,
      "followers": 46,
      "following": 26
    }
  },
  "gruebel": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-backward-compatible-parameters",
        "title": "Backward compatible parameters"
      },
      {
        "slug": "checkov-choose-optimal-algorithms",
        "title": "Choose optimal algorithms"
      },
      {
        "slug": "checkov-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "checkov-comprehensive-security-scanning",
        "title": "Comprehensive security scanning"
      },
      {
        "slug": "checkov-configure-security-scanners-completely",
        "title": "Configure security scanners completely"
      },
      {
        "slug": "checkov-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "checkov-document-configuration-consistently",
        "title": "Document configuration consistently"
      },
      {
        "slug": "checkov-document-configuration-options",
        "title": "Document configuration options"
      },
      {
        "slug": "checkov-ensure-dependency-compatibility",
        "title": "Ensure dependency compatibility"
      },
      {
        "slug": "checkov-meaningful-identifier-names",
        "title": "Meaningful identifier names"
      },
      {
        "slug": "checkov-precise-configuration-validation",
        "title": "Precise configuration validation"
      },
      {
        "slug": "checkov-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "checkov-safe-dictionary-access",
        "title": "Safe dictionary access"
      },
      {
        "slug": "checkov-safe-dictionary-navigation",
        "title": "Safe dictionary navigation"
      },
      {
        "slug": "checkov-strategic-error-handling",
        "title": "Strategic error handling"
      },
      {
        "slug": "checkov-strategic-exception-management",
        "title": "Strategic exception management"
      },
      {
        "slug": "checkov-support-all-target-environments",
        "title": "Support all target environments"
      },
      {
        "slug": "checkov-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "checkov-use-pytest-best-practices",
        "title": "Use pytest best practices"
      },
      {
        "slug": "checkov-validate-configurations-correctly",
        "title": "Validate configurations correctly"
      },
      {
        "slug": "checkov-write-pythonic-code",
        "title": "Write pythonic code"
      }
    ],
    "comments": {
      "checkov-choose-optimal-algorithms": [
        "you could also create a `defaultdict`, then you don't have to this check manually \r\n```python\r\ndirs_to_definitions = defaultdict(list)\r\ndirs_to_definitions[dir_path].append({tf_definition_key: tf_value})\r\n```",
        "how about making this a `set()` then you don't need to transform it to a set and back to list 🙂 ",
        "also thought the same, but I think it is a bigger change, because of the typing mismatch.",
        "```suggestion\r\n            for field, value in each[\"change\"][\"before\"].items():\r\n                if value != each[\"change\"][\"after\"].get(field):\r\n```",
        "also make sure to skip the field names `__startline__` and `__endline__`, there is aconstant `LINE_FIELD_NAMES` which stores them as a set.",
        "just double checking, this has no bad side effect, because we would create more often edges, right?",
        "if we for some reason get more than 1 `target_variables`, we didn't create an edge before and just continued. If the tests are passing, then it is probably not an issue 😄 "
      ],
      "checkov-support-all-target-environments": [
        "this will not work. this lock file was created with Python 3.10+ therefore any CI jobs running on 3.8 or 3.9 will fail"
      ],
      "checkov-document-configuration-options": [
        "move this to the `Settings` block as an env var",
        "nice catch!"
      ],
      "checkov-use-appropriate-logging-levels": [
        "```suggestion\r\n            logging.debug(f\"OpenAI request returned: {completion}\")\r\n```\r\nor something similar to have a direct context. Also debug level is more than enough.\r\n",
        "instead of print use `logging.info(...)`",
        "just thinking about the log level, if maybe `info` is enough, depends on how critical it is to the user. "
      ],
      "checkov-use-pytest-best-practices": [
        "the parent calss `TestBaseSolver` comes with a couple of convenient functions, which I didn't plan to duplicate, but I will double check what's the effort 🙂 "
      ],
      "checkov-document-configuration-consistently": [
        "move this to the `Settings` block as an env var",
        "nice catch!"
      ],
      "checkov-write-pythonic-code": [
        "```suggestion\r\n                if mod['Key']:\r\n```\r\nit is faster to check for truthiness, which means it is not empty.",
        "```suggestion\r\n        self._address_to_tf_vertex_map = {\r\n            vertex.attributes[TF_PLAN_RESOURCE_ADDRESS]: vertex\r\n            for vertex in self.tf_graph.vertices\r\n            if vertex.block_type == BlockType.RESOURCE:\r\n        }\r\n```\r\nI think you can do it via a dict comprehension",
        "```suggestion\r\n                if 'source_arn' in conf or 'source_account' in conf:  # If either of these are set, we're good and the check should pass.\r\n```\r\nno need to explicitly add `.keys()` because it is the default when doing a lookup in a dict."
      ],
      "checkov-preserve-api-compatibility": [
        "```suggestion\r\n                if runner.graph_manager:\r\n                    check_type_to_graph = {runner.check_type: runner.graph_manager.get_reader_endpoint()}\r\n                    return report, runner.check_type, runner.graph_manager.get_reader_endpoint()\r\n                return report, None, None\r\n```\r\nhow about returning all as a normal tuple, then you can check if both are not `None` and update `full_check_type_to_graph[check_type] = reader_endpoint`"
      ],
      "checkov-strategic-error-handling": [
        "@mikeurbanski1 any thoughts about raising an exception here or should we just log it and return the normal URL?",
        "do we really want to raise an exception or just log a message?",
        "in theory it could, but this would mean something went wrong during the context creation. But I will add a check and log a message."
      ],
      "checkov-configure-security-scanners-completely": [
        "we usually recommend to run docker with `--tty` for better output handle.",
        "```suggestion\r\n    entry: checkov -d . --framework secrets --enable-secret-scan-all-files\r\n```\r\ncould you also add the flag `--enable-secret-scan-all-files` so all the files are scanned."
      ],
      "checkov-precise-configuration-validation": [
        "```suggestion\r\n      operator: not_exists\r\n```\r\nshouldn't it be not set, because we don't want a public IP?",
        "```suggestion\r\n  operator: \"not_equals_ignore_case\"\r\n```\r\nit should not `true` right?",
        "you can change it to a 'not empty', but a not empty `origin_access_identity` is a broken configuration, but correct me, if I'm wrong.",
        "please add an or block, when `shared_access_key_enabled` is set to `false` then it should pass without setting the expiration"
      ],
      "checkov-consistent-naming-conventions": [
        "you can also use here `def get_evaluated_keys(self) -> List[str]:` instead of adding them dynamically",
        "```suggestion\r\n        self.ALLOW_KUSTOMIZE_FILE_EDITS = convert_str_to_bool(os.getenv(\"CHECKOV_ALLOW_KUSTOMIZE_FILE_EDITS\", False))\r\n```\r\nplease prefix it with `CHECKOV_` to make it clear, it is an internal env var.",
        "had the same thought 😄 ",
        "yeah, sure. I just named them identical to keep the code changes minimal 😄 "
      ],
      "checkov-backward-compatible-parameters": [
        "```suggestion\r\n                    messages=messages,\r\n```\r\notherwise you only send the first message, same a few lines lower"
      ],
      "checkov-ensure-dependency-compatibility": [
        "this will not work. this lock file was created with Python 3.10+ therefore any CI jobs running on 3.8 or 3.9 will fail"
      ],
      "checkov-safe-dictionary-access": [
        "```suggestion\r\n        if properties and isinstance(properties, dict):\r\n```\r\nlet's make sure, ew deal with a dictionary otherwise we will have a problem.",
        "```suggestion\r\n        principal = conf.get(\"principal\")\r\n        if principal and isintsance(principal, list) and isinstance(principal[0], str):\r\n            principal_parts = principal[0].split('.')\r\n```\r\nthis is a bit tricky, we need to be a bit more cautious on the types. Quite often Terraform plan files come with unexpected default values and break our checks.",
        "```suggestion\r\n    inline_suppressions_by_cve = inline_suppressions.get(\"cves\", {}).get(\"byCve\", {})\r\n    for cve_suppression in inline_suppressions_by_cve:\r\n        cve_id = cve_suppression.get(\"cveId\")\r\n        if cve_id:\r\n            cve_by_cve_map[cve_id] = cve_suppression\r\n```\r\n🙂 ",
        "`not` also works, but if it is an empty `dict` then there is no need to override it again\r\n```suggestion\r\n            if each[\"change\"][\"before\"] is None:\r\n                each[\"change\"][\"before\"] = {}\r\n            if each[\"change\"][\"after\"] is None:\r\n                each[\"change\"][\"after\"] = {}\r\n```"
      ],
      "checkov-safe-dictionary-navigation": [
        "```suggestion\r\n        if properties and isinstance(properties, dict):\r\n```\r\nlet's make sure, ew deal with a dictionary otherwise we will have a problem.",
        "```suggestion\r\n        principal = conf.get(\"principal\")\r\n        if principal and isintsance(principal, list) and isinstance(principal[0], str):\r\n            principal_parts = principal[0].split('.')\r\n```\r\nthis is a bit tricky, we need to be a bit more cautious on the types. Quite often Terraform plan files come with unexpected default values and break our checks.",
        "```suggestion\r\n    inline_suppressions_by_cve = inline_suppressions.get(\"cves\", {}).get(\"byCve\", {})\r\n    for cve_suppression in inline_suppressions_by_cve:\r\n        cve_id = cve_suppression.get(\"cveId\")\r\n        if cve_id:\r\n            cve_by_cve_map[cve_id] = cve_suppression\r\n```\r\n🙂 ",
        "`not` also works, but if it is an empty `dict` then there is no need to override it again\r\n```suggestion\r\n            if each[\"change\"][\"before\"] is None:\r\n                each[\"change\"][\"before\"] = {}\r\n            if each[\"change\"][\"after\"] is None:\r\n                each[\"change\"][\"after\"] = {}\r\n```"
      ],
      "checkov-validate-configurations-correctly": [
        "```suggestion\r\n      operator: not_exists\r\n```\r\nshouldn't it be not set, because we don't want a public IP?",
        "```suggestion\r\n  operator: \"not_equals_ignore_case\"\r\n```\r\nit should not `true` right?",
        "you can change it to a 'not empty', but a not empty `origin_access_identity` is a broken configuration, but correct me, if I'm wrong."
      ],
      "checkov-meaningful-identifier-names": [
        "yeah, sure. I just named them identical to keep the code changes minimal 😄 "
      ],
      "checkov-strategic-exception-management": [
        "@mikeurbanski1 any thoughts about raising an exception here or should we just log it and return the normal URL?",
        "do we really want to raise an exception or just log a message?",
        "in theory it could, but this would mean something went wrong during the context creation. But I will add a check and log a message."
      ],
      "checkov-comprehensive-security-scanning": [
        "we usually recommend to run docker with `--tty` for better output handle.",
        "```suggestion\r\n    entry: checkov -d . --framework secrets --enable-secret-scan-all-files\r\n```\r\ncould you also add the flag `--enable-secret-scan-all-files` so all the files are scanned."
      ],
      "checkov-choose-optimal-data-structures": [
        "you could also create a `defaultdict`, then you don't have to this check manually \r\n```python\r\ndirs_to_definitions = defaultdict(list)\r\ndirs_to_definitions[dir_path].append({tf_definition_key: tf_value})\r\n```",
        "how about making this a `set()` then you don't need to transform it to a set and back to list 🙂 ",
        "also thought the same, but I think it is a bigger change, because of the typing mismatch.",
        "just double checking, this has no bad side effect, because we would create more often edges, right?",
        "if we for some reason get more than 1 `target_variables`, we didn't create an edge before and just continued. If the tests are passing, then it is probably not an issue 😄 "
      ]
    },
    "profile": {
      "company": "@baz-scm",
      "blog": "",
      "site_admin": false,
      "followers": 58,
      "following": 2
    }
  },
  "radoering": {
    "repos": [
      "python-poetry/poetry"
    ],
    "entries": [
      {
        "slug": "poetry-api-backwards-compatibility",
        "title": "API backwards compatibility"
      },
      {
        "slug": "poetry-avoid-redundant-tool-configuration",
        "title": "avoid redundant tool configuration"
      },
      {
        "slug": "poetry-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "poetry-clear-actionable-error-messages",
        "title": "Clear actionable error messages"
      },
      {
        "slug": "poetry-complete-config-setting-integration",
        "title": "Complete config setting integration"
      },
      {
        "slug": "poetry-configure-http-requests-properly",
        "title": "Configure HTTP requests properly"
      },
      {
        "slug": "poetry-consistent-semantic-naming",
        "title": "consistent semantic naming"
      },
      {
        "slug": "poetry-dependency-constraint-consistency",
        "title": "dependency constraint consistency"
      },
      {
        "slug": "poetry-document-configuration-clearly",
        "title": "Document configuration clearly"
      },
      {
        "slug": "poetry-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "poetry-explicit-configuration-specification",
        "title": "explicit configuration specification"
      },
      {
        "slug": "poetry-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "poetry-maintain-security-constraints",
        "title": "maintain security constraints"
      },
      {
        "slug": "poetry-manage-testing-dependencies",
        "title": "manage testing dependencies"
      },
      {
        "slug": "poetry-network-request-configuration",
        "title": "Network request configuration"
      },
      {
        "slug": "poetry-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "poetry-parameterize-similar-tests",
        "title": "Parameterize similar tests"
      },
      {
        "slug": "poetry-pin-tool-versions-explicitly",
        "title": "Pin tool versions explicitly"
      },
      {
        "slug": "poetry-prefer-simple-readable-code",
        "title": "prefer simple readable code"
      },
      {
        "slug": "poetry-specify-tool-version-requirements",
        "title": "specify tool version requirements"
      },
      {
        "slug": "poetry-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "poetry-use-descriptive-names",
        "title": "Use descriptive names"
      }
    ],
    "comments": {
      "poetry-use-appropriate-logging-levels": [
        "We probably should at least add a debug log with the error message.",
        "I think it might be good to add a debug log for known exceptions because otherwise it might be difficult to tell why exactly lazy-wheel failed (e.g. no content-range).",
        "```suggestion\r\n\r\n    logger.debug('None of the hash types %s is in prioritized_hash_types', hash_types)\r\n\r\n    for hash_type in hash_types:\r\n        if hash_type in non_prioritized_hash_types:\r\n            return hash_type\r\n\r\n    logger.warning('None of the hash types %s is available in hashlib', hash_types)\r\n\r\n    return None\r\n```\r\n\r\nIf there is no prioritized hash type we just \"randomly\" choose an available hash type. That's why md5 and sha1 can be omitted in `prioritized_hash_types`.\r\n\r\nIf none of the hash types is available we probably should log a warning."
      ],
      "poetry-network-request-configuration": [
        "I'd omit everything from _\"As this environment variable is **unstable**, ...\"_ If we replace the environment variable, IMO we will deprecate it first anyway."
      ],
      "poetry-cache-expensive-computations": [
        "We may use `@cached_property` instead so we don't need `_installer_scheme_dict`.",
        "One more thing: Since `all_requires` is not an attribute but a property that iterates over groups and dependencies we should store it in a local variable outside of the loop so we have to calculate it just once. Further, I'd do the same for `self.option(\"top-level\")`. It's not consistent for all options, but it's done for some options at least.",
        "Maybe, we should make this a `cached_property` since this should not change during one invocation of poetry. Now, we are reading `pyvenv.cfg` once for each package."
      ],
      "poetry-complete-config-setting-integration": [
        "You have to add this setting in two more places:\r\n\r\n- Some lines below in `_get_normalizer`\r\n- In `ConfigCommand.unique_config_values`",
        "```suggestion\r\n                warning = (\r\n                    \"Found deprecated key 'default' or 'secondary' in\"\r\n                    \" pyproject.toml configuration for source\"\r\n                    f\" {source.get('name')}. Please provide the key 'priority'\"\r\n                    \" instead. Accepted values are:\"\r\n                    f\" {', '.join(p.name.lower() for p in Priority)}.\"\r\n                )\r\n                io.write_error_line(\"<warning>Warning: {warning}</warning>\")\r\n```\r\n\r\nor similar.\r\n\r\nDeprecation warnings are for (plugin) developers, but that's something that's relevant for the end user."
      ],
      "poetry-avoid-redundant-tool-configuration": [
        "```suggestion\r\n```\r\n\r\nThis seems to be the default order, so we don't have to configure it.",
        "The [documentation of isort](https://pycqa.github.io/isort/docs/configuration/custom_sections_and_ordering.html) seems to be better in this point:\r\n\r\n> You can change the section order with sections option from the default of:\r\n> \r\n> `FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER`\r\n\r\nI'm not good at Rust but after some trial and error and code spelunking, I'd say ruff does the same:\r\n[ImportType Enum](https://github.com/charliermarsh/ruff/blob/64b7280eb824d0e5f9da887e82dcda53838dd38d/crates/ruff/src/rules/isort/categorize.rs#L35C1-L41)\r\n[Default order 1](https://github.com/charliermarsh/ruff/blob/64b7280eb824d0e5f9da887e82dcda53838dd38d/crates/ruff/src/rules/isort/settings.rs#LL343C83-L343C83)\r\n[Default order 2](https://github.com/charliermarsh/ruff/blob/64b7280eb824d0e5f9da887e82dcda53838dd38d/crates/ruff/src/rules/isort/settings.rs#L351-L353)"
      ],
      "poetry-api-backwards-compatibility": [
        "If I remember correctly, this function is used by plugin authors (not only poetry-plugin-export). Maybe, we should deprecate the parameters, keep them functional and add `priority` as keyword-only argument for now.",
        "I suppose this will break some plugins. I'd like to deprecate it first.",
        "It's not the import, I'm concerned about. It's `PyProjectTOML.file`. Correct me if I'm wrong, but plugins relying on `PyProjectTOML.file.parent` will still work if we keep `__getattr__()`.",
        "You may not have to change the import if there is no import. Let's just look at the [docs](https://python-poetry.org/docs/plugins/#generic-plugins). You have an instance of the `Poetry` class in your hand and just could do `poetry.pyproject.file.parent`. I suppose many users will not import `PyProjectTOML` or `TOMLFile` but just use the instance they get from the Poetry object."
      ],
      "poetry-manage-testing-dependencies": [
        "Fortunately, it is only a dev dependency. That is why I would say it is not that relevant for the release. In the medium term, of course, we have to replace it."
      ],
      "poetry-explicit-configuration-specification": [
        "> @radoering is `(>=2.7,<2.8 || >=3.4)` defaults intended?\r\n\r\nI assume this has been the default since forever:\r\nhttps://github.com/python-poetry/poetry-core/blob/main/src/poetry/core/packages/project_package.py#L40-L41",
        "> What do you think?\r\n\r\nI think your observation is correct.\r\n\r\n> It is not a good default\r\n\r\nI agree. I think we should discourage not defining `project.requires-python` or `tool.poetry.dependency.python`, maybe at least print a warning in `poetry check`, but that is something for another PR.",
        "```suggestion\r\nrequires = [\"poetry-core\", \"cython\", \"setuptools\"]\r\n```",
        "Not quite correct. We shouldn't care about `default` too much here because I don't want to change default handling in this PR.",
        "Not sure if \"explicit packages\" makes sense. Maybe:\r\n\r\n```suggestion\r\nThis approach is strongly suggested for all packages that are expected to be provided only by one specific source to avoid dependency confusion attacks.\r\n```",
        "```suggestion\r\n**Omit the `url` when specifying PyPI explicitly.** Because PyPI is internally configured\r\nwith Poetry, the PyPI repository cannot be configured with a given URL. Remember, you can always use\r\n`poetry check` to ensure the validity of the `pyproject.toml` file.\r\n```",
        "```suggestion\r\n{{% warning %}}\r\nUse `--` to terminate option parsing, otherwise commands like\r\n`poetry config http-basic.custom-repo gitlab-ci-token ${GITLAB_JOB_TOKEN}`\r\nwill fail if `${GITLAB_JOB_TOKEN}` starts with a hyphen (`-`).\r\n{{% /warning%}}\r\n\r\n```"
      ],
      "poetry-pin-tool-versions-explicitly": [
        "Good catch. Tested it in python-poetry/website#162 and adopted it."
      ],
      "poetry-configure-http-requests-properly": [
        "I'm fine with adding them if you add a comment with a link so we see where these codes come from."
      ],
      "poetry-prefer-simple-readable-code": [
        "done",
        "```suggestion\r\n        return cls.run(\"checkout\", rev, folder=target)\r\n```\r\n\r\nThis should be sufficient because `None` is handled in `run()`. (The `args` variable in line 27 can be removed then.)"
      ],
      "poetry-document-configuration-clearly": [
        "I always use `--sync` so that's fine for me. 😉\r\n\r\nWe could also add two hooks `poetry-install` (without sync) and `poetry-sync` (with sync). Maybe, we just add `poetry-sync` now and poetry-install later only if someone requests it?",
        "That's fine with me, too. It has the benefit that it's more flexible and we only need one hook. 🤷 ",
        "Another spot that reads more like `poetry install --sync`. Considering the descriptions of the other hooks, maybe:\r\n\r\n```suggestion\r\n  description: run poetry install to install dependencies from the lock file\r\n```"
      ],
      "poetry-specify-tool-version-requirements": [
        "```suggestion\r\n**Yes**. Provided that you are using `tox` >= 4, you can use it in combination with\r\nthe PEP 517 compliant build system provided by Poetry. (With tox 3, you have to set the \r\n[isolated build](https://tox.wiki/en/3.27.1/config.html#conf-isolated_build) option.)\r\n```"
      ],
      "poetry-clear-actionable-error-messages": [
        "The output is not so nicely formatted. It contains a raw dict, e.g.:\r\n```\r\nValidation failed: {'errors': ['project.name must match pattern ^([a-zA-Z\\\\d]|[a-zA-Z\\\\d][\\\\w.-]*[a-zA-Z\\\\d])$'], 'warnings': []}\r\n```\r\n\r\nYou may take a look at https://github.com/python-poetry/poetry-core/blob/002aa3e16f98d21645bb9a45f698b55adc40f317/src/poetry/core/factory.py#L53-L60 for better formatting.",
        "Absolutely.",
        "My comment was just about formatting the output. At this point, we get a dict with lists of messages and the output should just be nicely formatted. The content of the single messages has been created before and is fixed at this point.\r\n\r\nIn my opinion, it is too much effort to improve each possible message that comes from schema validation - or at least this is clearly out of scope of this PR. The message is the same message if you run `poetry check` on such an invalid pyproject.toml.",
        "I assume we don't need `<error>` and `Error:` when raising an error. Further, we can remove `\"You may be getting improper dependencies.\"` because now you can't install anymore.",
        "Printing an error and raising an exception is a bit like \"Two is better than one.\" IMO one should suffice.\r\n\r\nPrinting an error and returning exit code 1 is probably not easily possible here, because we're not in the `handle` method of a command. Thus, I'd propose to only raise an exception.",
        "IMO it's not required to mention that only valid groups are allowed and it's not really necessary to mention the option to which the invalid group has been provided. The name of the invalid group should be enough for the user to notice what's wrong. I'd probably just print `Group(s) not found: batman, robin`\r\n\r\nOf course, I'm not the average user so if you think that's too consise please tell me. 😄 ",
        "I just noticed that we print this error even if there is no lockfile. Maybe, we should make a distinction between \"no lockfile\" and \"inconsistent lockfile\"?",
        "```suggestion\r\n                raise ValueError(f\"Failed to parse script entry point '{script}'\") from exc\r\n```",
        "> Several lint tools recommend against interpolation when instantiating Exceptions.\r\n\r\nSorry, I don't follow. Can you give an example?\r\n\r\nMy reasoning is that you get the original exception via `from exc`. Thus, you don't need `exc.args` it in the derived exception.",
        "Thanks for the example. I read the [reasoning to EM102](https://pypi.org/project/flake8-errmsg/0.4.0/) and I'm not sure I'm convinced (especially since we are not printing the traceback by default). Normally, we are using string literals and f-strings without assigning it to a variable first, so there is no reason to deviate from that here.\r\n\r\n> Poetry does not display the stack trace so you have to add the exception info to the message so the user can see what the symptom and cause is in one message\r\n\r\nYou will see the traceback by running poetry with `-vvv`. When not in verbose mode, we shouldn't print the original error message since it is not very helpful anyway. If you think that `f\"Failed to parse script entry point '{script}'\"` is not enough information, then what about `f\"Failed to parse script entry point '{script}', ':' not found\"` or similar?\r\n\r\nBy the way, we have the same script parsing with the same issue in https://github.com/python-poetry/poetry/blob/161b19cb4e4686fc5a0a7925001534a87f6c4052/src/poetry/console/commands/run.py#L73",
        "Good point, my proposal is not accurate enough. However, I still stand by my initial statement.\r\n\r\nIMO `('not enough values to unpack (expected 2, got 1)',)  - Failed to parse script entry point '...'` is not a good error message. Without any context `not enough values to unpack` does not really help. `Failed to parse script entry point '...'` should be good enough. If you want more details you have to run `poetry install -v`. That way you will get the `not enough values to unpack` with the line where it happened."
      ],
      "poetry-explicit-null-handling": [
        "Probably, a None check is easier to understand. I will change it.",
        "IIRC, this is written to the cache. It shouldn't matter if it's an empty list or None, should it?",
        "I was wondering where this warning came from, but was too lazy to go into the matter. 👍\r\n\r\nSince we are handling None as well as KeyError now, we should filter this deprecation warning, shouldn't we?",
        "However, it will take quite a while: https://github.com/python/importlib_metadata/blob/700f2c7d74543e3695163d5487155b92e6f04d65/importlib_metadata/_adapters.py#L11\r\n\r\nIt seems even though https://github.com/python/importlib_metadata/pull/416 has been merged into main it's not in main!?\r\n\r\nI'd prefer a filter and an explanatory comment. Even if we forget to remove it, I assume it will not really hurt. (Just added it.) Does that make sense to you?",
        "```suggestion\r\n        assert module_name(name) == package_include.get(\"include\")\r\n```\r\n\r\n`name` is always set isn't it?\r\n\r\nIf I don't miss anything the expression wouldn't work if it wasn't set because there is a missing bracket. It had to be:\r\n\r\n```\r\nassert module_name(name) == (package_include.get(\"include\") if name else \"\")\r\n```",
        "LGTM 👍 "
      ],
      "poetry-dependency-constraint-consistency": [
        "1. We do this since Poetry 1.3. IIRC, the idea behind this was that we will do a Poetry release anyway even if there is just a poetry-core bugfix release.\r\n2. I think so. That is what Poetry procudes for core metadata.",
        "Shall we require `\"^1.1.1\"` because `1.1.0` is yanked?",
        "Sure about that?\r\n\r\n<html><body>\r\n<!--StartFragment-->\r\n\r\nimportlib_metadata | stdlib\r\n-- | --\r\n4.8 | 3.11\r\n4.4 | 3.10\r\n1.4 | 3.8\r\n\r\n<!--EndFragment-->\r\n</body>\r\n</html>\r\n(from https://pypi.org/project/importlib-metadata/)\r\n\r\nIf we really want to do that, we'll have to change https://github.com/python-poetry/poetry/blob/0b71e4381445bf6b6c239bf0d88c8f5dfb4a32bd/src/poetry/utils/_compat.py#L10-L11",
        "I like <3.8 even if we don't support 3.6. I suppose that's a matter of taste. 🤷 "
      ],
      "poetry-documentation-clarity-standards": [
        "I would restructure the text a bit and do not refer to `eval` but \"the eval command of your shell\" (or similar). Proposal:\r\n\r\n```suggestion\r\nThe `poetry env activate` command prints the activate command of the virtual environment to the console.\r\nYou can run the output command manually or feed it to the eval command of your shell to activate the environment.\r\nThis way you won't leave the current shell.",
        "Is \"less significant\" a common term? (I know \"least significant\" but haven't heard \"less significant\" before.)\r\n\r\nAlternatively, we could just write \"... only modifies the numbers to the right ...\".",
        "```suggestion\r\nSimilar to `--no-root` you can use `--no-directory` to skip directory path dependencies:\r\n```\r\n\r\nCan you add a sentence that mentions that skipping directory path dependencies only makes sense for certain use cases and link to the FAQ entry?"
      ],
      "poetry-use-descriptive-names": [
        "If you want to refrain from taking advantage of the fact that the list is sorted and thus from the early return if the version is not found, you can simplify the loop as follows:\r\n\r\n```\r\nif locked is not None:\r\n    for version in packages:\r\n        if version.version == locked.version:\r\n            break\r\n```",
        "I like it, but it has to be:\r\n\r\n```\r\nversion = next((p for p in packages if p.version == locked.version), None)\r\n```\r\n\r\nMaybe a bit confusing that the variable `version` is a `package`. Should we rename it to `package`?",
        "@neersighted Sorry for bothering, but I had the impression that the review of this PR has already been well advanced - maybe, the naming of the variable being the only point to be clarified before merging. (Correct me if I'm wrong.) I'd appreciate if you find some minutes so that we may finalize it.",
        "I understand the confusion.\r\n\r\nOn the one hand, `SystemEnv` is the env Poetry is installed in. `poetry env use system` tells Poetry to use it's own Python version as default for new venvs. Thus, in our wording `system` is Poetry's own env.\r\n\r\nOne the other hand, `virtualenvs.options.system-site-packages` does not refer to Poetry's own env. And most people will probably not associate `system` with Poetry's own env.\r\n\r\nNot sure, if `get_poetry_python` is a better name or if it can be confused with the project's python. At least, `system` is quite consistent in our code. No strong opinion on this. 🤷",
        "Maybe, we should rename it to `use_in_project_venv` instead. At least to me, `root_env` is a bit vague.",
        "```suggestion\r\n    cached_wheel: bool,\r\n```\r\n\r\nMaybe, we should name it more clear because it doesn't matter if the sdist is cached, only if a generated wheel is cached or not.",
        "Ah, I thought caching is only turned on/off for the generated wheel and the sdist is always cached, but it is also turned on/off for the sdist. In that case `cached` would have been fine, but `is_artifact_cached` is better. 😄 "
      ],
      "poetry-maintain-security-constraints": [
        "Is this still necessary? Shouldn't there always be as hash now?",
        "We assumed it before this PR. If there the set of known hashes is empty, the hash is not known and we should fail for security reasons. I wouldn't weaken that constraint if it is not necessary."
      ],
      "poetry-parameterize-similar-tests": [
        "Can we merge these three `ref_spec_resolve` tests into one parameterized test?\r\n\r\nThere are still some uncovered branches in `_normalise`, e.g. specifying a `branch` as `revision`, which can be covered easily. If we add more test variants, a parameterized tests will make even more sense.",
        "This looks like a copy of `test_env_activate_prints_correct_script` that also works for Windows. Can we just edit the original test instead of creating a new one?",
        "Can you make the test `parametrized` to test passing `bytes` as well as `str`, please?",
        "Can you make the test `parametrized` to add an example with a trailing slash, e.g. `\"ssh://git@github.com/org/repo/\"`, please?",
        "```suggestion\r\ndef test_no_directory_is_passed_to_installer(tester: CommandTester, mocker: MockerFixture):\r\n```",
        "Can you move this somewhere near the other \"pass to the installer\" tests, e.g. after `test_compile_option_is_passed_to_the_installer` and add a negative test via `pytest.mark.parametrize`  (`_skip_directory` is `False` if option is not passed)",
        "Can you integrate this with `pytest.mark.parametrize` in the previous test?\r\n\r\n`skip_directory=False` -> `directory_installs`, `installations_count == 6`\r\n`skip_directory=True` -> `not directory_installs`, `installations_count == ...`",
        "```suggestion\r\n@pytest.mark.parametrize(\"command\", [\"foo\", \"foo --lock\"])\r\n@pytest.mark.parametrize((\"locked\", \"expected_docker\"), [(True, \"4.3.1\"), (False, \"4.3.2\")])\r\n```\r\n\r\nThinking about the `poetry.locker.locked(True)`, I just came up with an idea to make the test more robust.",
        "We should parametrize the test since it should make no difference if `--lock` is passed or not anymore.",
        "Is it necessary to have three similar test cases or is one parameterized test case sufficient?",
        "If you now add `packages_files` and `expected_url_reference` as parameters (like [that](https://github.com/python-poetry/poetry/blob/e31a9fc3ce5a7fc72eefa22040676bc66a35e5ad/tests/installation/test_executor.py#L247-L279)) that's what I meant. 😅"
      ],
      "poetry-optimize-algorithmic-efficiency": [
        "Probably not. I'm not even sure if there are real world examples for this edge case. Usually, there is just one reason. I'll change it to a `set` and add a `sorted` to the `join` (for reproducable output).",
        "```suggestion\r\n        return super().is_path_relative_to_lib(path) or (\r\n            self.includes_system_site_packages()\r\n            and SystemEnv(Path(sys.prefix)).is_path_relative_to_lib(path)\r\n        )\r\n```\r\n\r\nThat way we don't have to create a SystemEnv and call additional methods in the default case (where system site packages are not included).",
        "```suggestion\r\n\r\nnon_prioritized_hash_types = hashlib.algorithms_available - set(prioritized_hash_types)\r\n\r\n```"
      ],
      "poetry-consistent-semantic-naming": [
        "The name of the label is not consistent with our naming scheme (no spaces, but slashes). Maybe, just `test/export`?",
        "IMO, this ID/name is a bit misleading since the hook does not run `poetry update`. Maybe, `poetry-sync` would be a better name?"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 27,
      "following": 0
    }
  },
  "kimwnasptd": {
    "repos": [
      "kubeflow/kubeflow"
    ],
    "entries": [
      {
        "slug": "kubeflow-api-structure-balance",
        "title": "API structure balance"
      },
      {
        "slug": "kubeflow-automate-style-enforcement",
        "title": "Automate style enforcement"
      },
      {
        "slug": "kubeflow-centralize-configuration-constants",
        "title": "Centralize configuration constants"
      },
      {
        "slug": "kubeflow-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "kubeflow-centralize-dependency-configurations",
        "title": "Centralize dependency configurations"
      },
      {
        "slug": "kubeflow-check-before-use",
        "title": "Check before use"
      },
      {
        "slug": "kubeflow-configurable-security-with-defaults",
        "title": "Configurable security with defaults"
      },
      {
        "slug": "kubeflow-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "kubeflow-control-header-modification",
        "title": "Control header modification"
      },
      {
        "slug": "kubeflow-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "kubeflow-document-code-thoroughly",
        "title": "Document code thoroughly"
      },
      {
        "slug": "kubeflow-document-networking-annotations",
        "title": "Document networking annotations"
      },
      {
        "slug": "kubeflow-enforce-https-protocol",
        "title": "Enforce HTTPS protocol"
      },
      {
        "slug": "kubeflow-enforce-least-privilege",
        "title": "Enforce least privilege"
      },
      {
        "slug": "kubeflow-environment-aware-configuration-design",
        "title": "Environment-aware configuration design"
      },
      {
        "slug": "kubeflow-externalize-configuration-parameters",
        "title": "Externalize configuration parameters"
      },
      {
        "slug": "kubeflow-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "kubeflow-go-export-naming-conventions",
        "title": "Go export naming conventions"
      },
      {
        "slug": "kubeflow-manage-configuration-changes",
        "title": "Manage configuration changes"
      },
      {
        "slug": "kubeflow-mark-ui-text-i18n",
        "title": "Mark UI text i18n"
      },
      {
        "slug": "kubeflow-match-algorithms-to-purpose",
        "title": "Match algorithms to purpose"
      },
      {
        "slug": "kubeflow-normalize-url-paths",
        "title": "Normalize URL paths"
      },
      {
        "slug": "kubeflow-optimize-container-build-configurations",
        "title": "Optimize container build configurations"
      },
      {
        "slug": "kubeflow-pin-version-dependencies",
        "title": "Pin version dependencies"
      },
      {
        "slug": "kubeflow-precise-workflow-triggers",
        "title": "Precise workflow triggers"
      },
      {
        "slug": "kubeflow-prefer-external-configuration",
        "title": "Prefer external configuration"
      },
      {
        "slug": "kubeflow-prioritize-readability-over-brevity",
        "title": "Prioritize readability over brevity"
      },
      {
        "slug": "kubeflow-private-variable-naming-convention",
        "title": "Private variable naming convention"
      },
      {
        "slug": "kubeflow-simplify-code-structure",
        "title": "Simplify code structure"
      },
      {
        "slug": "kubeflow-specific-network-access-documentation",
        "title": "Specific network access documentation"
      },
      {
        "slug": "kubeflow-standardize-build-configurations",
        "title": "Standardize build configurations"
      },
      {
        "slug": "kubeflow-standardize-makefile-patterns",
        "title": "Standardize makefile patterns"
      },
      {
        "slug": "kubeflow-standardize-network-tools",
        "title": "Standardize network tools"
      },
      {
        "slug": "kubeflow-standardize-style-scripts",
        "title": "Standardize style scripts"
      },
      {
        "slug": "kubeflow-structured-owners-files",
        "title": "Structured OWNERS files"
      },
      {
        "slug": "kubeflow-type-appropriate-default-values",
        "title": "Type-appropriate default values"
      },
      {
        "slug": "kubeflow-unique-workflow-step-names",
        "title": "Unique workflow step names"
      },
      {
        "slug": "kubeflow-use-appropriate-log-levels",
        "title": "Use appropriate log levels"
      },
      {
        "slug": "kubeflow-use-css-classes-properly",
        "title": "Use CSS classes properly"
      },
      {
        "slug": "kubeflow-use-enums-for-state",
        "title": "Use enums for state"
      },
      {
        "slug": "kubeflow-use-modern-javascript-idioms",
        "title": "Use modern JavaScript idioms"
      },
      {
        "slug": "kubeflow-use-snake-case-in-python",
        "title": "Use snake_case in Python"
      },
      {
        "slug": "kubeflow-use-table-driven-tests",
        "title": "Use table-driven tests"
      },
      {
        "slug": "kubeflow-validate-inputs-explicitly",
        "title": "Validate inputs explicitly"
      }
    ],
    "comments": {
      "kubeflow-externalize-configuration-parameters": [
        "Also, ultimately we could use these CRs then to list who the contributors in the namespace are from the CentralDashboard?"
      ],
      "kubeflow-validate-inputs-explicitly": [
        "let's instead make a check if `NaN` is included in the value of either `cpu` or `cpu_limit`.\r\n\r\nAnd if that's the case then the backend should raise a `BadRequest` werkzeug exception (400)",
        "Lets also add some logic here that would handle incorrect values.\r\n\r\nThe expected values here, from the request, would be:\r\n* `None`, which means `\"jupyter\"`\r\n* `\"jupyter\"`\r\n* `\"rstudio\"`\r\n* `\"vscode\"`\r\n\r\nIf this field is present in the request but has a different value from the expected ones then the backend should raise a `400` error",
        "Indeed, the frontend sends only specific values but it's a good practice for the backend to make it explicit to the user/frontend that the data it received was not formatted as expected.",
        "And also you could look at this snippet for how to raise this error\r\n\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/jupyter/backend/apps/common/form.py#L31-L34\r\n\r\nEDIT: was typing it before I saw your comment above ",
        "You are not wasting anyone's time. Your efforts are as important as mine and no releases were delayed because of a question :) "
      ],
      "kubeflow-precise-workflow-triggers": [
        "Let's re-use the makefile rules here, to avoid duplication of code in makefiles and GH Actions. The layers are already cached so rebuilding the image should be very fast\r\n```bash\r\nexport TAG=$(cat releasing/version/VERSION)\r\ncd components/centraldashboard-angular\r\nmake docker-build docker-push\r\n```",
        "@apo-ger let's leave the notebook images completely out for this effort.\r\n\r\nI believe we would be able to use the Makefiles if we'd use a `REGISTRY` env var in each Makefile. But let's discuss this in a follow up PR, since this one is already getting big",
        "just the changes from this PR, and we can send another PR for the latest tag in a separate PR that:\r\n1. Uses a `REGISTRY` env var in all makefiles (let's see if we need to do more)\r\n2. Use the Makefiles to build each notebook, but with a different TAG each time like we do for the rest of the components",
        "Since we are testing manifests this time we don't want to trigger the workflows when code changes, but rather when the **manifests** change.\r\n\r\nSo in this case it will be `components/centraldashboard/manifests/**`",
        "I think you missed this one. It should be `components/profile-controller/config/**`",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Let's also trigger this workflow for the `/components/common/**` directory, since it's using common code from there"
      ],
      "kubeflow-automate-style-enforcement": [
        "We should look into moving away from `tslint` in the backend as well, now that we've decoupled the frontend and backend code"
      ],
      "kubeflow-api-structure-balance": [
        "I'd propose to avoid introducing a handler for the InferenceServices in the common backend code.\r\n\r\nThis would mean we'd need to add more handlers for list/delete/get/logs as well, which as a result will make the MWA depend even more in this common code. I'd propose to instead keep on using the `custom_resource.py` file's handlers for the ISVC logic in the MWA",
        "Could you add a `get_age(k8s_object)` in our common [helpers.py](https://github.com/kubeflow/kubeflow/blob/e99b5e18697a15088abea12543bd4e3f180ff984/components/crud-web-apps/common/backend/kubeflow/kubeflow/crud_backend/helpers.py) file that would return a dictionary with the `uptime` and `timestamp` values? Since other web apps will want to use this convention, for returning age information to their frontends, it would be a good idea to add this logic into a common place.\r\n\r\nFor now it could only work with dict objects, like CRs, and later one we could extend it to work with class objects as well, such as PVCs for example."
      ],
      "kubeflow-centralize-configuration-values": [
        "Could you add the urls for the icons into the environment files instead? https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/jupyter/frontend/src/environments\r\n\r\nLets keep these links in a central place, where we might also add other links in the future like the svgs for the main page",
        "Thanks for the ping! \r\n\r\nGood catch on the CORS issue. Lets dump the SVGs in the `static/assets` folder. And also define the links that the frontend will use in the `environment` files as mentioned above."
      ],
      "kubeflow-standardize-build-configurations": [
        "I don't like the phrasing of this sentence, as it implies that for development purposes we need to run the command directly and can't use the Makefifle.\r\n\r\nWe can use the Makefile to build Jupyter app's image by running\r\n```bash\r\nREGISTRY_PROJECT=my-repo make docker-build\r\n```\r\n\r\n[ Tensorboard's makefile uses slightly different vars but we should iron them out and have the same template when we setup the CI/CD ]"
      ],
      "kubeflow-go-export-naming-conventions": [
        "small nit, shouldn't this function start from lowercase since it's not meant to be used from outside this go module?"
      ],
      "kubeflow-use-modern-javascript-idioms": [
        "Could you restructure the if statements in order to reduce the nesting levels?\r\n\r\nSomething like:\r\n```javascript\r\nif (!queryParams || !queryParams[\"ns\"]) {\r\n  return this.buildHref(href, this.queryParams);\r\n}\r\n\r\nreturn this.buildHref(href.replace('{ns}', queryParams[\"ns\"]), queryParams);\r\n```",
        "LGTM!"
      ],
      "kubeflow-prefer-external-configuration": [
        "@tasos-ale let's have this as an empty list initially, since we don't have an app that supports it at this point in time",
        "Ideally I'd like this to info (which web app supports all namespaces) to be transmitted via the common library, so that the dashboard can dynamically know if an app supports this once it loads it.\r\n\r\nBut we can do with hardcoding the urls for now. If in the future we see a bigger need for this we can look into it",
        "Do we need all of these new imports or only a subset of them?",
        "Makes sense, lets keep them then"
      ],
      "kubeflow-use-table-driven-tests": [
        "Let's change the name of this function to `createNotebookStatus` and update it's signature to:\r\n```golang\r\nfunc createNotebookStatus(r *NotebookReconciler, nb *v1beta1.Notebook,\r\n\tsts *appsv1.StatefulSet, pod *corev1.Pod, req ctrl.Request) (v1beta1.NotebookStatus, error) {\r\n```\r\n\r\nThis means that:\r\n1. We will be passing all necessary objects, statefulset, pod, as arguments\r\n2. The function will be just calculating the status\r\n\r\nThis will also allow us to write some unit tests to ensure we calculate the status as expected.",
        "Nice! Let's also include another unit test for the case where the Notebook's Pod is unschedulable"
      ],
      "kubeflow-standardize-makefile-patterns": [
        "Can you also remove this rule? We don't use `build-gcb` anywhere in the project.\r\n\r\nIt's a good chance to clean up the project",
        "Can you add a `TAG ?= $(shell git describe --tags --always)` here? This is the one we use in the web apps as well. Let's standardize on this one",
        "nit: Could you also create an `image` rule, which builds and pushes the image?\r\n\r\nWe have this for the web apps as well and would be nice to keep this convention across all components\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/jupyter/Makefile#L11",
        "@elikatsis ACK, this is a very good suggestion!\r\n\r\nI'll add a commit for Volumes web app in this PR, since it's fixing the formatting for this component. I'll then open up a new issue to keep track of the uniformity in the dockerfiles for the other web apps as well."
      ],
      "kubeflow-use-appropriate-log-levels": [
        "I agree with @yanniszark.\r\nAlthough AFAIK `logr` does not have a [Warning level](https://github.com/go-logr/logr#why-not-more-named-levels-like-warning). So we should do this an Error instead.\r\n\r\n@gilbeckers could you also make the error message more explicit as to why it couldn't find the Notebook Container? Something like: `Could not find the Notebook container. No container has the same name with the CR '{notebook-name}'. Will not update the status of the CR.`"
      ],
      "kubeflow-structured-owners-files": [
        "@TobiasGoerke why are you not an approver? :) \r\n\r\nWe want the OWNERS file to depict the folks that are driving the component. Considering that you, @apo-ger and I have worked on the proposal of this I'd create the OWNERS file like this:\r\n\r\n```yaml\r\napprovers:\r\n  - apo-ger\r\n  - kimwnasptd\r\n  - TobiasGoerke\r\n```",
        "My bad, I just copied the reviewers from the jupyter web app.\r\nLets start with an empty list of reviewers for now",
        "I think I'm missing something here, where should approvers be listed?",
        "I see. Which people should I assign as reviewers?"
      ],
      "kubeflow-environment-aware-configuration-design": [
        "It makes sure the image build, and stored in docker, will use the TAG defined in the `env` and not the default one which is\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/centraldashboard/Makefile#L2",
        "> @kimwnasptd I don't like that this applies the manifests and then patches them after, this will result in a generation of the pod that fails to run, quickly followed by one with the correct image.\r\n\r\nWhy is this a hard problem? The deployment will be patched immediately, and we wait for the Deployment object either way\r\n\r\n> I don't understand what was not working with the old sed approach, which modified the manifests before applying them?\r\n\r\nPreviously the flow was:\r\n1. build the manifests\r\n    1. These manifests in `master` would have `latest` tag in images\r\n    2. In a release branch we update the tag, so it would be something like `v1.8.0-rc.0`\r\n3. `sed` was used change the image with tag `latest`, to `${{env.TAG}}`\r\n4. The above worked for `master`, which has tag `latest` but doesn't for release branches\r\n\r\nSo the script works for `master`, because `latest` image tag is used in manifests, but fails in release branches which use a different tag in manifests.",
        "Lets remove the `logo` sections in the ConfigMap. As mentioned above they add a lot of boilerplate code that most of the users won't need to configure.",
        "@DavidSpek taking your comment https://github.com/kubeflow/kubeflow/pull/5646#issuecomment-801308346 here to have everything coordinated in this place\r\n\r\n> @kimwnasptd Could we do some sort of combination of the things you describe and what is being done now? More specifically, allow the user to set the URLs for the icons in the config map. \r\n\r\nThis is the initial idea I had https://github.com/kubeflow/kubeflow/pull/5646#issuecomment-797521345, although thinking about it more I believe it will be more involved to get it working. In this case we will need to first fetch the yaml [ and the request could fail ] and then inject the urls and use them.\r\n\r\n> Due to the use of trademarks and the applicable guidelines I think it is import to make it very easy for people to remove the logos (preferably without needing to build their own image).\r\n\r\nCould you elaborate a little bit more on this? How does the trademarks and applicable guidelines fit into the picture?\r\n\r\nI agree that we should make it easy for people to swap the logos they prefer, but these users are the minority and most probably won't have a hard time to rebuild the image.",
        "From what I understand the issue is not how the icon will appear to the UI [ via the backend directly, fetched from somewhere public etc ] but the fact that we use this logo in the software. With that I mean that if someone would use the app he would end up seeing the logo, which could imply that this is endorsed from RStudio etc.\r\n\r\nAlso an issue with fetching resources in the UI directly from the public internet is that the app unable will not work in air-gaped environments, where the users would only have access to specific IPs.\r\n\r\n",
        "So judging from our current situation, where we are tied by time constraints and the trademark situation would need some communication and tip toeing into what it's acceptable I would propose the following solution to unblock ourselves:\r\n\r\n1. Don't use the logos in this PR. We could use the [group options](https://v8.material.angular.io/components/select/overview#creating-groups-of-options) from Angular material, or just use buttons with text [ would prefer the first approach tbh ] to show the images for the different servers\r\n2. Open an issue about it and involve @castrojo in this. And once we figure out what we are allowed to do with the rstudio trademark we can open a new PR and add this functionality. We've done it once here so this won't be difficult to do in the future\r\n\r\nWDYT?",
        "Let's go with using the svgs. We'll use links to fetch them from where they publicly host them. Distributions could make a small change and add their svgs to the app and simply change these links, which is something that we were OK with from the beginning. Lets just add a small section for this in the app's README.\r\n\r\nRegarding the responsibility part, I'm not a layer on this so not an expert but we are a team and all of us move forward together. We are not going to point fingers to people if things go south. And also since I'm a reviewer and agreed to move forward with this so I'm also responsible.\r\n\r\nBut if indeed people complain about the usage then we will sincerely apologize and just change this to something like we discussed above. "
      ],
      "kubeflow-manage-configuration-changes": [
        "I'm not sure I understand what the question or suggested change is here. Could you rephrase this?",
        "I see what you mean. The `start` script should be completely removed, since the users should run the UI locally with `build:watch` for now.\r\n\r\nI'll push a commit to remove this script",
        "Added a commit that removes the unused npm script",
        "Let's exclude this change from this PR and keep the touched lines to the minimum. This will also remove the 15.000 modified lines to the `package-lock.json` because of this change.\r\n\r\nAlthough I'd like to progressively replace the use of `lodash` with `lodash-es`, but until I get to it we can remove this from the `package.json`, but lets just do it in another PR."
      ],
      "kubeflow-consistent-descriptive-naming": [
        "nit: can we make this `pvcviewer-` instead? In the labels as well we do `pvcviewers` so let's be uniform",
        "Let's rename this to `CentralDashboard-angular Frontend Tests` and also the file to `centraldb_angular_frontend_test.yaml`"
      ],
      "kubeflow-pin-version-dependencies": [
        "let's remove the dependency on the `test` rule. This would require users to have a configured Go environment to build the controller. \r\n\r\nIf in the future we'd like to run these tests then we should do this inside of the Dockerfile, just like we do for the CentralDashboard",
        "When I tried applying the new CRD on a cluster with the previous CRD I got the following errors back:\r\n```\r\nThe CustomResourceDefinition \"notebooks.kubeflow.org\" is invalid:\r\n* metadata.annotations: Too long: must have at most 262144 bytes\r\n* spec.preserveUnknownFields: Invalid value: true: must be false in order to use defaults in the schema\r\n```\r\n\r\nRegarding the `preserveUnknownFIelds`, could you explicitly set it to `false`?\r\n\r\nThis is also the recommended way in the docs to have compatibility: \r\nhttps://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning\r\nhttps://github.com/kubernetes-sigs/controller-tools/issues/476#issuecomment-691519936\r\n\r\nAlthough with a quick look at the kubebuilder docs I'm not 100% sure whether we should do this with a [marker](https://book.kubebuilder.io/reference/markers/crd-processing.html) or via the [makefile](https://book.kubebuilder.io/reference/generating-crd.html#multiple-versions)",
        "After looking at this again I propose that we manually \r\n1. add the `spec.preserveUnknownFields: false` just for this release\r\n2. remove this field in KF 1.7, since it's only needed for the `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1` transition.\r\n\r\nThe `controller-gen` does not allow us to explicitly set this to false https://book.kubebuilder.io/reference/markers/crd-processing.html\r\n\r\n",
        "This is awesome @samuelvl!\r\n\r\nOne final nit, I see that you've commented out the `crd` part in the manifests, which results in the CRD to not be included when generating `base` or `overlays/kubeflow` \r\n\r\nhttps://github.com/kubeflow/kubeflow/pull/6374/files#diff-28481732533c7c75d2d2ba504e9670d90e72ed98f999115fd92a0f8e8a5aace2R20\r\n\r\nCould you revert it back, so that the CRD is included as well? We should be ready to merge afterwards",
        "Hmm, indeed in this PR we made the CRD quite bigger since we now actually include the full PodSpec for validation.\r\n\r\nBut the `maxDescLen=0` seems like a good way to cut down significantly the size of it. The spec is currently only the PodSpec, for which descriptions are widely available so we are OK.\r\n\r\nGood job @samuelvl!",
        "I tried to `kustomize build config/crd` with a 3.2 version, since this was the one [used from manifests](https://github.com/kubeflow/manifests#prerequisites) but I got the following errors:\r\n```\r\nError: no matches for OriginalId apiextensions.k8s.io_v1beta1_CustomResourceDefinition|~X|notebooks.kubeflow.org; no matches for CurrentId apiextensions.k8s.io_v1beta1_CustomResourceDefinition|~X|notebooks.kubeflow.org; failed to find unique target for patch apiextensions.k8s.io_v1beta1_CustomResourceDefinition|notebooks.kubeflow.org\r\n``` \r\n\r\nDo you know why these occur? They were not shows in the previous iteration of the controller, so maybe something slightly changed in the autogenerated manifests?\r\n\r\nIf I use the 3.8 version from the makefile this error goes away. Also it's not shown even with 3.2 when I build the `base`, so it's not that critical. But let's try to understand why this happens. I'll try to look into it within the next days as well",
        "I agree to remove the patch altogether. The patch was making the validation more lax, by removing validation from the CPU/Memory fields.\r\n\r\n@samuelvl can you double check that creating a Notebook with the Jupyter web app also works as expected after this change? To make sure the backend is submitting objects that are valid.",
        "Perfect!"
      ],
      "kubeflow-centralize-configuration-constants": [
        "That's a good idea, it will help us have all the names organized in one place.\r\nAdding it",
        "@PatrickXYS I added another commit that introduces this `config.py` file.\r\nPTAL and if you are OK I can resolve this conversation",
        "OK, I'll just omit this field entirely. There no issue by completely omitting the field and the configmap's value will be used right?"
      ],
      "kubeflow-type-appropriate-default-values": [
        "In this case the default value for the conditions should be a `[]`, instead of `\"\"` since this var is a list"
      ],
      "kubeflow-private-variable-naming-convention": [
        "let's make this `private`, to comply with the var's name"
      ],
      "kubeflow-configurable-security-with-defaults": [
        "Lets make it configurable then, it should be a small change.\r\n\r\nI'll make it look for a `CSRF_SAMESITE` env var with a default value to `Strict`. If the value provided by the user is not `Strict`, `Lax` or `None` then again it will default to `Strict`\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#SameSite",
        "force pushed. The PR should only contain one commit responsible for the CSRF functionality.\r\nI also made the `SameSite` attribute of the cookie configurable via the `CSRF_SAMESITE` variable.\r\n\r\nI also think a good next step would be to document the ENV Vars that each web app uses in the respective READMEs. This will make clear to the users how they can configure each app.",
        "ACK! I modified the README of the common code.\r\nI also created #5483 as an umbrella issue for the web apps"
      ],
      "kubeflow-mark-ui-text-i18n": [
        "Pushed a commit",
        "pushed a commit",
        "Pushed a commit"
      ],
      "kubeflow-centralize-dependency-configurations": [
        "Should we drop this package and install the corresponding kserve package instead?",
        "Could you instead make this change into the common code, so that it takes effect for all the web apps?\r\n\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/common/backend/setup.py#L6"
      ],
      "kubeflow-use-css-classes-properly": [
        "Don't modify html elements directly with styles, always use a class instead that wraps what the style changes are. Also try to avoid as much as possible `!important` in CSS. Instead try to use more specific css selectors.\r\n\r\nHere you could introduce and use the following CSS class for each `<mav-icon>`\r\n```css\r\n.server-type {\r\n  height: 32px;\r\n  width: 150px;\r\n}\r\n```",
        "Add a wrapper class here to give this group a bottom margin\r\n\r\n```css\r\n.server-type-wrapper {\r\n  margin-bottom: 1rem;\r\n}"
      ],
      "kubeflow-enforce-https-protocol": [
        "nit: `Invalid HOST url provided, must be like https*://*` (the protocol should be https in the end)"
      ],
      "kubeflow-specific-network-access-documentation": [
        "Could you instead have a list for this numbered step and expose each `kubectl port-forward` command that a user needs to run?\r\n\r\nWe can just have commands for the Services that we proxy in the webpack config",
        "Let's add this explanation at the numbered step, since it explains how the proxying works. And, as mentioned above, have the bullet list for the different proxying commands",
        ">This continuous failure to mutate the target pod may block other target pods from mutation\r\nuntil the failing process ends.\r\n\r\nCould you clarify a little bit more what you mean here? By other Pods you are not referring to other unrelated Pods in the cluster right?\r\n\r\nIIUC, as you described, the Deployment/StatefulSet will be retrying to create the Pod which will keep failing. Are there other side effects to other pods?"
      ],
      "kubeflow-document-code-thoroughly": [
        "Could you add a comment here to document that the status of the CR will be updated based on the `ContainerState` of the container that has the same name with the CR?"
      ],
      "kubeflow-optimize-container-build-configurations": [
        "I understand that by omitting the GOARCH env var we let golang decide on the architecture based on the node environment (machine). \r\n\r\nI'm just trying to wrap my mind around the next step. Is it going to be the process described in https://docs.docker.com/build/building/multi-platform/ to build images that have manifests for different platforms?",
        "@lehrig this was a *very* thorough explanation of the suggested approach! I'd also suggest to cross post it in the umbrella issue, so that it's readily available for anyone wondering how to e2e approach is going to be.\r\n\r\nAlso LGTM"
      ],
      "kubeflow-simplify-code-structure": [
        "Let's remove this function. We only need this initialization in one place and can simplify with something like:\r\n\r\n```golang\r\nstatus := v1beta1.NotebookStatus{\r\n    Conditions:     make([]v1beta1.NotebookCondition, 0),\r\n    ReadyReplicas:  sts.Status.ReadyReplicas,\r\n    ContainerState: corev1.ContainerState{},\r\n}\r\n```",
        "Could you break this if statement to the following ones:\r\n\r\n```golang\r\nif pod.Status.ContainerStatuses[i].Name != instance.Name {\r\n    continue\r\n}\r\n\r\nif pod.Status.ContainerStatuses[i].State == instance.Status.ContainerState {\r\n    continue\r\n}\r\n\r\nlog.Info(\"Updating Notebook CR state: \", \"namespace\", instance.Namespace, \"name\", instance.Name)\r\ncs := pod.Status.ContainerStatuses[i].State\r\n...\r\n```",
        "nit: You could have an `if nodename == \"\" {` here and return nil if that holds.\r\n\r\nThen you could move the code below one tab to the left, outside of an if clause, to make it a little bit more simple to read"
      ],
      "kubeflow-follow-api-conventions": [
        "@DavidSpek show your comment below about the error with Unmarshal https://github.com/kubeflow/kubeflow/pull/5660#issuecomment-797378906. Writing it here to keep it in one place.\r\n\r\nI'm not sure how you used the Unmarshal function but I believe you could also try and catch the error and not let it panic. Here's a very similar code I've seen https://github.com/kubeflow/katib/blob/master/pkg/new-ui/v1beta1/backend.go#L73"
      ],
      "kubeflow-match-algorithms-to-purpose": [
        "Why does this need to be a `while` loop and not an `if`? \r\n\r\nAlso, shouldn't we have the condition also check if the status is \"empty\"? Since we want to have this message only when the controller hasn't set the status? \r\n\r\nElse won't it always expose the `Waiting for StatefulSet to create the underlying Pod.` message for the first 10 seconds independently of whether the status is set?",
        "We can simplify this and remove the `for` loop for the `item` var.\r\n\r\nThe `coditions` are a list of dict objects. We only need to iterate over each condition dict, from the list, and check if that condition object contains the `reason` key"
      ],
      "kubeflow-standardize-network-tools": [
        "@nrchakradhar thank you for your comments. I'm planning on tackling the IPv6 support for all the web apps after 1.3.\r\n\r\nThe first step is to bind the IPv6 equivalent addresses as well, as you've pointed out in your links. But I want to do a good deep dive on this and test some edge cases for which I don't have the cycles right now. "
      ],
      "kubeflow-unique-workflow-step-names": [
        "Could you provide some more details as to why this change is needed?",
        "Makes sense",
        "@PatrickXYS to make sure I'm synced with the proposed changes; the argument is to create a common base class for building a workflow that always has a step for kaniko-build, with `--no-push`, since we essentially use it in all of the component CI files?",
        "Regarding the changes for using a different class I agree with @DavidSpek that this isn't a blocker, so let's go with this implementation for now and if in the future we find ourselves repeating too much code then lets go over this argument.\r\nDoes this sound OK to you @PatrickXYS ?\r\n",
        "> Should the registry be called access-management or should it be kfam?\r\n\r\n@yanniszark what are your thoughts on this? I guess we should stick to `access-management` for now?\r\n\r\nI'll also give a heads up to our tracking issue to add this registry as well ",
        "> @kimwnasptd I actually just ran into an issue where using the common kaniko_builder.py actually complicates things slightly. For this situation specifically it is easily fixed\r\n\r\nCould you provide some more details for this? Just so that we can identify the pattern/pain-point once we see it reoccurring "
      ],
      "kubeflow-prioritize-readability-over-brevity": [
        "I was thinking that If in the future we want to handle other cases in the PATCH request, then each one of them should be its own function and not have all the logic unfolded in the PATCH handler",
        "I was thinking that If in the future we want to handle other cases in the PATCH request, then each one of them should be its own function and not have all the logic unfolded in the PATCH handler",
        "We should not ignore the E501 warnings.\r\nIf the line is too long you could put each argument in a distinct line.",
        "nit: @lalithvaka could you add this logic into a distinct function on its own?"
      ],
      "kubeflow-standardize-style-scripts": [
        "Why do we introduce rules for `tslint`, which is getting deprecated?\r\n\r\nLet's instead have the following 2 rules:\r\n```json\r\n    \"lint-check\": \"ng lint\",\r\n    \"lint\": \"ng lint --fix\",\r\n```\r\nsimilarly to KWA https://github.com/kubeflow/katib/blob/master/pkg/new-ui/v1beta1/frontend/package.json#L12-L13\r\n\r\nWe might also need to make some small changes to the `angular.json`. Take a look at the previous effort for this in https://github.com/kubeflow/kubeflow/pull/6464",
        "I see that we don't have this script in TWA, like we do in Katib\r\nhttps://github.com/kubeflow/katib/tree/master/pkg/new-ui/v1beta1/frontend/scripts\r\n\r\nYet the action didn't fail. Why is that?",
        "OK I see what happened. `||` means that the second command (node script in this case) will NOT be run if the first part was successful. But the node script will run if the prettier check fails.\r\n\r\nThat script is just there to instruct users to run the formatting. Let's include it in this PR"
      ],
      "kubeflow-use-enums-for-state": [
        "Thank you for the feedback @tasos-ale! \r\n\r\nThe reason I used `true` as a default value here is because if it starts as `null` then the app will think that the dashboard is not present and try to make a request to fetch the namespaces. \r\n\r\nSo in this case I take for granted the opposite, which is expect the dashboard to be present and if after the check it's missing then the apps will make the request to fetch the namespaces",
        "but indeed the enumeration is a better way to express this. I'll use this enum instead of a boolean value and the apps will fetch the namespaces only if the state is `Disconnected`",
        "pushed a commit for the described changes"
      ],
      "kubeflow-enforce-least-privilege": [
        "Should be good to go. It would also be a bug from the current code if it couldn't operate without listing namespaces, since it shouldn't need these permissions in the first place.\r\n\r\nNote that we might need to re-introduce them though, in a future iteration when we would want the apps to be self-standing. But we need to discuss other items for this effort. But I'm also mentioning this context to keep in mind"
      ],
      "kubeflow-document-networking-annotations": [
        "Can you add a comment here that for images in this group the backend will be:\r\n1. Adding the `notebooks.kubeflow.org/http-rewrite-uri` annotation to the CR, to rewrite the path that ends up in the running container to be `/`\r\n\r\nThis will help exposing to the users the constraints and logic that will be applied to images that belong to that group",
        "LGTM",
        "Can you add a comment here that for images in this group the backend will be:\r\n1. Adding the `notebooks.kubeflow.org/http-headers-request-set` annotation to the CR, for setting the `X-RStudio-Root-Path: ${ISTIO_PREFIX}`  header to each request to the running container\r\n2. Adding the `notebooks.kubeflow.org/http-rewrite-uri` annotation to the CR, to rewrite the path that ends up in the running container to be `/`\r\n\r\nThis will help exposing to the users the constraints and logic that will be applied to images that belong to that group",
        "LGTM"
      ],
      "kubeflow-control-header-modification": [
        "I think we should not let users arbitrarily add headers to the requests that pass from the ingress gateway. For example they could even override the ISTIO `userid-header`, the `X-Forwarded-Proto` so that the server assumes http instead of https etc.\r\n\r\nI think we should reconsider the design we take here\r\nalso cc @yanniszark who has a lot of experience on security",
        "I took a more extensive look at the docs and I propose the following solution:\r\n\r\nThe headers that are used to configure parts of R-Studio are all non-standard headers, starting with `X-*`. Such example headers are\r\n* `X-RStudio-Request`\r\n* `X-RStudio-Root-Path`\r\n* `X-Forwarded-Host`\r\n* `X-Forwarded-Proto`\r\n* `X-Auth-Token`\r\n\r\nSo I propose to go the other way around from blacklisting specific headers. Let's only allow the users to edit/set non-standard headers that start with `X-*`. This way we provide all the necessary configuration for R-Studio and at the same time don't allow the users to edit any header arbitrarily.\r\n\r\nAnd if in the future we find users that need to specifically set well defined headers for a Notebook then we can discuss based on the specific use case on how to proceed and allow users to add more headers.\r\n\r\nThis is the case for the docs I looked up to now\r\nhttps://docs.rstudio.com/ide/server-pro/access-and-security.htmlhttps://docs.rstudio.com/connect/admin/authentication/proxied/\r\nhttps://docs.rstudio.com/connect/admin/appendix/configuration/#ProxyAuth.Settings\r\n",
        "And lastly, the controller will set accordingly the `X-RStudio-Root-Path` header if the `server-type` is `rstudio`. But the user could still be able to override this by manually specifying a different value.\r\n\r\nThis way in order for a CR to support RStuido it will only need the `notebooks.kubeflow.org/server-type` annotation, which provides a good default mechanism and extensibility to the users.",
        "> Also, using X-* for headers seems to have been deprecated int 2012: see https://tools.ietf.org/html/rfc6648\r\n\r\nThis is a very good argument to make to the folks maintaining the R-Studio code :) \r\n\r\n> However, by backing this into the controller administrators cannot easily customize these values.\r\n\r\nWhat extra headers do you think they would need to set aside, from the `X-*`  headers R-Studio knows/expects?\r\n\r\n> Regarding your last comment, are you suggesting that the current mechanisms added to the controller stay as they are, but the controller itself adds the annotations to the CR when notebooks.kubeflow.org/server-type: rstudio is present?\r\n\r\nNo, the controller will not add any extra annotations if the `server-type` is present. It will only configure the vsvc accordingly.\r\nLet me try to clarify with two examples:\r\n\r\nCR 1:\r\n```yaml\r\nmetadata:\r\n  annotations:\r\n    notebooks.kubeflow.org/server-type: rstudio\r\n```\r\n\r\nIn this case the controller will see that the Notebook is for R-Studio so it will set the `/` url rewrite in the vsvc and also set the `X-RStudio-Root-Path` header in the vsvc for the prefix\r\n\r\nCR 2:\r\n```yaml\r\nmetadata:\r\n  annotations:\r\n    notebooks.kubeflow.org/server-type: rstudio\r\n    notebooks.kubeflow.org/http-rewrite-uri: /some/path\r\n```\r\n\r\nIn this case the controller will first do exactly what it would for CR 1. But then it would also detect the `notebooks.kubeflow.org/http-rewrite-uri` and use this value for the Istio rewrite, instead of the default it previously put. \r\n\r\nAlso the same approach will be used with the `notebooks.kubeflow.org/http-headers-request-set`. If this annotation is present then the controller will use the `X-*` headers mentioned in this annotations instead",
        "> Regarding the server-type thing, we can do that in the future but its not necessary right now.\r\n\r\n@thesuperzapper I can understand your urge to have this effort merged as soon as possible, but this is not a big feature so lets get it right from the beginning and not worry about it in the future. The functionality I described is almost there.\r\n\r\n> Adding a whitelist would only act to reduce the usefulness of this feature for users why might have their own container images.\r\n\r\nCould you elaborate on this? What use cases do you think we limit? With the mention mechanism, of allowing the user to set any `X-*` header they want, we still support all of the configuration options from R-Studio."
      ],
      "kubeflow-descriptive-consistent-naming": [
        "nit: Let's use `centraldashboard-angular` here as well, to make sure we use the same value everywhere.\r\n\r\nWe can revert to `centraldashboard` once we are confident with switching the web apps"
      ],
      "kubeflow-use-snake-case-in-python": [
        "Let's rename this var to `creation_timestamp`. Python is using snake_case for var names \r\nhttps://peps.python.org/pep-0008/#descriptive-naming-styles",
        "let's not use `camelCase` for Python variables. This should be `cpu_limit`",
        "same with `camelCase` as above. This should be `memory_limit`"
      ],
      "kubeflow-check-before-use": [
        "Let's move this part completely outside of this function and treat the Pod as the StatefulSet. \r\n\r\nThe main reconciliation logic is responsible for finding the objects (Pod, StatefulSet) and our function will need to check if they are `nil` or not",
        "This nesting level can be simplified by checking the incoming `pod` object:\r\n\r\n```golang\r\nif pod == nil {\r\n    log.Info(\"No pod found. Won't update notebook conditions and containerState\")\r\n    return status, nil\r\n}\r\n\r\n```"
      ],
      "kubeflow-normalize-url-paths": [
        "Let's rephrase this to something like the following:\r\n\r\nWhen Istio exports Services it always expects a `/` at the end. SO we'll need to make sure the links propagated to the iframe end with a `/`",
        "Let's add a comment here to explain why we need this function, which is to handle the sometimes missing `/` from some urls"
      ]
    },
    "profile": {
      "location": "Athens",
      "company": "@Canonical",
      "blog": "https://www.linkedin.com/in/kimonas-sotirchos-1ba45b155/",
      "site_admin": false,
      "followers": 74,
      "following": 7
    }
  },
  "dougwilson": {
    "repos": [
      "expressjs/express"
    ],
    "entries": [
      {
        "slug": "express-access-settings-properly",
        "title": "Access settings properly"
      },
      {
        "slug": "express-accurate-jsdoc-documentation",
        "title": "Accurate JSDoc documentation"
      },
      {
        "slug": "express-clear-array-operations",
        "title": "Clear array operations"
      },
      {
        "slug": "express-clear-intention-in-names",
        "title": "Clear intention in names"
      },
      {
        "slug": "express-enforce-null-safety-patterns",
        "title": "Enforce null safety patterns"
      },
      {
        "slug": "express-ensure-test-completion",
        "title": "Ensure test completion"
      },
      {
        "slug": "express-exclude-sensitive-configurations",
        "title": "Exclude sensitive configurations"
      },
      {
        "slug": "express-follow-standardjs-when-modifying",
        "title": "Follow StandardJS when modifying"
      },
      {
        "slug": "express-handle-streams-properly",
        "title": "Handle streams properly"
      },
      {
        "slug": "express-optimize-hot-paths",
        "title": "Optimize hot paths"
      },
      {
        "slug": "express-propagate-errors-properly",
        "title": "Propagate errors properly"
      },
      {
        "slug": "express-purposeful-style-changes",
        "title": "Purposeful style changes"
      },
      {
        "slug": "express-rest-principles-first",
        "title": "REST principles first"
      },
      {
        "slug": "express-single-source-documentation",
        "title": "Single source documentation"
      },
      {
        "slug": "express-standardize-dependency-version-notation",
        "title": "Standardize dependency version notation"
      },
      {
        "slug": "express-structured-release-workflows",
        "title": "Structured release workflows"
      },
      {
        "slug": "express-use-unique-password-salts",
        "title": "Use unique password salts"
      }
    ],
    "comments": {
      "express-propagate-errors-properly": [
        "If there is an error, it should ideally be forwarded to the error handling pipeline rather than swallow and just ending the response wirh no info.",
        "It should forward the error to the error handling pipeline, not just output the error directly. May sites what users to see a customized error page and have the error written to their logs, not displayed to the end user. The error pipeline (through error handling middlewares) allows this to be customized.",
        "You probably need to remove this handlers before proceeding. The error handler itself that `next` may try to write to `res` and cause an error, and this would end up involing `next` a second time.",
        "Also on this res error, it probably means we need to stop the pipe from the blob so it doesn't keep writing to res. ",
        "I'm not sure if the examples should demonstrate that any possible error above should result in a 400. What if the data source to get the note has an issue (like cannot connect to db)? One would normally expect it to 500 rather than 400. I also wonder if these should be using the Express error handling mechanism to respond rather than demo putting a try...catch in every single handler (except `getAll` that doesn't need one for some reason)? The reason I ask is because ideally we want to demo using Express.js and it's features in our demo apps as much as possible.",
        "the aborted error should only go to `fn` if there is one; never to `next`. just change it so it doesn't go to `next` :)\n"
      ],
      "express-follow-standardjs-when-modifying": [
        "I would just leave this off the pr since it's a style only change.",
        "All net new code should be written in StandardJS style per https://github.com/expressjs/express/blob/master/Collaborator-Guide.md#prs-and-code-contributions",
        "Typically we do not indent these lines, keeping the `.` at the same indent as the function call. Can you revert all the re-formatting of the file that wasn't related to your changes?"
      ],
      "express-handle-streams-properly": [
        "Should we add a code path here to use https://nodejs.org/api/buffer.html#blobstream when supported? Not sure how usable that is, but seems Node.js is adding the ability to make a Blob from a file which could be huge.",
        "I'm not sure why a custom write stream needs to be added, but it is missing backpressure handling, which is very important in http servers like this because the clients can stop reading if they know this is a large response, leading to a dos vector. But I would think you don't need to implement that at all, as you should be able to use `.toWeb()`/`.fromWeb()`, right? I ask because I'm not familiar with the new APIs for web streams (yet). So either we should use that or this custom stream needs to have backpressure handling added",
        "Just to note, if it doesn't work or make sense to land in the 4.x line due to back compat, that's no problem, as we have the 5.x line thay doesn't have that concern and as soon as I finish getting out a sec path for a diff module, the last 5.x pre release will be published so we can publish the final.",
        "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "remove your aborted listener when done\n"
      ],
      "express-accurate-jsdoc-documentation": [
        "This jsdoc does not reflect what you are doing. It documents the call as .param('id', [fn1, fn2])\n",
        "We had a PR about this right now, so we should make sure we land it right this time :) The `options` argument needs to be optional in the JSDoc here since it's optional in the code 👍 "
      ],
      "express-use-unique-password-salts": [
        "> here '10' is the salt used for encryption\r\n\r\nI thought your PR said you don't need a salt? Can you vlarify this comment and/or PR description?",
        "Gotcha. So then the comment right above this seems misleading/confusing:\r\n\r\n> A better Way to Hash Password without passing any salt using Bcrypt.Js\r\n\r\nit says \"without passing any salt\", but this line's comment says \"here '10' is the salt\""
      ],
      "express-access-settings-properly": [
        "I would expect that if the user explicitly set the `etag` option to `res.sendFile`, it would not be overwritten by the setting.\n",
        "You should use the setting functions to access settings, not direct access to the object. Direct object access was deprecated in 3.0.\n"
      ],
      "express-exclude-sensitive-configurations": [
        "Since certs have an expiration date, I would suggest not checking them in and either leave it as an exercise for the user running the example to create it with the instructions you provided or use one of those npm modules that auto generate them."
      ],
      "express-clear-intention-in-names": [
        "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "It is, but the change here is to use `this.get`, not `this.getHeader`. In addition, they are both case-insensitive, but we use the standard header casing in the code (as you can see from all the other parts).",
        "They are private, which is why they are not documented and the testing for them is tested though the public APIs that utilize them 👍 ",
        "Yes, it is curious 😀 They are before my time so cannot answer the why, lol. Even an underscore doesn't actually stop folks from using stuff, and a large project even ends up needing to be csreful modifying those things. Really only way to protect is an inside out obect, clever usage of closures, or the newer private class members. Idk if 5.x will change it, as it is late in the dev on it and probably need to scout round for the usages to make sure they are still provided in some way and truely private them in like 6.\r\n\r\nI bet people are accessing settings directly, at least. Perhaps too cache if they wanted to clear it since there is no API to do so.",
        "This should probably have a different name, otherwise people will try to `app.dispatch(req, res)` with it.\n"
      ],
      "express-standardize-dependency-version-notation": [
        "Be sure to keep the `~` symbol; if a 2.0.7 is published, then a user upgrading from a 4.17 version of a 4.18 version would end up downgrading the `proxy-addr` installed.",
        "You cannot use `^` in our `package.json` file.\n",
        "`^` is not compatible with all the base installed for 0.10.x. people would have to upgrade their npm just to use express, which is not always possible in certain enterprise environments. plus we don't only hard-pin external dependencies since it's too easy for them to break people's express otherwise\n"
      ],
      "express-purposeful-style-changes": [
        "I know the PR was updated already, but I figured I'll reply for just information sharing: the short answer is no, but the long answer is that PRs are accepted as long as they provide some kind of merit; for example, if there is a demonstrable perf improvement."
      ],
      "express-single-source-documentation": [
        "Should this section link to https://github.com/expressjs/express/blob/master/Contributing.md#tc-process , just replace that part of that other document, or something else? It seems like a duplication and could easily get lost where we make a change to one but forget the other, so the less duplication probably the better.",
        "All of these \"you\"s need to be re-worded in to the third person from the second person, as the document is meant to be in the third person form."
      ],
      "express-optimize-hot-paths": [
        "For any Node.js version where `require('stream/web')` doesn't work, this is going to cause a sync walk of the file system every time, since a `require()` failure is not added to the module cache like a success is. This would ideally be moved to only happen on the loading of this module, not on every single response write.",
        "this creating fake contexts is expensive; could use refactoring somewhere to not need to do this.\n"
      ],
      "express-rest-principles-first": [
        "I agree. The `Blob` is just a byte stream, so whatever the `type` is shouldn't be touched, as Express has no way to know what the charset is of the bytestream.",
        "Makes sense. I think that is what this pr is already doing, but if not, it shouldn't set the type if already set, which is what all the other arguments to send do.",
        "I'm not sure why this was marked resolved, as nothing was fixed and the issue @jimmywarting mentioned with adding charset is still there.",
        "We cal always be more restrictive than the spec and then loosen up later (but not the other way around), so alphanumerics would at least be a start :)\n"
      ],
      "express-ensure-test-completion": [
        "without an `else` here, a failure of the check leaves the test suite hanging. if you are testing the body contents, just use supertest to do it rather than manually checking res.body which would simplify it.",
        "Darn. That particular tests doesn't do much at all. It seems like you likely just need to blanket increase the timeout for the environment you're running in. Are you about to run the test suite like the following? `npm test -- --timeout 7500` ?",
        "Ah, I see, there is a an issue with npm passing arguments around. Any reason you are using `npm run test-ci` ? That is meant just for the usage of the GitHub Actions CI here. Why not just use `npm test` ?",
        "Since this is not an async test, you don't need to use `done` here and just throw the errors",
        "use mocha, not try-catch\n"
      ],
      "express-enforce-null-safety-patterns": [
        "Probably don't want to reassign values linked to arguments, since it silently alters `arguments[0]`. Usually I would just name the argument `options` and this line as `var opts = options || {}`\r\n```\r\n$ node -pe '(function(foo){foo=foo||{};return arguments[0] === null}(null))'\r\nfalse\r\n```",
        "Usually not best practice to alter the object someone is passing in; the object could potentially be frozen, even, causing an exception here. `var createServer = opts.createServer || http.createServer` is probably fine (and then changing the vars below).",
        "Pleas do not use in for option detection. This makes the interface really confusing because you have to ensure that whatever you are passing in does not have that key anywhere in the prototype chain.\n",
        "Do not pass in non-booleans to the sub module, because that is just asking for undefined behavior later. The etag option only accepts true or false.\n"
      ],
      "express-structured-release-workflows": [
        "I'm not sure if this was the way it has ever been done. Typically there are two main release flows: patch and non-patch.\n\nThe patch flow is pretty simple: it's just extremely simple updates, like typo fixes, patch dependency updates, and maybe a fix, depending on how risky it is. Every other change is always a non-patch flow.\n\nThe non-patch flow is done using a separate branch (for example `4.13`, `5.0`, etc.) and tracked using a release pull request (for example, https://github.com/expressjs/express/pull/2682, https://github.com/expressjs/express/pull/2237, etc.).\n"
      ],
      "express-clear-array-operations": [
        "`[key].concat(fns)` will inadvertinately allow for the arguments to be arrays, because it'll flatten in certain cases, i.e.:\n\n``` js\n$ node -pe '[2].concat([1,2])'\n[ 2, 1, 2 ]\n```\n"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 3715,
      "following": 12
    }
  },
  "fmeum": {
    "repos": [
      "bazelbuild/bazel"
    ],
    "entries": [
      {
        "slug": "bazel-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "bazel-automate-sensitive-ci-artifacts",
        "title": "Automate sensitive CI artifacts"
      },
      {
        "slug": "bazel-avoid-brittle-algorithms",
        "title": "avoid brittle algorithms"
      },
      {
        "slug": "bazel-avoid-eager-cache-invalidation",
        "title": "Avoid eager cache invalidation"
      },
      {
        "slug": "bazel-avoid-unnecessary-computations",
        "title": "Avoid unnecessary computations"
      },
      {
        "slug": "bazel-build-action-separation",
        "title": "build action separation"
      },
      {
        "slug": "bazel-check-operation-failures",
        "title": "Check operation failures"
      },
      {
        "slug": "bazel-clarify-configuration-examples",
        "title": "Clarify configuration examples"
      },
      {
        "slug": "bazel-comprehensive-edge-case-testing",
        "title": "comprehensive edge case testing"
      },
      {
        "slug": "bazel-configuration-clarity-standards",
        "title": "Configuration clarity standards"
      },
      {
        "slug": "bazel-document-network-interface-purposes",
        "title": "Document network interface purposes"
      },
      {
        "slug": "bazel-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "bazel-ensure-documentation-completeness",
        "title": "Ensure documentation completeness"
      },
      {
        "slug": "bazel-environment-aware-configuration-testing",
        "title": "Environment-aware configuration testing"
      },
      {
        "slug": "bazel-executor-service-lifecycle",
        "title": "Executor service lifecycle"
      },
      {
        "slug": "bazel-explicit-null-checks",
        "title": "explicit null checks"
      },
      {
        "slug": "bazel-optimize-algorithm-choices",
        "title": "optimize algorithm choices"
      },
      {
        "slug": "bazel-prefer-simple-api-designs",
        "title": "prefer simple API designs"
      },
      {
        "slug": "bazel-prefer-simple-readable-code",
        "title": "prefer simple readable code"
      },
      {
        "slug": "bazel-preserve-exception-causes",
        "title": "preserve exception causes"
      },
      {
        "slug": "bazel-simplify-complex-code",
        "title": "Simplify complex code"
      },
      {
        "slug": "bazel-temporary-security-workarounds",
        "title": "temporary security workarounds"
      },
      {
        "slug": "bazel-use-descriptive-unambiguous-names",
        "title": "Use descriptive, unambiguous names"
      },
      {
        "slug": "bazel-use-semantically-clear-names",
        "title": "Use semantically clear names"
      },
      {
        "slug": "bazel-use-standard-api-abstractions",
        "title": "Use standard API abstractions"
      },
      {
        "slug": "bazel-validate-feature-configurations",
        "title": "validate feature configurations"
      },
      {
        "slug": "bazel-write-specific-test-assertions",
        "title": "Write specific test assertions"
      }
    ],
    "comments": {
      "bazel-use-standard-api-abstractions": [
        "Please use `args` for this as well - it will allow this action to opt into [path mapping](https://github.com/bazelbuild/bazel/discussions/22658) later as it avoids using `.path`."
      ],
      "bazel-configuration-clarity-standards": [
        "I reworded this a little to hopefully clarify the scope. I would be interested in integrating a less low-level version of this with `bazel coverage`, but don't really know how to accomplish this yet.",
        "Failures are usually preferred over warnings, both to reduce the amount of \"warning spam\" and also because it makes it easier to change the behavior later in a compatible way (for example, we could introduce new logic that chooses a reasonable default based on host machine info if this is set to 0).\r\n\r\nSee https://github.com/bazelbuild/bazel/blob/0e220f03f154cbb5e52190e1085b01173f301c02/src/main/java/com/google/devtools/build/lib/bazel/BazelRepositoryModule.java#L396-L404 for an example."
      ],
      "bazel-optimize-algorithm-choices": [
        "The `generate-modmap` tool uses `unsorted_set` extensively, which doesn't guarantee any particular ordering (I am not even sure whether its guaranteed to be deterministic on a single machine). Could you either use a data structure that preserves insertion order or just use `set` to get sort order?",
        "@tjgq This made me realize that the internal string encoding can cause Unicode matchers in regex to not match as expected. To be fully correct, we would need to reencode all paths before matching them and also reencode the pattern. Do you think that's relevant enough to change? (just to be clear, this is preexisting)",
        "https://github.com/bazelbuild/bazel/issues/26070",
        "While `Pattern.compile(\"abc$\").matcher(\"abc\\n\").matches()` is `false`, `Pattern.compile(\"abc$\").matcher(\"abc\\n\").find()` is `true`. That's because `$` does indeed match *before* a final line terminator (which contradicts the docs, I think), but the implicit anchoring bounds of `matches` still result in `false` since the line terminator itself remains unmatched.\r\n\r\nThat's why we need the negative lookahead and `UNIX_LINES` just happens to simplify that lookahead as we only have to match a single character.\r\n\r\n(If you agree that the docs are wrong, let me know and I'll send a PR to the JDK)",
        "Could you add a test for a fulfilled no-dep that crosses a compat level boundary (and thus causes the build to fail)?",
        "Sorry for the confusion, it took me some time to figure out how to specify the behavior I would like to see. It's not directly related to compatibility level crossing, but its implementation may constrain how we handle that.\r\n\r\nI would like to see the following test pass (when pasted into `DiscoveryTest.java`). Crucically, `ddd@1.0` never has its module file downloaded and also doesn't show up in the dep graph. This would make nodeps even less costly to add. \r\n\r\nIf we can't get this behavior that's also fine, I would just like to make sure we have tried at a point where we won't need a backwards incompatible change.\r\n\r\n```java\r\n  @Test\r\n  public void testNodep_ineffectiveUpdate() throws Exception {\r\n    scratch.file(\r\n        workspaceRoot.getRelative(\"MODULE.bazel\").getPathString(),\r\n        \"\"\"\r\n        module(name='aaa',version='0.1')\r\n        bazel_dep(name='bbb',version='1.0')\r\n        bazel_dep(name='ccc',version='1.0')\r\n        \"\"\");\r\n\r\n    FakeRegistry registry =\r\n        registryFactory\r\n            .newFakeRegistry(\"/foo\")\r\n            .addModule(\r\n                createModuleKey(\"bbb\", \"1.0\"),\r\n                \"module(name='bbb', version='1.0');bazel_dep(name='ddd', version='2.0')\")\r\n            .addModule(\r\n                createModuleKey(\"ccc\", \"1.0\"),\r\n                \"module(name='ccc', version='1.0');bazel_dep(name='ddd',version='1.0',repo_name=None)\")\r\n            .addModule(createModuleKey(\"ddd\", \"1.0\"), \"module(name='ddd', version='1.0')\")\r\n            .addModule(createModuleKey(\"ddd\", \"2.0\"), \"module(name='ddd', version='2.0')\");\r\n    ModuleFileFunction.REGISTRIES.set(differencer, ImmutableSet.of(registry.getUrl()));\r\n\r\n    EvaluationResult<DiscoveryValue> result =\r\n        evaluator.evaluate(ImmutableList.of(DiscoveryValue.KEY), evaluationContext);\r\n    if (result.hasError()) {\r\n      fail(result.getError().toString());\r\n    }\r\n    DiscoveryValue discoveryValue = result.get(DiscoveryValue.KEY);\r\n    assertThat(discoveryValue.depGraph().entrySet())\r\n        .containsExactly(\r\n            InterimModuleBuilder.create(\"aaa\", \"0.1\")\r\n                .setKey(ModuleKey.ROOT)\r\n                .addDep(\"bbb\", createModuleKey(\"bbb\", \"1.0\"))\r\n                .addDep(\"ccc\", createModuleKey(\"ccc\", \"1.0\"))\r\n                .buildEntry(),\r\n            InterimModuleBuilder.create(\"bbb\", \"1.0\")\r\n                .addDep(\"ddd\", createModuleKey(\"ddd\", \"2.0\"))\r\n                .setRegistry(registry)\r\n                .buildEntry(),\r\n            InterimModuleBuilder.create(\"ccc\", \"1.0\")\r\n                .setRegistry(registry)\r\n                .addNodepDep(createModuleKey(\"ddd\", \"1.0\"))\r\n                .buildEntry(),\r\n            InterimModuleBuilder.create(\"ddd\", \"2.0\").setRegistry(registry).buildEntry());\r\n    assertThat(discoveryValue.registryFileHashes().keySet())\r\n        .containsExactly(\r\n            registry.getUrl() + \"/modules/bbb/1.0/MODULE.bazel\",\r\n            registry.getUrl() + \"/modules/ccc/1.0/MODULE.bazel\",\r\n            registry.getUrl() + \"/modules/ddd/2.0/MODULE.bazel\");\r\n  }\r\n```",
        "1. Isn't that only unsafe in the sense that changing this behavior later would be a breaking change? I don't think that taking the versions of deps of \"ineffective\" nodeps into account is required for the resolution result to be considered reasonable.\r\n2. That's correct and I certainly don't want to change that. But for both a \"nodep\" dep or a \"lower version constraint\" (whatever we brand this new feature as), it's potentially wasteful.\r\n\r\nI don't consider making nodeps behave like deps in this sense a big problem, but it could come back to hurt us later. Nodeps will mostly be used in N:1 dep situations with somewhat large (protobuf) or really large (boost) N. In many cases, users at a slightly larger scale will use many of these N modules and likely have them updated to latest anyway, which would mean that nodeps as is would roughly double the number of module files visited while not being relevant for the resolution outcome most of the time. Gradual performance degradation is what worries me most here as it is 1) hard to attribute to nodeps and 2) hard to fix retroactively as per your point 1. I may be arguing for premature optimization here, but it's unfortunately not just an internal algorithm :-)"
      ],
      "bazel-simplify-complex-code": [
        "Could we share the common part of the construction of the builder between the branches?",
        "This can use a switch expression for an exhaustiveness test.",
        "Same as above, use a switch expression."
      ],
      "bazel-ensure-documentation-completeness": [
        "```suggestion\nThe functionality is documented inside the header file.\n```\n",
        "Could you add a comment here explaining the purpose of this function? Doesn't have to be user-facing as it is private, but dev-facing.",
        "It could make sense to mention that C++20 is a requirement (if that's actually true) to give users an idea when they should expect this to be available."
      ],
      "bazel-use-semantically-clear-names": [
        "Drop `_src`, that suffix is only there before packaging.\n\nPlease also mention the target under `@bazel_tools` that provides this header."
      ],
      "bazel-use-descriptive-unambiguous-names": [
        "Consider calling this `isExecutableNonTestRule` since that's what folks will wonder about at the call site.",
        "I agree that's better at this point, it can always be generalized later.",
        "@tetromino Note that the string representation of `Label` is currently misleading since it uses canonical and not unambiguous canonical labels. @Wyverald is working on a CL that fixes this, but in the meantime, I converted this to match on Java objects.",
        "It's only wiped out by a `bazel clean --expunge`, just like the persistent action cache (which I named this after). But I like \"hidden\" better.",
        "The parameter doesn't exist on the interface, but I added docs to `RemoteOutputChecker`. Let me know if that's not what you meant."
      ],
      "bazel-automate-sensitive-ci-artifacts": [
        "This file is pretty large and could potentially be used to hide exploits (I promise I didn't :-)). It would be great if this could be regenerated on import, followed by a rerun of the benchmark.",
        "We could probably replace the `bazel run` script with an aspect that runs a modified Turbine action to collect the profile as an output file. But CI has a lot of other stuff running in parallel, so profiling may not lead to accurate results anymore. Profile generation is highly non-reproducible. Alternatively, we could run this in the release pipeline for java_tools.",
        "Do you mean that I should move the file into `tools`, but keep the `genrule` referencing it in `third_party`? Wouldn't that cause the file to show up under `@bazel_tools//tools`?",
        "I moved it into `tools` and added it to `exclude` so that excluding it is documented as an explicit choice."
      ],
      "bazel-prefer-simple-api-designs": [
        "I made `FileWriteOutputArtifactValue` private, but also added `writeTo` to `FileArtifactValue` with a default implementation in terms of `getInputStream`. Having it implement `StreamWriter` just like `VirtualActionInput` is just very convenient for the use sites. Let me know if you find this too intrusive and I can refactor this further.",
        "Yes, there is a check in Skyframe that specifically guards against this.",
        "It's still enough and I just switched back. `InputMetadataProvider` is now always a transitive dep of `BulkTransferException`, which is what I initially wanted to avoid - but `LostArtifacts` brings that with it.",
        "Nit: Could you change the signature of this method to accept a `PathFragment` instead of a `String`?",
        "I am now passing in the execution info map directly. I still need some extra logic to save a field (which would result in a larger instance size), PTAL."
      ],
      "bazel-temporary-security-workarounds": [
        "I wouldn't want users to do this as it may break with future Ubuntu versions. If they need to take explicit action, they should use one of the sanctioned ways to disable the restriction.",
        "Don't worry, thanks for raising these questions. \n\nThe busybox workaround may go away in the future and yes, the new restriction is totally pointless until then (and until the other bypasses have been removed). \n\nMy plan is not to keep bypassing valid security restrictions, but exactly to keep Bazel working until 1) the restriction isn't pointless anymore and 2) there's a better story around allowlisting user applications. That's why I think that relying on a workaround is fine for now, without us committing to keep this working without user action indefinitely."
      ],
      "bazel-preserve-exception-causes": [
        "Should be fixed",
        "This is nicer indeed.",
        "It looks like it does throw via the call to `Substitution#getValue`.",
        "```suggestion\n          throw new WrappedIOException(e);\n```\n"
      ],
      "bazel-document-network-interface-purposes": [
        "I don't know enough about the use cases of `GrpcRemoteDownloader` to decide whether we could expect it to have access to Bazel registries, but I would guess that this should work and it was only left out of the Bzlmod implementation because nobody was certain that it would work. @Wyverald Is that correct? :-)\r\n\r\nI'm pretty sure `--repository_disable_download` isn't related to this since I only very recently made it so that this flag doesn't stop registry access. I'm also pretty sure that it doesn't interact in any way with remote build artifact downloads.\r\n"
      ],
      "bazel-write-specific-test-assertions": [
        "Yes, otherwise the `touch` would not necessarily verify that the directory is writable. Updated the PR description to call this out."
      ],
      "bazel-explicit-null-checks": [
        "Do we ever expect `source_artifact not in source_to_ddi_file_map`? If so, please add a comment explaining what happens in that case, if not, use `[...]` instead of `get` to make this an error.",
        "In that case please use `[...]` instead.",
        "Could this use `startswith`? In any case it would need bounds checking as `file` may be too short.",
        "I don't know, but the tests should tell us."
      ],
      "bazel-comprehensive-edge-case-testing": [
        "This fallback shouldn't be necessary - could you share the error you see if you don't add it?",
        "If you address the other comments and add an integration test that runs on Windows (you could also just drop the `no-windows` tag on the existing coverage tests), I can look into the root cause of the failure without this fallback. ",
        "I started a parallel PR that adds support for multiple files in `collect_coverage`. There's still one issue left that I need to debug, but once that's done, you should be able to base your PR on it.",
        "https://github.com/bazelbuild/bazel/pull/26603",
        "The PR now passes CI and includes an updated integration test that you can use as the basis for getting the others to pass. ",
        "This was both a nightmare (see the comments I added) and super helpful as the previous implementation had a number of bugs, including for the existing UTF-16 string case. PTAL."
      ],
      "bazel-avoid-eager-cache-invalidation": [
        "I tried that first but it doesn't work: the intermediate Bazel invocation doesn't update the action cache (because it uses a different version in the reproducer and because of `--nouse_action_cache` in the test), so the first and third run would always see the same GUID no matter how we change it.",
        "We don't have to, but I think it's worth doing: We know that the remote cache doesn't have the artifact anymore (as opposed to just knowing that it's TTL expired) and deleting the entry means that we can avoid build rewinding, which has an associated cost (mostly a loss in observability).",
        "That makes sense, I'll remove this part.",
        "```suggestion\r\n                  Stream.concat(\r\n                          entry.getOutputFiles().values().stream(),\r\n                          entry.getOutputTrees().values().stream()\r\n                              .flatMap(tv -> tv.childValues().values().stream()))\r\n                      .anyMatch(e -> e.getExpirationTime() == RemoteFileArtifactValue.SERVER_EXPIRATION_SENTINEL));\r\n```"
      ],
      "bazel-prefer-simple-readable-code": [
        "This duplication is errorprone. What do you think of extending the existing `_create_cc_compile_actions` with the new `module_interfaces_sources` parameter and, in that function, branching on whether it is non-empty?",
        "This looks good as as, thanks",
        "Avoid the duplicated long call via logic like this:\r\n```starlark\r\nuse_pic_values = []\r\nif generate_no_pic_action:\r\n  use_pic_values.append(false)\r\nif generate_pic_action:\r\n  use_pic_values.append(true)\r\n\r\nfor use_pic in use_pic_values:\r\n  ...\r\n```",
        "1. We need to match this part so that it is consumed and replaced, otherwise it would end up in the `sed` output. I could use another wildcard, but using two wildcards around a part we need to match exactly seemed too brittle to me.\n2. It may appear in file names, which is why i wanted this match to be as precise as possible. It is only relevant in edge cases though (and maybe not even there).\n3. `t` and `d` additionally drop unmatched lines, which is required to drop some symbol types that should be ignored. Default sed doesn't do that as far as I know.",
        "Top-level ifs are not allowed:\r\n```\r\n./src/test/java/net/starlark/java/eval/testdata/json.star:29:1: if statements are not allowed at the top level. You may move it inside a function or use an if expression (x if condition else y).\r\n```"
      ],
      "bazel-clarify-configuration-examples": [
        "Could make sense to say `Bazel modules` here as Go modules would be fetched with Gazelle.\n\nCould you also mention external deps and reference the rules_go `bzlmod.md` guide for that?"
      ],
      "bazel-check-operation-failures": [
        "What do you think of moving the escaping logic to https://cs.opensource.google/bazel/bazel/+/4196862e32ec1591cc2815a2a13fe7c2867dc2b4:src/tools/launcher/launcher.cc;l=218? These arguments aren't special, other arguments in launchers for other languages may equally be affected by this issue.",
        "Good catch. I like the idea of introducing this virtual method. ",
        "It already is in the case of `bazel ''`, as demonstrated by the test. Which other case are you referring to? Just `bazel` probably shouldn't fail as users may rely on it as a simple \"does Bazel work\" check."
      ],
      "bazel-ensure-comprehensive-test-coverage": [
        "The recent refactorings made wiring this up much simpler - the change only affects `DirectoryArtifactTraversalRequest` now. Do you still think I should add a test there?"
      ],
      "bazel-executor-service-lifecycle": [
        "@Wyverald I suggested this and am okay with it being changed back if you prefer that.\r\n\r\nTo me, Futures are a low-level primitive that's best avoided if a more high-level, structured alternative exists. `invokeAny`'s behavior regarding cancellation is pretty much determined by its type signature (it returns a bare value), which helps readability and avoids misuse. For example, a Future would need to be cancelled manually when `get` times out.",
        "I read a bit of JDK code and the only difference I could find is that `myWorkerFuture.cancel(true)` ensures that the `Future` both has its carrying thread interrupted *and* has its `done()` method called, which signals the `ExecutorService` that it is done. `workerExecutorService.shutdownNow()` instead seems to just interrupt the carrying threads and then waits for that to result in the `Future`s signalling that they are done. This could be a sign that something is still swallowing `InterruptedException`s in the worker threads.",
        "Yes, this wasn't clear on my part, I meant the (virtual) thread \"carrying\" the `Future`.",
        "Instead of combining a semaphore and a latch, maybe we could use a single shared `Phaser` instead? A download can `register` at the start of the lambda and `arriveAndDeregister` in the `finally` block. Then `close` can wait on `arriveAndAwaitAdvance`. You can search for `Phaser` in the codebase to find another very similar usage.",
        "I may very well be missing something, but why do we need to track the `init` phase at all with a `Phaser`? If the task has never been scheduled by the executor, we also don't need to cancel it. If it has started, it will either have registered with the `Phaser` (and will thus be awaited with `awaitAndAdvance`) or not since it hasn't even reached the `register` call (you can check the return value to avoid registrations happening after the advance).\n\nJust to clarify: With the `Phaser` approach, `addListener` also shouldn't be needed.",
        "I think that we could even use a single `Phaser` per context, see https://github.com/fmeum/bazel/tree/23837-phaser. It doesn't give use the opportunity the log the time taken to wait for a particular download cancellation though.",
        "```suggestion\r\n        downloadPhaser.arriveAndAwaitAdvance();\r\n```\r\nshould suffice as we don't care about deregistering (there won't be another phase)."
      ],
      "bazel-validate-feature-configurations": [
        "```suggestion\r\n    if module_interfaces and not feature_configuration.is_enabled(\"cpp_modules\"):\r\n```",
        "I guess this won't be supported when combined with C++ 20 modules? Could you add an appropriate `fail` to the `if` above so that users don't end up mixing both types of modules?",
        "I only rarely work on Windows builds, but even I got into situations where every single character counts. What do you think of also gating this abbreviation behind the new feature and giving it a more generic name that doesn't promise a particular scheme, say `shorten_virtual_includes`?"
      ],
      "bazel-avoid-unnecessary-computations": [
        "You can use `Iterables.filter` to avoid the overhead of allocating all the singleton lists + an extra iteration.",
        "I don't have any data to assess whether this causes a performance degradation even if coverage is not being recorded (which means that the instance returned will have a no-op `recordVirtualJump`). If this should turn out to be prohibitive, I could make the instance `static final` and populate it e.g. via an environment variable rather than a Bazel flag.",
        "How long does this take on a larger repo? It's a cost paid even by someone who doesn't use the new feature, so we need to be careful.",
        "I think I would prefer that, a slowdown of 1.5s just for providing this feature (even if unused) is something we should try to avoid.\n\n@tjgq as he worked on action cache GC logic recently",
        "@coeuvre I'm curious which files you had in mind when you introduced this in https://github.com/bazelbuild/bazel/commit/ebd6e58ac186d5137a4ba4450ae48a28b1e111f7. Is this perhaps about spawn outputs that aren't action outputs (e.g. `test.xml`)? If so, could we optimize this further by skipping the checks for non-test actions? It seems likely that this would also benefit from using the digest cache, but it could also pollute it when the digest mismatches often.",
        "In that case, what do you think of adding a check for whether the artifact is an output of the action (which we have available) and skipping I/O in that case? ",
        "Assuming that the `name` field only contains normalized file paths (which I guess it does?), this meant to be a pure optimization."
      ],
      "bazel-build-action-separation": [
        "This part of the change looks unnecessary as the followup PR does not set `dotdFile` on the builder for the codegen action. The action itself should contain as little action-specific branching as possible, so if we do need to tweak something, that should happen in the builder.",
        "I assume that you don't want to support include scanning with C++20 modules (that probably doesn't make sense). How about ensuring that include scanning is disabled for the deps scanning action at https://github.com/bazelbuild/bazel/blob/b07549d774d6768ada95dc2d4715557bce53bf2c/src/main/java/com/google/devtools/build/lib/bazel/rules/cpp/BazelCppSemantics.java#L114? Then we could move this computation into the branch below.",
        "This doesn't update the `ccCompilationContext` member, which is read by other methods. It's not clear to me whether that's a problem."
      ],
      "bazel-avoid-brittle-algorithms": [
        "I checked that it's zero (the call above also doesn't pass `allow_failure=True`). \r\n\r\nOur observations are indeed consistent with Skyframe only setting the error bit if a top-level key evaluation fails. I agree that the exit code should be non-zero if any failing extension has been encountered, but I don't know how to pull that off. If we figure it out, we may be able to just get rid of the injected value.\r\n\r\nI updated the test to verify exit codes (hence it fails).",
        "Thanks, this explanation was very helpful. I made the required changes."
      ],
      "bazel-environment-aware-configuration-testing": [
        "I think that the test fails because Bazel uses C++11 by default. You probably have to add `--copt=c++20`.",
        "We could, but I think that the test just wouldn't run if we did: Since Bazel passes `--std=0x` on Unix by default, it just won't be available on any of the runners.",
        "I think that it would remain relevant, this failure originates in remote code and it's not so clear what to do with dangling symlinks in hashed directories. Or would you expect that to fail early in some other way?",
        "Added a comment"
      ],
      "bazel-add-explanatory-comments": [
        "Could you add that parent directories are created as needed? Please also document what happens if the target already exists."
      ]
    },
    "profile": {
      "location": "Germany",
      "company": "@buildbuddy-io",
      "blog": "https://hen.ne.ke",
      "twitter_username": "fhenneke",
      "site_admin": false,
      "followers": 168,
      "following": 60
    }
  },
  "VladLazar": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-balance-flexibility-with-performance",
        "title": "Balance flexibility with performance"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-connection-pooling-with-pipelining",
        "title": "Connection pooling with pipelining"
      },
      {
        "slug": "neon-database-before-memory",
        "title": "Database before memory"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-concurrency-design-decisions",
        "title": "Document concurrency design decisions"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-ensure-algorithm-robustness",
        "title": "Ensure algorithm robustness"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-log-level-appropriately",
        "title": "Log level appropriately"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-reliable-concurrency-synchronization",
        "title": "Reliable concurrency synchronization"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      }
    ],
    "comments": {
      "neon-environment-specific-config-defaults": [
        "The production defaults don't make sense for the test env.\r\nThis will likely become a tenant config at some point, so we can remove this oddity then."
      ],
      "neon-stage-configuration-changes-gradually": [
        "Why do we support propagating these configs in two ways?",
        "Should these be optional? It allows us to try it out in staging/pre-prod without deploying the new endpoint everywhere in prod."
      ],
      "neon-database-before-memory": [
        "Another general guideline: aim to update the database state __before__ the in-memory state. Otherwise, if you crash in between it will look like you time travelled since storcon rebuilds the world state from the db.",
        "`schedule_compaction_update`, yeah that should, but the name is bad :laughing: \r\n\r\nFair point. I'll just unlink here.",
        "https://github.com/neondatabase/neon/pull/12038/commits/2053e7f4a4b8f838159008d0e7841c454bda4459"
      ],
      "neon-document-concurrency-design-decisions": [
        "https://github.com/neondatabase/neon/pull/11937/commits/9377e9af65921e06e25e4cec5607150faad56a1a\r\n\r\nI don't think a deadlock is possible here. The only place where we grab both locks at the same time is `write_to_disk` and in that case they're read locks.",
        "> We're relying on inner being append-only for this to be safe, yeah?\r\n\r\nCorrect.\r\n\r\n> Let's specify that as part of the locking protocol.\r\n\r\nDon't see how you'd express that via the locking protocol. Can you elaborate?",
        "Makes sense. Done in https://github.com/neondatabase/neon/pull/11937/commits/721e8989b0968023d06f8b52e6b559fc6af2cded",
        "The API sucks.\r\n\r\n`freeze` and `put_batch` are synchronized via `Timeline::write_lock`.\r\nWe only `freeze` when there's no ongoing writes. There's two paths on which we freeze:\r\n1. Via the active `TimelineWriter`. This holds the `Timeline::write_lock` for its lifetime. The rolling is handled in `TimelineWriter::put_batch`. It's a `&mut self` function so can't be called from different threads.\r\n2. In the background via `Timeline::maybe_freeze_ephemeral_layer`. This only proceeds if `try_lock` on `Timeline::write_lock` succeeds (i.e. there's no active writer), hence there can be no concurrent writes.",
        "https://github.com/neondatabase/neon/pull/11855/commits/af641fe64ac386a712952f531076d476eae6555e"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "The compute is available during pre-warm, correct? Wondering if there's any interactions between the user workload and prewarm that are worth considering."
      ],
      "neon-structure-endpoints-for-rest": [
        "This should be `POST`.",
        "This should be:\r\n`PUT`: if the endpoint does the offload before returning\r\n`POST`: if the endpoint does the offload in the background"
      ],
      "neon-document-parameter-choices": [
        "Would be good to leave a comment about why `retry_on_failures` is required."
      ],
      "neon-design-metrics-for-insights": [
        "Is this metric useful at timeline granularity? If the goal is to get a sense of the overall PS background operations, you can look at `rate(pageserver_storage_operations_seconds_count)`, but that's a per-pageserver view.\r\n\r\nAsking, since this is pretty high cardinality.",
        "This metric is being bumped multiple times for the same request. I think it makes sense to remove the increment from resolve and keep just this one (this is when we actually tell the compute to reconnect).",
        "Pushed [b01579a](https://github.com/neondatabase/neon/pull/12467/commits/b01579afad6b604597c95234d59a031c8b5f2329). PTAL",
        "Not sure I understand this. If the current offloader is indeed lagging, then the call to `determine_offloader` would have already picked a different offloader. I mean `offloader_sk_id` might not actually be the current offloader."
      ],
      "neon-ensure-algorithm-robustness": [
        "This isn't entirely correct. It doesn't handle:\r\n* overlapping layers\r\n* delta layers containing a full page image\r\n\r\nIn some cases it would panic and in others it would just yield an incorrect result.\r\nThe correct way of doing this is to use a neon local setup, import the tenant state from s3 and then use the HTTP endpoints to get the reconstructed page: `GET v1/tenant/<tid>-<shard-id>/timeline/<tlid>/getpage?lsn=<lsn>`.",
        "Indeed the check was wrong. Good catch.\r\n\r\nIt's not about bound checking since we don't blindly index into the jobs.\r\nIt narrows collisions down to collisions on plans with the same length.\r\n\r\n> Wonder why we wouldn't go all the way and compute a cryptographic checksum\r\n\r\nI don't think we'll have enough on-going imports at any given time to worry about collisions here.\r\nFor 1 million the collision probability is ~2.71e-08. This is without accounting for imports of the same length check.",
        "Fixed in https://github.com/neondatabase/neon/pull/11862/commits/5ec58a723b11e0370dab9c8b6cd0d64ceab414e2",
        "Done: https://github.com/neondatabase/neon/pull/11862/commits/27458a203b18510e20c57bf6feb229a171f59cb1"
      ],
      "neon-reliable-concurrency-synchronization": [
        "Would calling `env.storage_controller.reconcile_until_idle()` to force the storage controller to run all pending reconciliations work?",
        "This will hang since `NeonPageserver.start` waits for the node to be marked as active in the storage controller by default. Re-attaching is a pre-condition for that. To avoid this, you can do:\r\n```\r\nps.stop()\r\nps.start(await_active=False)\r\n```"
      ],
      "neon-balance-flexibility-with-performance": [
        "> Let's consider an extreme case, for illustration: we send a batch request for pages 0-100, and at page 50 we have to download a layer which takes 3 seconds. I think there are cases where it's advantageous to send back pages 0-49 first, and pages 50-100 later:\r\n\r\nOn streaming: the pageserver doesn't currently support streaming. The current implementation collects all the deltas and images for _all_ keys in the batch and then does walredo for all keys concurrently. We could do true streaming, but it's not trivial.\r\n\r\nOn how scattered the reads should be: hard to tell. Generally speaking, the more clustered the pages in a request are, the more predictable performance will be:\r\n* in-memory layers are ingest ordered: clustering by key doesn't help much here\r\n* delta layers are ordered by (key, lsn): clustering by key helps with index reads, but for the reads themselves our ability to merge depends on how many key versions we have\r\n* in-memory layers are ordered by key: clustering keys clearly helps here\r\n\r\nOverall take:\r\n* I'm not sure streaming responses makes sense at this stage. We currently support a max of 32 keys in a batch (PS side). We could increase it, but I don't think we should go above 512 pages short term. I'd just return all the pages at once. Streaming protocol can be done in the future, but I see there's other reasons quoted for streaming.\r\n* I think it makes sense to keep the grpc spec somewhat flexible here. The implementation can enforce strict batching rules or implement heuristics based on key, lsn range.",
        "> think the more important point here is that when we do the batching client-side, we know whether it makes sense to batch or not (i.e. whether to prioritize throughput or latency)\r\n\r\nWith the more flexible protocol, the communicator could coalesce batches, but perhaps we don't care about that (yet?)."
      ],
      "neon-configurable-cache-parameters": [
        "I don't have good intuition on what the right size for this cache is. Therefore, I'd make this a tenant config since those are configurable at run-time. The data structures supports capacity resizing via `resize_capacity`.\r\n\r\nThis would give us a tool to address replicas with slow get pages due to rel size cache churn.",
        "Please plug into `Timeline::tenant_conf_updated` and resize the cache when the config changes.\r\nOtherwise, the config change still requires a restart."
      ],
      "neon-connection-pooling-with-pipelining": [
        "What's the point of this extra channel now? Can't we just plug `caller_rx` into `client.get_pages`?",
        "Makes sense. The extra stream still allows for pipelining as long as the previous request was pushed down the tcp pipe (response need not be received by that point)."
      ],
      "neon-minimize-unnecessary-allocations": [
        "Would be good to avoid an extra allocation here. Could track the block number in `Self::response` and put everything in the right place with `Vec::spare_capacity_mut`.",
        "Sounds good. Can optimize later if needed."
      ],
      "neon-log-level-appropriately": [
        "Let's log a warning here. It's not that big of a deal for draining since we expect the node to come back, but here we're removing it. Also, these comments mention draining. Can you update them to avoid confusion?",
        "Don't think it's useful to log thousands of layers that we are keeping. We log removed layers somewhere else - that's what could actually be useful. Could also contribute to the loop taking longer than it should.",
        "Please update this log line and comment to mention that we place a tombstone instead of deleting."
      ],
      "neon-document-api-specs-completely": [
        "Let's be kind to future readers and mention why it's time consuming."
      ],
      "neon-comprehensive-code-documentation": [
        "nit: a comment would be nice:\r\n* explain difference between effective and request lsn\r\n* explain that primary compute always uses `request_lsn == MAX`\r\n* hint that this is how the compute thinks about get page requests"
      ],
      "neon-cache-performance-preservation": [
        "We terminate the old primary before promoting the secondary. If I'm reading this right, there's a an availability gap between the termination and promotion. Is this reading correct? If so, can we avoid it?",
        "It would be great to aim for something where the happy path has no hiccups. If that's not possible, let's be explicit about it and say why.\r\n\r\n---\r\n\r\nProxy learns about about the new primary after the promotion of the new compute, so there should be no queries on it at that point (according to the diagram). It seems like we can swap promotion and termination around while maintaining this property.\r\n\r\nWhat would happen to the client in this case? Let's say we have an in-progress query when the old primary terminates. Client would probably retry and eventually get routed to the new primary.\r\n\r\n",
        "> Two computes can Never, NEVER, never both be Primary on the same timeline at the same time. CPlane is supposed to make sure of that, and Compute will fail if it doesn't. Promotion of the replica before the original Primary shut down will cause errors, panics, data loss, shutdowns, and/or nasal deamons on the Primary, this promoting Secondary, or both.\r\n\r\nAt the risk of being annoying: why? I understand it doesn't work in the case when two primaries are being written to. But what about if we could guarantee the new primary is idle? More specifically, by idle I mean it would have to not write any WAL and be in some sort of consensus observer role.\r\n\r\nI'm sure you're right about this working with the current state of afairs, but I'm curious about what the technical roadblock is."
      ]
    },
    "profile": {
      "location": "London, United Kingdom",
      "blog": "",
      "site_admin": false,
      "followers": 16,
      "following": 4
    }
  },
  "crenshaw-dev": {
    "repos": [
      "argoproj/argo-cd"
    ],
    "entries": [
      {
        "slug": "argo-cd-choose-appropriate-synchronization-primitives",
        "title": "Choose appropriate synchronization primitives"
      },
      {
        "slug": "argo-cd-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "argo-cd-configuration-ui-consistency",
        "title": "Configuration UI consistency"
      },
      {
        "slug": "argo-cd-design-extensible-apis",
        "title": "design extensible APIs"
      },
      {
        "slug": "argo-cd-follow-go-naming-conventions",
        "title": "Follow Go naming conventions"
      },
      {
        "slug": "argo-cd-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "argo-cd-prefer-early-returns",
        "title": "Prefer early returns"
      },
      {
        "slug": "argo-cd-prefer-modern-react-patterns",
        "title": "prefer modern React patterns"
      },
      {
        "slug": "argo-cd-prevent-silent-failures",
        "title": "Prevent silent failures"
      },
      {
        "slug": "argo-cd-provide-comprehensive-explanations",
        "title": "Provide comprehensive explanations"
      },
      {
        "slug": "argo-cd-simplify-code-readability",
        "title": "Simplify code readability"
      },
      {
        "slug": "argo-cd-structured-logging-practices",
        "title": "structured logging practices"
      },
      {
        "slug": "argo-cd-use-descriptive-constants",
        "title": "Use descriptive constants"
      },
      {
        "slug": "argo-cd-validate-conceptual-api-types",
        "title": "validate conceptual API types"
      },
      {
        "slug": "argo-cd-validate-configuration-appropriateness",
        "title": "Validate configuration appropriateness"
      },
      {
        "slug": "argo-cd-validate-external-urls",
        "title": "validate external URLs"
      },
      {
        "slug": "argo-cd-validate-untrusted-inputs",
        "title": "Validate untrusted inputs"
      },
      {
        "slug": "argo-cd-wrap-errors-with-context",
        "title": "Wrap errors with context"
      }
    ],
    "comments": {
      "argo-cd-optimize-algorithmic-complexity": [
        "While we're in here, I think we should fix this logic. Instead of constructing a single string and then comparing, we should compare the highest priority field, return if not equal, proceed to the next field if equal, etc.",
        "Copilot came up with this:\r\n\r\n```\r\nsort.Slice(newConditions, func(i, j int) bool {\r\n\tleft := newConditions[i]\r\n\tright := newConditions[j]\r\n\r\n\tif left.Type != right.Type {\r\n\t\treturn left.Type < right.Type\r\n\t}\r\n\tif left.Message != right.Message {\r\n\t\treturn left.Message < right.Message\r\n\t}\r\n\tif left.Status != right.Status {\r\n\t\treturn left.Status < right.Status\r\n\t}\r\n\tif left.Reason != right.Reason {\r\n\t\treturn left.Reason < right.Reason\r\n\t}\r\n\treturn left.LastTransitionTime.Before(right.LastTransitionTime)\r\n})\r\n```",
        "```suggestion\r\n\t\tif left.Status != right.Status {\r\n\t\t\treturn left.Status < right.Status\r\n\t\t}\r\n\t\tif left.Reason != right.Reason {\r\n\t\t\treturn left.Reason < right.Reason\r\n\t\t}\r\n\t\tif left.Message != right.Message {\r\n\t\t\treturn left.Message < right.Message\r\n\t\t}\r\n```\r\n\r\nThis should give us somewhat more stable order, since the messages are arbitrary."
      ],
      "argo-cd-simplify-code-readability": [
        "And one more... might be worth a lint rule? :-) "
      ],
      "argo-cd-configuration-ui-consistency": [
        "Could we have some unit tests for these utility functions? I think we're trying to mimic Helm's values file merging behavior. If we find later that our merge algorithms differ from Helm's, it would be good to have unit tests to help safely change our algorithm to match Helm's."
      ],
      "argo-cd-validate-untrusted-inputs": [
        "Do we need to be concerned about someone constructing a link that tries to access stuff they shouldn't be accessing? Do we need some validation on repo, chartName, and version beyond what already exists?",
        "I guess I'm thinking more of a malicious scenario, where a user passes `repoName=../../../etc/passwd` or similar. It's not an easy attack path, but it seems like it would be super easy to validate against.",
        "Ditto re: securejoin. If `relPath` refers to a symlink, this join could end up referring to something above `s.dest`.",
        "We should probably require that ConfigMaps outside the argocd namespace have some label advertising them as available for use as a plugin. Otherwise users might use the AppSet as a way to try to leak information from arbitrary ConfigMaps which they may or may not have access to."
      ],
      "argo-cd-use-descriptive-constants": [
        "Why 14?",
        "```suggestion\r\n                // Show \"sha256: \" plus the first 7 actual characters of the digest.\r\n                message += ' (' + revision.substring(0, 14) + ')';\r\n```"
      ],
      "argo-cd-prefer-modern-react-patterns": [
        "Should we have a linter for this? I see we use `<Consumer>` a lot."
      ],
      "argo-cd-provide-comprehensive-explanations": [
        "I think \"rather it is the values/valuesObject merged with parameters\" is still valuable to include. Maybe just rephrased to be easier to understand."
      ],
      "argo-cd-validate-external-urls": [
        "Need to be super careful about injection here... do we need to do any sanitization on imageUrl?",
        "I'd suggest dropping the image URL from the UI for now and add it as a follow-up enhancement. I think we might want to enforce domain allowlists or something like that."
      ],
      "argo-cd-structured-logging-practices": [
        "A couple suggestions:\r\n\r\n1) I'd unmarshal the patch into a map[any]any and log it as its own field, so it's parseable by log tools\r\n2) I'd exclude the patch from the event: some people put _huge_ stuff in their app spec",
        "```suggestion\r\n\t\t\t\tlogFields := log.Fields{}\r\n```\r\n\r\nProbably fine to start with an empty set and replace it if the type assertion is successful.",
        "```suggestion\r\n\t\t\tlogCtx.WithFields(applog.GetAppLogFields(&generatedApplications[i])).Errorf(\"validation error found during application validation: %s\", message)\r\n```\r\n\r\nSomething like that would get you the standard app log fields.",
        "Makes sense. At any rate, I'd go with \"application\" instead of \"app\" to match the standard: https://argo-cd.readthedocs.io/en/latest/operator-manual/security/#standard-application-log-fields",
        "added revision and repo url"
      ],
      "argo-cd-follow-go-naming-conventions": [
        "I think convention is to add a `_seconds` suffix to timestamp gauges."
      ],
      "argo-cd-prefer-early-returns": [
        "I'd short-circuit instead of nesting\r\n\r\n```go\r\nif kubeutil.IsCRD(live) {\r\n// CRDs don't get tracking annotations.\r\nreturn nil\r\n}\r\n```",
        "We can avoid the deep nesting by just short-circuiting here.\r\n\r\n```suggestion\r\n\tif !needToUpdateConditions {\r\n\t\treturn nil\r\n\t}\r\n```"
      ],
      "argo-cd-design-extensible-apis": [
        "Would it be possible to unmarshal the patch into a map[string]any so log tools aren't forced to parse a JSON string?"
      ],
      "argo-cd-validate-configuration-appropriateness": [
        "I really don't think we should do this. imo using a fork of a lightly-maintained library is worse than just using the lightly-maintained library."
      ],
      "argo-cd-comprehensive-function-documentation": [
        "I feel like this function is doing too much... the revisions and phase output parameters seem to be simple aliases of their respective fields in `app.Status.OperationState`. If we need short var names, we can just do\r\n\r\n```go\r\nopPhase := app.Status.OperationState.Phase\r\nopRevisions := app.Status.OperationState.SyncResult.GetRevisions() # would probably need a new receiver\r\n```\r\n\r\nThen this function could focus on the first return param, which seems to involve the more interesting logic.",
        "Maybe but I'm really struggling to understand what this function does due to it being so crowded.\r\n\r\nFor now I'd settle for a docstring thoroughly explaining the intent and a TODO to remove the unnecessary behavior.",
        "Not really, because I don't think I fully understand either the behavior or the intent of the function (regarding just the first output param).\r\n\r\n> alreadyAttemptedSync returns whether the most recent sync was performed against the desiredRevisions and with the same app source config which are currently set in the app.\r\n\r\nI think I basically follow that, but the actual behavior is:\r\n\r\n1) If there is no operation state, return false, because we can't confirm the above two things.\r\n2) If we have an operation state but not a sync result, we return true if and only if the phase was marked completed. This is weird, because we haven't confirmed whether the synced revisions match `desiredRevisions` or whether the source config has changed. \"Phase is completed\" wasn't even mentioned as a criteria in the docstring. Why return true if the phase is completed? There's a docstring above that `return`, but it seems to be explaining the behavior of the calling function, not the `alreadyAttemptedSync` function.\r\n3) If `newRevisionHasChanges`, return `false` if `desiredRevisions` doesn't match the synced revisions. What does `newRevisionHasChanges` mean? And why aren't we checking whether the synced source config has changed, like the function docs mentioned?\r\n4) If not `newRevisionHasChanges`, just return whether the synced source config matches current source config. But it's unclear why we're not comparing desired revisions to synced revisions. Is that already confirmed to be true because `newRevisionHasChanges` is false? If `newRevisionHasChanges` means \"desired revisions don't match synced revisions\", why are we checking that again in the previous point?",
        "> alreadyAttemptedSync is meant to help the caller understand whether an identical sync operation has been attempted, to avoid excessively retrying the exact same sync operation.\r\n>\r\n> alreadyAttemptedSync returns true if either 1) newRevisionHasChanges is true and the most recently synced revision(s) exactly match the given desiredRevisions, 2) newRevisionHasChanges is false and the most recently synced app source configuration matches exactly the current app source configuration, or 3) the most recent operation state is missing a sync result but the sync phase is completed (this can happen when there are errors that cause the sync result not to be persisted). The last case returns true, because the caller should treat such a failed sync as an attempt. \r\n>\r\n> alreadyAttemptedSync returns false if the operation state is not set at all. This happens when the app is brand new or when a new operation has started.\r\n>\r\n> alreadyAttemptedSync also returns the most recently synced revisions and the most recent sync operation's phase.\r\n>\r\n> TODO: remove the last two return parameters, since they're effectively just aliases for fields on the app object. If the nil checks are too cumbersome for the caller, they can be moved into separate utility functions to avoid crowding this one.\r\n\r\nThis would get closer. But I think the references to `newRevisionHasChanges` should be replaced with a plain-language explanation of what that parameters means, and that param should be documented.",
        "The description of `newRevisionHasChanges`. cleared up a lot for me.\r\n\r\nWould probably be good to standardize on one var name for that variable and document it in each function that uses it. But that can be a future enhancement."
      ],
      "argo-cd-choose-appropriate-synchronization-primitives": [
        "I haven't used `atomic` much... could we just use `atomic.Int64`? Docs seem to recommend that: https://pkg.go.dev/sync/atomic#AddInt64"
      ],
      "argo-cd-wrap-errors-with-context": [
        "```suggestion\r\n\t\treturn nil, fmt.Errorf(\"failed to initialize oci client: %w\", err)\r\n```",
        "```suggestion\r\n\t\treturn nil, \"\", fmt.Errorf(\"failed to initialize oci client: %w\", err)\r\n```",
        "Let's wrap this error for easier debugging."
      ],
      "argo-cd-validate-conceptual-api-types": [
        "Is this necessary? As far as I can tell, OCI isn't a source \"type\" in the same way that Helm or Kustomize are. An OCI repo could contain Helm, Kustomize, Directory, or Plugin-style manifetss."
      ],
      "argo-cd-prevent-silent-failures": [
        "Should we disable the button if there's an ongoing refresh? Otherwise looks like the click gets silently ignored."
      ]
    },
    "profile": {
      "company": "Intuit",
      "blog": "",
      "site_admin": false,
      "followers": 191,
      "following": 29
    }
  },
  "AlexDBlack": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-always-secure-your-locks",
        "title": "Always secure your locks"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-clear-descriptive-identifiers",
        "title": "Clear descriptive identifiers"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-ai-apis-completely",
        "title": "Document AI APIs completely"
      },
      {
        "slug": "deeplearning4j-document-ai-implementation-references",
        "title": "Document AI implementation references"
      },
      {
        "slug": "deeplearning4j-document-api-completely",
        "title": "Document API completely"
      },
      {
        "slug": "deeplearning4j-fail-fast-clearly",
        "title": "Fail fast clearly"
      },
      {
        "slug": "deeplearning4j-maintain-proper-capitalization",
        "title": "Maintain proper capitalization"
      },
      {
        "slug": "deeplearning4j-minimize-object-allocations",
        "title": "Minimize object allocations"
      },
      {
        "slug": "deeplearning4j-numerical-stability-practices",
        "title": "Numerical stability practices"
      },
      {
        "slug": "deeplearning4j-optimize-hardware-acceleration",
        "title": "Optimize hardware acceleration"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-use-modern-api-methods",
        "title": "Use modern API methods"
      },
      {
        "slug": "deeplearning4j-user-friendly-documentation-examples",
        "title": "User-friendly documentation examples"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-clear-descriptive-identifiers": [
        "What about ```var_``` prefix instead of ```sd_var_```? It's shorter, and no less descriptive.",
        "Yeah, good point. OK, sounds good :)"
      ],
      "deeplearning4j-optimize-hardware-acceleration": [
        "Can't have this dep here even under test scope, for CI CUDA-only builds:\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/a948b1364329c7281aa2302524e2b9dcd6858cf2/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/pom.xml#L77"
      ],
      "deeplearning4j-always-secure-your-locks": [
        "This doesn't look safe to me.\r\nIf the semaphore available permits is anything other than 0, this will overflow.\r\n\r\nSemaphore.release:\r\n```\r\n    public void release(int permits) {\r\n        if (permits < 0) throw new IllegalArgumentException();\r\n        sync.releaseShared(permits);\r\n    }\r\n```\r\n\r\nThis ultimately ends up calling this method, on Semaphore.Sync:\r\n```\r\n        protected final boolean tryReleaseShared(int releases) {\r\n            for (;;) {\r\n                int current = getState();\r\n                int next = current + releases;\r\n                if (next < current) // overflow\r\n                    throw new Error(\"Maximum permit count exceeded\");\r\n                if (compareAndSetState(current, next))\r\n                    return true;\r\n            }\r\n        }\r\n```\r\n\r\nUnless we can guarantee that the semaphore only ever has 0 permits at this point (unlikely), this is a bug.",
        "Unlock should be in finally block?\r\nOtherwise if there's any sort of an exception, any other threads waiting on that lock could be waiting forever.",
        "Same thing - unlock in finally block in case of exception.\r\nAlso un-indent line above.\r\nSame thing for other lock uses in this class."
      ],
      "deeplearning4j-document-ai-apis-completely": [
        "inputs() is good - that just tells you placeholders. Let's note placeholders, and maybe link to other SameDiff page that explains this.\r\nAs for outputs() - that will be going away. Yes it's in 1.0.0-beta5, but it's not robust enough - it's basically \"variables that don't feed into any other ops\" - which often won't be the real predictions that users want - but rather the loss function or some irrelevant unused output.\r\n\r\nSo users should just look at the summary instead and infer the outputs from that.",
        "\"For multiple outputs, use exec() instead of execSingle(), to return a `Map<String,INDArray>` of outputs instead.\r\nAlternatively, you can use methods such as `SameDiff.output(Map<String, INDArray> placeholders, String... outputs)` to get the same output.\"",
        "Let's add a section with this, which should be good enough for now. We'll get proper coverage info up at a later date.\r\n\r\n```\r\n## Operations Coverage\r\n\r\nSameDiff's TensorFlow import is still being developed, and does not yet have support for every single operation and datatype in TensorFlow.\r\nAlmost all of the common/standard operations are importable and tested, however - including almost everything in the tf, tf.math, tf.layers, tf.losses, tf.bitwise and tf.nn namespaces. The majority of existing pretrained models out there should be importable into SameDiff.\r\n\r\nIf you run into an operation that can't be imported, feel free to open an issue here: https://github.com/eclipse/deeplearning4j/issues\r\n```",
        "DifferentialFunctionFactory is an internal detail most users will never need to touch, let's not mention this."
      ],
      "deeplearning4j-use-modern-api-methods": [
        "Let's direct users to Nd4j.createFromArray instead.\r\nUnlike Nd4j.create, there's overloads for all java primitive types and Number (long, short, Long, Boolean etc), and so users are less likely to get confused with Nd4j.create(int[]), where they think the int[] is content just like Nd4j.create(double[]), but it's actually shape",
        "This is old. Nd4j.createFromArray has overloads up to 4d for all types",
        "Use `Nd4j.zeros(DataType.DOUBLE, 5)`,  no need to do it in 2 lines with array for shape.\r\nMaybe also show the import for DataType the first time it's used.",
        "Show casting example as solution to this:\r\n`INDArray x3 = x.add(x2.cast(DataType.DOUBLE))`"
      ],
      "deeplearning4j-document-ai-implementation-references": [
        "Maybe add a comment here - seemed a little strange on first glance.\r\nI gather you want masked steps to have very large negative attention weights as a way of doing \"masked softmax\"?\r\nHow was the 1e9 chosen? (just wondering if we'll even have numerical stability issues...)",
        "conv_padding is 'truncate' mode padding values, and conv_padding_r is same mode padding, right?\r\nIf so, not sure on the -pH and -pW here... Usually with same mode, we ignore the ph and pW args and just calculate what we should actually use.\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ConvolutionMode.java#L62-L63\r\n\r\nAlso present in a bunch of other places."
      ],
      "deeplearning4j-descriptive-error-context": [
        "Is \"no axis\" a legitimate use case? More likely to be a bug than \"I want to implement identity op via standardize\".\r\nIMO exception would be better."
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Not a big deal - but we can use Lombok `@Slf4j` annotation on the class to get the same thing",
        "Don't also print stack trace, log.error will already print it.\r\nAlso other instances below.",
        "I'm being picky here - can use ```@Slf4j``` annotation instead.",
        "Put exception as log.error arg"
      ],
      "deeplearning4j-document-api-completely": [
        "https://github.com/eclipse/deeplearning4j/issues/8031\r\n\r\nAlso `@see` is self-referential?\r\nShould be `#pad(INDArray, int[][], List<double[]>, PadMode)` I think? (there's other ones like this).\r\nAlso for future reference: when the signatures differ, we need to specify the default behaviour / arg values, otherwise the user has to try and dig this out of the code.\r\nA good comment would be: `As per {@link ...} with 'constantValues' being zeros (zero padding)`",
        "I get what you're going for here. But suppose the user comes directly to the argMax method (not via the argMin see/link).\r\nThey could reasonably intepret this to mean that *somehow* the minimum might be returned by this method.\r\n\r\nBetter: only talk about argmax here. And for argMin, do something like `An per {@link argMax(...)} but for minimum values`",
        "Technically not javadoc, but you might as well fix them when you see them.\r\nDeprecated methods should follow the following format (usually)\r\n1. An `@Deprecated` annotation on the method (or class if applicable)\r\n2. A `@deprecated` javadoc tag\r\n3. Javadoc tag should include what to use instead (+ why it was deprecated, if applicable)\r\n4. Usually - no other comments other than the deprecated information (to discourage users from using it at all)\r\n\r\nAlso a minor nitpick: I prefer the `{@link ...}` style directly in the `@deprecated` tag.\r\n`@see` gets rendered to javadoc as something like:\r\n```\r\nSee Also:\r\nsomeMethod(int, double)\r\n```\r\nSemantically - \"see also\" is different to \"use this instead\".",
        "Better javadoc would be good. Mainly clarify exactly what you mean by \"standardize\"",
        "As a general rule: any time we add a ```@Deprecated``` tag we should have javadoc to explain why and/or what to use instead. Even better: add a date/version (pretty sure we're going with 1.0.0-beta for next release)",
        "More javadoc - limitations, when to use.\r\nAlso how to build one (i.e., \"use PI builder configured with X, not this class\") - that might not be obvious to users at first glance (it wasn't for me)"
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "General point (applies everywhere) - if a method allows null values, it should be marked as such in the docs, with a brief explanation of the behaviour for null vs. non-null.\r\nSomething along the lines:\r\n`@param workspace Optional memory workspace to define the buffer in, may be null (buffer not created in workspace)`",
        "Handling of nulls? You have some in the later methods, but not here...\r\nIf null is acceptable, we should return null here. If not, let's either add a lombok ```@NonNull``` or a ```Preconditions.checkNotNull(```. At present, nulls will give a non-useful NPE.",
        "I don't think point(null) is valid?\r\nThe way you have it implemented currently (with masks) it won't return a single value along that dimension, I think.\r\nAnd if we don't allow null, switch arg to ```int```",
        "Switch all of these array methods to var-args - ```var(int... shape)``` etc\r\nMaybe also a ```Preconditions.checkArgument(shape != null  && shape.length > 0, \"Invalid shape: %s\", shape)```"
      ],
      "deeplearning4j-user-friendly-documentation-examples": [
        "Let's move these two \"don't do this\" to a section at the very end. Totally fine to mention them, but we shouldn't be opening with a bunch of things to *not* do that the user might not have even thought to try...",
        "Maybe add comment for args order: (start, stop, count)",
        "IndArray -> INDArray\r\n\"operations such as\" - mention they are called reduction/accumulation operations.",
        "`.data().asDouble()` etc isn't something I want in the quickstart, it's confusing for new users due to views and f/c order (not to mention permuted arrays).\r\nInstead, direct users to toFloatVector(), toDoubleMatrix() etc\r\n",
        "Missing createFromArray, valueArrayOf, createUninitialized, scalar. Remove copy method (I didn't even realize it was a thing, users can just use assign).\r\n\r\nMaybe also consider splitting out into new subsections:\r\n- Random array creation (randn, randomBernoulli, randomBinomial, randomExponential, choice)\r\n- (De)Serialization (read, readNumpy, createFromNpy/Npz etc) + write methods\r\n\r\nAlso, generally, note that this is just a few of the ops. We have literally hundreds of ops, too many to list here... we don't want users to think that this list is it :)"
      ],
      "deeplearning4j-minimize-object-allocations": [
        "When using the Preconditions class, do this instead:\r\nThis avoids object creation/garbage unless an error is actually thrown.\r\n```\r\nPreconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = %s) does not match output layer number of outputs (nOut = %s)\",  labels.size(1), preOutput.size(1));\r\n```",
        "This seems off.\r\nYou create a zeros array of shape [mb, inputCaps, caps, capsDimensions, 1]\r\nBut then proceed to immediately get a [mb, inputCaps, caps, 1, 1] subset from it?\r\nThat's unnecessarily inefficient. Why not make a ```[mb, inputCaps, caps, 1, 1]``` in the first place?",
        "Don't create this temporary ret array. Use ```gainParamView.assign(gainInit)``` instead.",
        "Nice regex :)\r\nA minor detail, but pattern is apparently immutable and thread safe... we can make it static? (Or make the one in BertWordPieceTokenizer static + public and use here too?)"
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "The reason it isn't equal without the toString is datatypes.\r\nBy design we don't cast on equals.\r\nI'd recommend either specifying datatypes on all INDArrays, or on none of them.\r\nTests default to double; you are comparing a float [10,10] with double [10,10] hence the failure.",
        "Let's replace these 5 lines with: ```return Double.compareDouble(lhs.getFitness(), rhs.getFitness())``` (possibly with order switched, if that's the intention here).\r\nNote I'm assuming the fitness field is protected/private and getter via ```@Data``` was added, as per earlier review comment.\r\nSame thing for other comparator."
      ],
      "deeplearning4j-maintain-proper-capitalization": [
        "Capitalization: `samediff` -> `SameDiff`\r\n`Tensorflow` -> `TensorFlow`\r\n`Samediff.importFrozenTF` -> `SameDiff.importFrozenTF`\r\n\r\nSame things in a few other places in the doc, I won't flag others...",
        "Technically it's a method overload, not an optional argument.\r\nLet's also mention generated name \"based on the operation name\" when not specified"
      ],
      "deeplearning4j-numerical-stability-practices": [
        "Why addition of 1e-5? Normally this is for numerical precision, but I don't believe we risk underflowing here even if l2 norm is 0? (We get 0/1 in that case)",
        "Ah, makes sense, let's leave it then.",
        "Again:\r\nBy definition, the score calculation _must_ use the SameDiff instance. The `output = activationFn.getActivation` part is ok, but after that just call SameDiff.output providing the placeholders instead.",
        "This is a little unintuitive, but it's definitely intended (and required).\r\nConsider the following graph:\r\n```\r\ninput -> split -> (A,B)\r\nB -> lossFn\r\n```\r\nHere, A is unused (in that it doesn't contribute to the loss function). But to do split op backprop, we need gradient variables/arrays for both outputs (A and B).\r\nWe know the shape and type of dL/dA must be exactly the same as A; we also know that the loss function doesn't depend on A. Hence, dL/dA == zerosLike(A)",
        "Done."
      ],
      "deeplearning4j-configurable-resource-locations": [
        "Let's make dir configurable. User might want it on SSD different to system drive.",
        "Do we want configurable location?\r\ni.e., for VMs etc with slow system  HDD but fast SSD available",
        "Also configurable location here?"
      ],
      "deeplearning4j-clean-up-your-code": [
        "Add private no-arg constructor to avoid users being able to do ```new ValidationUtils()```",
        "This comment applies to all of your uses of Preconditions case: don't create strings like this.\r\nThe correct way of using preconditions class doesn't (in most cases) result in any object creation unless the precondition fails:\r\n```\r\nPreconditions.checkArgument(data >= 0, \"Values for %s must be >= 0, got %s\", paramName, data);\r\n```"
      ],
      "deeplearning4j-fail-fast-clearly": [
        "Good check, but this can be optimized (and is not the correct way to use preconditions). This always results in 3 objects being created (2xlong[] shape, and a String) regardless of whether an error is encountered. Instead, use this (no object creation unless an error is encountered)\r\n```\r\nPreconditions.checkState(input.rank() == 3, \"3D ... got %s\", input.rank())\r\n```",
        "Unimplemented? Get the params then don't do anything with them...\r\nIf planned to be added later, maybe add UnsupportedOperationException?\r\n(Better to get exception than obscure issues or silent failure)",
        "Couple of things here:\r\n(a) we can use Preconditions.checkArgument/checkState - less verbose\r\n(b) When throwing exceptions, I think it's good practice to include useful information. What dimension? What's the shape of the array? Without that, I need a debugger to get that information, which adds a lot of time required to fix it..."
      ]
    },
    "profile": {
      "location": "Melbourne, Australia",
      "blog": "",
      "site_admin": false,
      "followers": 236,
      "following": 0
    }
  },
  "MMeent": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-connection-transitions",
        "title": "Document connection transitions"
      },
      {
        "slug": "neon-escape-sql-parameters",
        "title": "Escape SQL parameters"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-limit-concurrent-access-slots",
        "title": "Limit concurrent access slots"
      },
      {
        "slug": "neon-mind-transaction-boundaries",
        "title": "Mind transaction boundaries"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-data-structures",
        "title": "Optimize data structures"
      },
      {
        "slug": "neon-prefer-opt-in-security",
        "title": "Prefer opt-in security"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      }
    ],
    "comments": {
      "neon-optimize-data-structures": [
        "Suggestion: Why don't we store subxact depth in DdlHashTable, and maintain a SubtransLevel counter that we can check the DdlHashTable against?\r\n\r\nWe know we won't have to create a new DdlHashTable for level=1 when CurrentLevel=2, unless level 2's subxact is rolled back to level 1.\r\n\r\nSo the stack (of sorts) would look like `SubXactLevel=130; CurrentDHT = DHT (level=100) -> DHT (level=99) -> DHT (level=40) -> DHT (level=21) -> DHT (RootTable)` instead of `SubXactLevel=30; DHT 100 -> DHT 99 -> .. -> DHT 1 -> DHT RootTable`.\r\n\r\nWDYT?\r\n\r\nAs you can see, that will reduce the memory usage required for 1000 subxacts followed by one with DDL from 1000 DdlHashTables to just 1.",
        "Does this not have off-by-one issues? I'd prefer keeping a single counter, rather than one reset every time you push a table: you can't have more than 2^31 subxacts in one transaction, as you'd run out of XIDs.\r\n\r\nAnd in that case, you can change the table `if (CurrentDdlTable->subtrans_level > --SubtransDdlLevel)`"
      ],
      "neon-escape-sql-parameters": [
        "Please make sure you correctly escape the database and user parameters of this connection string."
      ],
      "neon-proactive-cache-warming": [
        "I really don't like this. Let's expose plain numbers, not rounded and divided values. It'll also allow (when used as metric) to compare absolute prewarm speed between projects and endpoints even when their sizes are very different.",
        "`pages_total` + `pages_processed`, probably?\r\nIIRC, we currently also have internal counters for \"ignored\" and \"dropped\" somewhere in the metrics, to cover those pages that we don't have to load because concurrent workloads already fetched the pages or that we can't load because there are no more LFC entries available, respectively.",
        "I don't like this. S3 writes are expensive to scale, and if we're about to write every N seconds it's going to be expensive to host read replicas.\r\n\r\nI'd _expected_ CPlane-triggered writes; either during shutdown (for warming up after start) or before shutdown (for hot restart); not systematic writes.",
        "I don't think checkpoint is a good place for this; it's much too frequent on highly loaded systems.",
        "We can make it part of endpoint shutdown procedures?\r\n\r\nNote that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.",
        "As for hot standby, it's probably good enough to \"just\" disable the replay filter and require replay of all WAL (rather than only those records which we already have in our caches)",
        "A calculation (S3 us-east-1) based on a setting of 60s:\r\n\r\n> 1 req/min * 60 min/hr * 730 hr/mon * $0.005 /1000req (S3 POST) = $0.219 per concurrently active compute per month, or $219 per 1000 concurrently active computes per month (annualized).\r\n\r\nI'd rather keep this frequency much lower than that even, as this seems like a significant potential cost for free tier operations: It'd be 7% of the monthly cost of a tiny general-purpose compute instance (t4g.micro) at AWS.\r\n\r\nI'm also a bit concerned about dumping LFC state every so often. Dumping that state is not free, and while it isn't all _that_ expensive it does block all IO operations to the LFC for a short while. Doing the dump regularly will likely cause some (small?) amount of degraded performance.\r\n\r\nIf instead of frequently dumping this state, we should dump the state only when needed (i.e. with pending shutdown or pending restart), so that we'll only degrade performance if and when needed."
      ],
      "neon-mind-transaction-boundaries": [
        "Could you please do better connection lifetime management here? It's quite bad to see the connections get leaked.",
        "Both `CHECKPOINT` and `COMMIT` should be sufficient to flush current WAL.\r\n\r\nAdditionally, the `last_flush_lsn` is quite out-of-date by the time `pg_switch_wal()` has succeeded. Better use `SELECT pg_current_wal_insert_lsn()` and then run `CHECKPOINT`."
      ],
      "neon-clear-consistent-identifier-names": [
        "I don't like using plain numbers for something that is more than just that plain number, unless context makes it abundantly clear. As Display doesn't guarantee context, the value of each type was clarified by adding the full enum tag to Display output.\r\n\r\nInstead of neutering the Display implementation, I think it's better to fix the clap usage so that it doesn't rely on `Display` for serializing strings - we implement Serialize and Deserialize for those purposes.",
        "Please rename this to `WalSegmentSize`. \r\n\r\nPostgreSQL stores data files in Segments of 1GB increments, which makes this value confusing."
      ],
      "neon-handle-all-error-paths": [
        "Please add an Assert(slot->response->tag == T_NeonGetPageResponse) below the `continue` code block, so that any misplaced response in slot->response is caught, at least in debug builds.",
        "Yes, but I'd rather have that check in place here as well, in case anything changes in other places that touch slot->response.",
        "So, is this the fix, or is there another component that's been fixed with this PR?",
        "Hmm, I see. How come we're not handling termination -related errors in PG_CATCH? Because we can't recover from them?"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "> or evicted from LFC after receiving the corresponding WAL record (in case of replica).\r\n\r\nThis is incorrect: WAL will be replayed for every page that's currently in the LFC or shared buffers.\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThe LFC can become stale, but only if the main page is still in shared buffers. The (modified) page from buffers will at some point be written, which will make the LFC lose its stale-ness.\r\n\r\nIn any case, the stale-ness of a page in LFC doesn't (shouldn't) matter for this RFC.",
        "I'm a bit concerned about issues that would arise from deletion on a weekly schedule while using the endpoint exclusively for monthly tasks - you'd want that endpoint to have good performance."
      ],
      "neon-database-replica-promotion-safeguards": [
        "Why does it matter whether this is a promoted replica or not? I see you use it in `walproposer.c`, but that seems like a workaround and bypass of a valid check, not a correct solution to an issue."
      ],
      "neon-handle-network-interrupts-safely": [
        "Note that moving this call to _before_ `page_server->send() || page_server->flush()` will cause requests sent by `page_server_request` to not be pipelined with getpage requests already in the connection, and so this may have 2 RTT latency, instead of 1 RTT: 1 RTT to finish all open GetPage requests, and 1 RTT for the actual NeonRequest.\r\n\r\nIt doesn't look like the behaviour here is very different, so why not put it immediately before the `page_server->receive()` call?",
        "OK, I think I got it. \r\nIf a connection is dropped during consume_prefetch_responses, then that signal is not carried on to the synchronous request path that requested `consume_prefetch_responses`, so putting consume_prefetch_responses between `page_server->sync()` and `page_server->receive()` could cause the connection to get stuck waiting on a newly created empty connection.\r\n\r\nIf we update `consume_prefetch_responses` to return the output of wait_for(), then we can use that to determine connection state, and continue handling everything correctly.\r\n\r\n```\r\nstatic bool\r\nconsume_prefetch_responses(void)\r\n{\r\n\tif (MyPState->ring_receive < MyPState->ring_unused)\r\n\t\treturn prefetch_wait_for(MyPState->ring_unused - 1);\r\n\treturn true;\r\n}\r\n```\r\n\r\n\r\nand then, in sync request paths, you'd do the following, inside a PG_TRY block to make sure to drop connections when the request gets cancelled:\r\n\r\n```\r\n\t\twhile (!page_server->send(shard_no, &request.hdr)\r\n\t\t\t\t|| !page_server->flush(shard_no)\r\n\t\t\t\t|| !consume_prefetch_responses())\r\n\t\t{\r\n\t\t\t/*\r\n\t\t\t * Loop until we've successfully\r\n\t\t\t *  1.) Written the request into the shard's connection,\r\n\t\t\t *  2.) Flushed that request to the network, and\r\n\t\t\t *  3.) Consumed all open prefetch requests still on the line.\r\n\t\t\t */\r\n\t\t};\r\n```\r\n\r\n(the sync request path in page_server_request and communicator_read_slru_segment)",
        "Hmm, ok."
      ],
      "neon-proper-option-type-usage": [
        "Shouldn't this be Option<Lsn>, given that it's only set during shutdown?"
      ],
      "neon-prefer-opt-in-security": [
        "Why is anon suddenly a default preloaded library?",
        "OK, but why does it need to be a default-on setting?\r\n\r\nI, as a user, don't want `anon` anywhere close to my data, due to its inherent nature to return the wrong data (i.e. anything but the actual stored data)."
      ],
      "neon-document-connection-transitions": [
        "This is missing the important element where the Primary that has to be shut down first, but the (old) primary does not show up in this diagram.",
        "Oh, it didn't have the same actor labeling as those of the secondary, so I'd failed to notice this.\r\n\r\nEither way, that needs additional details, as there are some compute_ctl-postgres interactions which we need to detail on the primary as well for this to work correctly and consistently.",
        "Not \"missing\" per se, but I do find it confusing that the RFC does go into detail for one side (the promoting replica) but not the other (the primary), while both sides are critial for correct functioning of the system. I'd expected both sides to have the same amount of detail."
      ],
      "neon-limit-concurrent-access-slots": [
        "True, but I wouldn't expect them to be working on the LFC _concurrently_. Similar to the shared buffers partition locks and the WAL writers locks, I think we only need a limited amount of concurrent slots to satisfy the workload (e.g. 128), as we only really need the lock once we're ready to start loading data.",
        "Yes, see https://github.com/neondatabase/neon/pull/10312#discussion_r1922225273 - we can limit this to a limited number of concurrent backends.\r\n\r\nNote that we'll only need a prewarm state once a backend has the prefetched data in memory (no need to hold an LFC entry for a not-yet-received prefetch entry; actually, holding that is dangerous for other backends that want to prefetch a set of pages from that chunk), and the turnaround time for prewarming is expected to be very small: it's quite unlikely you're waiting for a long time on concurrent backends to finish their work, as reasonably there are at most BLOCKS_PER_CHUNK backends that are doing IO on the LFC chunk, and each such IO probably won't take very long."
      ],
      "neon-guard-against-race-conditions": [
        "Shouldn't `IsUnderPostmaster` be a better approach than these rather arbitrary conditions?\r\n\r\nI.e. \r\n```suggestion\r\n\tif (newval && *newval != '\\0' && IsUnderPostmaster && RecoveryInProgress())\r\n```",
        "Yeah, it's weird that we're checking for the availability of shmem when we know that shmem should be available exactly when we run code with `IsUnderPostmaster`. AFAIK there is no valid reason why we should be unable to access shared memory in `IsUnderPostmaster` processes.",
        "Fixed.",
        "Fixed."
      ],
      "neon-proper-metrics-design": [
        "If a table is dropped, its count is presumably removed from the total, too."
      ],
      "neon-minimize-unnecessary-allocations": [
        "True, but that'd add a `format!()` overhead, and I'd rather prevent that. |\r\n\r\nThis allows 'static Strings, also further reducing alloc overhead."
      ],
      "neon-document-api-specs-completely": [
        "CPlane would be hitting this service.\r\n\r\n(Note that we're not necessarily using S3; any blob storage will do)"
      ],
      "neon-flexible-documented-configurations": [
        "Did you consider providing a single function for this, like the following?\r\n\r\n```suggestion\r\n+CREATE OR REPLACE FUNCTION anon.enable_transparent_masking_superuser(\r\n+  dbname TEXT,\r\n+  toggle BOOL DEFAULT = true,\r\n+)\r\n+RETURNS VOID AS\r\n+$$\r\n+BEGIN\r\n+  EXECUTE format('ALTER DATABASE %I SET anon.transparent_dynamic_masking TO %L', dbname, toggle::text);\r\n+END;\r\n+$$\r\n+  LANGUAGE plpgsql\r\n+  VOLATILE\r\n+  SECURITY DEFINER\r\n+  SET search_path=''\r\n+;\r\n```"
      ],
      "neon-cache-performance-preservation": [
        "Make primary report its flushLSN to CPlane on shutdown, and have CPlane pass that on with the `/promote` call so that compute_ctl can ensure that it has received that WAL before promotion.",
        "> If I'm reading this right, there's a an availability gap between the termination and promotion\r\n\r\nThe secondary *could* be made available for read-only queries before the primary shuts down. However, we can *not* allow write queries to the secondary before the primary has shut down, so there will always be a gap where there is no writer available.\r\n\r\nThe system should be fast enough, however, that this gap is no longer than a second at most; and probably much less.",
        "> It seems like we can swap promotion and termination around while maintaining this property.\r\n\r\nNo, Vlad, we can not do that.\r\n\r\nTwo computes can Never, NEVER, _never_ both be Primary on the same timeline at the same time.  CPlane is supposed to make sure of that, and Compute will fail if it doesn't. Promotion of the replica before the original Primary shut down will cause errors, panics, data loss, shutdowns, and/or nasal deamons on the Primary, this promoting Secondary, or both.\r\n\r\nWe don't want to test that theory.\r\n\r\n\r\n> What would happen to the client in this case? Let's say we have an in-progress query when the old primary terminates.\r\n\r\nTheir session would and should get disconnected. How that client handles disconnections is not something we care about here.",
        "> More specifically, by idle I mean it would have to not write any WAL and be in some sort of consensus observer role.\r\n\r\nThat's what a hot standby is for PostgreSQL - the hot standby is not allowed to write WAL, but could stop reading and applying WAL from the primary and start writing its own WAL at a moment's notice.\r\n\r\nThe issue is that you can't promote until you've replayed all acknowledged WAL from safekeepers, because otherwise you'd fail to replay commits that the primary may have already sent acknowledgements for to its clients, essentially losing commits."
      ],
      "neon-configuration-context-alignment": [
        "```suggestion\r\n\t\t\t\t\t\t!AmPrewarmWorker && /* do not configure the timeout-based pumping system in prewarm workers */\r\n```"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "The RFC for this storage service https://github.com/neondatabase/neon/pull/9661 implies that compute's tokens won't contain information about which endpoint it is, so the \"S3 proxy\" (which isn't just that, and thus probably shouldn't be called that) **can't** validate that the request came from a compute with the right endpoint_id.",
        "I'm not quite sure yet about the S3 design. \r\n\r\nYes, it'll have to be tenant-prefixed, and probably Endpoint-prefixed too, but I'm not yet 100% sure if it'll also be timeline-prefixed.",
        "> I think that from the security standpoint, the tenant should be enough as the tenant is our level of multi-tenancy,\r\n\r\nI don't think that's good enough. Compute's tokens should be bound to (Tenant, Timeline, Lsn >/=), so it can't ask for data created on completely disjoint timelines in the same tenant (e.g. `a` branches into `b` and `c`; compute on `b` shouldn't be able to query data in `c`).\r\n\r\n> and we use it for storage auth already\r\n\r\nIMV that's a bad argument. Having a bad practice doesn't mean we should adapt it in new projects if we can prevent it."
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 4,
      "following": 3
    }
  },
  "SomeoneToIgnore": {
    "repos": [
      "alacritty/alacritty",
      "zed-industries/zed"
    ],
    "entries": [
      {
        "slug": "alacritty-use-constraining-types",
        "title": "Use constraining types"
      },
      {
        "slug": "zed-background-process-blocking-operations",
        "title": "Background process blocking operations"
      },
      {
        "slug": "zed-choose-domain-specific-semantic-names",
        "title": "Choose domain-specific semantic names"
      },
      {
        "slug": "zed-consider-algorithmic-complexity",
        "title": "Consider algorithmic complexity"
      },
      {
        "slug": "zed-contextualize-dont-panic",
        "title": "Contextualize don't panic"
      },
      {
        "slug": "zed-design-interfaces-not-implementations",
        "title": "Design interfaces, not implementations"
      },
      {
        "slug": "zed-document-configuration-clearly",
        "title": "Document configuration clearly"
      },
      {
        "slug": "zed-document-configuration-constraints-clearly",
        "title": "Document configuration constraints clearly"
      },
      {
        "slug": "zed-hierarchical-configuration-organization",
        "title": "Hierarchical configuration organization"
      },
      {
        "slug": "zed-minimize-credential-exposure-lifetime",
        "title": "Minimize credential exposure lifetime"
      },
      {
        "slug": "zed-prefer-idiomatic-option-handling",
        "title": "Prefer idiomatic Option handling"
      },
      {
        "slug": "zed-prefer-rust-structural-patterns",
        "title": "Prefer Rust structural patterns"
      },
      {
        "slug": "zed-protect-render-loop-performance",
        "title": "Protect render loop performance"
      },
      {
        "slug": "zed-respect-language-specific-conventions",
        "title": "Respect language-specific conventions"
      },
      {
        "slug": "zed-scope-dependencies-appropriately",
        "title": "Scope dependencies appropriately"
      },
      {
        "slug": "zed-self-explanatory-identifier-names",
        "title": "Self-explanatory identifier names"
      },
      {
        "slug": "zed-standardize-platform-agnostic-configuration",
        "title": "Standardize platform-agnostic configuration"
      },
      {
        "slug": "zed-test-through-public-apis",
        "title": "Test through public APIs"
      }
    ],
    "comments": {
      "zed-minimize-credential-exposure-lifetime": [
        "Also, adding a code that explicitly stores credentials somewhere seems somewhat scary.\nI get it that we do `drop(askpass);` but wondering if we could somehow extract this all into:\n\n```rs\nlet (askpass, askpass_rx) = ....\n....\n\nlet Some(password) = askpass_rx.next().await else {...};\nlet socket = SshSocket::new(connection_options, &temp_dir, password)?;\n```\n\nthis way, nothing related to password ever gets stuck in memory for sure.\n\nGiven good Mikayla's ideas about deduplicating the logic with the posix impl and the fact that it managed to work without storing passwords in fields, seems reasonable?",
        "So, this is a password that we write into a file and never control the lifecycle of that tmp file.\nHow adequate and safe this is?\n\nIs there a way where we do not store the password in files and \"hardcoded\" memory such as fields?\nHaving some `rx` oneshot seems ok? \nThen we can pass that through as a \"askpass reply result\" and call once, where the password is needed?\nOr, is it possible complicate the `SSH_ASKPASS` script so that it gets the input from Zed? \nI'm sort of surprised that ssh itself cannot handle its askpass matters internally with the user."
      ],
      "zed-protect-render-loop-performance": [
        "It's not superb that we have to use this method during __rendering__ , that might happen fast, e.g. 120 times/second: and we loop over outlines in a potentially large file.\r\n\r\nWhat's more odd, is that we call `render_outline(` in a loop over every item to render, so should know whether the outline has children or not.\r\n\r\nSeems that we can rewrite it into something better.",
        "`show_diagnostics_inline` is what alters the rendering logic, so this code will \"blink\" the diagnostics, if the delay is long enough: e.g for `\"delay_ms\": 500` after an edit, the diagnostics will stop rendering and editor will \"blink\", showing none at all first and 500+Nms later coming up with the new diagnostics.\r\n\r\nWhat if, instead, we rework the approach a bit?\r\nWe'll keep the only `show_diagnostics_inline` in `Editor` (altered via project settings), remove `show_diagnostics_inline_enabled` and altering the delay logic similar to https://github.com/zed-industries/zed/pull/21463 ?\r\n\r\nThe idea is to have a single \"is it enabled?\" flat on the editor level which allows updating the collection of diagnostics to render (can be stored in the editor's element or editor itself).\r\nWhen the flag goes down, the collection is cleared and not updated until the settings change and re-enable the flag.\r\n\r\nThe diagnostics' update happen after buffer edits and on special events (e.g. editor creation), debounced (50ms default seems ok?), so we do not recalculate diagnostics needlessly on quick user typing (current part of the config seems ok syntaxically, but needs a bump from 0ms).\r\n\r\nIf there's something in the collection, it gets rendered straight away, without any complex computations.",
        "Seems like a good summarization, I indeed propose to do less work on each frame, in particular\r\n\r\n> grabs what should be currently visible, does a bit of math to translate from buffer coordinates to display coordinates, and then spits out whatever those flex box thingers are collectively known as, and paints them on screen\r\n\r\nspecifically \"does a bit of math\" part is very concerning to me: we have ~16.67ms per frame at best, and potentially very large documents with very large amount of errors, consider scenarios like project.rs from this repo which will have a particular `use` statement(s) malformed (which will trigger a lot of errors).\r\n\r\nWe also do sorting with `sorted_by_key` and all those coordinate transformations are logarithmic at best.\r\n\r\nI just cannot fathom any reason to do that bit of math if we can avoid doing that.\r\nSure, this can wait until other fixes, but has to be made eventually.",
        "This is a code with a lot of tree traversals (`display_point_to_point`, `point_to_display_point` and other coordinate transform, `diagnostics_in_range` lookup) and various allocations and whatever sorting on top.\r\n\r\nAll that has a lifecycle of `let diagnostics = self.gather_inline_diagnostics(`, and `layout.inline_diagnostics.drain()` on the other end, so sort of thrown away.\r\n\r\nThis all is done on each frame* potentially 120 times/second, and to me it seems we can try and rewrite the logic so that it scales better: traversing thousands of diagnostics with various tree-manipulations on the side does not seem fun to fit into a frame.\r\n\r\nI've mentioned a proposal in the editor.rs part: we might want to debounce any attempt to update the diagnostics data (unless it's a data purge after disabling the diagnostics); keep the old state as long as possible, swapping it to the new one which is calculated after an edit; keep renrendering that part",
        "Just to be more specific, I've got a file with a lot of diagnostics.\r\n\r\n[rust-analyzer-repro-long-diagnostics.zip](https://github.com/user-attachments/files/18411900/rust-analyzer-repro-long-diagnostics.zip)\r\n\r\nand there, I've opened the file both in `Nightly` and my local `--release` build of this branch.\r\nWaited until all diagnostics are shown, and started scrolling through the file, nothing else, profiling.\r\n\r\nBoth versions behave poorly, but the traces show drastic difference in the reasons to do so:\r\n\r\nNightly has some line layout and tree-sitter-query-related issues:\r\n![Nightly](https://github.com/user-attachments/assets/565631de-1b6a-41aa-8a24-0146f35fa880)\r\n\r\nThe branch build does all that diagnostics-related work including extra allocations, not explicitly mentioned in the comment above:\r\n![Dev](https://github.com/user-attachments/assets/9cc29382-9bca-4658-8c61-71a4e762d4b5)",
        "After new design, numbers are back to something comparable with the rest of the code for the same exaggerated example:\r\n\r\n![image](https://github.com/user-attachments/assets/72ce4f9d-1d73-427e-96be-8641b6d61ada)\r\n\r\nand the heaviest stacktrace is related to layouting again.",
        "> .exists()\r\n\r\n1. We prefer to avoid using `std::fs`-related methods and use `Fs` instead, https://github.com/zed-industries/zed/blob/5e210c083fc4d1a00c9a7dac7abf185351bb9b3e/crates/fs/src/fs.rs#L112 in this case.\r\n\r\n2. This is a rendering code, potentially called 120 times/second, calling whatever FS-related things here is madness.\r\nIs there another way we can check that file is outside of projects?\r\n\r\n3. We can be on remote, where the client will have no FS at all."
      ],
      "zed-respect-language-specific-conventions": [
        "One more small change here, we need to return that trailing newline back."
      ],
      "zed-contextualize-dont-panic": [
        "```suggestion\n        fs::write(&askpass_script_path, askpass_script).await.with_context(|| format!(\"Creating askpass script at {askpass_script_path:?}\"))?;\n```\n\nNot sure about `smol`, but stdlib's FS errors are notorious to not include a flie path.\n\nE.g. if script's parent directory is not created in an analogous method, the error returned would be smth. like `Err(\"not a directory\")`.\nTo be on a safe side, seems worth to add some context.",
        "I'm concerned by the amount of new `.unwrap()`, `.expect(..)` in the new code, esp. given that this function returns `Result`.\r\nLet's not write the code that may panic here, as it's quite a core feature, queried frequently, and we're parsing things from a potentially malicious server.",
        "We can remove this nesting with\r\n\r\n```rs\r\nuse anyhow::Context as _;\r\n\r\nlet (socks_proxy, version) = parse_socks_proxy(proxy).with_context(|| format!(\"Parsing proxy url '{proxy}'\"))?;\r\n// or use `.context(\"parsing proxy url\")?` if `url` is not secure to print due to the risk of leaking the credentials.\r\n```\r\n\r\nand might be good to do `.context` on more `?` around, such as `TcpStream::connect` or maybe even `.map_err(|err| anyhow!`?",
        "I would prefer `.context`/`.with_context` over `.map_err` and `?` usage anyway as indeed, mostly the Zed \"code style\" and the type system being rather explicit itself already. ",
        "I think it's fine, thank you for the rest of the fixes.",
        "I understand that we did a Python regex match and should not fail here, but the logic may have flaws (or gain, with later contributions) causing None to be returned here.\r\n\r\nFor the sake of less panics due to unwrap/expect's in \"prod\" code, let's replace it with `log_err()?` or, `debug_panic!` if you want to pay more attention to this in debug builds.\r\n\r\nI also think we can avoid this entirely, and redo this `else if let` into `else`, then get the `let file_line` and use the regular Rust's regex (see how unit tests do that with `fn re_test` now) to match the rest of the cases, with capture values extractions?"
      ],
      "zed-document-configuration-clearly": [
        "```suggestion\r\n    \"context\": \"Editor && can_scroll_signature_help\",\r\n```\r\n\r\nSeems that we can simplify this by giving a more readable name and only adding this context when no completions/whatever else conditions.\r\nWDYT?",
        "This 6 is somewhat confusing.\r\nIs it a default? Then we can uncomment the value.\r\nIs it a minimum allowed? Then we can explain this fact."
      ],
      "zed-prefer-rust-structural-patterns": [
        "NIT: we can do `if snapshot.mode != EditorMode::Full || scrollbars_layout.vertical.is_none()` with an early `return None` to avoid nested code and improve the readability.",
        "```suggestion\r\n```\r\n\r\nCan also do `.get_diagnostics(server_id).into_iter().flat_map(...` and avoid any extra nesting and `if let Some`.",
        "```suggestion\r\n            signature_help_task: None,\r\n```\r\n\r\nNIT: let's keep the compiler a bit less busy solving traits for simple cases? Same for the other one.",
        "```suggestion\r\n            let Some((buffer, buffer_position)) = self.buffer.read(cx).text_anchor_for_position(position, cx) else { return; }\r\n```\r\n\r\nNIT: we can destructure it right away, so that will be one less block to nest.",
        "NIT: that could be destructured in-place, without method field calls, right in the parameter declaration:\r\n\r\n```\r\npub fn create_signature_help_markdown_string(\r\n    SignatureHelp {\r\n        signatures,\r\n        active_signature,\r\n        active_parameter,\r\n    }: SignatureHelp,\r\n) -> ..."
      ],
      "zed-choose-domain-specific-semantic-names": [
        "It's not just the content but some way to launch askpass it seems?\nLet's name it differently to ensure we do not pass any passwords or any other sensitive things, at least on the semantics, naming level.\n\n```suggestion\n        let askpass_script = format!(\n```\n\nAlso later, we store this into `askpass` field which seems worth of having the same name.",
        "1. This method is used only once, so let's inline it instead.\r\n\r\n2. I like how `active` word is used here though, let's rename `FocusedPane` into `Active???`\r\nAnd let's uniformy state what the PR tries to do: here, it uses `Pane` but the title of the PR is \r\n> Add setting for minimap on active buffer only\r\n\r\nI think it's the latter based on the video and the discussion, so we can use\r\n\r\n`MinimapDisplayScope::ActivePane`\r\n\r\nBut then, `MinimapDisplayScope` is also wordy, can we do `Display` or `DisplayIn` or similar?\r\n\r\nAs a last naming thought, what if we use `editor` instead of `pane` in the new code?\r\nThen it will be `DisplayIn::AllEditors | ActiveEditor` and users won't have to guess what a `pane` is, is it a `panel` or not.",
        "```suggestion\r\n    pub fn semantic_tokens(&self) -> &Arc<SemanticTheme> {\r\n```\r\n\r\n`tokens` is quite broad, esp. in a text editor domain, so let's specify which tokens are these, as LSP case is relatively niche."
      ],
      "zed-prefer-idiomatic-option-handling": [
        "```suggestion\r\n            .is_some_and(|popover| popover.signature.len() > 1)\r\n```\r\n\r\nNIT",
        "> `if let Some`\r\n\r\nThis can be just `?`",
        "Here, we'd rather use the `to_string_lossy()` method, to try and show something for invalid UTF-8.",
        "`?`",
        "No, I mean for the 3rd time in a row, this is a method that returns `Option`, so you can `?` instead of `if let Some`",
        "> .unwrap();\r\n\r\nDo not ever use that, please.",
        "NIT: we could use `view.read(cx).active_editor.as_ref().is_some_and(` instead."
      ],
      "zed-consider-algorithmic-complexity": [
        "To note, `dedup` is the wrong method to deduplicate things in a `Vec`, as it only works for sequential items:\r\nhttps://doc.rust-lang.org/std/vec/struct.Vec.html#method.dedup\r\n\r\nIt seems that we could try and either use a HashSet (I had to implement a few `Hash` manually for document colors for this approach), or use a method from `Itertools` for a proper deduplication, [`dedup_by`](https://docs.rs/itertools/latest/itertools/trait.Itertools.html#method.dedup_by)",
        "Ahh, traits.\r\nThank you, I would not think of this method seeing a `Vec` around.",
        "This is odd: effectively, we go over all `fetched_outlines` and for each, we we-iteratethe outlines list again to find the children.\r\n\r\nIt's a plain O(N^2) for nothing and should be fixed.\r\nWe do `outlines.retain(|outline| {` right above and can prepare a depth map for any decision of a kind.",
        "Can we change the predicate, so we don't have to clone?\r\nSeems that `F: FnMut((&TaskSourceKind, &TaskTemplate)) -> bool + 'static,` is good enough?"
      ],
      "zed-hierarchical-configuration-organization": [
        "Let's either wait for https://github.com/zed-industries/zed/pull/29494 or incorporate the settings approach, as we seem to get more and more title bar settings and we'd better start namespacing them.",
        "Sure, less knobs to have in the settings is always good, thanks."
      ],
      "zed-design-interfaces-not-implementations": [
        "Let's add a static `fn .. -> Option<Workspace>` for all this extractions, as we might want to invoke things headless for ~10 different actions it seems.",
        "The code below seems to show that quite explicitly, so let's remove the comment.\r\n\r\nI would also argue that this function should accept `proxy: &Url` instead and `TcpStream::connect` can be done up the stack, at a single place that calls this function.\r\nShould we do that?",
        "This works, but we can reduce the diff and stick to the builder-like API with `.when(TitleBarSettings::get_global(cx).show_branch_icon, |branch_button| ...)`.\r\n\r\nLet's try doing that?",
        "Let's consolidate this with https://github.com/zed-industries/zed/blob/1cfbfc199cee551318b89a2f35853ed43b8ac52d/crates/gpui/src/app.rs#L1434-L1440 and it's the same thing implemented for macOS.\r\n\r\nAt least, the vocabulary better match in the \"interface\" API.",
        "I mean that there are two components shared in every OS (Linux is special but has similar concepts in certain DEs, but let's omit it as not implemented in the repo).\r\n\r\n* in the OS, there's a list of entries, ordered by \"last opened\"\r\n  * that list could be altered from the OS side (different ways to do that but still)\r\n* there's a list of last opened projects in Zed, and that list can be altered from Zed and needs to be propagated to the OS side\r\n\r\nAt let's call all these entries, lists, whatnot similarly, as of now there's \"jump_list\" for Windows only and something else for mac-only.\r\nWhat also would be good, is to keep the same, single \"add an entry\"/\"set entries\" workflow in the API if possible."
      ],
      "zed-background-process-blocking-operations": [
        "Nope, alas.\r\n\r\n1. To start with, `pull_diagnostic` has some `.detach()` inside, so here we spawn a task that calls a synchronous function that spawns a task.\r\nThis is sort of noop and odd (even the name is odd, as it actually updates the buffer with the new diagnostics).\r\n\r\nI think it's better to have `pull_diagnostic` to return `Task<anyhow::Result<Vec<Diagnostics>>>` and inside, it will either call to LSP or to protobuf (in the remote client side), similar to how inlays are doing this.\r\n\r\n2. Then, somewhere on this level, we'll have to handle the debounces and tasks better.\r\nRight now, every time we type, we spawn a task that waits for a debounce time, then queries for diagnostics and applies them to the buffer.\r\n\r\nSo, the debounce at its current form does nothing but the delay of the potential query + edit cascade.\r\n\r\nI think the whole approach of placing the pull code here might be a be a bit off-pace.\r\nInstead, we can alter `editor.rs` and adjust around the https://github.com/zed-industries/zed/blob/d53a86b01dd3d02980938cbce1bfd74e35901dda/crates/editor/src/editor.rs#L12167 line.\r\n\r\nWe can store another task and do debounces in it first, similar to what `_scroll_cursor_center_top_bottom_task` has: https://github.com/zed-industries/zed/blob/d53a86b01dd3d02980938cbce1bfd74e35901dda/crates/editor/src/scroll/actions.rs#L77-L87\r\n\r\ndoes, but query LSP store for diagnostics instead after the timeout.\r\nThen, update the diagnostics similar to what applying the diagnostics push + calling `refresh_active_diagnostics` does today.\r\n\r\nWe're lucky that buffer, before updating its diagnostics, checks `diagnostics_timestamp` that it's a newer set.",
        "One thing the new model implies though, is the need to explicitly query multiple servers.\r\nCurrent code sets up the listeners per language server, so what looks like a single LSP query can be N (e.g. tailwind-css project that will have TS langserver + tailwind langserver + maybe prettier/biome and/or eslint servers on top).\r\n\r\nThe new way will require more code and will make it more explicit with `MultiLspQuery`, but maybe it's even better as simpler to understand?\r\n\r\n",
        "This is the core place, combining multiple new feedback from this review.\r\n\r\nhttps://github.com/user-attachments/assets/99f8f320-2b24-44cc-a926-9ef928a08fa2\r\n\r\nInteresting, I've intuitively expected that `clear` will cause a lot more issues with flickering, if the diagnostics retrieval is slow due to the amount of the diagnostics, but even on the large example it's not that bad.\r\n\r\nI think we're saved here by the fact that Zed already has issues with large amounts of diagnostics, hence we're not seeing the new ones.\r\nI would propose still, to rework this a slightly:\r\n* accumulate new state (`inline_diagnostics`) first, and mutate the state it in the very end of the task, once\r\n* pass everything that does not depend on `self` and `cx` through `cx.background_spawn(async move { ... }).await` \r\n\r\nCombined, something like\r\n```rs\r\nlet new_inlay_hints = cx.background_spawn(async move {\r\n    let mut new_inlined_diagnostics: Vec<(Anchor, InlineDiagnostic)> = Vec::new();\r\n    let mut prev_diagnostic_line = None;\r\n    for diagnostic in diagnostics {\r\n        //........\r\n        new_inlined_diagnostics..binary_search_by(|probe| {\r\n            diagnostic_anchor.cmp(&probe.0, &buffer)\r\n        });\r\n        //........\r\n    }\r\n}).await;\r\n//........\r\nself.inline_diagnostics = new_inline_diagnostics;\r\n```\r\n\r\nThis way, we'll be usually on a background thread, debounced or computing, cancelled on concequent requests.\r\nOld state will be kept, and Anchor (from Editor) -> DisplayPoint (when laying out) conversion will keep the diagnostics placed on the right lines when rendering, even after adding newlines or undoing.\r\n\r\nLarge diagnostics sets to process will cause more stale diagnostics text during fast editing, but as a somewhat inevitable trade-off, which does not flicker at least."
      ],
      "zed-scope-dependencies-appropriately": [
        "This changes the behavior for all crates in the project, seems like an overkill for a dev build-related change.\n\nIf it intends a global change, let's extract it into a separate PR and keep this one scoped on a local dev workflow change.\nFor that, I imagine we need to add a new [feature](https://doc.rust-lang.org/cargo/reference/features.html) right inside the `remote_server` crate and use that feature when doing a dev build.",
        "IIRC you have to post this change (with another feature) right inside the `remote_server/Cargo.toml` and see if it brings the right results with `cargo tree` or your experiments.\r\nFeatures are additive, so this should work."
      ],
      "alacritty-use-constraining-types": [
        "There seems to be no `LPDWORD` or even `DWORD` type in the `windows_sys` crate, and looks like tjeu use `*mut u32` directly:\r\n\r\n* https://docs.rs/windows-sys/latest/windows_sys/Win32/System/Threading/fn.GetExitCodeProcess.html\r\n\r\n* https://github.com/microsoft/windows-rs/blob/1f40da8ffc44adba3f9a866287108e80223d0a81/crates/libs/sys/src/Windows/Win32/System/Threading/mod.rs#L140\r\n\r\n* https://github.com/microsoft/windows-rs/blob/1f40da8ffc44adba3f9a866287108e80223d0a81/crates/libs/windows/src/Windows/Win32/System/Threading/mod.rs#L840"
      ],
      "zed-self-explanatory-identifier-names": [
        "```suggestion\r\n      \"cmd-alt-=\": \"workspace::IncreaseDocksSize\",\r\n```\r\n\r\n* We have a way to define parameters in the keymap:\r\n\r\nhttps://github.com/zed-industries/zed/blob/0731097ee5780b3569980d7ba93f8fcf4eee097d/assets/keymaps/default-macos.json#L40\r\n\r\nlet's use this form for every new action that has parameters: this way we'll make it more obvious to the users that the parameter exists (the do not usually see the code), and document the default.\r\n\r\n* Let's also remove that `ByOffset` suffix from everywhere, as the `offset` parameter is clearly showing this, and `Open` part, for brevity.\r\n\r\nMaybe `Size` can also go away?",
        "* WDYT on `workspace::Increase/DecreaseActiveDock`, `workspace::Increase/DecreaseDocks` and `workspace::ResetDocks` as names?\r\n\r\n* We do have to write new entries similar to that `IncreaseBufferFontSize` example above and a comment, what `0` as a default will do.",
        "What about the previous name, `\"min_line_number_digits\"`?\r\n\r\nNot sure what a \"line number base\" is, but feels about a single line number?"
      ],
      "zed-document-configuration-constraints-clearly": [
        "> ///\r\n\r\nLet's remove that and mention ranges around, where it matters.",
        "More like [0.0, 1.0] range that's not anywhere here and in the `///` docs too, just in the json defaults?\r\n\r\nWhat's up with negatives, btw, are those clamped to 0.0?",
        "```suggestion\r\nNon-negative `float` values\r\n```\r\n\r\nhttps://github.com/zed-industries/zed/blob/f19e1e3b5f23ec7f64b692f4530825e0e50e5b95/crates/workspace/src/pane_group.rs#L1138-L1139",
        "* Can we use a diagnostics level here? \r\nLSP defines that there are 4 types of these: https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#diagnosticSeverity and a single boolean cannot cover them all.\r\n\r\nI think it's better to be flexible and allow users to show all or almost none of them, if the want to.\r\nWe could also move that into `inline` part of the settings?\r\n\r\n* If we do that, I wonder how useful `primary_only` would be: even now it seems redundant, have you found it useful?\r\n\r\nI feel that we remove `use_rendered` unless you have a good reason to leave it be, if we remove `primary_only`, we'll have only 3 settings knobs: on/off, diagnostics level, and diagnostics interactivity.\r\nThat set looks quite small and good to me.",
        "This is the best part of the impl to me, honestly, I wonder if we could \r\n* move it into `inline` as this settings knob is related to this part of the functionality only? (apart from the fact that it spoils f8 navigation sometimes 🙂, see another comment )\r\n* enable it by default, and explain that when disabled, same effect could be achieved with f8/shift-f8 or the new action (if we decide to leave it)",
        "I think we now have a very good set of settings and descriptions inside the default.json above, and we need to update this section respectfully.\r\n\r\nI agree with the workflow, where all toggling is made by existing f8/shift-f8 , to potentially jumpy autotoggling or special actions are needed.\r\n\r\nOne wish I used to voice in another related discussion here: we can add `\"minimum_level\"` knob `\"inline\"` section, to allow users to show only errors, only errors + warnings, etc.\r\nSimilar to people not fond of flickering, there is a notable group of people who are not fond of many popovers/colors around their code and it seems simple to add another `.filter` inside `update_inline_diagnostics` to support that + new diagnostics re-queried after level change."
      ],
      "zed-test-through-public-apis": [
        "Super nice to see the tests on the new functionality, thank you.\r\n\r\nOne thing I really lack in them is a visual part: instead of each \"visible count\" and other digits assertion, we should do what other tests do, comparing a file+outline string representations.\r\n(all these\r\n\r\n```\r\noutline: struct OutlineEntryExcerpt\r\n  outline: id\r\n  outline: buffer_id\r\n  outline: range\"\r\n```\r\nparts).",
        "We need much more tests for something as fundamental as \"show me all highlights for a random chunk of text in the buffer\".\r\n\r\nIf this panics or somehow misbehaves, it might be very frequent and frustrating, and overall, it's very scary to merge whatever related thing.\r\n\r\nWe should cover:\r\n\r\n* overlapping ranges (both LSP overlaps and LSP + tree-sitter highlights overlap LSP ones)\r\n* multi-line LSP highlights\r\n* something else? \r\nFor inlays, we have `test_random_inlays` fuzzy test checks how text behaves for random inlays' inserts.\r\nShould we do something for highlights too?",
        "Also, all that \r\n\r\n> The delta is now expressed on these number arrays without any form of interpretation what these numbers mean. \r\n\r\nsection in the spec is somewhat scary: if I've read it correctly, we should fiddle with digits on the client, when computing token deltas?\r\n\r\nDefinitely worth testing then, if/when the delta support arrives.",
        "I think it would be better to add these cases into `test_url_regex` and ensure we match out the right thing.\r\n`valid_url_ending` is pretty local method to test instead.",
        "The idea of unit tests is to test certain contracts of a single code unit, usually by using its public API for testing.\r\nObviously, nothing forbids doing otherwise and test the private api, but unit tests also slow down development, as every adjusment might require test rewrites/fixes — private, \"pretty local\", methods are meant to be moved/removed/adjusted/etc. thus testing them is not that great.\r\n\r\nJust a general rule of thumb: you're fixing the way regex captures strings, so test that, not something that checks whether the URL \"ending\" is valid.",
        "Done, and that lead to a better way to compare chars case-insensitively, thank you.",
        "There's a lot of rendering tests, which is amazing, thank you!\r\n\r\nWhat's lacking are new ones in the `editor_tests.rs`, and after we deal with the brackets and whatever else that triggers the pop-up display, it would be fantastic to have that logic tested too."
      ],
      "zed-standardize-platform-agnostic-configuration": [
        "That has to have `#[serde(default = \"default_true\")]` , as the default_settings.json mention default = true (which is also good to mention here) + the user setting overrides require this field always (which is not what we want): \r\n\r\n![image](https://github.com/user-attachments/assets/fadef7dc-8010-4a93-b638-2f15b8d4fc90)\r\n",
        "The error is gone, so thank you for fixing this."
      ]
    },
    "profile": {
      "location": "Turku, Finland",
      "company": "@zed-industries",
      "blog": "t.me/SomeoneToIgnore",
      "site_admin": false,
      "followers": 195,
      "following": 1
    }
  },
  "ClearlyClaire": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-accessibility-interaction-security",
        "title": "accessibility interaction security"
      },
      {
        "slug": "mastodon-api-parameter-design",
        "title": "API parameter design"
      },
      {
        "slug": "mastodon-avoid-deprecated-sass-syntax",
        "title": "avoid deprecated SASS syntax"
      },
      {
        "slug": "mastodon-batch-similar-operations",
        "title": "batch similar operations"
      },
      {
        "slug": "mastodon-centralize-configuration-management",
        "title": "centralize configuration management"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-complete-translatable-sentences",
        "title": "Complete translatable sentences"
      },
      {
        "slug": "mastodon-comprehensive-authorization-checks",
        "title": "comprehensive authorization checks"
      },
      {
        "slug": "mastodon-configuration-value-safety",
        "title": "Configuration value safety"
      },
      {
        "slug": "mastodon-early-nil-validation",
        "title": "early nil validation"
      },
      {
        "slug": "mastodon-environment-variable-descriptive-naming",
        "title": "Environment variable descriptive naming"
      },
      {
        "slug": "mastodon-framework-aware-text-composition",
        "title": "Framework-aware text composition"
      },
      {
        "slug": "mastodon-leverage-existing-configuration-sources",
        "title": "leverage existing configuration sources"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-minimize-html-attack-surface",
        "title": "Minimize HTML attack surface"
      },
      {
        "slug": "mastodon-network-resource-limits",
        "title": "Network resource limits"
      },
      {
        "slug": "mastodon-optimize-collection-iterations",
        "title": "optimize collection iterations"
      },
      {
        "slug": "mastodon-optimize-database-queries",
        "title": "Optimize database queries"
      },
      {
        "slug": "mastodon-optimize-react-hooks",
        "title": "Optimize React hooks"
      },
      {
        "slug": "mastodon-optimize-test-organization",
        "title": "optimize test organization"
      },
      {
        "slug": "mastodon-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "mastodon-referrer-header-privacy",
        "title": "referrer header privacy"
      },
      {
        "slug": "mastodon-review-configuration-currency",
        "title": "Review configuration currency"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-contextually-descriptive-names",
        "title": "Use contextually descriptive names"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-naming",
        "title": "Use semantic naming"
      },
      {
        "slug": "mastodon-use-semantic-null-handling",
        "title": "Use semantic null handling"
      }
    ],
    "comments": {
      "mastodon-prefer-early-returns": [
        "Where is the `status` coming from in this snippet?"
      ],
      "mastodon-batch-similar-operations": [
        "Thank you for your contribution!\r\n\r\nThis could grow into a pretty large in-memory `notification_jobs_args` array. If going this way, I'd probably use `find_in_batches` and build an array for each such group instead of for all affected groups. As you pointed out, `find_each` works with the same batch size as `perform_bulk`, so this would not actually change how jobs are sent to Sidekiq, it would just put a bound on the size of the in-memory array.",
        "Looks good, thanks!",
        "Should those two calls be pipelined to avoid back-and-forth with Redis?",
        "The superclass `vite_stylesheet_tag` calls `vite_asset_path` which itself makes a lookup. Maybe it would make sense rewriting this bit to avoid doing the lookup twice (once for the path, and once for the integrity)?",
        "This will still be re-parsed on every access… maybe memoize it?\r\n```suggestion\r\n    @@frontend_translations ||= JSON.parse(FRONTEND_TRANSLATIONS)\r\n```"
      ],
      "mastodon-environment-variable-descriptive-naming": [
        "I'd suggest MAX_TOOT_CHARS or something similar, as one may want custom maximum length for other things (such as bios)."
      ],
      "mastodon-complete-translatable-sentences": [
        "I am again concerned, with this panel in particular, that we are encouraging clout-chasing and popularity contests where we really should not be.\r\n\r\nIn addition, I have a few other concerns about this panel:\r\n- I'm afraid that breaking up the sentence in a few translatable strings and making layout assumptions based on the shape of the sentence is going to make that hard or even impossible to translate in some languages\r\n- “That puts you in the top X of Mastodon users” can be fairly confusing and misleading. What's the metric used here? Is that Mastodon users in general, fediverse users known to your server, or local users on your server?\r\n- (I vaguely understand what the Bernie thing is supposed to mean, but this doesn't land for me; maybe it's some kind of cultural reference I'm missing?)",
        "What if the whole thing was one translatable string with various placeholders?\r\n\r\nSomething like (untested):\r\n\r\n```jsx\r\n<FormattedMessage\r\n  id='annual_report.summary.percentile_text'\r\n  defaultMessage='<label>That puts you in the top</label><percentage /><label>of Mastodon users.</label>'\r\n  values={{\r\n   label: (text) => <div className='annual-report__summary__percentile__label'>{text}</div>,\r\n   percentage: () => <div className='annual-report__summary__percentile__number'><FormattedNumber value={percentile / 100} style='percent' maximumFractionDigits={1} /></div>,\r\n  }}\r\n/>\r\n```",
        "This is *probably* fine, but I'd be careful with this. It's usually way better to have full sentences in translatable strings, even if similar strings end up repeating themselves. This gives translators more context and helps with languages which have a different grammatical structure."
      ],
      "mastodon-use-contextually-descriptive-names": [
        "Should we use a different name than `api`? This is pretty non-specific and confusing."
      ],
      "mastodon-centralize-configuration-management": [
        "One thing I'm kind of worried about is that we don't test for the environment variables' effect anymore, although they are the documented way to do this.",
        "I am not sure how clean and future-proof adding our own stuff directly in `config` is. Otherwise, this looks good.",
        "I meant the use of the top-level Rails `configuration` namespace. `omniauth` is probably safe indeed, though."
      ],
      "mastodon-optimize-collection-iterations": [
        "Hm… maybe something like the following? It seems to perform about the same, but is much more verbose:\r\n```suggestion\r\n    usernames = []\r\n    ids = []\r\n\r\n    uris.each do |uri|\r\n      param, value = uri_to_local_account_params(uri)\r\n\r\n      case param\r\n      when :username\r\n        usernames << value.downcase\r\n      when :id\r\n        ids << value\r\n      end\r\n    end\r\n\r\n```",
        "Yeah I suppose the following is ok:\r\n```suggestion\r\n    usernames = []\r\n    ids = []\r\n\r\n    uris.each do |uri|\r\n      param, value = uri_to_local_account_params(uri)\r\n      usernames << value.downcase if param == :username\r\n      ids << value if param == :id\r\n    end\r\n```",
        "If I understand correctly, this will recursively fetch posts even if those have already been fetched recently? I mean, the `should_fetch_replies?` is only fetched for the root post in the recursive crawling. So basically, if you check the context for a post *then* for its parent post, you'd end up doing the work twice."
      ],
      "mastodon-leverage-existing-configuration-sources": [
        "Oh, that's good to know and it would indeed be appropriate to replace the computed `jsRoot`!\r\n\r\nHowever, it's not (in general) guaranteed to be set, so I'm not sure how to handle the case where `config.root` is `undefined`.\r\n\r\nFurthermore, it would mean that `themesFile` would be defined as something like `path.resolve(config.root, '../../config/themes.yml')`, which is rather awkward and brittle.",
        "Seems like we could use `config.root` and `config.envDir`. However, they both can be `undefined`, I am not sure what the behavior of the plugin should be in this case."
      ],
      "mastodon-optimize-react-hooks": [
        "Oh, yeah, good point for the refs, I'm always confused by that. It can't have a dependency of `[]` since `handleDocumentClick` depends on `onClose`."
      ],
      "mastodon-use-semantic-naming": [
        "I'm personally fine either ways. `MAX_STATUS_CHARS` would be more consistent with the source code, but the configuration option is intended for admins, not for developers.",
        "I'm having a look and it's not super obvious how this could be done. I mean we could have a `rule.translated_text_for(I18n.locale.to_s)` but then memoizing the correct translation would be pretty awkward imho.",
        "I went with something like that, thanks!"
      ],
      "mastodon-network-resource-limits": [
        "I think an infinite loop is possible if an attacker builds a collection of infinitely many empty pages.",
        "I think having a low recursion limit makes sense. Every request is costly (up to a few seconds of waiting on a remote server), and the more we do in the service, the longer we will keep a worker busy. This can easily become a DoS vector.\r\n\r\nAs for paginating the context, yes, that would be extremely useful, and it could be useful in this specific case as well, but as you may have noticed this is a massively complex issue that also involves a lot of UX considerations (for instance, Mastodon's display of replies is “flattened” so the pagination cutoffs are far less obvious to represent than on say, reddit)",
        "> Checking on how we are meaning recursion here, just because activitystreams also refers to just iterating through collection pages as recursion: do we mean a limit on paging or do we mean limiting the recursion depth within a reply tree (only get e.g. `depth<n` replies)?\r\n\r\nI meant a pagination limit, not specifically a reply tree depth limit.\r\n\r\n> I would think that some invisible barrier where someone finds out later 'oh i wasn't actually seeing all the replies, i needed to click around more' would be pretty frustrating.\r\n\r\nThis is always a risk, whatever limits we have, unfortunately.",
        "Wouldn't this change require that every caller that uses persistent connection fully consume the response? Is that something we're sure we're currently doing?",
        "So this changes persistent connections from never being closed by `Request` to being closed by `Request` if they return too large of a body. I wonder if this is safe. What about only closing if it's not a persistent connection?",
        "I was not completely sure how HTTP.rb handled closed connections. It seems it silently reopens a connection, which seems safe in this case."
      ],
      "mastodon-choose-appropriate-exception-types": [
        "Having something called `find_or_initialize_by_*` raise an error (especially an `ActiveRecord::RecordInvalid` error) is pretty surprising, as `find_or_initialize_by` does not perform validation and does not raise.",
        "I think I would be in favor of either ignoring the issue and only errorring on save, or further restrict the existing `constraints: { id: %r{[^/]+} }` constraint in the router (which I think would look like something like `constraints: ->(req) { TagManager.instance.normalize_domain(req.params['id']).present? }`), though that is a little awkward as well.",
        "I think I would just use `find_or_initialize_by(domain: )`, I don't really see the value of the method otherwise.",
        "I think this `save!` is what causes the settings page to return a bogus 422 by raising a `ActiveRecord::RecordInvalid` instead of properly annotating the model with an error."
      ],
      "mastodon-api-parameter-design": [
        "If you wanted to do it like that, you'd need:\r\n```suggestion\r\n    @filter.keywords.build(resource_params.dig(:keywords_attributes, '0'))\r\n```\r\n\r\nIndeed, using `params` would raise `ActiveModel::ForbiddenAttributesError` as the params would not be vetted through `StrongParameters` (that's what `params.expect` does), and `resource_params.dig(:keywords)` is an array (well, a hash) for multiple keywords, while you are setting up only one.\r\n\r\nThat being said, this is a lot of work for just a few attributes. I wonder if we shouldn't use some bespoke parameter instead, e.g. have `/filters/new?hashtag=blah`.",
        "You would need to add handling just like you did there, e.g. `@filter.keywords.build({ 'keyword' => \"##{params['hashtag']}\", 'whole_word' => true }) if params['hashtag'].present?`"
      ],
      "mastodon-use-semantic-null-handling": [
        "Maybe the following might be a little easier to read?\r\n```suggestion\r\n  return accountId ?? undefined;\r\n```",
        "But overall, the previous version had an actual issue where it could return a string, `undefined` or `null` but the types were overridden so Typescript was considering only `string` and `undefined`. I think in some places (probably non-Typescript only) we rely on the difference between `undefined` and `null`, as `undefined` means “we haven't fetched it yet” and `null` means “it does not exist”.\r\n\r\nIf I'm wrong and we never make the distinction, we probably should never store `null` and instead delete from the map, simplifying the type and having `string | undefined` everywhere.",
        "We indeed do make a distinction in at least `app/javascript/mastodon/features/account_timeline/index.jsx`, with `https://mastodon.social/@bogus@example.com` displaying a 404, but `app/javascript/mastodon/features/account_featured/index.tsx`, not making that distinction, displays an infinite spinner: `https://mastodon.social/@bogus@example.com/featured`"
      ],
      "mastodon-use-descriptive-specific-names": [
        "We have only briefly used values starting with `_:` and stopped doing so years ago, so the `value.start_with?('_:')` case would actually never occur afaik.\r\n\r\nAs for values starting with `_`, we currently have none, and would start using `_misskey_quote`, which needs to *not* be camelized.",
        "For context, this was added in #4585 for the `_:locked` attribute, which was dropped in #4767, and then renamed in #4779.",
        "Ok, why not, changed it to `start_with?('_misskey')`",
        "Maybe those scopes could use some renaming. `Fasp::Subscription.account` was a bit confusing, as on first read I did not think this was a scope, but an Account attribute.",
        "I think `category_account` and `category_content` are ever so slightly less confusing. But I don't have any better suggestion.",
        "I chose “closed federation” in opposition of “open federation”, but I have no strong feeling. I also considered `limited_federation` and `allowlist_federation` (possibly the most descriptive one here).",
        "Changed to `limited_federation`"
      ],
      "mastodon-framework-aware-text-composition": [
        "I tried to come up with something that would work both for previous and upcoming ToS changes, since someone could log in between the time a ToS change notification was triggered and the time the new ToS apply.\r\n\r\nUpdated to have a different copy for future and past ToS changes."
      ],
      "mastodon-comprehensive-authorization-checks": [
        "Indeed, because we don't have any support or representation for private groups at the moment."
      ],
      "mastodon-migration-data-dependencies": [
        "I think this needs to be moved to a pre-deployment migration. And now that we have migration breakpoints, it should be safe to do so.",
        "That is a good question. I guess the assumption now is that anyone running those will be on 4.3.0 or newer, or will have the services stopped while running the migrations. This means they should normally not lead to new entries with `thing_id` and `thing_type`. Just to be safe we could do the deletion in both a pre and post-deployment migration.\r\n\r\nThe columns removal still need to be in a post-deployment migration because the 4.3.0 code does not ignore those columns. Index changes should be safe either way I suppose, but if we are re-deleting stuff out of cautiousness in the post-deployment migrations, this means we would do the index changes there too.",
        "Those are loaded in a post-deployment migration (not sure why we did it that way), which means there is no guarantee the role exists at runtime (or in pre-deployment migrations).",
        "Yes, that's it!\r\n\r\nThough, once again, taking a step back, I would be happy with a migration-less approach that may do a N+1 once if the default role doesn't exist yet at runtime, as long as we can avoid N+1s when the role does exist.",
        "The main issue with `after_initialize` is that it would set the value to `-99` in the database, while we ideally do not want that to be persisted in the database, for index size reasons mainly."
      ],
      "mastodon-minimize-html-attack-surface": [
        "Allowing `style` attributes *should* be safe, although there have been vulnerabilities in our sanitizing library in the past, and limiting the surface area should be best.\r\n\r\nAnd if we do decide to allow `style` attributes, it probably makes sense to only allow specific properties like `border`. And keep support for `frameborder` as well.",
        "Afaik the `html` bit you modified still goes through sanitization, so by using `style` instead, it will get sanitized away and won't work.",
        "I'm not sure if we want to risk adding support for `style` (it's *supposed* to be safe but significantly increases the attack surface), I'll let @Gargron decide on this.\r\n\r\nBut if we're doing this, we should very much use something like:\r\n```ruby\r\n  css: {\r\n    properties: ['border']\r\n  }\r\n```\r\n\r\nto limit attributes to a list of explicitly-vetted properties, to avoid an attacker getting creative and using confusing UI elements, and so on."
      ],
      "mastodon-configuration-value-safety": [
        "afaik what we need is:\r\n```suggestion\r\n    password: <%= ENV.fetch('SMTP_PASSWORD', nil).to_json %>\r\n```\r\n\r\nas we do in `config/database.yml`"
      ],
      "mastodon-review-configuration-currency": [
        "This is still a discrepancy with the Ruby handling, isn't it?"
      ],
      "mastodon-optimize-database-queries": [
        "I wonder if we wouldn't be better off using raw SQL here:\r\n```suggestion\r\n  scope :matches_partially, ->(str) { where(exact: false).where(\"'%' || username || '%'\") }\r\n```\r\n\r\nI'm also slightly worried about the performance implications: if the table of blocked usernames grows big, looking up with partial matches is going to be increasingly expensive, as it will require going through the whole table. Unfortunately, I do not have any suggestion how to improve that.",
        "@Gargron both would need to go through the whole table… I guess I'd stick with the current approach for now\r\n\r\n@mjankowski I'm not against that, but it would just move those decisions to later, and my first review pass is already done, so while it would have helped, it's now a bit late.",
        "Good catch. Fixed!",
        "There is a slight (probably pretty much unobservable) overhead to checking for blocks, but the overall query is actually much faster by avoiding a sequential scan (`Account.local.where(username: local_usernames)` did not use the index we have, because the index is on lowered usernames).",
        "I suppose we could do that in a follow-up PR, yeah."
      ],
      "mastodon-early-nil-validation": [
        "Good catch, this would indeed be a weird edge case, but it should probably be gracefully handled rather than raising an exception and clogging queues with retried jobs which are extremely unlikely to ever proceed. Will have a look!",
        "I changed it to silently return if the post is missing.",
        "`effective_date` can be `nil`, for instance for ToS entries created before this PR. In such cases, this will raise `NoMethodError (undefined method 'past?' for nil)`.",
        "I think this should go even earlier, you can avoid the Redis lock.",
        "What about just defining `uncached = {}` just a few lines above, so that it's never `nil`?",
        "No! This would precisely not work, because the failing test passes an array as `scope`, which causes `no implicit conversion of Symbol into Integer`, an error that would remain the same with `dig`!"
      ],
      "mastodon-accessibility-interaction-security": [
        "> The `aria-describedby` property is appropriate when the associated content contains plain text. If the content is extensive, contains useful semantics, or has a complex structure requiring user navigation, use [aria-details](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Reference/Attributes/aria-details) instead.\r\n\r\nContent warnings are mostly textual but they are allowed to include emojis, including custom ones, so I wonder if we should use `aria-details` instead?"
      ],
      "mastodon-optimize-test-organization": [
        "It's a minor thing, but we're not checking empty events anywhere.",
        "Oh yeah, it's testing empty URL and empty events at the same time. Maybe it should be split in two, and made a little more explicit with comments.",
        "Sorry if I haven't been clear, but I was suggesting testing both case failures (empty events and empty URL) separately.",
        "Yeah, I suppose you're right, if the model tests cover this, it does not matter much."
      ],
      "mastodon-avoid-deprecated-sass-syntax": [
        "This leads to the following warning:\r\n```\r\nDEPRECATION WARNING [slash-div]: Using / for division outside of calc() is deprecated and will be removed in Dart Sass 2.0.0.\r\n\r\nRecommendation: math.div(24px - 12px, 2) or calc((24px - 12px) / 2)\r\n\r\nMore info and automated migrator: https://sass-lang.com/d/slash-div\r\n\r\n     ╷\r\n4623 │   margin: (24px - 12px) / 2;\r\n     │           ^^^^^^^^^^^^^^^^^\r\n     ╵\r\n    app/javascript/styles/mastodon/components.scss 4623:11  @use\r\n    app/javascript/styles/application.scss 16:1             root stylesheet\r\n```",
        "There's a deprecation warning for `lighten`."
      ],
      "mastodon-use-accessible-terminology": [
        "This appears technically correct, however I think it is unnecessary, as `count` here is a constant (currently set to 20000). It may change in the future, but it will always be a large number.",
        "Same thing with `limit` VS `count`, and same thing with the limit being a hardcoded value (currently 25)."
      ],
      "mastodon-referrer-header-privacy": [
        "This looks ok, but things I'm worried are less uniquely identifying users, but having them inadvertently out something on them when clicking a link to an external service they are registered on (e.g. user on a LGBTQ instance logging in on a place where that could be an issue for them)"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 403,
      "following": 0
    }
  },
  "gaby": {
    "repos": [
      "gofiber/fiber"
    ],
    "entries": [
      {
        "slug": "fiber-api-design-clarity",
        "title": "API design clarity"
      },
      {
        "slug": "fiber-avoid-count1-flag",
        "title": "avoid `-count=1` flag"
      },
      {
        "slug": "fiber-check-all-error-returns",
        "title": "check all error returns"
      },
      {
        "slug": "fiber-choose-descriptive-property-names",
        "title": "Choose descriptive property names"
      },
      {
        "slug": "fiber-clear-network-api-documentation",
        "title": "Clear network API documentation"
      },
      {
        "slug": "fiber-document-mutex-usage",
        "title": "Document mutex usage"
      },
      {
        "slug": "fiber-enforce-secure-tls-configuration",
        "title": "Enforce secure TLS configuration"
      },
      {
        "slug": "fiber-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "fiber-evaluate-nil-check-necessity",
        "title": "Evaluate nil check necessity"
      },
      {
        "slug": "fiber-explicit-cicd-configuration",
        "title": "explicit CI/CD configuration"
      },
      {
        "slug": "fiber-extract-duplicate-logic",
        "title": "Extract duplicate logic"
      },
      {
        "slug": "fiber-follow-naming-patterns",
        "title": "Follow naming patterns"
      },
      {
        "slug": "fiber-include-practical-examples",
        "title": "Include practical examples"
      },
      {
        "slug": "fiber-maintain-clean-linter-configs",
        "title": "maintain clean linter configs"
      },
      {
        "slug": "fiber-minimize-memory-allocations",
        "title": "minimize memory allocations"
      },
      {
        "slug": "fiber-minimize-unsafe-reference-lifetime",
        "title": "minimize unsafe reference lifetime"
      },
      {
        "slug": "fiber-simplify-logging-integrations",
        "title": "simplify logging integrations"
      },
      {
        "slug": "fiber-use-context-for-configuration",
        "title": "Use context for configuration"
      },
      {
        "slug": "fiber-validate-configuration-defaults",
        "title": "Validate configuration defaults"
      },
      {
        "slug": "fiber-validate-security-inputs",
        "title": "Validate security inputs"
      }
    ],
    "comments": {
      "fiber-choose-descriptive-property-names": [
        "Agree, the `Format` field is enough to provide predefined formats and user can provide a custom format through it. So there's no need for 2 fields.",
        "@edvardsanta Yes, the `Format` field is enough. Will keep things simple/easy to use.",
        "I'd probably name this `ExtraKeyLookup`, `ExtraKeyLookups`,  or something like that?",
        "This should be \"LivenessEndpoint\"",
        "Yes",
        "That way its consistent internally and public"
      ],
      "fiber-simplify-logging-integrations": [
        "Yeah, this looks too complicated for just a logger. Unless i'm looking at it wrong? ",
        "Having it in the code would be ideal, a lot of users don't check the docs ",
        "@haochunchang True, that makes sense"
      ],
      "fiber-avoid-count1-flag": [
        "You are missing  `-count=1`",
        "Why is `count=1` removed?"
      ],
      "fiber-include-practical-examples": [
        "General comment for docs:\r\n\r\nWe mention in multiple places the usage of `maps.Collect()` but never provide an example. Probably worth adding an example to avoid follow-on questions from users."
      ],
      "fiber-evaluate-nil-check-necessity": [
        "@mitulagr2 Can you add the nil check suggested including the panic.",
        "Also add a test where you call the function with `nil`, and test that it panics. We use testify you can check this with:\r\n\r\n```go\r\nrequire.Panics(t, func(){\r\n    // create client with nil here to throw panic\r\n})\r\n```",
        "Missing safety check found by https://github.com/uber-go/nilaway",
        "Will remove",
        "Done",
        "Missing safety check found by https://github.com/uber-go/nilaway"
      ],
      "fiber-validate-security-inputs": [
        "I feel like this could be abused for DoS by sending big values via cookies",
        "It might be better to log a generic message with the cookie name, instead of logging the whole cookie value. ",
        "Just add a comment for now in the code. Do not log it.",
        "@gofiber/maintainers Is the correct approach to \"clean\" the cookie for the user. Or should we return HTTP 400?\r\n\r\n@c00kie17 how does net/http handle this?",
        "Before this line we should add something like this to validate the proxies are http/https.\r\n\r\n```go\r\n    // Validate the URL scheme\r\n    if pURL.Scheme != \"http\" && pURL.Scheme != \"https\" {\r\n        return errors.New(\"unsupported proxy scheme\")\r\n    }\r\n```"
      ],
      "fiber-extract-duplicate-logic": [
        "@ReneWerner87 Do we want to add `DoTimeout` and `DoDeadline` to this PR? ",
        "@ReneWerner87 Implementef the changes for DoTimeout and DoDeadlind. Having a hard time writing the unit-tests. Seems like I need to reuse the `createProxyServer` since `app.Test()` is returning 404 unless I use a real domain name."
      ],
      "fiber-clear-network-api-documentation": [
        "Replace this with:\r\n\r\n-  Added Unix socket support\r\n+ - Added support for Unix domain sockets via `ListenerNetwork` and `UnixSocketFileMode`"
      ],
      "fiber-maintain-clean-linter-configs": [
        "Fixed, thanks!"
      ],
      "fiber-document-mutex-usage": [
        "Do we need the Mutex Locks here too?",
        "add comment for `sendfilesMutex`",
        "This is an interface, we shouldn't rename it to avoid breaking changes.",
        "https://github.com/gofiber/storage/blob/main/storage.go\r\n\r\nWe may need to discuss this, since it's implemented differently in multiple places 🤔"
      ],
      "fiber-api-design-clarity": [
        "@ReneWerner87 What if we remove the handler param, and require the middleware to be used with `app.Use()` ?",
        "@Andrei-hub11 The `main` branch is tracking the new major version of `GoFiber`. Breaking changes are `allowed`.",
        "It is not that simple to implement, it requires using the router within the handler."
      ],
      "fiber-follow-naming-patterns": [
        "All the benchmarks and tests should have an underscore \"Benchmark_\" and \"Test_\" for consistency with the other files.",
        "The naming here could use some help:\r\n\r\n- DefaultFormat\r\n- CommonLogFormat (CLF)\r\n- CombinedLogFormat\r\n- JSONFormat\r\n- ECSFormat",
        "We should probably rename this to `FromContext()`",
        "Thoughts? ",
        "We should rename this since `Datas` is not an english word. An option is `AllFormData()`",
        "We should rename this to `GetLogger()` or `Logger()` ?",
        "Probably `Logger()`  to match `ControlLogger`",
        "Consider renaming this to `RegisterConstraint()` like the original issue suggested.",
        "Yeah,  fair if we have functions with that convention to keep it as `RegisterCustomConstraint()`"
      ],
      "fiber-validate-configuration-defaults": [
        "This should be 770 by default. If the users the server to be accessible by the whole host they can change it to 775",
        "I think default should be 10s. Having infinite will cause for example Docker to send a `SIGKILL`.\r\n\r\nSee: https://docs.docker.com/reference/cli/docker/container/stop/#time",
        "@efectn @ReneWerner87 Thoughts? "
      ],
      "fiber-check-all-error-returns": [
        "@copilot the error code from ln.Close() needs to be check",
        "We should log the error here and return the error not the client without a CA set",
        "Missing error check:\r\n\r\n```\r\nif !config.RootCAs.AppendCertsFromPEM(certData) {\r\n    return errors.New(\"failed to append certificate\")\r\n}\r\n```",
        "Missing error check:\r\n\r\n```\r\nif !config.RootCAs.AppendCertsFromPEM(certData) {\r\n    return errors.New(\"failed to append certificate\")\r\n}\r\n```"
      ],
      "fiber-minimize-memory-allocations": [
        "@sixcolors I'm closing this PR then.",
        "Why not wrap \"/\" in bytes? Instead of converting patternPretty to a string",
        "Same with calling string(patternPretty) multiple times in the same function",
        "This should be a map, for o(1) lookups. Ex:\r\n\r\n```go\r\nvar cacheableStatusCodes = map[int]bool{\r\n    fiber.StatusOK:                          true,\r\n    fiber.StatusNonAuthoritativeInformation: true,\r\n    // etc...\r\n}\r\n```"
      ],
      "fiber-use-context-for-configuration": [
        "Done"
      ],
      "fiber-enforce-secure-tls-configuration": [
        "This should use TLSv1.2 min.",
        "Does this enforce ClientCert Auth? We should add an option for setting ClientAuth type https://pkg.go.dev/crypto/tls#ClientAuthType",
        "@efectn What it is talking about is for example:\r\n\r\nFiber Server ONLY allows TLSv1.3\r\nFiber Client tries to connect using only TLSv1.2\r\n\r\n^ This is not covered by the tests and should result in a handshake failure. "
      ],
      "fiber-minimize-unsafe-reference-lifetime": [
        "This should probably be: `safe := c.CopyString(c.Cookies(\"name\"))`",
        "This should probably be: safe := c.CopyBytes(c.Body())`"
      ],
      "fiber-ensure-comprehensive-test-coverage": [
        "Makes sense, will add that.",
        "Added test for -1 and 0",
        "@coderabbitai Great, now write a unit-test for when token is \"\".\r\n\r\nAdded lines #L148 - L149 were not covered by tests",
        "This is missing a test using the default CBOR Marshall and UnMarshall functions as noted by `codecov` below.",
        "Missing hook test for `cbor`",
        "This whole file is missing tests",
        "Agree",
        "We should add a test were the flag is set to False, and make sure the Header is not present.",
        "If the function is public, we may need to add a test."
      ],
      "fiber-explicit-cicd-configuration": [
        "Done, points to 1.64.7 now.",
        "This action hasn't been updated since early 2023. See this https://github.com/autero1/action-gotestsum/issues/46",
        "I think we better of by running `go install gotest.tools/gotestsum@latest`",
        "That's too cumbersome, just use `go install`, we already do that for `gosec`and `govulncheck`.",
        "Un-neccesary comment/change"
      ]
    },
    "profile": {
      "location": "United States",
      "blog": "",
      "site_admin": false,
      "followers": 131,
      "following": 21
    }
  },
  "wilkinsona": {
    "repos": [
      "spring-projects/spring-boot"
    ],
    "entries": [
      {
        "slug": "spring-boot-alphabetical-ordering-requirement",
        "title": "Alphabetical ordering requirement"
      },
      {
        "slug": "spring-boot-bean-lifecycle-management",
        "title": "Bean lifecycle management"
      },
      {
        "slug": "spring-boot-clear-structured-logging-documentation",
        "title": "Clear structured logging documentation"
      },
      {
        "slug": "spring-boot-concrete-bean-return-types",
        "title": "Concrete bean return types"
      },
      {
        "slug": "spring-boot-consistent-observability-data",
        "title": "Consistent observability data"
      },
      {
        "slug": "spring-boot-consistent-terminology-usage",
        "title": "Consistent terminology usage"
      },
      {
        "slug": "spring-boot-document-configuration-properties-completely",
        "title": "Document configuration properties completely"
      },
      {
        "slug": "spring-boot-documentation-clarity-principles",
        "title": "Documentation clarity principles"
      },
      {
        "slug": "spring-boot-environment-variables-best-practices",
        "title": "Environment variables best practices"
      },
      {
        "slug": "spring-boot-explicit-security-configurations",
        "title": "Explicit security configurations"
      },
      {
        "slug": "spring-boot-explicit-security-documentation",
        "title": "Explicit security documentation"
      },
      {
        "slug": "spring-boot-follow-consistent-style-conventions",
        "title": "Follow consistent style conventions"
      },
      {
        "slug": "spring-boot-include-database-specific-migration-dependencies",
        "title": "Include database-specific migration dependencies"
      },
      {
        "slug": "spring-boot-inherit-organization-security-policies",
        "title": "Inherit organization security policies"
      },
      {
        "slug": "spring-boot-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "spring-boot-meaningful-exception-design",
        "title": "Meaningful exception design"
      },
      {
        "slug": "spring-boot-optimize-test-case-design",
        "title": "Optimize test case design"
      },
      {
        "slug": "spring-boot-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "spring-boot-property-description-conventions",
        "title": "Property description conventions"
      },
      {
        "slug": "spring-boot-reference-existing-configurations",
        "title": "Reference existing configurations"
      },
      {
        "slug": "spring-boot-stable-observability-components",
        "title": "Stable observability components"
      }
    ],
    "comments": {
      "spring-boot-bean-lifecycle-management": [
        "Is it possible to not expose the imperative components as beans in a reactive app?",
        "> The reactive components leverage the reactive client which adapts the original client (which is imperative and auto-configured)\r\n\r\nI wonder if the original, imperative client has to be a bean in the reactive case. Perhaps it does for ease of configuration property binding and the like. Not something to worry about for this PR, but probably worth revisiting down the road before 3.2 GAs.\r\n\r\n>  there are other technologies auto-configured in Boot whose reactive components leverage the imperative ones, no?\r\n\r\nWe have some situations where there's a low-level bean that's shared between imperative and reactive (Elasticsearch client transport and the ElasticsearchClient and ReactiveElasticsearchClient that use it, for example) but I can't think of a situation where we have an imperative bean that we expect applications to use extensively that's then wrapped by a reactive bean. I may well be forgetting something though."
      ],
      "spring-boot-optimize-test-case-design": [
        "This is largely testing the underlying SpEL support. It could be simplified to test a single valid expression.",
        "This is largely testing the underlying SpEL support. It could be simplified to test a single invalid expression.",
        "+1. For this to warrant being a Docker-based test, I think it should somehow check that setting the `application_name` has had the desired effect on the Postgres side of things.\r\n\r\nI'd then prefer that we only have a single Docker-based test that checks this and that the precedence of the different sources of the application name is unit tested without involving Docker. Right now we're adding a lot to the build time and I don't think the extra test coverage warrants that increase.",
        "I don't think so. Given that all three values are passed through and then returned by `getMaxParameterCount()` without any having special treatment, I think testing with a single value would be sufficient.",
        "The paths aren't right in this example, but Framwork 6.2's `JsonContent` is an option here:\r\n\r\n```java\r\nJsonContent body = new JsonContent(entity.getBody());\r\nassertThat(body).extractingPath(\"alias\").isEqualTo(\"spring-boot-ssl-sample\");\r\nassertThat(body).extractingPath(\"status\").isEqualTo(\"EXPIRED\");\r\nassertThat(body).extractingPath(\"subject\")\r\n\t.isEqualTo(\"CN=localhost,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown\");\r\n```",
        "I don't feel strongly one way or the other. That said, I do wonder if the intent would be clearer with four separate methods with names that make the expectations clear. They could all then delegate to a common method that looks a lot like this one."
      ],
      "spring-boot-environment-variables-best-practices": [
        "I'm not in favor of trying to mock environment variables so +1 for @nosan's suggestion.\r\n\r\nMore generally, where we need to test environment variables, we try to allow the `Map` to be injected for testing purposes. If third-party code doesn't allow that then I'd prefer that we accept that it can't be tested rather than relying on reflection to hack into `System.getenv`.",
        "I don't think spaces in the default makes any difference. For example, you can do this:\r\n\r\n```\r\nproperty.example=${some.other.property:The default value}\r\n```\r\n\r\nAssuming that `some.other.property` hasn't been set the value of `property.example` will default to `The default value`."
      ],
      "spring-boot-reference-existing-configurations": [
        "Is it possible to tie this to the version that's configured in `.sdkmanrc`? That's particularly important for branches other than `main` where a Java 8 install is required.",
        "To avoid the risk of editing conflicts, I think we might prefer to have this disabled too.",
        "Those docs, which is where I learned of the potential for a conflict, say that adding the badge \"can also create a concurrent editing conflict when the bot and a user try to edit the description of a pull request at the same time\"."
      ],
      "spring-boot-inherit-organization-security-policies": [
        "I don't think we need this as we already inherit the policy from spring-projects/.github. We can just link to https://github.com/spring-projects/spring-boot/security/policy."
      ],
      "spring-boot-follow-consistent-style-conventions": [
        "We don't really use `Objects.equals`. This could be `return \"trust\".equals(postgresAuthMethod);` instead.",
        "We don't use `var` in Boot's code base. Please declare variables' types explicitly.",
        "We don't use `var` in Spring Boot's code.",
        "There are [several other places where we use `@Order(Ordered.LOWEST_PRECEDENCE)`. If we're going to change one, we should change them all. Personally, I don't think we should change any of them as I prefer to see the order explicitly declared.",
        "`(exporter) ->` is Spring Boot's code style and this doesn't pass Checkstyle. Please build your changes before submitting them.",
        "We're still escaping the opening `[` but the closing `]` is no longer escaped. I think the parser may cope with that but I find the expression harder to read as you have to know that the parser treats the `]` as a plain `]` rather than closing a never-opened character set. Please revert."
      ],
      "spring-boot-preserve-api-compatibility": [
        "This is a breaking API change. A separate method for the timeout would be better.",
        "This is a breaking API change. A separate method for the timeout would be better."
      ],
      "spring-boot-clear-structured-logging-documentation": [
        "Thanks, @quaff. I wonder if people might miss this? They could read the paragraph above and then stop, happy that they know which configuration property to set. I think it might be more noticeable if we changed the paragraph above instead to start with something like \"To enable structured logging, ensure that you aren't using custom log configuration and set the property…\". WDYT?",
        "I agree with @mhalbritter. We shouldn't over-complicate the documentation by showing the use of conditionals if they aren't always needed."
      ],
      "spring-boot-concrete-bean-return-types": [
        "The return type of a `@Bean` method should be as specific as possible. Could this be `DefaultPulsarConsumerFactory` with `@ConditionalOnMissingBean(PulsarConsumerFactory.class)`, similar to `topicResolver` above?",
        "Yes. This provides as much type information as possible to the bean factory while allow the bean to back off when any `PulsarConsumerFactory` bean is defined.",
        "Rather than changing the return type, which deprives the bean factory of type information, it would be better to change `@ConditionalOnMissingBean` to `@ConditionalOnMissingBean(ExamplarSampler.class)`. This will cause `exemplarSampler` to back off if there's an `ExamplarEampler` bean defined, while also still telling the bean factory that `exemplarSampler` is a `DefaultExemplarSampler`.",
        "I think this could be considered a bug and, therefore, it should be handled separately. Could you please split this out into a new PR, along with a test that verifies that the auto-configured `spanReporter` bean backs off when you define a custom `Reporter` bean?",
        "To provide as much type information as possible to the bean factory, the signature should show that it returns a `B3Propagator`."
      ],
      "spring-boot-document-configuration-properties-completely": [
        "The annotation processor the generates configuration property metadata can't handle enums due to a limitation of the annotation processing model. Please add an entry to `additional-spring-configuration-metadata.json` for this property's default value.",
        "This should be an `int` with a default value that matches that of `InMemoryWebSessionStore`. Providing a default improves the metadata that's used for auto-completion when editing application properties and YAML files. A test should be added to assert that the defaults are in sync.",
        "Unfortunately, we can't use javadoc tags in the descriptions of configuration properties as it doesn't work well with configuration property metadata generation. We'll have to use `AggregationTemporality` or even just `Aggregation temporality` instead.",
        "This default should be declared in the additional metadata as the annotation processor cannot detect default enum values."
      ],
      "spring-boot-consistent-terminology-usage": [
        "+1 for changing to \"autowire\". The Framework docs use \"autowire\" as a verb and we should follow suit, particularly as the example that follows doesn't even use `@Autowired` as it does not need to do so.",
        "Thanks for the suggestion. I think it would be better to use Kubernetes here rather than the K8S abbreviation."
      ],
      "spring-boot-consistent-observability-data": [
        "I wonder if something like \"Duration of HTTP server request handling\" would be better here? To me, it makes it clearer that the metric is for a server handling a request from a client rather than a client making a request to a server."
      ],
      "spring-boot-explicit-security-configurations": [
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here."
      ],
      "spring-boot-alphabetical-ordering-requirement": [
        "The modules should be listed alphabetically",
        "We try to list modules in alphabetical order."
      ],
      "spring-boot-include-database-specific-migration-dependencies": [
        "This wildcard search doesn't work for me and reports 0 results. I think we may have to link to https://documentation.red-gate.com/flyway/flyway-cli-and-api/supported-databases instead. There are also some DB-specific modules that are not named `flyway-database-*` such as the [module for MySQL](https://documentation.red-gate.com/flyway/flyway-cli-and-api/supported-databases/mysql)."
      ],
      "spring-boot-stable-observability-components": [
        "Should we link to something stable (a tag or specific SHA) here? Perhaps https://github.com/OpenObservability/OpenMetrics/blob/v1.0.0/specification/OpenMetrics.md#exemplars."
      ],
      "spring-boot-meaningful-exception-design": [
        "Let's consider this separately please. If we do it, it should be applied on the servlet side as well.",
        "We think that adding the stop or destroy failure as a suppressed exception is a good idea. Would you like to open a PR for it or would you prefer that we take care of it?",
        "Thanks!",
        "I think an `IllegalStateException` or `IllegalArgumentException` (it depends if you consider `target` or `type` to be the problem) might be better here and perhaps the error message should include `target`. Something like `\"Cannot unwrap \" + target + \" to \" + type.getName()`",
        "I'd rather this failed with an NPE as the initialize method should always be present.",
        "If the field's not found something's gone really wrong. I think it would be better to fail hard.",
        "If Hikari doesn't meet expectations then I think we should just give up as we have no way of knowing that our approach is valid. We'll know, through tests, that it works with the version of Hikari in dependency management. If someone overrides that version and Hikari has changed incompatibility I think we'd be better to fail fast. That'll make the problem much easier to diagnose rather than dealing with a checkpoint failure and trying to figure out why it occurred.",
        "`NoSuchJobException` is a `JobExecutionException` so I don't think there's any need to create a new exception. Instead, I think the `try` and `catch` could be removed completely so that the method just throws the original `NoSuchJobException`."
      ],
      "spring-boot-maintain-consistent-naming-patterns": [
        "I wonder if it would be better if we ignored that fact that it's `BatchSpanProcessor` that's the component that's doing the exporting and focussed instead on the properties affecting span export.\r\n\r\n\r\n```\r\nmanagement.tracing.opentelemetry.span.export.include-unsampled\r\nmanagement.tracing.opentelemetry.span.export.schedule-delay\r\nmanagement.tracing.opentelemetry.span.export.timeout\r\n```",
        "> However, these properties only impact the BatchSpanProcessor and cannot be applied, for instance, to the SimpleSpanProcessor or other processors.\r\n\r\nBoot doesn't auto-configure the `SimpleSpanProcessor` so hopefully that confusion will not arise. The property names make no mention of processing so hopefully no one will expect them to apply to other processors either.\r\n\r\n>  In my opinion, there should be some indication that these properties are intended for batch processing\r\n\r\nDoesn't the `max-batch-size` property (that we omitted in the property examples above) do that?\r\n\r\nThe complete set of properties would be:\r\n\r\n```\r\nmanagement.tracing.opentelemetry.span.export.include-unsampled\r\nmanagement.tracing.opentelemetry.span.export.max-batch-size\r\nmanagement.tracing.opentelemetry.span.export.max-queue-size\r\nmanagement.tracing.opentelemetry.span.export.schedule-delay\r\nmanagement.tracing.opentelemetry.span.export.timeout\r\n```\r\n\r\nI'm not totally sold on the prefix here. In part, it's the `otlp` vs `opentelemetry` problem again. I think this export may use Zipkin or OTLP depending on which of `ZipkinSpanExporter`, `OtlpHttpSpanExporter`, and `OtlpGrpcSpanExporter` has been auto-configured. We already have a couple of export-related properties for those:\r\n\r\n- `management.otlp.tracing.export.enabled`\r\n- `management.zipkin.tracing.export.enabled`",
        "I think `opentelemetry` is better here as this is configuring parts of the OpenTelemetry SDK. The protocol used for the export won't necessarily by OTLP so I think `otlp` would be inaccurate. This naming and its subtleties are more complex than we'd like but that complexity isn't of our making and there's only so much we can do to hide it.\r\n\r\n+1 for dropping `.span.` from the names.",
        "We prefer longer names for test methods that describe what's being tested. Something like `whenCustomizerBeanIsDefinedThenItIsConfiguredOnSpringLiquibase`.",
        "I think this should be called `SdkTracerProviderBuilderCustomizer`. We can take care of that as part of merging it.",
        "I think we should aim for consistency with other similar interfaces that already exist. We have many that are named `…BuilderCustomizer` because the class that they are customising is a `…Builder`. The same applies here.",
        "For consistency with `alpha-numeric -> alphanumeric`, this method should be renamed to `remainderIsNotAlphanumeric`.",
        "Is white box a recognised Wavefront term? We try to avoid white box and black box if we can as they're jargon that can confuse people, particularly those with English as a second language."
      ],
      "spring-boot-property-description-conventions": [
        "Given that the property's prefix is `management.tracing.brave` do we need `(tracing library)` in the description? Put another way, is there another Brave that could cause confusion?",
        "We try to have `boolean` properties start with `Whether the…`. Perhaps this could be something like this instead:\r\n\r\n> Whether the propagation type and tracing backend support sharing the span ID between client and server spans. Requires B3 propagation and a compatible backend."
      ],
      "spring-boot-documentation-clarity-principles": [
        "What failure did you get? That `xref` seems to work for me.",
        "I just pushed a change to your branch. Let's see what CI makes of it.",
        "It failed. `:spring-boot-project:spring-boot-docs:antora` which pulls in the Actuator API docs worked for me locally. It's `spring-boot-project:spring-boot-actuator-autoconfigure:antora` that's failing on CI though.\r\n\r\nAny ideas, @philwebb? The reference works fine when built as part of `spring-boot-docs` but not when just building the Actuator API docs on their own.",
        "I think this was intentionally singular. If it's to change, I think it should be changed to \"container\".",
        "Oops. Yeah, I think we can just remove it as we're already in a section about Testcontainers. I think \"for Container beans\" reads better and there's enough context to know that it's a Testcontainers container.",
        "I'd just document the map-based approach.",
        "Could we use \"over\" here? I think it's more idiomatic when talking about a protocol.",
        "This looks to be unrelated to the rest of the changes. Could it please be made separately?",
        "> If you still insist, I can move it to separate PR.\r\n\r\nYes please, Artem. It sounds like we should get rid of that sentence in 2.4.x and later whereas the rest of the changes proposed here will land in 2.6."
      ],
      "spring-boot-explicit-security-documentation": [
        "This property name is incorrect. It should be `spring.security.oauth2.resourceserver.jwt.audiences`. As previously requested, please use the `configprop:spring.security.oauth2.resourceserver.jwt.audiences[]` syntax so that the name is checked as part of the build.",
        "This (line 234) should not be here. It should be part of line 226.",
        "Sorry, it's still not right and you've moved it to line 229 rather than updating 226 as I requested above. Please don't make any more changes. I'll take things from here.",
        "That looks better. Please go ahead.",
        "I agree. In the context of CSRF, it would be better to just disable Security's protection rather than ignoring entirely. This is what the old (1.x) auto-configuration for the console used to do:\r\n\r\nhttps://github.com/spring-projects/spring-boot/blob/10a5cef4ef33e7c86d18e1f92793c2aaa57d5a82/spring-boot-autoconfigure/src/main/java/org/springframework/boot/autoconfigure/h2/H2ConsoleAutoConfiguration.java#L97-L113",
        "The goal here is to consistently use the term \"expose\" but this change uses \"secret\". I think this sentence would be better if it was something like the following:\r\n\r\n> For security purposes, only the `/health` endpoint is exposed over HTTP by default."
      ]
    },
    "profile": {
      "location": "UK",
      "company": "Broadcom",
      "blog": "",
      "site_admin": false,
      "followers": 2181,
      "following": 0
    }
  },
  "kchibisov": {
    "repos": [
      "alacritty/alacritty"
    ],
    "entries": [
      {
        "slug": "alacritty-assess-security-trade-offs",
        "title": "Assess security trade-offs"
      },
      {
        "slug": "alacritty-avoid-unnecessary-operations",
        "title": "avoid unnecessary operations"
      },
      {
        "slug": "alacritty-avoid-unwrap-on-nullables",
        "title": "Avoid unwrap on nullables"
      },
      {
        "slug": "alacritty-centralize-workspace-dependencies",
        "title": "centralize workspace dependencies"
      },
      {
        "slug": "alacritty-choose-familiar-intuitive-names",
        "title": "Choose familiar, intuitive names"
      },
      {
        "slug": "alacritty-configuration-documentation-accuracy",
        "title": "Configuration documentation accuracy"
      },
      {
        "slug": "alacritty-configuration-validation-feedback",
        "title": "Configuration validation feedback"
      },
      {
        "slug": "alacritty-consistent-error-handling",
        "title": "consistent error handling"
      },
      {
        "slug": "alacritty-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "alacritty-document-configuration-specifics",
        "title": "Document configuration specifics"
      },
      {
        "slug": "alacritty-explain-code-intent",
        "title": "Explain code intent"
      },
      {
        "slug": "alacritty-follow-metadata-specifications",
        "title": "Follow metadata specifications"
      },
      {
        "slug": "alacritty-keep-documentation-together",
        "title": "Keep documentation together"
      },
      {
        "slug": "alacritty-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "alacritty-platform-specific-api-documentation",
        "title": "Platform-specific API documentation"
      },
      {
        "slug": "alacritty-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "alacritty-unsafe-code-practices",
        "title": "unsafe code practices"
      },
      {
        "slug": "alacritty-use-constraining-types",
        "title": "Use constraining types"
      },
      {
        "slug": "alacritty-use-descriptive-contextual-names",
        "title": "Use descriptive contextual names"
      },
      {
        "slug": "alacritty-write-audience-appropriate-documentation",
        "title": "Write audience-appropriate documentation"
      }
    ],
    "comments": {
      "alacritty-write-audience-appropriate-documentation": [
        "I'd suggest migration for bigger stuff, you can look at the changelog we have in winit now. For such a small change, I don't think you need it?\r\n\r\nhttps://github.com/rust-windowing/winit/blob/master/src/changelog/unreleased.md\r\n\r\n",
        ">I mean that's exactly the issue right there. A ton of changes but no clear indication which changes require my attention.\r\n\r\nIt has a big chunk of code and explanation for what you need. So it's just 2 changes where you need attention.\r\n\r\nWe can do a separate section with breaking changes.",
        ">It doesn't though. It states nowhere that these are the only changes that require my attention. It just tells me what I need to do for those.\r\n\r\nHm, that's fair, because it'll start to warn you, since we don't remove the API. For real breaking change I just did `bold` iirc.\r\n\r\n>Do you suggest adding an extra section with its own Fixed/Added/etc?\r\n\r\nJust `Breaking`.",
        "Maybe something like that\r\n\r\n```markdown\r\n**_I'm on Windows and applications don't work like in other terminals?_**\r\n\r\nAlacritty on Windows is relying on the system libraries by default, which are\r\nquite out of date most of the time, for example your mouse might not work in\r\nvim on Windows 10. Other applications, like Windows Terminal, ship updated\r\nversions of the required libraries which are known to work better, however\r\nthey are not officially provided by the microsoft [yet].\r\n\r\nAlacritty doesn't ship those libraries, but it could still load them if they are\r\npresent in your `PATH`. Some other terminal emulators for windows do build their\r\nown copies of such libraries, so you could take those libs from them.\r\n\r\n\r\n[yet]: https://github.com/microsoft/terminal/issues/15065\r\n\r\n```"
      ],
      "alacritty-centralize-workspace-dependencies": [
        "It seems like our deps are using dirs 2, and given that dirs are unmaintained, we should either use dirs 2, or use `dirs-next`, since you're adding one more dependency anyway.\r\n\r\nIn general, we can go for dirs 2 just to not add a dependency for now.",
        "Seems fine to me."
      ],
      "alacritty-avoid-unwrap-on-nullables": [
        "Yeah, maybe checking for empty should be better on callee side.",
        "I still can't remove the len check due to compose input, since it could be an actual string in the end of the day."
      ],
      "alacritty-keep-documentation-together": [
        "But the whole point was to make it like that so you can copy sections into the config and probably assemble the config out of man page pretty reliably?  I don't see any issue with sections like that."
      ],
      "alacritty-optimize-algorithmic-efficiency": [
        "What the reason to check `is_some`, I think it's already covered? ",
        "Could you simply take the code which was there before and do 2 loops? The amount of bindings is very low so it's better than `vec` approach.\r\n\r\nJust do normal loop without `mouse_mode` part and set some flag to `true` when you actually triggered some binding.\r\n\r\nAnd then, `if mouse_mode && !sent_binding` do the ho the same loop, but with `SHIFT` applied? First loop won't need to `clone` even, I think.",
        "The `find_map` finds next unique hyperlink, the `while let` loop finds the end of that particular hyperlink, the point here is to compute meaningful bounds.\r\n\r\nHowever what we can do if we don't care about bounds at all is to always use `find_map` and report ranges for hint as 1 cell. That should probably make some sense, I guess?",
        "We can, but of form `Option<Vec<char>>; 2`, since we should occupy the entire it? I can adjust that, I think."
      ],
      "alacritty-platform-specific-api-documentation": [
        "Maybe something like that?\r\n\r\n```suggestion\r\n    Use EGL as display API if the current platform allows it. Note that\r\n    transparency may not work with EGL on Linux/BSD.\r\n```",
        "```suggestion\r\n\tOn Wayland the general class sets the _app\\_id_, while the instance class is ignored.\r\n```"
      ],
      "alacritty-follow-metadata-specifications": [
        "I guess one that could work here is MIT, since it's all permissive and compatible with APACHE, given that we don't use copyleft, requiring permission of the author is not needed, thus even cc may work.",
        "The current text was mostly written by @chrisduerr according to git blame, with the exception on how markup is placed. I'm not sure there's a need to contact original author, if most of the lines are changed and only _standard defined markup_ was left in place, which is there just because of standard and you can not really change it. One can update `keywords`.\r\n\r\nThe copyright at the top also a bit doesn't really matter, since there's no entity like alacritty project contributors, since there's no CONTRIBUTORS.md or other entity to contact/etc. Though, on github one can say that it's contributors that popup from the contributors menu, but it was a topic of the past as well iirc.\r\n\r\nAlso, is this field mandatory? ",
        "we're not dual licensed, that's the thing if we were, that was not a problem.",
        "they just lay there, but the application of them is explicitly specified. this particular file is apache licensed anyway.",
        "The issue is that you still need to ask actually, even if they are compatible.",
        "alacritty is apache-2.0 only software, and always been. MIT is used by 2 small crates and it lays in project root only because it's simple to link to it.",
        "The person who originally submitted it, which is no longer on github and has no email, that one should be asked, as it was outlined above. we can not contant them, so nothing could be done unless the file is rewritten."
      ],
      "alacritty-use-descriptive-contextual-names": [
        "This should be called just `level`, since it's in `window` section already.",
        "I could name it `winit_event`.",
        "Could you name the variables based on the location in the shape, like `top-left`, `top-right`, `mid-left`, etc?"
      ],
      "alacritty-configuration-validation-feedback": [
        "Don't set a variable like that, it's user controllable. So completely remove those lines.",
        "I actually had a setting for a year which was unused, because I've added it during `dev` and then we changed it at the last second and I forgot about it.\r\n\r\nProbably should add that we warn on unused config keys?",
        "In the `CHANGELOG`."
      ],
      "alacritty-use-constraining-types": [
        "Could you use the actual type of this value? `LPDWORD` or however windows-sys calls it.",
        "then it's fine."
      ],
      "alacritty-assess-security-trade-offs": [
        "Actually, could you roll back this part of the update? We'll drop yml anyway, and I don't want to deal with this mess. It's a libyaml transpiled into rust, so basically a 'nice way to add a security nightmare' sort of things.",
        "I mean, they are both not maintained. It's just one is a massive transpilation of a C library, and the rust crate is a small safe rust parser. They're not maintained from the same year."
      ],
      "alacritty-avoid-unnecessary-operations": [
        "why this got changed?",
        "Hm, the reason this works is because the logic is bit inconsistent. We basically set the cursor from the renderer and from this function, so that's why the code here works even though it doesn't check the regular region anyway.\r\n\r\nIn the current state of things you can remove all the logic around `hyperlink` and `::Pointer` and it'll still work. \r\n\r\nI don't really like that it's like that though since it's a bit error prone?",
        "Pretty much yes. I'm not sure what to do about this function though, because clearly the check could be removed, though, maybe there's an edge case..."
      ],
      "alacritty-consistent-naming-conventions": [
        "it's sort of weird that we have places with `none` starting with capital later and some without. Probably should be `None` for consistency."
      ],
      "alacritty-document-configuration-specifics": [
        "```suggestion\r\n## 0.25.0-dev\r\n\r\n### Changed\r\n\r\n- Replaced `Options::hold` with `Options::drain_on_exit` that drains, but doesn't hold, since holding could be easily done by user.\r\n```"
      ],
      "alacritty-choose-familiar-intuitive-names": [
        "I think it should be called `CreateNewWindow`, I do understand why spawn was used, but I think it's more natural to create windows and not spawn them.",
        "Could add a default binding to open a new window for each platform, like command + n for macOS. I think there's something common for such things on Windows as well. For Linux could add `ctrl + shift +n`.",
        "I disagree. I'd say that we should use `CreateWindow` for  `command + n` on macOS, since we're using `SpawnNewInstance` right now, which is not desired action for the standard hotkey on macOS.",
        "My point is that it's expected behavior on macOS, since the current one fills up the dock. But if you don't want to change the behavior we might just merge it as it is right now, since it's been working quite stable for me(I'm not sure that I've ever seen a crash on this PR).",
        "I just copied the exact same line from the different option, so I'd change both of them."
      ],
      "alacritty-configuration-documentation-accuracy": [
        "```suggestion\r\n\t\tIf the dim foreground color is not set, it is automatically\r\n```\r\n\r\nI think it's `dim`?",
        "Also, the default in the config is `None`, when you look into color.rs, so it could be changed either here or in the configuration to specify the value explicitly. It's also a bit weird that we have `Dim` colors defined in the config, but not use `None`. \r\n\r\nit's also weird that we have `Dim` colors defined in the `color.rs`, but they actually never used, but still being computed on demand. Should likely remove definition from `color.rs` for them while at it.",
        "```suggestion\r\n*position* \"None\" | { x = <integer>, y = <integer> }\r\n\r\n\tWindow startup position\r\n\r\n\tSpecified in number of pixels.\r\n\r\n\tIf the position is _\"None\"_, the window manager will handle placement.\r\n\t\r\n\tDefault: _\"None\"_\r\n```",
        "```suggestion\r\n\t\t\tUse _-1_ to apply this change to all windows.\r\n```"
      ],
      "alacritty-unsafe-code-practices": [
        "You'd be surprised, but we already do that. It's unsafe because it touches env variables, as in mutates the global state. You can't do that transparently, because it's again, global variables and it's all racy.\r\n\r\nFor example if you try to create a window and spawn the process at the same time, winit could implicitly remove the variables, and your process won't activate, so that's why it's all explicit.",
        "I mean, the function simply calls `std::env::remove_var` that's all it does and that's the reason it's unsafe, because it removes the `env` which you could use for launching, so you should take care. Nothing will really break or you can't lead to undefined state, it's just global state mutation basically, which is fine in application code, but I think is unsafe in library code, because it could remove a state set by application.",
        "mutates a global state. The variables may change over time. I don't understand what's wrong with adding `unsafe` on a function you where you should at least read what it does before using. ",
        "The issue is that it's not like the mutation itself is unsafe, it's just that if you spawn a process using those variables from thread `A` and then try to remove them on thread `B`, you could simply remove them for `A` and it won't start up notify anything, because the tokens were simply removed.\r\n\r\nSo the extra care should be taken and set and extra synchronization on top of that should be used.",
        "Like it won't break anything or result in UB, it just you could have sometimes things launched with the wrong env, if you don't add extra `Mutex` to sync threads here, thus it was decided to have it unsafe in winit.",
        "```\r\nThe unsafe keyword has two uses:\r\n\r\n    to declare the existence of contracts the compiler can’t check (unsafe fn and unsafe trait),\r\n    and to declare that a programmer has checked that these contracts have been upheld (unsafe {} and unsafe impl, but also unsafe fn – see below).\r\n```\r\n\r\nSo it clearly a contract which compiler can't check."
      ],
      "alacritty-consistent-error-handling": [
        "No need to do any sort of panics like that it's just error prone, simply handle good value and use prefer otherwise use standard behavior.\r\n\r\nSo it'll be \r\n```rust\r\nOk(\"1\") => DisplayApiPreference::EglThenGlx(Box::new(x11::register_xlib_error_hook)), // Or wgl\r\n_ => // default\r\n```",
        "Do we really want to do `process::exit(1)` instead of doing the flow we have with any other command with returning an error? ",
        "I mean, it's a bit weird that we have normal error handling in other sub command and have lazy `eprintln` + `exit` here.",
        "That's true, but the same you could say for `msg`, yet `msg` propagates error, they are being called from the exact same `match` handler in the end.",
        "I'd still suggest to be consistent with other subcommands.",
        "there's only one place and it's an event loop builder, but I think message box won't show if you don't build event loop on windows. Not sure about that though.\r\n\r\nIt's also tricky that you need to call it before `FreeConsole`, otherwise you won't log into pty, so I don't see other place you'd through it on windows.\r\n\r\nThe good thing about side stepping panic like that, that it's visible for the users so they can report something at least. ",
        "The fatal error could be due to crash of the event loop, so it's effectively not possible to do so.\r\n\r\nOnly windows always has windows around, but not X11/Wayland, you need to somehow draw that sort of thing. In general, you should send information to crash report service, which is system log, basically, so you'll see crashes inside the `journalctl`, etc, since rust's panic doesn't do that sort of thing.\r\n\r\n",
        "You should run `FreeConsole` from inside the `fn main`, from the `match` there you should get a `let result` and then do the same panic logic there.",
        "Also, do we really need a `panic`? Can't we call some `Win32` API function with panic message and do a normal return?"
      ],
      "alacritty-prefer-early-returns": [
        "Last time the style for thing like that was the one I picked.\r\n\r\nI can do early return, but I don't think in this particular case it's really needed, because indention is low."
      ],
      "alacritty-explain-code-intent": [
        "Document why it's done like that. Like it's not really clear why you need `0`, `0` at the end."
      ]
    },
    "profile": {
      "location": "Tokyo, Japan",
      "blog": "",
      "site_admin": false,
      "followers": 149,
      "following": 4
    }
  },
  "louis-menlo": {
    "repos": [
      "menloresearch/jan"
    ],
    "entries": [
      {
        "slug": "jan-ai-model-configuration-completeness",
        "title": "AI model configuration completeness"
      },
      {
        "slug": "jan-api-interface-consistency",
        "title": "API interface consistency"
      },
      {
        "slug": "jan-avoid-external-urls",
        "title": "Avoid external URLs"
      },
      {
        "slug": "jan-consistent-localhost-addressing",
        "title": "consistent localhost addressing"
      },
      {
        "slug": "jan-consolidate-build-scripts",
        "title": "consolidate build scripts"
      },
      {
        "slug": "jan-cross-platform-configuration-simplification",
        "title": "Cross-platform configuration simplification"
      },
      {
        "slug": "jan-eliminate-code-redundancy",
        "title": "eliminate code redundancy"
      },
      {
        "slug": "jan-ensure-proper-event-sequencing",
        "title": "Ensure proper event sequencing"
      },
      {
        "slug": "jan-externalize-configuration-values",
        "title": "externalize configuration values"
      },
      {
        "slug": "jan-externalize-hardcoded-configurations",
        "title": "Externalize hardcoded configurations"
      },
      {
        "slug": "jan-implement-graceful-error-fallbacks",
        "title": "Implement graceful error fallbacks"
      },
      {
        "slug": "jan-leverage-existing-solutions",
        "title": "leverage existing solutions"
      },
      {
        "slug": "jan-model-description-accuracy",
        "title": "model description accuracy"
      },
      {
        "slug": "jan-simplify-error-handling",
        "title": "simplify error handling"
      },
      {
        "slug": "jan-use-centralized-logging-framework",
        "title": "Use centralized logging framework"
      },
      {
        "slug": "jan-use-centralized-model-state",
        "title": "Use centralized model state"
      },
      {
        "slug": "jan-use-configuration-constants",
        "title": "Use configuration constants"
      },
      {
        "slug": "jan-use-descriptive-names",
        "title": "Use descriptive names"
      }
    ],
    "comments": {
      "jan-avoid-external-urls": [
        "Thank you for your contribution. We could not process this change as it directs visitors to an external URL, which could result in URL hijacking.\r\n```suggestion\r\n  - <a href=\"https://github.com/janhq/jan/blob/dev/README_CN.md\">简体中文</a>\r\n```"
      ],
      "jan-cross-platform-configuration-simplification": [
        "It would be better to enhance with run-script-os for multiple platform support instead of adding JS code for run scripts.",
        "```suggestion\r\n    \"version-restore:darwin:linux\": \"jq --arg ver $(cat .version.bak) '.version = $ver' package.json > package.tmp && mv package.tmp package.json && rm .version.bak\",\r\n```",
        "Hi @bxdoan, please combine these two scripts into a single one. Then it's good to go.",
        "Ah yes, forgot this."
      ],
      "jan-externalize-hardcoded-configurations": [
        "Inject from app, `Core` should not define these.",
        "move to .env",
        "move to .env"
      ],
      "jan-use-centralized-logging-framework": [
        "I was really impressed the first time I saw the name of this function. However, it turned out to be quite messy, and debugging with a logger in place would be simpler. It's cumbersome to add it to every function call like this here and there:\r\n\r\nprocess.env.DEBUG ? debugInspector(setBinPath) : setBinPath",
        "Yeah, I can get the idea of avoiding uninteded side-effects, but just keep it clean. The log function in @janhq/core aims to route to a log file. It would be great to add a log-level option there, rather than introducing so many logging systems.",
        "Nice, thanks!",
        "@vuonghoainam, this task should only be handled by the Jan Log Manager (to be implemented later). Please refer to the 'jan-001-log-framework' JIP for more details. For now we disabled log output to file from Nitro, why try to enable from app?"
      ],
      "jan-consolidate-build-scripts": [
        "Same, we would not build deps anymore, combine build steps should work.\r\n```json\r\n {\r\n    \"build\": \"tsc -b . && webpack --config webpack.config.js\",\r\n    \"build:publish\": \"npm run build && rimraf *.tgz --glob && npm pack && cpx *.tgz ../../electron/core/pre-install\"\r\n}\r\n```\r\ncc @hiento09 ",
        "Since we are not building deps anymore, combining build steps should work. This is also to shorten root build command. \r\n```json\r\n {\r\n    \"build\": \"tsc -b . && webpack --config webpack.config.js\",\r\n    \"build:publish\": \"npm run build && rimraf *.tgz --glob && npm pack && cpx *.tgz ../../electron/core/pre-install\"\r\n}\r\n```",
        "cc @hiento09 "
      ],
      "jan-use-descriptive-names": [
        "We should use camel case for all properties to avoid inserting the product name everywhere. This is not a good approach for an open source project, as users can fork our repository and give it a new product name. This makes `find and replace` difficult. Simply using `appVersion` would be better"
      ],
      "jan-leverage-existing-solutions": [
        "Can import classnames, no need to redefine",
        "This is general ConfirmationModal, @urmauur @namchuai. Don't have to create every modal for each action (delete conversation / model), let just make use of this one. Please help remove other modals @urmauur "
      ],
      "jan-api-interface-consistency": [
        "The function does not return Promise",
        "This would look very clean at the interface design level, but quite complicated at implementation level, the app/extension would not be able to access JSONConversationalExtension to invoke its functions.",
        "i.e. pull extension from map using its type(). What's the type of this extension now.\r\nFurthermore, an extension should only extend one parent extension and can implement many features",
        "@0xSage that's why I mentioned an extension can implement many features such as `thread, message & inference` but not many parent extensions. In OOP, an object would just be able to inherit 1 parent only. A Dog can inherit Animal but not Plant, same as extension. I'm seeing Thread & Message as features not extensions for now.\r\n\r\nThread & Message should not extends BaseExtension as well, but Conversational, so: \r\n```ts\r\nclass ConversationExtension extends BaseExtension implements Thread, Message {\r\n}\r\n```\r\nBut I have no idea how to name the Thread and Message here without extension suffix.",
        "For the 1, its about runtime type check as I mentioned before, can't check ConversationExtension inherited ThreadExtension or not 🫨",
        "1. Yeah, that's correct, but app can use default implemented extensions directly instead of just EventHandler. e.g.\r\n```ts\r\n// in useCreateAssistanth() \r\nconst threads = await extensionManager.get<ConversationalExtension>().getThreads() // here is runtime type check needed\r\n```\r\n2. Precompile extensions in app should work, we don't need to manage anonymous extensions, but quite anti-pattern (Registry). \r\nIt works with singleton extensions only\r\n```ts\r\nconst convExt = new ConversationalExtension() // singleton - global access\r\n\r\nconst threads = await convExt.getThreads()\r\n```\r\n3. Since it `implements` features, I have no idea for now on how to deal with that. `Implements` do not mean much in TS after compiled.\r\n\r\n4. Since `core` module is extensively used from the app client to the Node server and every single extension, any update to the core could potentially affect other components. For example, if we rename the API routes, all extensions dependent on these routes would require updates, otherwise, they may break. I believe developers might struggle to make it work without proper alignment and granted permissions",
        "2. I mean to access precompile extensions without checking their type, we have to force to constraint the logic with type casting `(use singleton + cast type -> anti pattern)` and not to put them in extension registry, otherwise we could not know how to pull the correct extension out of the registry to use (could not inject)\r\n\r\n3. I mean `extension` implements `thread / message / assistant`\r\n4. Yes, totally correct."
      ],
      "jan-ensure-proper-event-sequencing": [
        "This function is defined for a single-response purpose and returns the result in place (the same as stream = false), which means it should not work with event pub-sub. The function would not be used in the chat box, so broadcasting a message event will result in an incorrect message response. E.g. it's used for thread summarization. Currently, the function is deprecated and not being used anywhere. Please revert the irrelevant change.",
        "nits: should we do it after first message response? (Move to EventHandler > OnMessageResponse) This would lead to a race condition here"
      ],
      "jan-ai-model-configuration-completeness": [
        "We need to pass CPU/GPU settings from the app (previously read from settings.json). It will also need to load setting arguments, such as ctx_len and ngl.",
        "```suggestion\r\n        const llama_model_path = path.join(app.getPath(\"userData\"), fileName);\r\n        const config = {\r\n          llama_model_path,\r\n          ctx_len: 2048,\r\n          ngl: 100,\r\n          embedding: true // Always enable embedding mode on\r\n```",
        "CRITICAL: App spawn Nitro before finished unloading model / killing nitro process",
        "Separated function so next PR will allow app to load model when failed to generate response (nitro process is killed)"
      ],
      "jan-eliminate-code-redundancy": [
        "Thanks for the comment. I noticed that eos_id appears to be legacy code, which we should consider cleaning up. Removed.",
        "I think we can avoid code duplication. Let's say we can remove these, also, instead of onFirstPrompt, add a message type into MessageRequestBody, and the app's EventHandler can switch logics there. The PR would be very short and clean then.",
        "this will be used in system wide, should be an util?"
      ],
      "jan-consistent-localhost-addressing": [
        "There are many more providers, not just these ports.",
        "Yes for /models and /chat/completions.",
        "```suggestion\r\n    await fetch(`http://127.0.0.1:${cortexJsPort}/v1/system`, {\r\n```"
      ],
      "jan-implement-graceful-error-fallbacks": [
        "Since this function returns a promise, there is no need to try catch in place\n```suggestion\n    return listSupportedBackends().then(this.determineBestBackend)\n```",
        "This would lead to an issue where app subprocesses, such as Nitro, loaded models, API server, are not being killed properly.",
        "```suggestion\r\n    return fetch(NITRO_HTTP_KILL_URL, {\r\n      method: \"DELETE\",\r\n    }).catch((err) => {\r\n      console.error(err);\r\n      subprocess.kill();\r\n      subprocess = null;\r\n      return kill(PORT, \"tcp\").then(console.log).catch(console.log);\r\n});\r\n```",
        "TODO: Display a toast notification to inform the user of these types of errors. cc @urmauur "
      ],
      "jan-externalize-configuration-values": [
        "this should be set from ENV file",
        "cc @hiento09 we will need to update GitHub secret for this"
      ],
      "jan-use-centralized-model-state": [
        "model can be retrieved using i-m states, e.g. `currentActiveModel`"
      ],
      "jan-simplify-error-handling": [
        "@cmppoon Can remove try catch block here",
        "```suggestion\r\n        toaster({\r\n            title: 'Failed to get Hugging Face models',\r\n            description: err instanceof Error ? err.message : 'Unexpected Error',\r\n            type: 'error',\r\n        })\r\n```"
      ],
      "jan-use-configuration-constants": [
        "status === EngineStatus.ready",
        "Please rebase + rebuild core to use this enum",
        "```suggestion\r\n   setMenus([\r\n     'My Models', \r\n     'My Settings',\r\n     'Advanced Settings',\r\n     ...(window.electronAPI ? ['Extensions'] : [])\r\n    ])\r\n```",
        "@markmehere, that's a really great PR. I'd like to apply this change to fix the compilation error, and then we'll be good to go.\r\n```suggestion\r\n      localStorage.getItem(HTTPS_PROXY_FEATURE) ?? \"\"\r\n```"
      ],
      "jan-model-description-accuracy": [
        "```suggestion\r\n    \"description\": \"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized - like Synthetic Text Generation, Code Generation, RAG, or Agents.\",\r\n```",
        "```suggestion\r\n    \"description\": \"Mistral Small is the ideal choice for simpe tasks that one can do in builk - like Classification, Customer Support, or Text Generation. It offers excellent performance at an affordable price point.\",\r\n```"
      ]
    },
    "profile": {
      "location": "HCMC",
      "blog": "",
      "twitter_username": "louis_le_gg",
      "site_admin": false,
      "followers": 102,
      "following": 2
    }
  },
  "Narsil": {
    "repos": [
      "huggingface/tokenizers"
    ],
    "entries": [
      {
        "slug": "tokenizers-avoid-unsafe-code",
        "title": "Avoid unsafe code"
      },
      {
        "slug": "tokenizers-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "tokenizers-choose-semantically-clear-identifiers",
        "title": "Choose semantically clear identifiers"
      },
      {
        "slug": "tokenizers-consistent-api-design",
        "title": "Consistent API design"
      },
      {
        "slug": "tokenizers-document-for-comprehension",
        "title": "Document for comprehension"
      },
      {
        "slug": "tokenizers-flexible-tokenizer-implementation",
        "title": "Flexible tokenizer implementation"
      },
      {
        "slug": "tokenizers-handle-nullable-types-idiomatically",
        "title": "Handle nullable types idiomatically"
      },
      {
        "slug": "tokenizers-manage-version-constraints",
        "title": "Manage version constraints"
      },
      {
        "slug": "tokenizers-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "tokenizers-modular-model-components",
        "title": "Modular model components"
      },
      {
        "slug": "tokenizers-optimize-workflow-triggers",
        "title": "Optimize workflow triggers"
      },
      {
        "slug": "tokenizers-prefer-explicit-api-design",
        "title": "Prefer explicit API design"
      },
      {
        "slug": "tokenizers-prioritize-tokenizer-simplicity",
        "title": "Prioritize tokenizer simplicity"
      },
      {
        "slug": "tokenizers-purpose-indicating-descriptive-names",
        "title": "Purpose-indicating descriptive names"
      },
      {
        "slug": "tokenizers-pythonic-api-design",
        "title": "Pythonic API design"
      },
      {
        "slug": "tokenizers-return-results-not-panics",
        "title": "Return results not panics"
      },
      {
        "slug": "tokenizers-robust-workflow-configurations",
        "title": "Robust workflow configurations"
      },
      {
        "slug": "tokenizers-simplify-for-readability",
        "title": "Simplify for readability"
      },
      {
        "slug": "tokenizers-test-algorithmic-behavior",
        "title": "Test algorithmic behavior"
      },
      {
        "slug": "tokenizers-thread-safe-resource-sharing",
        "title": "Thread-safe resource sharing"
      },
      {
        "slug": "tokenizers-use-explicit-assertions",
        "title": "Use explicit assertions"
      }
    ],
    "comments": {
      "tokenizers-simplify-for-readability": [
        "```suggestion\r\n    pub fn from_string(content: &str) -> Result<Self> {\r\n        serde_json::from_str(content)\r\n    }\r\n```\r\n\r\nSeems simpler:\r\n\r\nNo need to pass owned data, no `?`+   `Ok`.",
        "Other option which feel slightly cleaner IMO:\r\n\r\n```rust\r\n        if let Some(&id) = self.vocab.get(token) {\r\n            Ok(vec![Token {\r\n                id,\r\n                value: token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else if let Some(&unk_id) = self.vocab.get(&self.unk_token) {\r\n            Ok(vec![Token {\r\n                id: unk_id,\r\n                value: self.unk_token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else {\r\n            Err(Box::new(Error::MissingUnkToken))\r\n        }\r\n```\r\nIt's personal taste, probably gets compiled the exact same way.",
        "Perfect, I'll go ahead an merge.",
        "```suggestion\r\n            let direction = match direction.as_str(){\r\n                \"left\"  => Truncate::Left,\r\n                \"right\" => Truncate::Right,\r\n                other => panic!(\"Invalid truncation direction value : {}\", other);\r\n            };\r\n```\r\nSeems slightly more rusty (and it should compile)"
      ],
      "tokenizers-choose-semantically-clear-identifiers": [
        "The argument name is odd to me.\r\n\r\ntruncate_left([0, 1, 2], 2) -> [1, 2]\r\ntruncate_right([0, 1, 2], 2) -> [0, 1]\r\n\r\nIMO. I think from the tests that you're doing the opposite.\r\n\r\nrenaming `left` to `right` is enough as a first approximation.\r\n\r\nIn `transformers` there's actually a `direction` name which would be an enum might clarify things a bit.",
        "Please factor this code out ! :D. You shouldn't need to have this twice, Make it a simple function.\r\n\r\nAlso why change `\"First\"`  ? `\"first\"` is much more pythonic imo.",
        "```suggestion\r\nfn from_string(string: String) -> Result<PrependScheme, PyErr> {\r\n```\r\n\r\nFunctions are private by default. `pub` means public."
      ],
      "tokenizers-manage-version-constraints": [
        "No, we need to allow for breaking changes in huggingface_hub meaning we can do `>=0.17,<0.18` instead.",
        "I would very much rather we update dependencies in separate PRs. \r\n\r\nWhen updating dependencies, I need to make sure nothing breaks upstream either.\r\nThis PR will update `pyo3` but if we could leave out the other dependencies not required for other PRs it would be easier to check. (One single PR for all other dependencies is fine)."
      ],
      "tokenizers-use-explicit-assertions": [
        "Thats a great test ! Good idea on the UTF-8 intense examples ! \r\n\r\nCould you add the explicit vocab as a result ? It makes the test more readable and more robust.\r\nLet's keep the functional part too, but having the explicit values prevents \"under the radar\" bugs, where behavior is modified unintentionally.|\r\n\r\n\r\nYou could reduce the size of those strings and `max_token_length` in order to keep things readable maybe.",
        "No 1 test, but explicit assert.\r\n\r\n`assert_eq!(tokenizer.get_vocab(), HashMapFrom([....]))`\r\n\r\nIt's really much better imo. Tests don't change that often, and I have seen many bugs be silent for too long for this lack of explicit value testing.",
        "Going along with the `#ignore` if we keep this `print` we 're not checking anything actually during tests. If possible/compatible with a fast testing iteration, we probably should actually change those into real asserts.\r\n\r\nIt hinders readability only by a slight margin I feel, but it impacts forward compat by quite a bit."
      ],
      "tokenizers-purpose-indicating-descriptive-names": [
        "I'd like to keep the dictionary format, at least for uniformity within the tests fixtures that all use dictionaries.\r\n\r\nIn terms of naming, because we might add follow ups saved tokenizers, what about `serialized_files` it makes `serialized_files[\"albert_base\"]` more understandable. I'd rather add another tokenizer in the tests than naming specifically for albert IMO. (We probably should have a handful of serialized tokenizers in those tests to make sure will load the old ones, with BertNormalizer, BPE, and so on). What do you think ?",
        "Good argument for downloading separately, and that changes the names, which makes your solution the best in the end.\r\n"
      ],
      "tokenizers-choose-optimal-data-structures": [
        "Is the hashSet `inserted` really necessary ? `required_chars` is already a HashMap, so we shouldn't get duplicates anyway, no ?",
        "My bad, did not see that change to `Vec`",
        "Yes. It complains that `needless_collect`. Basically that we collect something that we iterate over afterwards.\r\n\r\nWe could definitely use `[allow(needless_collect)]` But I think we should try to fix them anyway. We probably also could have only one forward pass for this one.\r\n\r\nBut how does `iter().rev()` handle cache locality though ? ",
        "I fixed it to only forward pass anyway, it's just better."
      ],
      "tokenizers-test-algorithmic-behavior": [
        "Could you assert the full list of `ids` and `tokens` (like the other test ?)",
        "So if the tests are not deterministic definitely let's not add them.\r\n\r\nHowever it's suprising that it's not determinstic though..... It shouldn't be ....\r\n\r\n▁ != _  \r\nfirst is a special unicode character (rarely used that's why google chose it)\r\nSecond is regular underscore.",
        "<unk> instead of 🤗 is indeed expected",
        "That's better, and OK I understand better why IDs are not deterministic, they essentially have the same score, so no particular reason that ids should be in a particular order."
      ],
      "tokenizers-flexible-tokenizer-implementation": [
        "```suggestion\r\n            tokenizer = Tokenizer(BPE(unk_token=str(unk_token), dropout=dropout, end_of_word_suffix=suffx))\r\n```\r\n\r\nno ?"
      ],
      "tokenizers-optimize-workflow-triggers": [
        "It's in the default template for `wasm` app. I don't really see a lot of downsides of keeping them."
      ],
      "tokenizers-handle-nullable-types-idiomatically": [
        "Why `Option<bool>` ? It should be `bool` no ?\r\n\r\nThere are no optional arguments in Rust (and it's good)\r\n\r\n`unk_id` is really an Option, it's not at all forced (but it will cause errors if you haven't one and are triggering an unk).",
        "```suggestion\r\n           if let Some(max_token_length) = max_token_length{\r\n               if new_token.chars().count() > max_token_length{\r\n                   continue;\r\n               }\r\n           }\r\n```\r\n\r\nThis is more idiomatic imo.\r\n0 is NOT a special value. `None`  means ignore, `zero` does mean zero. If things starts to do weird things it's not the problem of this function, it's respecting the value which is more important.\r\n\r\nTry to switch to `usize` it makes everything easier to read, and the actual \"size\" isn't important optimization wise."
      ],
      "tokenizers-document-for-comprehension": [
        "Yes, totally fine. ",
        "We probably should add a little comment here, jieba's behavior is not super straightforward here.\r\n\r\nAlso we should probably explain what this code does.\r\n\r\n```python\r\nclass JiebaPreTokenizer:\r\n    def jieba_split(self, pretoken_index, pretoken):\r\n        new_tokens = []\r\n        # Why do we need `str(normalized)?`\r\n        pretoken_string = str(pretoken)\r\n        for token, start, stop in jieba.tokenize(pretoken_string):\r\n            new_tokens.append(normalized[start:stop])\r\n        return new_tokens\r\n\r\n    def pre_tokenize(self, pretok):\r\n        # Let's call split on the PreTokenizedString to split using `self.split`\r\n        # pretok.split takes in a function, that receives `i` that is the current token index\r\n        # and `pretoken` that is a substring of th original string, that might have been normalized,\r\n        # and already cut up by previous pretokenizer. It should return a list of subtokens.\r\n        # `pretok.split` changes it inplace.\r\n\r\n        # Checkout X to see all available primitives to do a custom pretokenization\r\n        pretok.split(self.jieba_split)\r\n```"
      ],
      "tokenizers-return-results-not-panics": [
        "Panicking is NOT okay in transformers. We should NEVER panic since we're a library.\r\n\r\nFor that there is `TryFrom` which returns `Result<T>`. However I don't think this should be done in the Rust layer but in the Python layer instead.",
        "Remove every `unwrap` and every `vec`.\r\n\r\nThere is 1 `collect` tolerated ( I think it's done that way in BPE) and it's *only* to check that ALL bytes have a token id (you're allowed to use a single `vec` or `collect` in that branch, not in the others.\r\n\r\n:)",
        "Well if the regexp doesn't match, it's strictly equivalent to matching nothing, right ?\r\n\r\nI do that to avoid adding and `unwrap` which could potential `panic!` which makes users pretty unhappy.\r\n\r\nI don't think the pathway should be taken, but if it ever is, then I think `0` is a valid default which prevents panicking and is still, correct. Wdyt ?",
        "> I understand adding Results all the way up to the public API is cumbersome but to me is the cleaner approach : notify the user there was an error / undefined / unexpected behavior and let him handle it.\r\n\r\nIn that case it's not true. Finding 0 match is different than an error.\r\nI could very well change the regex to be `\\s+` and then the `else` branch would be expected and working as intended.\r\n\r\nIs adding a `warn!(\"AddedToken with `single_word` seems to have an issue, Please report this\")` in that code path OK ?\r\nIt it warning the user, but still prevent catastrophing failure ?\r\n\r\nWe don't expect the code path to be taken, but it's not preventing the algorithm from working.",
        "No, the else clause will trigger, when `REGEXP.find()` returns `None`.\r\n\r\nThis is normal when the REGEX fails to find any match in the submitted string.\r\nThis particular brand of REGEX should match all the time (since it should match the empty string being `^\\s*` and `\\s*$`).\r\nSo I don't expect it to not match. But if it doesn't match, it's roughly the same as saying no space where found.",
        "Oh no no necessarily.\r\n\r\n`AddedTokens(\"<mask>\", lstrip=True)` means you want it to match `\"Something <mask>  else\"` -> `['Something\", \" <mask>\", \" else\"]`. Meaning you are actually capturing the left spaces in addition to your token (effectively tripping it from being seen for your model).\r\n\r\nBut `\"Something<mask>else\"` will also capture `[\"Something\", \"<mask>\", \"else\"]`. no space have been deleted.\r\nIf you want this to NOT capture you need to activate `single_word=True` too.\r\n\r\nBut both options are orthogonal and don´t  necessarily need one another (although I think in practice they are probably reasonned about in conjunction)\r\n\r\n\r\nEdit: `<mask>` is a comment in GH markdown easy to miss :D\r\n",
        "@McPatate I merged since I am starting the necessary work to release this before transformers's own release on thursday, but we can continue the discussion to make this clear.\r\n",
        "`.unwrap()` is something I tend to avoid within logic code at all if possible, since having the program panic, is never a great user experience.\r\n\r\nMy answer : https://github.com/huggingface/tokenizers/pull/919#discussion_r813862919 applies too. I think returning `Option` would be preferrable to panicking, but the result would be the same IMO, if the regex doesn't match, I just don't capture anything.",
        "I think this would go away using `aho-corasick` (it returns the ID with the match).",
        "Also there are other `unwrap` which would panic too."
      ],
      "tokenizers-pythonic-api-design": [
        "As long as we're breaking signature, I would argue we have a different signature like\r\n\r\n`train(files, options=bpe_train_options)`, or `train(files, vocab_size=X, ....)` what do you think ?\r\n\r\nI like the second version better, the only trouble is the exact description of those options if going to get fuzzy pretty fast and error handling a bit hard. But it \"feels\" more pythonic, what do you think ?\r\n\r\nEither that, or if we keep the `trainer` concept, we should stick to something closer to Rust, with `trainer.train(tokenizer, files)` I actually like that last version better at this moment, the control flow feels more natural.",
        "I'm merely exposing the rust api which requires this format.\r\nIt's also what `BPE::read_files()` returns.\r\n\r\nFollowing what you said we should mostly expose the rust api, no ?\r\nShould I change the rust API ? (I don't think we should)"
      ],
      "tokenizers-consistent-api-design": [
        "Is that really something we need to expose ?"
      ],
      "tokenizers-prefer-explicit-api-design": [
        "Just make that `PrependScheme`.\r\n\r\n`impl Into<T>` has an associated cost with it. Which is that there will be a concrete function for EVERY possible caller that might arise (here at least `PrependScheme`, `&str` and `String`. \r\n\r\nThis is nice to make an API easier to use when the concrete underlying type is relatively verbose to make, or there are many structs implementing the given trait that might be useful (something like `impl Response` in a web framework.\r\n\r\nHere everything is a simple `enum`. Importing the enum and using a variant makes the function  so that only 1 exists, prevents all kind of runtiem errors (all will become compile time errors). And since only 1 function exists, it can be inline more easily by the compiler.\r\n\r\nSimple is better here IMO",
        "TBH variant return types always give me shivers. It's usually so much better to have 2 different functions instead. Saves so much headaches for users....\r\n\r\nIf we want to save backcompat:\r\n\r\n```python\r\ndef token_to_word(token) -> int:\r\n     if self.num_sequences() > 1:\r\n         raise Exception(\"Can't use this in multiple sequences encoding, use token_to_word_seq instead\")\r\n     word, seq = token_to_word_seq(token)\r\n     return word\r\n\r\ndef token_to_word_seq(token) -> Tuple[int, int]:\r\n    ....\r\n```"
      ],
      "tokenizers-thread-safe-resource-sharing": [
        "Because it's python a python object holding a ref to a rust object.\r\n\r\nSince Python has the GIL we don't know if we're on the same thread or not, hence `Arc<RwLock<>>` To become `Send+Sync`. This is already what the `PyTrainer` owns as an object.",
        "Could you explain why all those RwLock are need ? I'm not sure why we would need them.",
        "I'm always a bit scared by adding that much `unwrap` everywhere... Do you think there's a way we could avoid them ?"
      ],
      "tokenizers-prioritize-tokenizer-simplicity": [
        "Yes I removed 1 feature:\r\n\r\nPreTokenized inputs. Which are presplitted strings (so list of strings).\r\n\r\nWhy did I remove it :\r\n\r\n- It bloats quite a lot the code (it would require rewriting the argument parsing since it doesn't work by default with the macro, or at least I wasn't able to).\r\n- It's a rarely used feature which have lots of caveats.\r\n- We can readd later, right now, running on more recent versions in reasonable time was the goal.",
        "That's not True, at least it was not in my case, all the tests were failing because the test string was not splitted through whitespace, so the added vocabulary was not handled, and those tests were failing because UNK_TOKEN was not defined.\r\n\r\nWe probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise."
      ],
      "tokenizers-minimize-memory-allocations": [
        "`iter_mut()` + `for_each` makes sure there are not reallocations.\r\n\r\n`.collect()` is like `.clone()`, avoid if possible.\r\n\r\nIt might be optimized away but I would rather not count on it.",
        "Probably, but if the reduce is not itself parallel, then it's ok to have `global` be mutable.",
        "Don't clone on behalf of users, never.\r\n\r\nEither return a reference `&PrependScheme`.\r\nOr make `PrependScheme`  ` Copy`.\r\n\r\nMaking something Copy, is something you should do only when the size makes it worthwile.\r\nA reference is a pointer of size `usize` so copying usize is usually much faster than copying an entire struct.\r\nFor an enum like PrependScheme, it' s only 3 possible values, encoded internally by rust as `u8` or `usize` (Don't remember which). In any case, it's prefereable to copy the value than to pass references around (which cost a pointer dereference)\r\n",
        "Let's remove this. Vec allocs is costly, you don't want to do this.",
        "Done."
      ],
      "tokenizers-modular-model-components": [
        "Why to we need this ? `encode` is enough, no ?"
      ],
      "tokenizers-robust-workflow-configurations": [
        "Hmm it does, just moves the location of this information. Will have to check about `click` too.",
        "Done."
      ],
      "tokenizers-avoid-unsafe-code": [
        "Ouch, not sure we need `unsafe` in this library :)",
        "I think staying away from unsafe is probably best, even if it means 1 clone. That's my opinion at least."
      ]
    },
    "profile": {
      "company": "@huggingface ",
      "blog": "",
      "site_admin": false,
      "followers": 794,
      "following": 29
    }
  },
  "jkotas": {
    "repos": [
      "dotnet/runtime"
    ],
    "entries": [
      {
        "slug": "runtime-avoid-busy-waiting",
        "title": "Avoid busy waiting"
      },
      {
        "slug": "runtime-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "runtime-centralize-platform-configurations",
        "title": "Centralize platform configurations"
      },
      {
        "slug": "runtime-choose-appropriate-error-mechanisms",
        "title": "Choose appropriate error mechanisms"
      },
      {
        "slug": "runtime-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "runtime-document-code-meaningfully",
        "title": "Document code meaningfully"
      },
      {
        "slug": "runtime-document-configuration-intent",
        "title": "Document configuration intent"
      },
      {
        "slug": "runtime-explicit-api-versioning",
        "title": "Explicit API versioning"
      },
      {
        "slug": "runtime-follow-naming-patterns",
        "title": "Follow naming patterns"
      },
      {
        "slug": "runtime-maintain-configuration-compatibility",
        "title": "Maintain configuration compatibility"
      },
      {
        "slug": "runtime-memory-barrier-pairing",
        "title": "Memory barrier pairing"
      },
      {
        "slug": "runtime-memory-ordering-matters",
        "title": "Memory ordering matters"
      },
      {
        "slug": "runtime-model-actual-hardware-costs",
        "title": "Model actual hardware costs"
      },
      {
        "slug": "runtime-names-reflect-actual-purpose",
        "title": "Names reflect actual purpose"
      },
      {
        "slug": "runtime-optimize-aligned-simd-operations",
        "title": "Optimize aligned SIMD operations"
      },
      {
        "slug": "runtime-optimize-common-paths",
        "title": "Optimize common paths"
      },
      {
        "slug": "runtime-optimize-for-readability",
        "title": "Optimize for readability"
      },
      {
        "slug": "runtime-optimize-memory-access",
        "title": "Optimize memory access"
      },
      {
        "slug": "runtime-preserve-pointer-authentication",
        "title": "Preserve pointer authentication"
      },
      {
        "slug": "runtime-simplify-code-expressions",
        "title": "Simplify code expressions"
      },
      {
        "slug": "runtime-specific-exceptions-with-context",
        "title": "Specific exceptions with context"
      },
      {
        "slug": "runtime-structure-configuration-options",
        "title": "Structure configuration options"
      }
    ],
    "comments": {
      "runtime-document-configuration-intent": [
        "Are any of these documented in official docs? If they are not, I do not think we need to keep them.",
        "`IlcInstructionSet` is explicitly documented in the repo docs as [subject to change without a breaking change notice](https://github.com/dotnet/runtime/blob/main/src/coreclr/nativeaot/docs/optimizing.md).",
        "Command line arguments for direct invocation of crossgen2/ilc are not officially documented/supported."
      ],
      "runtime-optimize-for-readability": [
        "> RuntimeMethodInfo doesn't overload those operators so it doesn't matter\r\n\r\nMethodInfo does overload those operators. MethodInfo operators are going to be selected to compare RuntimeMethodInfo [quick test](https://sharplab.io/#v2:EYLgtghglgdgNAFxFANgHwAIAYAEGCMAdAEoCmAZiqQMYJQD2MA3ALABQ7EwAzggE4RaeAEw5iAVxh0wpALKkEAC3oATAJIxy9HCBzylqjVvYBvdjgt4AzHnwA2HMHr0UOACqleACglSoM/WV1TW0wAEpzSzM2S1i8AHYcMBwAXhScGHEUFFYYywBfdnygA=)"
      ],
      "runtime-specific-exceptions-with-context": [
        "\"Dynamic entrypoint allocation is not supported in the current environment.\"?",
        "Can we throw the exception immediately here instead of propagating it manually? The manual error propagation looks like a left-over from the .NET Native MRT/app split.",
        "This should be `PlatformNotSupportedException`. From .NET Framework design guidelines:\r\n\r\nDO throw PlatformNotSupportedException to indicate the operation cannot complete in the current runtime environment but could on a different runtime or operating system."
      ],
      "runtime-names-reflect-actual-purpose": [
        "FWIW, the user facing property to enable event pipe (and related features) is called `EventSourceSupport` in NativeAOT (https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/diagnostics#observability-and-telemetry).",
        "> Yeah, I find EventSourceSupport confusing for all that Event Pipe / Diagnostic Server can do.\r\n\r\nI agree that it is not the most descriptive name. On the other hand, it gets complicated to explain what works and does not work in the matrix of fine-grained combinations. It is why we have opted to have one config switch that controls it all."
      ],
      "runtime-memory-barrier-pairing": [
        "Do we need a counter-part barrier on the writer sides (e.g. in DynamicHelperFixup)?"
      ],
      "runtime-optimize-common-paths": [
        "We build with both llvm and gcc successfully. What's the compiler used by SunOS that this needs fixing?",
        "Why does this compile fine with gcc on Linux?",
        "> it would be nice if NAOT could target this as the default\r\n\r\nI do not think we want to have the baseline supported piecemeal (it is unlikely we would be able to it correctly since it is impossible to test). If we want to raise the baseline, we should do it for the whole product. I would not be opposed to raising the product baseline to x86-64-v2.\r\n\r\n>  its an 18 year old baseline\r\n\r\nWhen did Intel/Amd stop selling the last processor without x86-64-v2? It is the more interesting date for this discussion.\r\n\r\n> Win11 prior to 24H2 is documented as requiring SSE4.1 otherwise\r\n\r\nWhere is it documented?"
      ],
      "runtime-maintain-configuration-compatibility": [
        "> The two big things are that we have some unused gaps now that could be filled in and I don't see why we need to use unique IDs across X86 and Arm64 (that is X86Base and ArmBase can't currently both be R2R ID 1).\r\n\r\nThis would make r2rdump more complicated. Right now, r2rdump just does Enum.ToString() to dump the instruction set. \r\n\r\nAlso, r2rdump is designed to be able to dump older versions of r2r images. Removing the definitions of the existing enum members breaks that. These removals should be reverted.",
        "> we should be able to do much like we do in NAOT\r\n\r\nNAOT does not have the versioning problem like R2R. There is no mixing and matching of versions, etc.",
        "> Right now we're using 60 bits for R2R, when we should only need 34 total. If we split it by platform then it's 11 for Arm64 and 23 for x64. Just given the pending work around SVE and AVX, we will likely go over 64-bits in the next release and need to take a break anyways.\r\n\r\nReadyToRunInstructionSet enum is not used to form a bit mask. Going above 64 should work just fine."
      ],
      "runtime-optimize-memory-access": [
        "Should this be done as unsigned division to match CoreCLR? \r\n\r\nSigned division is extra instructions: https://godbolt.org/z/cxxWP67n6",
        ">  Also validating that byteCount % alignment == 0 would be fairly expensive.\r\n\r\nYou do not need to use `%` once you know that the alignment is power of 2.",
        "Does the formula for the second argument need to use the alignment after it has been bumped up to `sizeof(void*)`?",
        "> basically, all MemoryMarshal.Read/Write/Cast, Unsafe.Read/ReadUnligned/Write/WriteUnligned, Unsafe.As\r\n\r\nMemoryMarshal.Cast and Unsafe.As are the only ones with the potential alignment problem, there are not that many, and we have been accepting fixes to make the core more portable (e.g. https://github.com/dotnet/runtime/pull/98812).\r\n\r\n> The only tricky cases\r\n\r\nAnd the platforms/architecture that we never heard of that Unity may run on.",
        "> it still is going to fail on a platform where such reads aren't fixed by the OS. At least on CoreCLR/NAOT\r\n\r\nIf we end up targeting a platform like that, the JIT will need to be fixed up to make `Unsafe.ReadUnaligned` work as appropriate.",
        "> I guess it's one of those cases when safe code doesn't read better than unsafe\r\n\r\nIt is because of we have not designed safe APIs that make this kind of code look good.\r\n\r\nI think that that best you can do using existing .NET APIs is approximation of \"eat the span\" pattern that's idiomatic in golang:\r\n```csharp\r\nwhile (span.Length >= 8)\r\n{\r\n   ulong v = BitConverter.ToInt64(span);\r\n\r\n   ...\r\n\r\n   span = span.Slice(8);\r\n}\r\n```\r\n\r\nNew APIs can make it look better:\r\n```\r\nwhile (BitConverter.TryRead(span, out long v))\r\n{\r\n   ...\r\n\r\n   span = span.Slice(8);\r\n}\r\n```\r\n\r\nAnother alternative is to introduce iterators that are idiomatic in Rust:\r\n```\r\n// Rust equivalent is for chunk in data.array_chunks::<8>()\r\nforeach (long v in span.IterateAsInt64())\r\n{\r\n...\r\n}\r\n```",
        "> `=> Aggregate<T, IdentityOperator<T>, Crc32Operator<T>>(x)`\r\n\r\nI get that this pattern is centralizing the unsafe code, but I do not think that it reads well."
      ],
      "runtime-cache-expensive-computations": [
        "This is an expensive way to compute a bool. Should we have a virtual property for this?",
        "Pass the result of `type.GetClassLayout()` to the worked methods to avoid recomputing it? (All except `ComputeCStructFieldLayout` needs it on some path.) It can be passed around as `in` since it is likely going grow into a large struct over time.",
        "Pre-allocate this HashSet and Dictionary to avoid re-hashing?",
        "```suggestion\r\n                symbolRemapping = new Dictionary<ISymbolNode, ISymbolNode>((int)(1.05 * (previousSymbolRemapping?.Count ?? 0)));\r\n```",
        "Otherwise, we are guaranteed to rehash the whole thing on most iterations just to add a few more elements."
      ],
      "runtime-choose-appropriate-error-mechanisms": [
        "It is unusual to return error code via out argument, and only allow that error code to be OOM.\r\n\r\nCan we return proper error code, and return the pointer via `[out]` argument instead?\r\n\r\nThe error code can be an HRESULT or an enum specific to this call. (I know HRESULTs are Windows-specific, but they are part of BCL Exception so they are not going away x-plat, so there is not much value in trying hard to avoid them.)",
        "I think it is fine add back what you need to get this to compile. "
      ],
      "runtime-document-code-meaningfully": [
        "What's the license on the file that this was copied from?",
        "We prefer to err on the side of doing more attribution.\r\n\r\nThis is more than 100 lines copied nearly verbatim, including docs. I think it is above the threshold."
      ],
      "runtime-optimize-aligned-simd-operations": [
        ">  I don't believe trying to polyfill via malloc/free is reliable as there is no guarantee that free works with arbitrary pointers:\r\n\r\nThe polyfill would have to be for both aligned alloc and free, something like this: https://github.com/dotnet/runtime/issues/33244#issuecomment-595848832"
      ],
      "runtime-avoid-busy-waiting": [
        "Yes, somebody tried really hard to make `Volatile.Read` for 64-bit values atomic on x86, and this is the price that was paid for it."
      ],
      "runtime-choose-descriptive-names": [
        "```suggestion\r\n                            trace::error(_X(\"The application '%s' is not a managed .dll.\"), app_candidate.c_str());\r\n```\r\nI do not think managed .exes are a thing in modern .NET. None of the tooling will produce them. Should we drop `or .exe` here to avoid suggesting that managed .exes are a thing?\r\n\r\nAlternatively, we can say `is not a .NET binary.\". \"binary\" looks less Windows-specific, but also less descriptive. It won't give you a hint that you need to pass in .dll."
      ],
      "runtime-memory-ordering-matters": [
        "```suggestion\r\n        void* pvHIjackAddr = (void*)pfnHijackFunction;\r\n#if defined(TARGET_ARM64)\r\n        pvHIjackAddr = PacSignPtr(pvHIjackAddr);\r\n#endif // TARGET_ARM64\r\n        *ppvRetAddrLocation = pvHIjackAddr;\r\n```\r\nWe should avoid writing the wrong value first, and then overwriting it with the correct value. It can cause interesting race conditions."
      ],
      "runtime-simplify-code-expressions": [
        "```suggestion\r\n    Volatile<bool> g_GCBridgeActive = false;\r\n```\r\nNit: It is fine to use native C/C++ bool for internal details."
      ],
      "runtime-follow-naming-patterns": [
        "This method is not returning anything after this change, so `Get...` name does not fit what it does anymore. \r\n\r\nI would rename it to `PrintHelp`, `DisplayHelp` or something similar.\r\n\r\n",
        "(I have not commented on all places with this name. All of them should be fixed.)"
      ],
      "runtime-preserve-pointer-authentication": [
        "I see that you have `Sign with SP` at the end of the list. I am not sure whether you want to wait with tackling it as the last item. I expect that you will need to revisit and retest number of changes in this PR to make signing with SP work. ",
        "Signing return addresses with `SP` is required to deliver on security promise of PAC and it changes how things like hijacking need to be handled. I expect that you will need to redo a large part of change to make it work and then retest everything again. You can certainly do that, but I do not think it is the most efficient way to deliver this feature.",
        "Would it make sense to keep the bits on `m_pvHJRetAddr` unstripped so that the original return address is protected at all times? I think stripping the bits here creates a hole in the protection. Should we strip the bits later when we read `m_pvHJRetAddr` for stackwalking, but not when we use it to store/restore the original return address so that the return address is protected for the whole time?",
        "Once the code actually returns to the restored return address, it is going to authenticate. It should not be necessary to do an extra authentication during hijacking.",
        "> the original address is preserved so that after GC we can return to it, and the updated address would take the execution flow to desired new address\r\n\r\nRight. My point is that it would be better to keep the signed address intact through the whole process:\r\n- Delete PacStripPtr in Thread::HijackThread and store the original signed address in m_pvHJRetAddr instead \r\n- Delete PacSignPtr in Thread::UnhijackThread\r\n- Add PacStripPtr as necessary to places that read m_pvHJRetAddr and do not expect the signature in the upper bits\r\n\r\nIt will make the original return address protected while it is stored in m_pvHJRetAddr. As implemented currently, the return address is not protected while it is stored in m_pvHJRetAddr."
      ],
      "runtime-centralize-platform-configurations": [
        "So this will need ifdefs controlled by configure script based on what's available on different Unix flavors. You can always do polyfill using malloc when there is no native API.",
        "Here is the existing place where all similar adjustment are concentrated today:\r\n\r\nhttps://github.com/dotnet/runtime/blob/9d771a26f058a9fa4a49850d4778bbab7aa79a22/src/libraries/Native/Unix/configure.cmake#L532-L558\r\n\r\nWe may want to move this one to this central place too."
      ],
      "runtime-explicit-api-versioning": [
        "Do we need bump `EE_INTERFACE_MAJOR_VERSION` for this this?",
        "I am worried about the existing standalone GC scenarios that the GC team cares about being broken. As implemented in the PR currently, I do not think you can use new GC with old runtime, and vice versa."
      ],
      "runtime-structure-configuration-options": [
        "```suggestion\r\n#if !defined(DACCESS_COMPILE) && !defined(FEATURE_CDAC_UNWINDER)`\r\n```\r\nWe have two builds of out-of-proc unwinders: DACCESS and CDAC. I think we want to take the offline path for both out-of-proc unwinders. `TARGET_ARM64` should not be needed in the condition once you do enable to the offline path for all out-of-proc unwinders.\r\n\r\n(All similar ifdefs in this file should be changed like this.)",
        "We want to be able to test interpreter-only mode on Windows and Linux even with support JIT compiled in. This should be based on some config switch - like `DOTNET_Interpter=*` that should make us to use the interpreter for all methods.",
        "@janvorli Do you have thoughts how we want to test interpreter-only mode?\r\n\r\nI am thinking we may want to introduce a new dedicated environment variable: `DOTNET_InterpreterMode`:\r\n\r\n`DOTNET_InterpreterMode=0`: default, do not use interpreter except explicit opt-in via `DOTNET_Interpreter`\r\n`DOTNET_InterpreterMode=1`: use interpreter for everything except (1) methods that have R2R compiled code and (2) all code in System.Private.CoreLib. All code in System.Private.CoreLib falls back to JIT if there is no R2R available for it. This can replace the testing mode introduced in https://github.com/dotnet/runtime/pull/116570/files#diff-3e5a329159ca5b2268e62be8a0d776b6092681e9b241210cb4d57e3454816abcR403 since it will cover code in non-entrypoint assemblies too and thus will be more comprehensive. This mode should have good balance between speed and coverage. We may want to use it for running libraries tests with interpreter eventually.\r\n`DOTNET_InterpreterMode=2`: use interpreter for everything except intrinsics. All intrinsics fallback to JIT. Implies `DOTNET_ReadyToRun=0`. I am not sure how much this will be useful in practice, but it sounds like interesting mode to have.\r\n`DOTNET_InterpreterMode=3`: use interpreter for everything, the full interpreter-only mode, no fallbacks to R2R or JIT whatsoever. Implies `DOTNET_ReadyToRun=0`, `DOTNET_EnableHWIntrinsic=0`, \r\n\r\nAn alternative is to piece together solutions from existing environment variables (`DOTNET_Interpreter`, `DOTNET_ReadyToRun`, `DOTNET_EnableHWIntrinsic`, ...), but it may be hard to make it do what we want exactly.\r\n\r\n> Should I introduce that flag as part of this PR?\r\n\r\nI do not have an opinion - it is fine with me to introduce it as part of this PR once we agree on how it should work.",
        "> I am not sure if the mode 1 would replace the testing mode for coreclr tests though, as it would also interpret the xunit stuff which I assume would be quite slow.\r\n\r\ncoreclr tests should have the xunit stuff source generated. It should not be a lot of code - it does not run reflection and other heavy lifting like regular xunit."
      ],
      "runtime-model-actual-hardware-costs": [
        "This should be enabled for crossgen too.",
        "Yep, as @AndyAyersMS said. PREJIT means AOT compilation in general. R2R means specific ABI for AOT compiled code, e.g. code sequence to access generic dictionaries, etc."
      ]
    },
    "profile": {
      "company": "Microsoft",
      "blog": "",
      "site_admin": false,
      "followers": 662,
      "following": 1
    }
  },
  "asmorkalov": {
    "repos": [
      "opencv/opencv"
    ],
    "entries": [
      {
        "slug": "opencv-cleanup-before-errors",
        "title": "Cleanup before errors"
      },
      {
        "slug": "opencv-clear-api-contracts",
        "title": "Clear API contracts"
      },
      {
        "slug": "opencv-code-for-readability",
        "title": "Code for readability"
      },
      {
        "slug": "opencv-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "opencv-cross-platform-api-design-rules",
        "title": "Cross-platform API design rules"
      },
      {
        "slug": "opencv-document-configuration-version-requirements",
        "title": "Document configuration version requirements"
      },
      {
        "slug": "opencv-document-properly-with-references",
        "title": "Document properly with references"
      },
      {
        "slug": "opencv-feature-flag-convention",
        "title": "Feature flag convention"
      },
      {
        "slug": "opencv-framework-synchronization-practices",
        "title": "Framework synchronization practices"
      },
      {
        "slug": "opencv-maintain-build-compatibility",
        "title": "Maintain build compatibility"
      },
      {
        "slug": "opencv-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "opencv-meaningful-semantic-naming",
        "title": "Meaningful semantic naming"
      },
      {
        "slug": "opencv-optimize-container-access",
        "title": "Optimize container access"
      },
      {
        "slug": "opencv-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "opencv-prevent-null-vulnerabilities",
        "title": "Prevent null vulnerabilities"
      },
      {
        "slug": "opencv-use-opencv-error-mechanisms",
        "title": "Use OpenCV error mechanisms"
      },
      {
        "slug": "opencv-use-optimized-functions",
        "title": "Use optimized functions"
      },
      {
        "slug": "opencv-use-proper-assertions",
        "title": "Use proper assertions"
      },
      {
        "slug": "opencv-validate-tensor-dimensions",
        "title": "Validate tensor dimensions"
      }
    ],
    "comments": {
      "opencv-optimize-memory-allocation-patterns": [
        "It's more efficient to use  std::vector<int> dims(dim_count); and assign values, rather than call push_back and trigger reallocations.",
        "If I understood correctly, The m_read_buffer is resized only once here ant it's size is always 16k. The `m_read_buffer` is used in ::read() only. M.b. it's better to make it local variable there. If you need the buffer shared between readHeader and readData then it makes sense to initialize the buffer in decoder constructor.",
        "I propose to use local variable on stack. new is redundant here. Also it's not deleted afterwards.",
        "I propose to use local variable on stack. new is redundant here. Also it's not deleted afterwards."
      ],
      "opencv-maintain-build-compatibility": [
        "It should break static linkage, if OpenCV is build against own zlib-ng, but not system-wide.",
        "> protobuf_generate Added in version 3.13.\r\n\r\nIt breaks build with older CMake. I propose to add CMake version check and presume the old branch for old CMake.\r\n"
      ],
      "opencv-consistent-descriptive-naming": [
        "We use all capital letters for constant names.",
        "The same note about capital letters,\r\ne.g. `COLOR_SPACE_AppleRGB` -> `COLOR_SPACE_APPLE_RGB`",
        "The name is very controversial. I propose `THRESH_DRYRUN` or something similar."
      ],
      "opencv-meaningful-semantic-naming": [
        "It makes sense to move key name to the function parameters. E.g.\r\ncheck_cmake_flag_enabled(cmake_file, \"HAVE_IPP\")\r\n\r\nThe same function may be used for KleidiCV and other dependencies.",
        "It breaks compatibility. M.b. rename the method to detectMarkersMultiDict?",
        "What if use `namespace_prefix_override` solution like for the functions? Namespace is already appended to the function names, if it's not overridden by config. It's less hacky and more obvious."
      ],
      "opencv-cross-platform-api-design-rules": [
        "size_t is not wrapped to Python and Java correctly. Java even does not support unsigned types. Please use int instead.",
        "size_t does not work well with Java and Python bindings. let's use just int.\r\n",
        "Please use `CV_EXPORTS_W` and `CV_WRAP` to make it available from Python,  Java and other binded languages."
      ],
      "opencv-document-properly-with-references": [
        "Please move the text to tutorial (markdown) and add references to the header file.",
        "It should go to make ccm header to be included into documentation and bindings. Please CV_EXPORTS_W to generate Java and Python bindings for it too.",
        "I propose to use the same description as for cv::solvePnpRansac with fisheye model reference and add reference to SOLVEPNP_XXX constants instead of copy."
      ],
      "opencv-document-configuration-version-requirements": [
        "I propose to add command line option, e.g. \"--strict-dependencies\" to make the check optional.",
        "I want to switch CI to the default configuration and always build SDK and AAR packages with 16kb pages support. The packages will work with both old and new memory configuration. It just tunes ELF sections alignment a bit.  The option will be ignored with NDK before 27."
      ],
      "opencv-framework-synchronization-practices": [
        "`stripeHeight = nThreads * 10;` sounds strange. More threads -> larger piece for each thread.",
        "threads are 8. It's not true:\r\n- Android build uses 2 threads by default. It's done to prevent overheating, but may be changed in future.\r\n- Linux builds use all available cores.\r\n- parallel_for_ serializes nested parallel_for_ calls. So you can easily get 1 here.",
        "We usually set granularity to some reasonable size for single thread. OpenCV uses dynamic scheduling, so all other steps are done automatically.",
        "I propose to use `num_worker_threads = cv::getNumThreads()` to make it manageable outside. See https://docs.opencv.org/4.x/db/de0/group__core__utils.html#ga2db334ec41d98da3129ef4a2342fc4d4 "
      ],
      "opencv-optimize-container-access": [
        "just local `std::vector<>` outside of `if` should be enough here. It's empty by default also you save one new/delete pair if condition is true."
      ],
      "opencv-use-optimized-functions": [
        "inRange should be enough instead of it: https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981",
        "No need to spit cv::Mat by channels for it. cv::sum supports channels: https://docs.opencv.org/5.x/d2/de8/group__core__array.html#ga716e10a2dd9e228e4d3c95818f106722",
        "I propose to convert Gamma correction to public function with InputArray and OutputArray. It's useful independently from the pipeline and also may be significantly optimized with universal intrinsics.",
        "CV_SIMD_WIDTH is compile time constant. It may not work correctly with _SCALABLE branch. please use `VTraits<xxx>::max_nlanes` instead. For fixed-size SIMD it works in the same way.",
        "@fengyuentau please correct me, if I'm wrong.",
        "Yes, it does not work, because `w` value is not known in compile time. `VTraits<xxx>::max_nlanes` is compile time constant. It's equal to `CV_SIMD_WIDTH` for fixed SIMD size architectures (x86). RISC-V RVV vector size is not known in compile time, but we know maximum vector length and use it for intermediate buffers to fit any feasible vector size.",
        "Please use cv::PSNR instead: https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga3119e3ea73010a6f810bb05aa36ac8d6",
        "I propose to use cv::RNG for it to make it manageable outside:\r\n- User may set seed to get deterministic behaviour\r\n- OpenCV test system fixes cv::RNG seed for each test independently."
      ],
      "opencv-clear-api-contracts": [
        "Need a warning, that GDAL does not support metadata for now.",
        "I propose to Move BRG/RGB logic into model with a flag (default to bgr). All OpenCV functions are designed for BGR. The flag allows to make optimizations without API change.",
        "The `p` parameter does not have default value like `Point p = Point()`. It means that the point is user provided value. I do not think that we sound handle `Point()`, a.k.a. (0,0) as special case.",
        "Why do we need special cases in code? User should provide location, shouldn't he/she?"
      ],
      "opencv-prevent-null-vulnerabilities": [
        "please use std::vector, std::array or cv::AutoBuffer to prevent memory leaks in cases of parser failrue.",
        "Let's use `std::vector<>` or `cv::AutoBuffer` for locals to prevent leaks.",
        "I propose to use `std::vector` or `cv::AutoBuffer` to prevent memory leaks. ",
        "Pleas use `cv::AutoBuffer` or `std::vector` for temporary buffers instead new/delete to prevent memory leaks. `cv::AutoBuffer` is preferable, it may use stack space, if buffer is small enough."
      ],
      "opencv-maintain-code-consistency": [
        "using namespace is very bad practice. It affects everything, even, if the header is not included directly.",
        "Please remove dead code, or guard it with macro, if you want to use it later."
      ],
      "opencv-validate-tensor-dimensions": [
        "```\r\nvector<Mat> channels = {\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(0)),\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(1)),\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(2))\r\n};\r\n```"
      ],
      "opencv-cleanup-before-errors": [
        "Looks like the assert will be disabled in regular release builds: https://en.cppreference.com/w/cpp/error/assert.\r\nWhy not just CV_Assert? It's defined in `opencv2/core/base.hpp`"
      ],
      "opencv-feature-flag-convention": [
        "Good point. Fixed.",
        "Looks like I cannot make it public for now. We get redefinition issue. The macros are defined by both HAL and IPP core. I made it private for now to exclude the redefinition issue and added note to CMake."
      ],
      "opencv-use-opencv-error-mechanisms": [
        "Please do not use try-catch. OpenCV uses CV_Assert, CV_Check for the function input validation and and CV_LOG_DEBUG/CV_LOG_INFO for notifications. Function should not silently ignore obvious invalid inputs like not enough points or wrong data types and ranges.  ",
        "OpenCV uses CV_Assert for such purposes.",
        "please add CV_Assert with input type checks at least. The function is public API now and should handle invalid input correctly.",
        "It's still relevant.",
        "Please use CV_Error, CV_Assert, etc. See https://docs.opencv.org/5.x/db/de0/group__core__utils.html#ga5b48c333c777666e076bd7052799f891",
        "It's error. I propose to include amount of channels to error message in printf style like https://github.com/opencv/opencv/blob/c21d0ad9d08d364542bb4a6eb971ee3051ccba63/modules/imgcodecs/src/grfmt_jpeg.cpp#L771",
        "OpenCV's `imread` is exception safe. Please use CV_LOG_WARNING and analogs and return satus instead of exception."
      ],
      "opencv-use-proper-assertions": [
        "`ASSERT_FALSE(chartsRGB.empty());`",
        "`ASSERT_FALSE(chartsRGB.empty());` here and bellow.",
        "`EXPECT_FALSE(src.empty()) << Cannot open test image perf/1920x1080.png;`"
      ],
      "opencv-code-for-readability": [
        "Let's extract extension search as dedicated static function. It should make the code more readable."
      ]
    },
    "profile": {
      "company": "OpenCV.AI",
      "blog": "",
      "site_admin": false,
      "followers": 255,
      "following": 0
    }
  },
  "Rich-Harris": {
    "repos": [
      "sveltejs/svelte"
    ],
    "entries": [
      {
        "slug": "svelte-analyze-transitive-dependencies",
        "title": "analyze transitive dependencies"
      },
      {
        "slug": "svelte-api-flexibility-balance",
        "title": "API flexibility balance"
      },
      {
        "slug": "svelte-async-cleanup-safety",
        "title": "async cleanup safety"
      },
      {
        "slug": "svelte-avoid-expensive-operations",
        "title": "avoid expensive operations"
      },
      {
        "slug": "svelte-avoid-unclear-abbreviations",
        "title": "avoid unclear abbreviations"
      },
      {
        "slug": "svelte-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "svelte-complete-code-examples",
        "title": "Complete code examples"
      },
      {
        "slug": "svelte-defensive-error-handling",
        "title": "defensive error handling"
      },
      {
        "slug": "svelte-document-complex-apis",
        "title": "Document complex APIs"
      },
      {
        "slug": "svelte-document-configuration-hierarchies",
        "title": "document configuration hierarchies"
      },
      {
        "slug": "svelte-document-non-obvious-behavior",
        "title": "Document non-obvious behavior"
      },
      {
        "slug": "svelte-measure-performance-impact",
        "title": "Measure performance impact"
      },
      {
        "slug": "svelte-prefer-simple-code-patterns",
        "title": "prefer simple code patterns"
      },
      {
        "slug": "svelte-prefer-testing-libraries",
        "title": "prefer testing libraries"
      },
      {
        "slug": "svelte-preserve-user-input-focus",
        "title": "preserve user input focus"
      },
      {
        "slug": "svelte-realistic-documentation-examples",
        "title": "realistic documentation examples"
      },
      {
        "slug": "svelte-runtime-html-escaping",
        "title": "Runtime HTML escaping"
      },
      {
        "slug": "svelte-simplify-and-deduplicate-code",
        "title": "Simplify and deduplicate code"
      },
      {
        "slug": "svelte-ssr-documentation-clarity",
        "title": "SSR documentation clarity"
      },
      {
        "slug": "svelte-state-boundary-management",
        "title": "state boundary management"
      },
      {
        "slug": "svelte-use-modern-null-safe-operators",
        "title": "use modern null-safe operators"
      }
    ],
    "comments": {
      "svelte-choose-descriptive-names": [
        "please, anything but `clazz` 😆 \r\n\r\n```suggestion\r\nexport function to_class(value, hash, classes) {\r\n\tlet class_name = value == null ? '' : '' + value;\r\n```",
        "wondering if this is the right name for this function. `StartStopNotifier` is slightly esoteric. what if it was `this.#subscribe = createSubscriber(fn)` instead? would that make sense?",
        "this signature (`node_or_nodes`) feels like a code smell. can we change it to this?\r\n\r\n```suggestion\r\n * @param {AST.Attribute} attribute\r\n * @param {Scope} scope\r\n */\r\nexport function is_inlinable_attribute(attribute, scope) {\r\n```\r\n\r\n(note that I got rid of the `import('...')` as well — we should use `/** @import ... */` instead)",
        "Really not a fan of a function returning `false | { need_to_escape: boolean }` — it's a messy and confusing type. If a function name begins with `is_` then it ought to return a boolean; if it needs to return more granular information than that then something like `analyze_expression` would probably be better",
        "I tend to think it's better to avoid negatives in variable names (even though a few other `skip_` things have slipped in) since you have to mentally invert stuff to understand what's happening. At first I thought replacing `if (!skip_derived_source ...)` with `if (track_owner ...)` might be clearer...\r\n\r\n```suggestion\r\nexport function source(v, track_owner = true) {\r\n```\r\n\r\n...but then it occurred to me that we could even do this...\r\n\r\n```suggestion\r\nexport function source(v, owner = current_reaction) {\r\n```\r\n\r\n...and then just do this below:\r\n\r\n```js\r\nif (owner !== null && (owner.f & DERIVED) !== 0) {...}\r\n```\r\n\r\nOne less thing to check. Thoughts?",
        "made that change"
      ],
      "svelte-use-modern-null-safe-operators": [
        "```suggestion\r\n\t\tparent: parent_derived ?? active_effect\r\n```",
        "```suggestion\r\n\t\t\t\t(to_animate ??= new Set()).add(item);\r\n```",
        "```suggestion\r\n\t\t\t\t(to_animate ??= new Set()).delete(item);\r\n```",
        "```suggestion\r\n\t\t\t\t(seen ??= new Set()).add(current);\r\n```",
        "I think this whole block could become a one-liner?\r\n\r\n```js\r\n(dependency.reactions ??= new Set()).add(reaction);\r\n```",
        "this makes me a bit nervous — if this is now being called before `a` exists, then will `a` still get created (but never aborted) in the microtask?"
      ],
      "svelte-state-boundary-management": [
        "this wording feels a bit strong. if we're so adamant that you shouldn't update state inside an effect, why do we allow it? it also feels quite vague — what is the nightmare exactly? it would be good if we could articulate _why_ we warn against this"
      ],
      "svelte-runtime-html-escaping": [
        "The escaping [needs to happen at runtime](https://svelte.dev/playground/hello-world?version=pr-14269#H4sIAAAAAAAAE23NMQ7CMAwF0KtEXrogupc0EucgDKU1IlLqRIlbgaLcnRgY2ayn7_8L0LQiDBBCzHCAu_OYYbgU4FcUF2j-S51jPOYdPYvdpoz_fA7ESNxqQOc5ucjqQ08eLaxh2TxaMJYsN86sZFqNqtOPZLqTJd1_vyRCenG7KRKpupfbUpuQMhg4bViv9Q3ti_IKxAAAAA==), not here"
      ],
      "svelte-defensive-error-handling": [
        "It won't work — the update is still in progress and everything gets torn. The `svelte_boundary_reset_onerror` message has an example of fixing it by waiting for a `tick()` before calling `reset()`"
      ],
      "svelte-simplify-and-deduplicate-code": [
        "no need to repeat this, we're extending `Value` which has this property\r\n\r\n```suggestion\r\n```"
      ],
      "svelte-realistic-documentation-examples": [
        "even though localStorage was one of the primary motivators for this work I think we need to omit it from these docs, because it's just too hard to illustrate concisely without cutting corners:\r\n\r\n- needs to accept initial data _without_ blowing up in SSR (i.e. needs `typeof localStorage` checks etc)\r\n- needs to be composable\r\n- needs to react to `storage` events, which basically means using `createSubscriber` which is a whole thing\r\n\r\nI think a simpler and clearer example would be validation. Will push a change",
        "You could imagine someone doing something cute like\r\n\r\n```js\r\nexport const messages = [\r\n  \"reticulating splines...\",\r\n  \"generating witty dialog...\",\r\n  \"swapping time and space...\",\r\n  \"640K ought to be enough for anybody\",\r\n  \"checking the gravitational constant in your locale...\",\r\n  \"keep calm and npm install\",\r\n  \"counting backwards from Infinity\",\r\n  \"I'm sorry Dave, I can't do that.\",\r\n  \"adjusting flux capacitor...\",\r\n  \"constructing additional pylons...\",\r\n  \"rm -rf /\",\r\n];\r\n\r\nconst loading_message = $derived(messages[$effect.pending() % messages.length]);\r\n```",
        "yeah, that was my thought process too — figured this would get the point across most concisely",
        "thought about something like [this](https://svelte.dev/playground/hello-world?version=5.36.1#H4sIAAAAAAAAE3VSwY6bMBD9lamzWpltEtg9sgRpL1UvlXrorenBC8PGWmMjewiJvPx7DYSkFe0BbL95780bsGda1MhS9hWVMtAZq0rgWErCMmJrVkmFjqU_PaNzM_AGIOAX1UvTbN0RFQ3Yq3D4L7wwmlBTsGGZK6xsKN_rPSkk6A6C8IgWdnDnKOx5Ej3PRTIk1KIyPFWrC5JGgxW6NDWPwI8lskit1aCxg-_W1NIh51UEu3wm7Mkh_ZA1mpY4HysV_ybosJ2ttmS-yBOW_CmK1vBXCR7gKUnmHNRPmz68wnKHVYXFbHpp98d8jxN51PyPfp14Km4b1KXUb4Fzfz9FqcWJj6z1khTdOmTx7UPr7LUNzhqMLpQs3nd-6nnN9jmE6_O2KcM5iyfyJGzysVcKflz7LG7yEV70DpQFdqEHwafNBrLGmjeLzsFRqBZDiuWU8fTPI_j4gKTPg_6iyWGzGa38Sla35HlgDTP71Tuer3A_fUy_QlEc4MVaceaPSdTPNyDk96ITkq7XZ046qOJBNbnGwTXsfCyrsISbTHgilpJtsf8VTkKqTuqSpZVQDvvf-aQbl0wDAAA=) and discovered that a) it doesn't decrement when it should and b) there's some weird NaN action happening 🤔 \r\n\r\ninvestigating",
        "```suggestion\r\nOptionally, they can return a function that is called before the attachment re-runs, or when the element is later removed from the DOM.\r\n```",
        "```suggestion\r\nOptionally, they can return a function that is called before the attachment re-runs, or after the element is later removed from the DOM.\r\n```",
        "I think this is probably a niche enough thing that we could move it to a separate 'Controlling when attachments re-run' section further down — thoughts?",
        "```suggestion\r\nTo fix this, either silence the warning with a [`svelte-ignore`](basic-markup#Comments) comment, or ensure that the value stays the same between server and client. If you really need the value to change on hydration, you can force an update like this:\r\n```",
        "```suggestion\r\nTo fix this, either silence the warning with a [`svelte-ignore`](basic-markup#Comments) comment, or ensure that the value stays the same between server and client. If you really need the value to change on hydration, you can force an update like this:\r\n```",
        "I think it probably makes sense to talk more abstractly about form resets, since in the provided example we're not using the `form.reset()` method (at least, not explicitly)\r\n\r\n```suggestion\r\nIf an `<input>` has a `defaultValue` and is part of a form, it will revert to that value instead of the empty string when the form is reset. Note that for the initial render the value of the binding takes precedence unless it is `null` or `undefined`.\r\n```",
        "MDN [has a note about default values](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/reset) which I think it worth reiterating here\r\n\r\n````suggestion\r\n```\r\n\r\n> [!NOTE]\r\n> Use reset buttons sparingly, and ensure that users won't accidentally click them while trying to submit the form.\r\n````",
        "'instead of the empty string' doesn't really make sense here, because the default behaviour if no option has the `selected` attribute is to select the first option. I think we can just leave that implied\r\n\r\n```suggestion\r\nYou can give the `<select>` a default value by adding a `selected` attribute to the`<option>` (or options, in the case of `<select multiple>`) that should be initially selected. If the `<select>` is part of a form, it will revert to that selection when the form is reset. Note that for the initial render the value of the binding takes precedence if it's not `undefined`.\r\n```",
        "We're in the compiler warnings section of the (hypothetical future) docs, I don't think we need to say that Svelte will issue a compiler warning — if someone is here it's most likely because they just experienced one\r\n\r\n```suggestion\r\nHTML restricts where certain elements can appear. For example, a `<p>` element cannot contain another `<p>`, or a `<div>`, or any other block-level element. The browser will 'repair' the HTML in a way that breaks Svelte's assumptions about the structure of your components. Some more examples:\r\n\r\n- `<option><div>...</div></option>` will result in `<option>...</option>` (everything except text is removed)\r\n- `<table><tr><td>...</td></tr></table>` will result in `<table><tbody><tr><td>...</td></tr></tbody></table>` (a `<tbody>` is auto-inserted)\r\n\r\nThis code will work when the component is rendered on the client, but if you use server rendering it will cause hydration to fail.\r\n```",
        "what about\r\n\r\n> This code will work when the component is rendered on the client (which is why this is a warning rather than an error), but if you use server rendering it will cause hydration to fail."
      ],
      "svelte-ssr-documentation-clarity": [
        "```suggestion\r\nIf the `{@html ...}` value changes between the server and the client, it will not be repaired during hydration, i.e. the server value will be kept. That's because change detection during hydration is expensive and usually unnecessary.\r\n```",
        "```suggestion\r\nCertain methods such as `mount` cannot be invoked while running in a server context. Avoid calling them eagerly, i.e. not during render.\r\n```"
      ],
      "svelte-api-flexibility-balance": [
        "FWIW we don't need to expose function wrappers to get values from different modules, only to set them. we can just `export let derived_writes`",
        "Someone might have an existing classname in their HTML template, in which case giving them a string would make it awkward to combine stuff. Should we return an object instead of a string, so that they have more flexibility?",
        "it only really matters for new projects, I think, since we can't retroactively add `%htmlAttributes%` anyway. I think we just replace `lang=\"en\"` with `%htmlAttributes%` in the template project's `app.html`, and add this to the root layout:\r\n\r\n```svelte\r\n<svelte:html lang=\"en\" />\r\n```"
      ],
      "svelte-async-cleanup-safety": [
        "I think we should get rid of `simple_set`, it will be a bug magnet. Case in point: `onchange` fires [here](https://svelte.dev/playground/hello-world?version=pr-15069#H4sIAAAAAAAAE22Qy2rDMBBFf2VQu7AhJGTrOIbu-gmBugtFntQGRRKecZJi9O-V5NemC4G4d-6ZxyiMvKMoxCdqbeFpe91Ahk3H2ORiJ26dRhLF1yj418W6KAR9Tn04t6cHao7aVRL-pytrGA0HjChJ9Z3jqjY1a2RQrTQ_2MAZ3oklY3aTmjA_RT8-pSURXGBMAr89pB5wqz7uFqdmayZYAVkO52ozAmXtwv2Ap8Xw08endukbJqVQotj2AbISuO1ov_Y-zoAU91M0BeEVXINPuGQRWR62ZU3pqmWKjFo7hCtfEaZtCxhnz5cHV4WLMb5YFHFY_-3_ALWpfmCjAQAA) when it shouldn't...\r\n\r\n```js\r\nclass X {\r\n  #value = $state(1, {\r\n    onchange: () => {\r\n      changed = true;\r\n    }\r\n  });\r\n\r\n  constructor() {\r\n    this.#value = 1;\r\n  }\r\n}\r\n```\r\n\r\n...because we're missing the `!source.equals(value)` check. We could add that in, but we're likely to run into similar cases if we (for example) add forking and so on. The very slight optimisation probably isn't worth the more complicated code.\r\n\r\nOne wrinkle: while no tests fail if we replace `$.simple_set` with `$.set` (other than a harmless snapshot test), it _is_ technically a breaking change, because it would mean that private state behaves the same as public state with regards to effects:\r\n\r\n```js\r\nclass X {\r\n  value = $state(0);\r\n  #value = $state(0);\r\n\r\n  constructor() {\r\n    this.value = 1;\r\n    this.#value = 1;\r\n  }\r\n\r\n  get private_value() {\r\n    return this.#value;\r\n  }\r\n}\r\n\r\n$effect(() => {\r\n  // this creates an infinite loop\r\n  const x = new X();\r\n  x.value;\r\n});\r\n\r\n$effect(() => {\r\n  // this does not (but probably should)\r\n  const x = new X();\r\n  x.private_value;\r\n});\r\n```\r\n\r\nI think that's _probably_ okay, since a) you really shouldn't be reading a signal that you just created inside an effect (in fact perhaps we should have a dedicated error for that, rather than just the infinite loop detection, [like we do with deriveds](https://svelte.dev/playground/hello-world?version=5.23.2#H4sIAAAAAAAAE22OQQqDMBBFrxKGLiIW7dpGobveoXahZgqBNIZktBXJ3WtUKJQu5_03M38G0zwRCrii1j179U5LxlEqQpnAER5Ko4fiNgNNNnoRLHzfulib-RE1RdY2Hv_xrjeEhpYzIHznlKWqNjVpJNb1gyFWsoNEp0aUWTtxnrCyYnNUfiRPDSE_JectW3ma7pNDGpzZ5BWF6In8-9EIW81rHkRuq6UZ4ZugIDdguIcP1dCucgsBAAA=)), and b) I'd be very surprised if anyone was relying on that behaviour. But it gives me pause.\r\n\r\nIf we wanted to be super cautious about it then we could pass a bitmask as the third option instead of a boolean, where the `1` bit means 'proxy this please' and the `2` bit means 'don't mark reactions'.",
        "A reversal of my previous position: neither effect in the code above should run in a loop https://github.com/sveltejs/svelte/pull/15553\r\n\r\nIf we merge that, we can safely get rid of `simple_set` IIUC",
        "There was some drift between this and `main` after #15553, so I merged everything and removed `simple_set` in the process on the assumption that #15300 (and #15564) will happen ",
        "the whole tuple thing is a bit confusing for starters but that'll need to be fixed on main",
        "https://github.com/sveltejs/svelte/pull/16287",
        "honestly... i have no idea what the `async_mode_flag` clause is doing here. maybe @dummdidumm knows since it was apparently introduced with #16197.\r\n\r\ni deleted it to see if anything would break and it didn't, so unless we can articulate why it needs to be there my inclination is to leave it deleted"
      ],
      "svelte-prefer-simple-code-patterns": [
        "we can use `&&` here because trailing undefined arguments to `b.call` are ignored\r\n\r\n```suggestion\r\n\t\t\tadditional && b.object(Object.entries(additional).map(([k, v]) => b.init(k, b.literal(v))))\r\n```",
        "using an implicit return will keep the statement on a single line in many cases \r\n\r\n```suggestion\r\n\t\t\tb.arrow([], call_expression),\r\n```",
        "in my experience it's vanishingly rare that this kind of indirection makes sense compared to writing more obvious code. if we do this...\r\n\r\n```suggestion\r\n\t/**\r\n\t * @template {Node} T\r\n\t * @param {T} child\r\n\t */\r\n\tfunction insert(child) {\r\n\t\tif (last_current_element) {\r\n\t\t\tlast_current_element.children ??= [];\r\n\t\t\tlast_current_element.children.push(child);\r\n\t\t} else {\r\n\t\t\telements.push(/** @type {Element} */ (child));\r\n\t\t}\r\n\r\n\t\treturn child;\r\n\t}\r\n\r\n\tfor (let instruction of items) {\r\n\t\tswitch (instruction.kind) {\r\n\t\t\tcase 'push_element':\r\n\t\t\t\telements_stack.push(/** @type {Element} */ (last_current_element));\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'pop_element':\r\n\t\t\t\telements_stack.pop();\r\n\t\t\t\tlast_current_element = elements_stack.at(-1);\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'create_element':\r\n\t\t\t\tlast_current_element = insert({\r\n\t\t\t\t\tkind: 'element',\r\n\t\t\t\t\telement: /** @type {string[]} */ (instruction.args)[0]\r\n\t\t\t\t});\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'create_text':\r\n\t\t\t\tinsert({\r\n\t\t\t\t\tkind: 'text',\r\n\t\t\t\t\tvalue: /** @type {string[]} */ (instruction.args)[0]\r\n\t\t\t\t});\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'create_anchor':\r\n\t\t\t\tinsert({\r\n\t\t\t\t\tkind: 'anchor',\r\n\t\t\t\t\tdata: instruction.args?.[0]\r\n\t\t\t\t});\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'set_prop': {\r\n\t\t\t\tconst el = /** @type {Element} */ (last_current_element);\r\n\t\t\t\tconst [prop, value] = /** @type {string[]} */ (instruction.args);\r\n\t\t\t\tel.props ??= {};\r\n\t\t\t\tel.props[prop] = value;\r\n\t\t\t\tbreak;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n```",
        "unrelated to the PR but I hadn't noticed this code before — any reason we're using an enum here rather than a boolean? Feels like it could be this instead, with a corresponding change on line 53:\r\n\r\n```js\r\nconst dirty = signal.w_version > current_reaction.w_version || current_reaction.w_version === 0;\r\n```",
        "What's the reason to use `0 | 1` instead of booleans here? If the signature was this...\r\n\r\n```suggestion\r\n\tconst set_branch = (/** @type {(anchor: Node) => void} */ fn, flag = false) => {\r\n\t\thas_branch = true;\r\n\t\tupdate_branch(flag, fn);\r\n\t};\r\n```\r\n\r\n...then in the common case where there's no alternate, we could generate this code — a small win but it feels like it better reflects the binary nature of `if` blocks\r\n\r\n```diff\r\n$.if(node, ($$branch) => {\r\n-\tif (Math.random() < 0.5) $$branch(0, consequent);\r\n+\tif (Math.random() < 0.5) $$branch(consequent);\r\n});\r\n```",
        "storing this feels like overkill when we only need to do `status === MAYBE_DIRTY` below (no need for bitwise shenanigans)\r\n\r\n```suggestion\r\n```"
      ],
      "svelte-measure-performance-impact": [
        "it was probably me (I used to be quite fond of the `~array.indexOf` trick), and I probably wrote it before `includes` was implemented everywhere",
        "For svelte.dev it's 147, for svelte.dev/playground it's 260, for svelte.dev/tutorial it's 302. If you interact with the search box it will increase by a few hundred. Obviously if you had a very large list then it would be more, but the upshot is that we're typically talking about hundreds of operations.\r\n\r\nI don't know if this is a valid microbenchmark, but on this M1 MBP I can run the check 10,000 times (i.e. `run(1e4)` before it takes more than a single millisecond:\r\n\r\n```js\r\nfunction run(i = 1e6) {\r\n  console.time('test');\r\n  let div = document.createElement('div');\r\n  while (i--) {\r\n    div.nodeName.includes('-');\r\n    div.namespaceURI === 'http://www.w3.org/1999/xhtml';\r\n  }\r\n  console.timeEnd('test');\r\n}\r\n```\r\n\r\nSo... it's measurable, but seems pretty negligible. I don't know, what do you think?",
        "Makes no real difference \r\n\r\n```js\r\nfunction run(i = 1e6) {\r\n  let div = document.createElement('div');\r\n  document.body.append(div);\r\n  console.time('test');\r\n  while (i--) {\r\n    div.nodeName.includes('-');\r\n    div.namespaceURI === 'http://www.w3.org/1999/xhtml';\r\n  }\r\n  console.timeEnd('test');\r\n}\r\n```",
        "Try it! The results are basically the same. Though the code above isn't correct, it should be this:\r\n\r\n```js\r\nfunction run(n = 1e6) {\r\n  const els = Array(n);\r\n  let i = n;\r\n  while (i--) {\r\n    let div = document.createElement('div');\r\n    document.body.append(div);\r\n    els[i] = div;\r\n  }\r\n  console.time('test');\r\n  i = n;\r\n  while (i--) {\r\n    const div = els[i];\r\n    div.nodeName.includes('-');\r\n    div.namespaceURI === 'http://www.w3.org/1999/xhtml';\r\n  }\r\n  console.timeEnd('test');\r\n}\r\n```",
        "I think it was you who taught me that we should avoid reassigning parameters, since mutating the `arguments` object carries overhead. Should it be something like this instead?\r\n\r\n```suggestion\r\n\tvar str = value == null ? '' : typeof value === 'object' ? value + '' : value;\r\n```",
        "probably. there's still a few places where we need to expose these properties to the world (via the `STATE_SYMBOL` property) so we can't totally get rid of it, but will make that change"
      ],
      "svelte-document-non-obvious-behavior": [
        "'Deeply tracks' isn't 100% clear — maybe something like this?\r\n\r\n```suggestion\r\n * Logs the arguments whenever they, or the properties they contain, change. Example:\r\n```",
        "not sure why we would need to specify 'attribute may be omitted when false' — for one thing non-boolean attributes are never `false`, they either exist or they don't\r\n\r\n```suggestion\r\n\t * Quoted/string values are represented by an array, even if they contain a single expression like `\"{x}\"`\r\n```"
      ],
      "svelte-prefer-testing-libraries": [
        "```suggestion\r\nWhile the process is very straightforward, it is also low level and somewhat brittle, as the precise structure of your component may change frequently. Tools like [@testing-library/svelte](https://testing-library.com/docs/svelte-testing-library/intro/) can help streamline your tests.\r\n```",
        "how does it know when to unmount it?",
        "```suggestion\r\nWhen writing component tests that involve two-way bindings, context or snippet props, it's best to create a wrapper component for your specific test and interact with that. `@testing-library/svelte` contains some [examples](https://testing-library.com/docs/svelte-testing-library/example).\r\n```"
      ],
      "svelte-preserve-user-input-focus": [
        "in a case like this, if you type 'potato' and then `find('p')` resolves, Svelte will replace the input contents with 'p' because that's the `query` that goes with the resolved promise. Most of the time that's what you want, but it's absolutely _not_ what you want if the input itself was the source of the change — instead you want the UI to 'catch up' to a focused input\r\n\r\n```svelte\r\n<input bind:value={query}>\r\n<p>{await find(query)}</p>\r\n```\r\n\r\nadded an explanatory comment"
      ],
      "svelte-document-complex-apis": [
        "I've added a bunch of JSDoc comments around the less self-explanatory stuff. No doubt there's more that could be added but I'll mark this resolved for now",
        "unrelated to this PR but I think this comment is out of date? write versions are used for everything, not just unowned deriveds.\r\n\r\nseparately, would be good to have a comment explaining what `read_version` is"
      ],
      "svelte-analyze-transitive-dependencies": [
        "```suggestion\r\nfix: treat transitive dependencies of each blocks as mutable in legacy mode if item is mutated\r\n```",
        "```suggestion\r\nfix: treat pure call expressions as potentially reactive if they reference local bindings\r\n```"
      ],
      "svelte-avoid-unclear-abbreviations": [
        "I really dislike these sorts of abbreviations, they feel clunky. The convention in mathematical notation is to use the letter `k` for this sort of constant\r\n\r\n```suggestion\r\n * @param {number} k\r\n */\r\nexport function multiplier(initial, k) {\r\n```"
      ],
      "svelte-complete-code-examples": [
        "it doesn't make sense for `count` to not be state (this is more apparent now that we're actually showing the code). by extension, we'll need to change the filename, both here and in the import directly above\r\n\r\n```suggestion\r\n\tlet count = $state(initial);\r\n```",
        "if we have a `<script>` for the imports, we also need to declare `visible` since it's no longer implicit\r\n\r\n```suggestion\r\n  import { fade, fly } from 'svelte/transition';\r\n  \r\n  let visible = $state(false);\r\n```",
        "we also need some way to change the value of `visible`\r\n\r\n```suggestion\r\n<label>\r\n  <input type=\"checkbox\" bind:checked={visible}>\r\n  visible\r\n</label>\r\n\r\n{#if visible}\r\n```"
      ],
      "svelte-document-configuration-hierarchies": [
        "good catch, thanks",
        "```suggestion\r\n- Use a [`target`](https://www.typescriptlang.org/tsconfig/#target) of at least `ES2022`, or a `target` of at least `ES2015` alongside [`useDefineForClassFields`](https://www.typescriptlang.org/tsconfig/#useDefineForClassFields). This ensures that rune declarations on class fields are not messed with, which would break the Svelte compiler\r\n```",
        "```suggestion\r\nThere are several ways to set this option:\r\n\r\n- Globally, via the `compilerOptions.css` option in your `svelte.config.js` or the options passed to `svelte.compile`\r\n- Dynamically, using [`dynamicCompileOptions`](https://github.com/sveltejs/vite-plugin-svelte/blob/main/docs/config.md#dynamiccompileoptions) in `vite-plugin-svelte`\r\n- Per-component, with `<svelte:options css=\"injected\" />` (this will override options set any other way)\r\n```\r\n\r\n"
      ],
      "svelte-avoid-expensive-operations": [
        "Walking is expensive, we want to do it as infrequently as possible. Luckily we don't need this function anyway, since the necessary information exists on `value.metadata.expression.dependencies`.\r\n\r\nAs a side-note, the check in the `Identifier` visitor is insufficient. If you want to know if an identifier is a _reference_ then you can use [`is-reference`](https://github.com/Rich-Harris/is-reference/)\r\n\r\n```suggestion\r\n```",
        "...although `value.metadata.expression.dependencies` excludes globals, which means that things like `{location}` get inlined incorrectly. We may need to treat globals as bindings",
        "Still trying to wrap my head round this, it's very cryptic. If I understand correctly, we're saying that since branch effects have no dependencies and can therefore never be dirty, we can (ab)use the `MAYBE_DIRTY` flag on branches to stop traversing upwards to the root. Is that right?\r\n\r\nIf so, it feels suboptimal — it means we traverse upwards unnecessarily until we hit a branch, and it means we call `check_dirtiness` for branches unnecessarily. What if we had an `EFFECT_HAS_DIRTY_CHILDREN` flag or similar instead?",
        "> That would just mean having more code in the codebase\r\n\r\nWould it? It seems we could probably _simplify_ `process_effects` if 'this effect is/could be dirty' and 'this effect has dirty children' were separate concepts, instead of needing to check `is_branch` at numerous points inside that function. And `schedule_effect` would get simpler, especially since we wouldn't need a chunky comment explaining what's going on.\r\n\r\nThe performance benefits might be negligible but the real win would be more self-explanatory code, I think",
        "we could do this instead which would presumably be a tiny bit faster, but less readable\r\n\r\n```suggestion\r\n\t\tvar is_skippable_branch = (flags & (CLEAN | BRANCH_EFFECT)) === (CLEAN | BRANCH_EFFECT);\r\n```"
      ]
    },
    "profile": {
      "location": "NYC",
      "blog": "",
      "site_admin": false,
      "followers": 19879,
      "following": 0
    }
  },
  "jmcdo29": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-avoid-testing-anti-patterns",
        "title": "Avoid testing anti-patterns"
      },
      {
        "slug": "nest-benchmark-before-optimizing",
        "title": "Benchmark before optimizing"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-manage-testing-dependencies",
        "title": "Manage testing dependencies"
      },
      {
        "slug": "nest-optimize-critical-path-iterations",
        "title": "Optimize critical path iterations"
      },
      {
        "slug": "nest-parameterize-version-requirements",
        "title": "Parameterize version requirements"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-preserve-api-interface-stability",
        "title": "Preserve API interface stability"
      },
      {
        "slug": "nest-preserve-public-api-stability",
        "title": "Preserve public API stability"
      },
      {
        "slug": "nest-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "nest-prevent-race-conditions",
        "title": "Prevent race conditions"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-test-dependency-management",
        "title": "Test dependency management"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      }
    ],
    "comments": {
      "nest-configurable-log-formatting": [
        "This is _mostly_ only used by the `ConsoleLogger`, but there are references to it in the `Injector.ts` file as well. If it was just the `ConsoleLogger` I'd say this is okay, or even checking `process.stdout.hasColors()` directly instead of needing the prototype, but I'm not sure how to handle if this check is coming from the `Injector.ts`\r\n\r\nOverall, I don't _think_ this should be an issue, my only concern would be if someone is in an environment where `hasColors()` returns `false` but they want to force the use of colors. "
      ],
      "nest-benchmark-before-optimizing": [
        "If you're going to refactor this to a raw for loop, we should do the same to the `getAll` for squeezing as much performance out as we can. \r\n\r\nSide note: do you have any benchmarks for how much of an improvement this is? Just curious if the improvement is going to be worth the readability "
      ],
      "nest-preserve-public-api-stability": [
        "Adding non-optional methods to a public interface that should be implemented for adapters means that anyone who is maintaining their own adapter, say for Koa, or hyper-express, etc, now **must** make these changes or possibly have their packages broken by a transient upgrade.\r\n\r\nThis is a braking change, and while you may have been waiting for it for a while, that doesn't mean it's any more urgent to get out, as we want to avoid making several small breaking changes and would rather lump them together over a major upgrade."
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "```suggestion\r\n      expect(usersController.create(createUserDto)).resolves.toEqual({\r\n        id: 'a id',\r\n        ...createUserDto,\r\n      });\r\n      expect(usersService.create).toHaveBeenCalled();\r\n      expect(usersService.create).toHaveBeenCalledWith(createUserDto);\r\n```\r\nNo need to call `usersController.create` twice. "
      ],
      "nest-test-dependency-management": [
        "Looks like it's for a new integration test. As it's in `devDeps` it should be fine"
      ],
      "nest-follow-protocol-standards": [
        "If you were to have a falsy header value, such as `false`, `0` or `''`, no browser that I'm aware of would know how to properly handle that response, and as such the body might be unrecoverable, or at least encoded in an unexpected way.\n\nI think think is a decent restriction to keep from giving the dev a nice foot gun, but I'd be willing to hear what the browser should do if the header was one of those values, especially if there's an RFC for it as well",
        "I assume this will automatically be `chunk`? If not, should we set it to that?"
      ],
      "nest-manage-testing-dependencies": [
        "Looks like it's for a new integration test. As it's in `devDeps` it should be fine"
      ],
      "nest-avoid-testing-anti-patterns": [
        "```suggestion\r\n      expect(usersController.create(createUserDto)).resolves.toEqual({\r\n        id: 'a id',\r\n        ...createUserDto,\r\n      });\r\n      expect(usersService.create).toHaveBeenCalled();\r\n      expect(usersService.create).toHaveBeenCalledWith(createUserDto);\r\n```\r\nNo need to call `usersController.create` twice. ",
        "There's no need to call `controller.findAll()` twice in the test. Just call it in the assertion and then do the jest check after it\r\n```suggestion\r\n      expect(controller.findAll()).resolves.toEqual([\r\n        {\r\n          name: 'Cat #1',\r\n          breed: 'Bread #1',\r\n          age: 4,\r\n        },\r\n        {\r\n          name: 'Cat #2',\r\n          breed: 'Breed #2',\r\n          age: 3,\r\n        },\r\n        {\r\n          name: 'Cat #3',\r\n          breed: 'Breed #3',\r\n          age: 2,\r\n        },\r\n      ]);\r\n      expect(service.findAll).toHaveBeenCalled();\r\n```",
        "This isn't an actual test of anything. You mock the value you want to call and assert the test just called the service method. This should instead mock the repository method used in `service.remove`"
      ],
      "nest-prevent-race-conditions": [
        "Rather than mutating the client, could I add a new `args` entry and let `getPattern()` retrieve `args[2]`? Would that be more \"stable\"? ",
        "Great call by the way! I moved `getPattern` to be a method on `WsArgumentHost` instead of under the `getClient()` so that it should be unique per request as the `WsArgumentHost` already is. Tests are still passing and the interfaces are updated. Let me know if you think of any other issues with this approach :smile_cat:"
      ],
      "nest-use-consistent-control-structures": [
        "Would you mind updating the code to wrap the internals of the if in curly braces? It leads the code to be easier to follow and modify if we need to\r\n\r\n```suggestion\r\n    if (this.flushLogsOnOverride) {\r\n      this.flushLogs();\r\n    }\r\n```",
        "If these are going to be single line ifs, would a ternary make more sense?\r\n```\r\ndata += message.type ? `event: ${message.type}\\n` : ''\r\n```\r\n",
        "Personally, I'm not a fan of inline ifs, so if we stay with using `if` statements it should be changed to\r\n```ts\r\nif (message.type) {\r\n  data += `event: ${message.type}\\n`;\r\n}\r\n```\r\nAnd this is what most of Nest's codebase does already even for single line. So ternary, or using braces as necessary should be fine. Just to stay consistent with the rest of the codebase"
      ],
      "nest-pin-dependency-versions": [
        "npm v9 was released on 2022-10-24 is is not compatible with node 12. Today (2022-11-09) it was set to the `@latest` tag for `npm` so our CircleCI builds started breaking. \r\n\r\n> Wednesday Nov. 9th (General Availability)\r\n> To ensure npm@9.x is considered \"non-breaking\" for Node.js LTS we will codify a set of exit criteria in collaboration with the [Release WG](https://github.com/nodejs/release)\r\n> npm@9.x will be set to the latest dist-tag (becoming the latest, maintained version of npm)\r\n> A PR will be opened to land npm@9.x in nodejs/node's main branch (exposing experimental/nightly users to this latest version)\r\n\r\nhttps://github.blog/changelog/2022-10-24-npm-v9-0-0-released/",
        "What are the differences between v2 and v3?"
      ],
      "nest-http-header-management": [
        "If you were to have a falsy header value, such as `false`, `0` or `''`, no browser that I'm aware of would know how to properly handle that response, and as such the body might be unrecoverable, or at least encoded in an unexpected way.\n\nI think think is a decent restriction to keep from giving the dev a nice foot gun, but I'd be willing to hear what the browser should do if the header was one of those values, especially if there's an RFC for it as well",
        "I assume this will automatically be `chunk`? If not, should we set it to that?"
      ],
      "nest-use-consistent-curly-braces": [
        "Would you mind updating the code to wrap the internals of the if in curly braces? It leads the code to be easier to follow and modify if we need to\r\n\r\n```suggestion\r\n    if (this.flushLogsOnOverride) {\r\n      this.flushLogs();\r\n    }\r\n```",
        "If these are going to be single line ifs, would a ternary make more sense?\r\n```\r\ndata += message.type ? `event: ${message.type}\\n` : ''\r\n```\r\n",
        "Personally, I'm not a fan of inline ifs, so if we stay with using `if` statements it should be changed to\r\n```ts\r\nif (message.type) {\r\n  data += `event: ${message.type}\\n`;\r\n}\r\n```\r\nAnd this is what most of Nest's codebase does already even for single line. So ternary, or using braces as necessary should be fine. Just to stay consistent with the rest of the codebase"
      ],
      "nest-use-factory-providers": [
        "I'd much rather we direct the user to the issue rather than have them dig into the source code to find the issue reference ",
        "[This wouldn't be the first time we've had a link in the exception](https://github.com/nestjs/nest/blob/ef5344826f23b5377a2f2599cf9efda0303e1f33/packages/core/errors/messages.ts#L163)",
        "And I'm certain we've linked to issues from within our docs. If we _really_ don't want to link to the issue directly, then we should make a new section of the FAQ > Common Errors for this"
      ],
      "nest-optimize-critical-path-iterations": [
        "If you're going to refactor this to a raw for loop, we should do the same to the `getAll` for squeezing as much performance out as we can. \r\n\r\nSide note: do you have any benchmarks for how much of an improvement this is? Just curious if the improvement is going to be worth the readability "
      ],
      "nest-explicit-default-configurations": [
        "Instead of marking this as optional, could we give it a default value?\r\n\r\n```suggestion\r\n    appOptions: NestApplicationOptions = {\r\n      disableInstanceLoaderLogs: false\r\n    },\r\n  ) {\r\n    const instanceLoader = new InstanceLoader(container);\r\n\r\n    if (appOptions.disableInstanceLoaderLogs) {\r\n      instanceLoader.disableLogs();\r\n    }\r\n```\r\n\r\nAlso, do we need the full options here, or just the `disableInstanceLoaderLogs` option? We might be able to cut this down to a simple `boolean` instead of the full object"
      ],
      "nest-preserve-api-interface-stability": [
        "Adding non-optional methods to a public interface that should be implemented for adapters means that anyone who is maintaining their own adapter, say for Koa, or hyper-express, etc, now **must** make these changes or possibly have their packages broken by a transient upgrade.\r\n\r\nThis is a braking change, and while you may have been waiting for it for a while, that doesn't mean it's any more urgent to get out, as we want to avoid making several small breaking changes and would rather lump them together over a major upgrade."
      ],
      "nest-parameterize-version-requirements": [
        "npm v9 was released on 2022-10-24 is is not compatible with node 12. Today (2022-11-09) it was set to the `@latest` tag for `npm` so our CircleCI builds started breaking. \r\n\r\n> Wednesday Nov. 9th (General Availability)\r\n> To ensure npm@9.x is considered \"non-breaking\" for Node.js LTS we will codify a set of exit criteria in collaboration with the [Release WG](https://github.com/nodejs/release)\r\n> npm@9.x will be set to the latest dist-tag (becoming the latest, maintained version of npm)\r\n> A PR will be opened to land npm@9.x in nodejs/node's main branch (exposing experimental/nightly users to this latest version)\r\n\r\nhttps://github.blog/changelog/2022-10-24-npm-v9-0-0-released/"
      ],
      "nest-prevent-async-race-conditions": [
        "Rather than mutating the client, could I add a new `args` entry and let `getPattern()` retrieve `args[2]`? Would that be more \"stable\"? ",
        "Great call by the way! I moved `getPattern` to be a method on `WsArgumentHost` instead of under the `getClient()` so that it should be unique per request as the `WsArgumentHost` already is. Tests are still passing and the interfaces are updated. Let me know if you think of any other issues with this approach :smile_cat:"
      ]
    },
    "profile": {
      "location": "Concrete, WA",
      "blog": "",
      "twitter_username": "jmcdo29",
      "site_admin": false,
      "followers": 1031,
      "following": 15
    }
  }
}