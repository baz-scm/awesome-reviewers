---
layout: default
title: Methodology
description: How the Baz team built Awesome Reviewers from thousands of open source code review comments.
---

<section class="page-container">
  <div class="container">
    <h1>From Review Thread to Team Standard</h1>
    <p class="lead">
      This methodology distills the process the Baz team followed to turn raw open source code review discussions into the
      reusable reviewer prompts that power Awesome Reviewers. It captures the original blog post
      <em>“From review thread to team standard: how we built AwesomeReviewers.”</em>
    </p>

    <h2>Why we built Awesome Reviewers</h2>
    <p>
      Baz customers repeatedly told us the same story: strong teams rely on a small group of staff reviewers who know every
      repository’s nuances, but that tribal knowledge rarely scales. We wanted to preserve the best feedback patterns from
      experienced maintainers and make them reusable—first inside Baz’s review agents, and then for the wider developer
      community. Awesome Reviewers is the public library of those distilled review playbooks.
    </p>

    <h2>1. Collecting the raw review corpus</h2>
    <p>
      We started by gathering tens of thousands of public pull request conversations from maintainers we trust. The selection
      criteria focused on projects with a high signal-to-noise ratio: active review culture, consistent maintainers, and
      repositories that document architectural intent in the code review itself. Every comment thread we ingested included the
      code diff, the reviewer's note, the eventual fix, and any discussion that explained the rationale behind the change.
    </p>
    <p>
      This gave us an anonymized but context-rich dataset that reflected how real practitioners reinforce quality in production
      systems across languages and frameworks.
    </p>

    <h2>2. Normalizing conversations into review patterns</h2>
    <p>
      Raw threads can be noisy, so we broke each exchange into atomic “review moves.” Reviewers often stack several ideas in a
      single comment—spotting a race condition, nudging toward better logging, and clarifying API contracts in one go. We
      untangled those into discrete units, capturing:
    </p>
    <ul>
      <li>What triggered the review (a diff pattern, a missing test, an unsafe default).</li>
      <li>The principle the maintainer defended (performance budgets, readability, security posture, etc.).</li>
      <li>The concrete remediation guidance that eventually landed.</li>
    </ul>
    <p>
      Each move was labeled with repository metadata (language, framework, subsystem) so we could later target prompts to the
      right stack.
    </p>

    <h2>3. Building a shared taxonomy with maintainers</h2>
    <p>
      To keep things grounded in what practitioners actually care about, we looped maintainers into weekly taxonomy reviews. We
      proposed an initial set of categories—testing, performance, reliability, platform, documentation, and ergonomics—and then
      iterated until the language felt natural to them. The maintainer panel also flagged anti-patterns: advice that only
      applied to one-off incidents or that would not generalize to the rest of the codebase. Only guidance that multiple
      maintainers endorsed as “team standard worthy” survived.
    </p>

    <h2>4. Distilling reviewer prompts</h2>
    <p>
      With the taxonomy locked, we turned each high-signal review move into a reusable reviewer. That required translating
      prose conversations into structured prompts that an AI agent can execute. The prompts capture three things:
    </p>
    <ol>
      <li>A short mission statement that mirrors how the maintainer frames the responsibility.</li>
      <li>Concrete heuristics tied to code signals (“flag any concurrency primitive that skips await”, “require explicit
      validation on external-facing endpoints”).</li>
      <li>Preferred tone and collaboration style, so automated feedback lands the way the maintainer would deliver it.</li>
    </ol>
    <p>
      We validated every prompt by replaying it on fresh pull requests from the same repository and confirming it spotted the
      same issues the maintainer previously caught.
    </p>

    <h2>5. Shipping with feedback loops</h2>
    <p>
      Before publishing, we ran closed pilots with Baz customers. They plugged the reviewers into their CI, compared agent
      feedback against human comments, and logged false positives. Those sessions surfaced edge cases—framework upgrades,
      generated code, dependency vendoring—that required opt-out rules. Each reviewer ships with the maintainer-approved guard
      rails baked in.
    </p>
    <p>
      Once quality cleared the bar, we published the reviewers both inside the Baz product and here in the open-source library
      so any developer can adopt the same standards.
    </p>

    <h2>6. What’s next</h2>
    <p>
      Awesome Reviewers is a living project. We continue to ingest new review threads, expand the taxonomy to fresh domains
      (mobile, data, infrastructure), and invite maintainers to submit their own review playbooks. If you would like to
      contribute, open an issue or share a repository for analysis—we are always adding new reviewers as teams codify their
      standards.
    </p>
  </div>
</section>
